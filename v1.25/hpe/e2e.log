I0105 07:22:49.593280      23 e2e.go:116] Starting e2e run "77228e4c-8e3b-412e-b6c7-acee0a58df34" on Ginkgo node 1
Jan  5 07:22:49.605: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1672903369 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan  5 07:22:49.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:22:49.665: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0105 07:22:49.670430      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
E0105 07:22:49.670430      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
Jan  5 07:22:49.674: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan  5 07:22:49.684: INFO: 5 / 5 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan  5 07:22:49.684: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jan  5 07:22:49.684: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan  5 07:22:49.687: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Jan  5 07:22:49.687: INFO: e2e test version: v1.25.5
Jan  5 07:22:49.687: INFO: kube-apiserver version: v1.25.5-hpe1
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan  5 07:22:49.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:22:49.690: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.026 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan  5 07:22:49.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:22:49.665: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0105 07:22:49.670430      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
    Jan  5 07:22:49.674: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan  5 07:22:49.684: INFO: 5 / 5 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan  5 07:22:49.684: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Jan  5 07:22:49.684: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan  5 07:22:49.687: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
    Jan  5 07:22:49.687: INFO: e2e test version: v1.25.5
    Jan  5 07:22:49.687: INFO: kube-apiserver version: v1.25.5-hpe1
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan  5 07:22:49.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:22:49.690: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:22:49.709
Jan  5 07:22:49.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 07:22:49.71
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:22:49.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:22:49.728
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan  5 07:22:49.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 07:22:50.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2102" for this suite. 01/05/23 07:22:50.753
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":1,"skipped":47,"failed":0}
------------------------------
â€¢ [1.048 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:22:49.709
    Jan  5 07:22:49.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 07:22:49.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:22:49.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:22:49.728
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan  5 07:22:49.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 07:22:50.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2102" for this suite. 01/05/23 07:22:50.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:22:50.759
Jan  5 07:22:50.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename security-context 01/05/23 07:22:50.76
E0105 07:22:50.763341      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
E0105 07:22:50.763341      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:22:50.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:22:50.778
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 07:22:50.78
Jan  5 07:22:50.788: INFO: Waiting up to 5m0s for pod "security-context-11288876-2951-4948-ad3b-8275ce452975" in namespace "security-context-831" to be "Succeeded or Failed"
Jan  5 07:22:50.790: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 1.672492ms
Jan  5 07:22:52.793: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004754042s
Jan  5 07:22:54.818: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029685795s
Jan  5 07:22:56.794: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005795806s
Jan  5 07:22:58.793: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004894684s
Jan  5 07:23:00.792: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004457787s
Jan  5 07:23:02.794: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005864935s
Jan  5 07:23:04.792: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004431535s
Jan  5 07:23:06.794: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005781426s
Jan  5 07:23:08.793: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.005373872s
STEP: Saw pod success 01/05/23 07:23:08.793
Jan  5 07:23:08.793: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975" satisfied condition "Succeeded or Failed"
Jan  5 07:23:08.795: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod security-context-11288876-2951-4948-ad3b-8275ce452975 container test-container: <nil>
STEP: delete the pod 01/05/23 07:23:08.806
Jan  5 07:23:08.818: INFO: Waiting for pod security-context-11288876-2951-4948-ad3b-8275ce452975 to disappear
Jan  5 07:23:08.819: INFO: Pod security-context-11288876-2951-4948-ad3b-8275ce452975 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 07:23:08.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-831" for this suite. 01/05/23 07:23:08.821
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":2,"skipped":114,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.070 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:22:50.759
    Jan  5 07:22:50.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename security-context 01/05/23 07:22:50.76
    E0105 07:22:50.763341      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:22:50.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:22:50.778
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 07:22:50.78
    Jan  5 07:22:50.788: INFO: Waiting up to 5m0s for pod "security-context-11288876-2951-4948-ad3b-8275ce452975" in namespace "security-context-831" to be "Succeeded or Failed"
    Jan  5 07:22:50.790: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 1.672492ms
    Jan  5 07:22:52.793: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004754042s
    Jan  5 07:22:54.818: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029685795s
    Jan  5 07:22:56.794: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005795806s
    Jan  5 07:22:58.793: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004894684s
    Jan  5 07:23:00.792: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004457787s
    Jan  5 07:23:02.794: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005864935s
    Jan  5 07:23:04.792: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004431535s
    Jan  5 07:23:06.794: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005781426s
    Jan  5 07:23:08.793: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.005373872s
    STEP: Saw pod success 01/05/23 07:23:08.793
    Jan  5 07:23:08.793: INFO: Pod "security-context-11288876-2951-4948-ad3b-8275ce452975" satisfied condition "Succeeded or Failed"
    Jan  5 07:23:08.795: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod security-context-11288876-2951-4948-ad3b-8275ce452975 container test-container: <nil>
    STEP: delete the pod 01/05/23 07:23:08.806
    Jan  5 07:23:08.818: INFO: Waiting for pod security-context-11288876-2951-4948-ad3b-8275ce452975 to disappear
    Jan  5 07:23:08.819: INFO: Pod security-context-11288876-2951-4948-ad3b-8275ce452975 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 07:23:08.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-831" for this suite. 01/05/23 07:23:08.821
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:23:08.832
Jan  5 07:23:08.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:23:08.832
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:08.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:08.849
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-49a0061d-c490-45c6-999b-ddf45f8440af 01/05/23 07:23:08.851
STEP: Creating a pod to test consume secrets 01/05/23 07:23:08.859
Jan  5 07:23:08.870: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a" in namespace "projected-7314" to be "Succeeded or Failed"
Jan  5 07:23:08.872: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.829427ms
Jan  5 07:23:10.876: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005876151s
Jan  5 07:23:12.876: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005632304s
Jan  5 07:23:14.875: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004743192s
Jan  5 07:23:16.875: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004590145s
Jan  5 07:23:18.876: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.005707337s
STEP: Saw pod success 01/05/23 07:23:18.876
Jan  5 07:23:18.876: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a" satisfied condition "Succeeded or Failed"
Jan  5 07:23:18.878: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 07:23:18.881
Jan  5 07:23:18.900: INFO: Waiting for pod pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a to disappear
Jan  5 07:23:18.908: INFO: Pod pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 07:23:18.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7314" for this suite. 01/05/23 07:23:18.91
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":3,"skipped":167,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.083 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:23:08.832
    Jan  5 07:23:08.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:23:08.832
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:08.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:08.849
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-49a0061d-c490-45c6-999b-ddf45f8440af 01/05/23 07:23:08.851
    STEP: Creating a pod to test consume secrets 01/05/23 07:23:08.859
    Jan  5 07:23:08.870: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a" in namespace "projected-7314" to be "Succeeded or Failed"
    Jan  5 07:23:08.872: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.829427ms
    Jan  5 07:23:10.876: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005876151s
    Jan  5 07:23:12.876: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005632304s
    Jan  5 07:23:14.875: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004743192s
    Jan  5 07:23:16.875: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004590145s
    Jan  5 07:23:18.876: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.005707337s
    STEP: Saw pod success 01/05/23 07:23:18.876
    Jan  5 07:23:18.876: INFO: Pod "pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a" satisfied condition "Succeeded or Failed"
    Jan  5 07:23:18.878: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 07:23:18.881
    Jan  5 07:23:18.900: INFO: Waiting for pod pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a to disappear
    Jan  5 07:23:18.908: INFO: Pod pod-projected-secrets-e0baf7a6-2f1d-4f16-8bbb-8caa5727383a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 07:23:18.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7314" for this suite. 01/05/23 07:23:18.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:23:18.915
Jan  5 07:23:18.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 07:23:18.916
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:18.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:18.951
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/05/23 07:23:18.953
Jan  5 07:23:18.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8819 cluster-info'
Jan  5 07:23:19.025: INFO: stderr: ""
Jan  5 07:23:19.025: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 07:23:19.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8819" for this suite. 01/05/23 07:23:19.027
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":4,"skipped":196,"failed":0}
------------------------------
â€¢ [0.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:23:18.915
    Jan  5 07:23:18.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 07:23:18.916
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:18.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:18.951
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/05/23 07:23:18.953
    Jan  5 07:23:18.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8819 cluster-info'
    Jan  5 07:23:19.025: INFO: stderr: ""
    Jan  5 07:23:19.025: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 07:23:19.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8819" for this suite. 01/05/23 07:23:19.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:23:19.032
Jan  5 07:23:19.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:23:19.032
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:19.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:19.061
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/05/23 07:23:19.063
Jan  5 07:23:19.073: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298" in namespace "projected-2248" to be "Succeeded or Failed"
Jan  5 07:23:19.075: INFO: Pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298": Phase="Pending", Reason="", readiness=false. Elapsed: 1.975361ms
Jan  5 07:23:21.079: INFO: Pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005320816s
Jan  5 07:23:23.079: INFO: Pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005700553s
Jan  5 07:23:25.079: INFO: Pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005324657s
STEP: Saw pod success 01/05/23 07:23:25.079
Jan  5 07:23:25.079: INFO: Pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298" satisfied condition "Succeeded or Failed"
Jan  5 07:23:25.081: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298 container client-container: <nil>
STEP: delete the pod 01/05/23 07:23:25.086
Jan  5 07:23:25.104: INFO: Waiting for pod downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298 to disappear
Jan  5 07:23:25.106: INFO: Pod downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 07:23:25.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2248" for this suite. 01/05/23 07:23:25.108
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":5,"skipped":209,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.085 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:23:19.032
    Jan  5 07:23:19.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:23:19.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:19.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:19.061
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/05/23 07:23:19.063
    Jan  5 07:23:19.073: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298" in namespace "projected-2248" to be "Succeeded or Failed"
    Jan  5 07:23:19.075: INFO: Pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298": Phase="Pending", Reason="", readiness=false. Elapsed: 1.975361ms
    Jan  5 07:23:21.079: INFO: Pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005320816s
    Jan  5 07:23:23.079: INFO: Pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005700553s
    Jan  5 07:23:25.079: INFO: Pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005324657s
    STEP: Saw pod success 01/05/23 07:23:25.079
    Jan  5 07:23:25.079: INFO: Pod "downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298" satisfied condition "Succeeded or Failed"
    Jan  5 07:23:25.081: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298 container client-container: <nil>
    STEP: delete the pod 01/05/23 07:23:25.086
    Jan  5 07:23:25.104: INFO: Waiting for pod downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298 to disappear
    Jan  5 07:23:25.106: INFO: Pod downwardapi-volume-b76addad-efdb-42b4-a698-94d6e0420298 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 07:23:25.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2248" for this suite. 01/05/23 07:23:25.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:23:25.117
Jan  5 07:23:25.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename watch 01/05/23 07:23:25.118
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:25.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:25.135
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/05/23 07:23:25.137
STEP: modifying the configmap once 01/05/23 07:23:25.144
STEP: modifying the configmap a second time 01/05/23 07:23:25.192
STEP: deleting the configmap 01/05/23 07:23:25.206
STEP: creating a watch on configmaps from the resource version returned by the first update 01/05/23 07:23:25.209
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/05/23 07:23:25.211
Jan  5 07:23:25.211: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6625  5a1fd41d-3f90-4e2f-aa2e-b1f54d26f5bd 2375 0 2023-01-05 07:23:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 07:23:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 07:23:25.211: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6625  5a1fd41d-3f90-4e2f-aa2e-b1f54d26f5bd 2376 0 2023-01-05 07:23:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 07:23:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  5 07:23:25.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6625" for this suite. 01/05/23 07:23:25.213
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":6,"skipped":219,"failed":0}
------------------------------
â€¢ [0.107 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:23:25.117
    Jan  5 07:23:25.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename watch 01/05/23 07:23:25.118
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:25.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:25.135
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/05/23 07:23:25.137
    STEP: modifying the configmap once 01/05/23 07:23:25.144
    STEP: modifying the configmap a second time 01/05/23 07:23:25.192
    STEP: deleting the configmap 01/05/23 07:23:25.206
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/05/23 07:23:25.209
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/05/23 07:23:25.211
    Jan  5 07:23:25.211: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6625  5a1fd41d-3f90-4e2f-aa2e-b1f54d26f5bd 2375 0 2023-01-05 07:23:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 07:23:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 07:23:25.211: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6625  5a1fd41d-3f90-4e2f-aa2e-b1f54d26f5bd 2376 0 2023-01-05 07:23:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 07:23:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  5 07:23:25.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6625" for this suite. 01/05/23 07:23:25.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:23:25.224
Jan  5 07:23:25.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 07:23:25.225
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:25.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:25.243
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/05/23 07:23:25.245
Jan  5 07:23:25.245: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan  5 07:23:25.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
Jan  5 07:23:25.436: INFO: stderr: ""
Jan  5 07:23:25.437: INFO: stdout: "service/agnhost-replica created\n"
Jan  5 07:23:25.437: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan  5 07:23:25.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
Jan  5 07:23:25.621: INFO: stderr: ""
Jan  5 07:23:25.621: INFO: stdout: "service/agnhost-primary created\n"
Jan  5 07:23:25.621: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan  5 07:23:25.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
Jan  5 07:23:25.815: INFO: stderr: ""
Jan  5 07:23:25.815: INFO: stdout: "service/frontend created\n"
Jan  5 07:23:25.815: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan  5 07:23:25.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
Jan  5 07:23:25.995: INFO: stderr: ""
Jan  5 07:23:25.995: INFO: stdout: "deployment.apps/frontend created\n"
Jan  5 07:23:25.996: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  5 07:23:25.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
Jan  5 07:23:26.176: INFO: stderr: ""
Jan  5 07:23:26.176: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan  5 07:23:26.176: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  5 07:23:26.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
Jan  5 07:23:26.349: INFO: stderr: ""
Jan  5 07:23:26.349: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/05/23 07:23:26.349
Jan  5 07:23:26.349: INFO: Waiting for all frontend pods to be Running.
Jan  5 07:23:36.401: INFO: Waiting for frontend to serve content.
Jan  5 07:23:36.410: INFO: Trying to add a new entry to the guestbook.
Jan  5 07:23:36.420: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/05/23 07:23:36.425
Jan  5 07:23:36.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
Jan  5 07:23:36.530: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 07:23:36.530: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 07:23:36.53
Jan  5 07:23:36.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
Jan  5 07:23:36.631: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 07:23:36.631: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 07:23:36.631
Jan  5 07:23:36.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
Jan  5 07:23:36.724: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 07:23:36.724: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 07:23:36.724
Jan  5 07:23:36.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
Jan  5 07:23:36.790: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 07:23:36.790: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 07:23:36.79
Jan  5 07:23:36.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
Jan  5 07:23:36.858: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 07:23:36.858: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 07:23:36.858
Jan  5 07:23:36.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
Jan  5 07:23:36.929: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 07:23:36.929: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 07:23:36.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2003" for this suite. 01/05/23 07:23:36.934
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":7,"skipped":236,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.723 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:23:25.224
    Jan  5 07:23:25.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 07:23:25.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:25.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:25.243
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/05/23 07:23:25.245
    Jan  5 07:23:25.245: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan  5 07:23:25.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
    Jan  5 07:23:25.436: INFO: stderr: ""
    Jan  5 07:23:25.437: INFO: stdout: "service/agnhost-replica created\n"
    Jan  5 07:23:25.437: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan  5 07:23:25.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
    Jan  5 07:23:25.621: INFO: stderr: ""
    Jan  5 07:23:25.621: INFO: stdout: "service/agnhost-primary created\n"
    Jan  5 07:23:25.621: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan  5 07:23:25.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
    Jan  5 07:23:25.815: INFO: stderr: ""
    Jan  5 07:23:25.815: INFO: stdout: "service/frontend created\n"
    Jan  5 07:23:25.815: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan  5 07:23:25.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
    Jan  5 07:23:25.995: INFO: stderr: ""
    Jan  5 07:23:25.995: INFO: stdout: "deployment.apps/frontend created\n"
    Jan  5 07:23:25.996: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  5 07:23:25.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
    Jan  5 07:23:26.176: INFO: stderr: ""
    Jan  5 07:23:26.176: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan  5 07:23:26.176: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  5 07:23:26.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 create -f -'
    Jan  5 07:23:26.349: INFO: stderr: ""
    Jan  5 07:23:26.349: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/05/23 07:23:26.349
    Jan  5 07:23:26.349: INFO: Waiting for all frontend pods to be Running.
    Jan  5 07:23:36.401: INFO: Waiting for frontend to serve content.
    Jan  5 07:23:36.410: INFO: Trying to add a new entry to the guestbook.
    Jan  5 07:23:36.420: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/05/23 07:23:36.425
    Jan  5 07:23:36.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
    Jan  5 07:23:36.530: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 07:23:36.530: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 07:23:36.53
    Jan  5 07:23:36.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
    Jan  5 07:23:36.631: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 07:23:36.631: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 07:23:36.631
    Jan  5 07:23:36.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
    Jan  5 07:23:36.724: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 07:23:36.724: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 07:23:36.724
    Jan  5 07:23:36.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
    Jan  5 07:23:36.790: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 07:23:36.790: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 07:23:36.79
    Jan  5 07:23:36.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
    Jan  5 07:23:36.858: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 07:23:36.858: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 07:23:36.858
    Jan  5 07:23:36.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-2003 delete --grace-period=0 --force -f -'
    Jan  5 07:23:36.929: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 07:23:36.929: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 07:23:36.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2003" for this suite. 01/05/23 07:23:36.934
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:23:36.949
Jan  5 07:23:36.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename replicaset 01/05/23 07:23:36.95
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:36.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:36.993
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/05/23 07:23:36.994
Jan  5 07:23:37.003: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7559" to be "running and ready"
Jan  5 07:23:37.004: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.434103ms
Jan  5 07:23:37.004: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:23:39.007: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004231908s
Jan  5 07:23:39.007: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:23:41.007: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00424948s
Jan  5 07:23:41.007: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:23:43.009: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005621925s
Jan  5 07:23:43.009: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:23:45.007: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004150054s
Jan  5 07:23:45.007: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:23:47.008: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 10.005120858s
Jan  5 07:23:47.008: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan  5 07:23:47.008: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/05/23 07:23:47.01
STEP: Then the orphan pod is adopted 01/05/23 07:23:47.014
STEP: When the matched label of one of its pods change 01/05/23 07:23:48.02
Jan  5 07:23:48.023: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/05/23 07:23:48.035
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 07:23:49.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7559" for this suite. 01/05/23 07:23:49.042
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":8,"skipped":239,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.108 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:23:36.949
    Jan  5 07:23:36.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename replicaset 01/05/23 07:23:36.95
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:36.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:36.993
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/05/23 07:23:36.994
    Jan  5 07:23:37.003: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7559" to be "running and ready"
    Jan  5 07:23:37.004: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.434103ms
    Jan  5 07:23:37.004: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:23:39.007: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004231908s
    Jan  5 07:23:39.007: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:23:41.007: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00424948s
    Jan  5 07:23:41.007: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:23:43.009: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005621925s
    Jan  5 07:23:43.009: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:23:45.007: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004150054s
    Jan  5 07:23:45.007: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:23:47.008: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 10.005120858s
    Jan  5 07:23:47.008: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan  5 07:23:47.008: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/05/23 07:23:47.01
    STEP: Then the orphan pod is adopted 01/05/23 07:23:47.014
    STEP: When the matched label of one of its pods change 01/05/23 07:23:48.02
    Jan  5 07:23:48.023: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/05/23 07:23:48.035
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 07:23:49.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7559" for this suite. 01/05/23 07:23:49.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:23:49.058
Jan  5 07:23:49.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 07:23:49.059
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:49.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:49.075
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/05/23 07:23:49.077
Jan  5 07:23:49.087: INFO: Waiting up to 5m0s for pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684" in namespace "downward-api-6768" to be "Succeeded or Failed"
Jan  5 07:23:49.089: INFO: Pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684": Phase="Pending", Reason="", readiness=false. Elapsed: 1.553533ms
Jan  5 07:23:51.092: INFO: Pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005224322s
Jan  5 07:23:53.094: INFO: Pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006581257s
Jan  5 07:23:55.097: INFO: Pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009823457s
STEP: Saw pod success 01/05/23 07:23:55.097
Jan  5 07:23:55.097: INFO: Pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684" satisfied condition "Succeeded or Failed"
Jan  5 07:23:55.099: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downward-api-21d06c85-f971-4173-bfdf-36b68ff87684 container dapi-container: <nil>
STEP: delete the pod 01/05/23 07:23:55.103
Jan  5 07:23:55.120: INFO: Waiting for pod downward-api-21d06c85-f971-4173-bfdf-36b68ff87684 to disappear
Jan  5 07:23:55.122: INFO: Pod downward-api-21d06c85-f971-4173-bfdf-36b68ff87684 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  5 07:23:55.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6768" for this suite. 01/05/23 07:23:55.124
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":9,"skipped":273,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.071 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:23:49.058
    Jan  5 07:23:49.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 07:23:49.059
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:49.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:49.075
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/05/23 07:23:49.077
    Jan  5 07:23:49.087: INFO: Waiting up to 5m0s for pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684" in namespace "downward-api-6768" to be "Succeeded or Failed"
    Jan  5 07:23:49.089: INFO: Pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684": Phase="Pending", Reason="", readiness=false. Elapsed: 1.553533ms
    Jan  5 07:23:51.092: INFO: Pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005224322s
    Jan  5 07:23:53.094: INFO: Pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006581257s
    Jan  5 07:23:55.097: INFO: Pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009823457s
    STEP: Saw pod success 01/05/23 07:23:55.097
    Jan  5 07:23:55.097: INFO: Pod "downward-api-21d06c85-f971-4173-bfdf-36b68ff87684" satisfied condition "Succeeded or Failed"
    Jan  5 07:23:55.099: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downward-api-21d06c85-f971-4173-bfdf-36b68ff87684 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 07:23:55.103
    Jan  5 07:23:55.120: INFO: Waiting for pod downward-api-21d06c85-f971-4173-bfdf-36b68ff87684 to disappear
    Jan  5 07:23:55.122: INFO: Pod downward-api-21d06c85-f971-4173-bfdf-36b68ff87684 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  5 07:23:55.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6768" for this suite. 01/05/23 07:23:55.124
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:23:55.129
Jan  5 07:23:55.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename svcaccounts 01/05/23 07:23:55.129
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:55.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:55.143
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan  5 07:23:55.151: INFO: Got root ca configmap in namespace "svcaccounts-3591"
Jan  5 07:23:55.170: INFO: Deleted root ca configmap in namespace "svcaccounts-3591"
STEP: waiting for a new root ca configmap created 01/05/23 07:23:55.671
Jan  5 07:23:55.673: INFO: Recreated root ca configmap in namespace "svcaccounts-3591"
Jan  5 07:23:55.677: INFO: Updated root ca configmap in namespace "svcaccounts-3591"
STEP: waiting for the root ca configmap reconciled 01/05/23 07:23:56.177
Jan  5 07:23:56.180: INFO: Reconciled root ca configmap in namespace "svcaccounts-3591"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 07:23:56.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3591" for this suite. 01/05/23 07:23:56.182
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":10,"skipped":275,"failed":0}
------------------------------
â€¢ [1.058 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:23:55.129
    Jan  5 07:23:55.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 07:23:55.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:55.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:55.143
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan  5 07:23:55.151: INFO: Got root ca configmap in namespace "svcaccounts-3591"
    Jan  5 07:23:55.170: INFO: Deleted root ca configmap in namespace "svcaccounts-3591"
    STEP: waiting for a new root ca configmap created 01/05/23 07:23:55.671
    Jan  5 07:23:55.673: INFO: Recreated root ca configmap in namespace "svcaccounts-3591"
    Jan  5 07:23:55.677: INFO: Updated root ca configmap in namespace "svcaccounts-3591"
    STEP: waiting for the root ca configmap reconciled 01/05/23 07:23:56.177
    Jan  5 07:23:56.180: INFO: Reconciled root ca configmap in namespace "svcaccounts-3591"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 07:23:56.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3591" for this suite. 01/05/23 07:23:56.182
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:23:56.186
Jan  5 07:23:56.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 07:23:56.187
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:56.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:56.2
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 07:23:56.219
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 07:23:56.426
STEP: Deploying the webhook pod 01/05/23 07:23:56.432
STEP: Wait for the deployment to be ready 01/05/23 07:23:56.462
Jan  5 07:23:56.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 07:23:58.476
STEP: Verifying the service has paired with the endpoint 01/05/23 07:23:58.488
Jan  5 07:23:59.489: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/05/23 07:23:59.492
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/05/23 07:23:59.505
STEP: Creating a configMap that should not be mutated 01/05/23 07:23:59.513
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/05/23 07:23:59.523
STEP: Creating a configMap that should be mutated 01/05/23 07:23:59.528
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 07:23:59.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4719" for this suite. 01/05/23 07:23:59.557
STEP: Destroying namespace "webhook-4719-markers" for this suite. 01/05/23 07:23:59.568
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":11,"skipped":278,"failed":0}
------------------------------
â€¢ [3.429 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:23:56.186
    Jan  5 07:23:56.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 07:23:56.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:56.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:56.2
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 07:23:56.219
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 07:23:56.426
    STEP: Deploying the webhook pod 01/05/23 07:23:56.432
    STEP: Wait for the deployment to be ready 01/05/23 07:23:56.462
    Jan  5 07:23:56.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 07:23:58.476
    STEP: Verifying the service has paired with the endpoint 01/05/23 07:23:58.488
    Jan  5 07:23:59.489: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/05/23 07:23:59.492
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/05/23 07:23:59.505
    STEP: Creating a configMap that should not be mutated 01/05/23 07:23:59.513
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/05/23 07:23:59.523
    STEP: Creating a configMap that should be mutated 01/05/23 07:23:59.528
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 07:23:59.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4719" for this suite. 01/05/23 07:23:59.557
    STEP: Destroying namespace "webhook-4719-markers" for this suite. 01/05/23 07:23:59.568
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:23:59.617
Jan  5 07:23:59.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename job 01/05/23 07:23:59.618
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:59.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:59.647
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/05/23 07:23:59.649
STEP: Ensure pods equal to paralellism count is attached to the job 01/05/23 07:23:59.658
STEP: patching /status 01/05/23 07:24:03.662
STEP: updating /status 01/05/23 07:24:03.667
STEP: get /status 01/05/23 07:24:03.672
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 07:24:03.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9656" for this suite. 01/05/23 07:24:03.676
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":12,"skipped":320,"failed":0}
------------------------------
â€¢ [4.071 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:23:59.617
    Jan  5 07:23:59.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename job 01/05/23 07:23:59.618
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:23:59.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:23:59.647
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/05/23 07:23:59.649
    STEP: Ensure pods equal to paralellism count is attached to the job 01/05/23 07:23:59.658
    STEP: patching /status 01/05/23 07:24:03.662
    STEP: updating /status 01/05/23 07:24:03.667
    STEP: get /status 01/05/23 07:24:03.672
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 07:24:03.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9656" for this suite. 01/05/23 07:24:03.676
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:24:03.688
Jan  5 07:24:03.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 07:24:03.689
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:24:03.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:24:03.719
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-78e2f674-43a3-4453-86d2-b45cc7799f58 01/05/23 07:24:03.721
STEP: Creating a pod to test consume secrets 01/05/23 07:24:03.734
Jan  5 07:24:03.739: INFO: Waiting up to 5m0s for pod "pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d" in namespace "secrets-7781" to be "Succeeded or Failed"
Jan  5 07:24:03.741: INFO: Pod "pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702054ms
Jan  5 07:24:05.744: INFO: Pod "pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004752088s
Jan  5 07:24:07.743: INFO: Pod "pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004089912s
STEP: Saw pod success 01/05/23 07:24:07.743
Jan  5 07:24:07.743: INFO: Pod "pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d" satisfied condition "Succeeded or Failed"
Jan  5 07:24:07.745: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d container secret-env-test: <nil>
STEP: delete the pod 01/05/23 07:24:07.748
Jan  5 07:24:07.761: INFO: Waiting for pod pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d to disappear
Jan  5 07:24:07.763: INFO: Pod pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  5 07:24:07.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7781" for this suite. 01/05/23 07:24:07.765
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":13,"skipped":320,"failed":0}
------------------------------
â€¢ [4.086 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:24:03.688
    Jan  5 07:24:03.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 07:24:03.689
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:24:03.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:24:03.719
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-78e2f674-43a3-4453-86d2-b45cc7799f58 01/05/23 07:24:03.721
    STEP: Creating a pod to test consume secrets 01/05/23 07:24:03.734
    Jan  5 07:24:03.739: INFO: Waiting up to 5m0s for pod "pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d" in namespace "secrets-7781" to be "Succeeded or Failed"
    Jan  5 07:24:03.741: INFO: Pod "pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702054ms
    Jan  5 07:24:05.744: INFO: Pod "pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004752088s
    Jan  5 07:24:07.743: INFO: Pod "pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004089912s
    STEP: Saw pod success 01/05/23 07:24:07.743
    Jan  5 07:24:07.743: INFO: Pod "pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d" satisfied condition "Succeeded or Failed"
    Jan  5 07:24:07.745: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d container secret-env-test: <nil>
    STEP: delete the pod 01/05/23 07:24:07.748
    Jan  5 07:24:07.761: INFO: Waiting for pod pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d to disappear
    Jan  5 07:24:07.763: INFO: Pod pod-secrets-6634dee5-a9e2-46f6-b7f6-2dfb4f920d3d no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 07:24:07.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7781" for this suite. 01/05/23 07:24:07.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:24:07.774
Jan  5 07:24:07.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename podtemplate 01/05/23 07:24:07.775
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:24:07.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:24:07.791
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/05/23 07:24:07.793
STEP: Replace a pod template 01/05/23 07:24:07.812
Jan  5 07:24:07.817: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan  5 07:24:07.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2933" for this suite. 01/05/23 07:24:07.819
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":14,"skipped":332,"failed":0}
------------------------------
â€¢ [0.052 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:24:07.774
    Jan  5 07:24:07.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename podtemplate 01/05/23 07:24:07.775
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:24:07.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:24:07.791
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/05/23 07:24:07.793
    STEP: Replace a pod template 01/05/23 07:24:07.812
    Jan  5 07:24:07.817: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan  5 07:24:07.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-2933" for this suite. 01/05/23 07:24:07.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:24:07.827
Jan  5 07:24:07.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename svc-latency 01/05/23 07:24:07.828
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:24:07.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:24:07.85
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan  5 07:24:07.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6538 01/05/23 07:24:07.854
I0105 07:24:07.863471      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6538, replica count: 1
I0105 07:24:08.915220      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0105 07:24:09.915744      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0105 07:24:10.915950      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0105 07:24:11.916254      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 07:24:12.033: INFO: Created: latency-svc-79bsh
Jan  5 07:24:12.037: INFO: Got endpoints: latency-svc-79bsh [20.113621ms]
Jan  5 07:24:12.054: INFO: Created: latency-svc-lbdlt
Jan  5 07:24:12.079: INFO: Got endpoints: latency-svc-lbdlt [41.968191ms]
Jan  5 07:24:12.079: INFO: Created: latency-svc-7btfs
Jan  5 07:24:12.099: INFO: Got endpoints: latency-svc-7btfs [61.83692ms]
Jan  5 07:24:12.102: INFO: Created: latency-svc-sb4tv
Jan  5 07:24:12.111: INFO: Got endpoints: latency-svc-sb4tv [73.981238ms]
Jan  5 07:24:12.122: INFO: Created: latency-svc-968w2
Jan  5 07:24:12.132: INFO: Created: latency-svc-x9p59
Jan  5 07:24:12.134: INFO: Got endpoints: latency-svc-968w2 [97.33964ms]
Jan  5 07:24:12.159: INFO: Got endpoints: latency-svc-x9p59 [121.67512ms]
Jan  5 07:24:12.161: INFO: Created: latency-svc-vm8fm
Jan  5 07:24:12.168: INFO: Got endpoints: latency-svc-vm8fm [130.849586ms]
Jan  5 07:24:12.192: INFO: Created: latency-svc-t8vg8
Jan  5 07:24:12.195: INFO: Created: latency-svc-dh87k
Jan  5 07:24:12.216: INFO: Got endpoints: latency-svc-t8vg8 [179.530886ms]
Jan  5 07:24:12.218: INFO: Got endpoints: latency-svc-dh87k [181.189397ms]
Jan  5 07:24:12.225: INFO: Created: latency-svc-2rm59
Jan  5 07:24:12.236: INFO: Got endpoints: latency-svc-2rm59 [199.394113ms]
Jan  5 07:24:12.246: INFO: Created: latency-svc-t8gpg
Jan  5 07:24:12.257: INFO: Got endpoints: latency-svc-t8gpg [220.383824ms]
Jan  5 07:24:12.259: INFO: Created: latency-svc-l9j2j
Jan  5 07:24:12.268: INFO: Got endpoints: latency-svc-l9j2j [231.372425ms]
Jan  5 07:24:12.277: INFO: Created: latency-svc-kbbgr
Jan  5 07:24:12.288: INFO: Got endpoints: latency-svc-kbbgr [250.774109ms]
Jan  5 07:24:12.290: INFO: Created: latency-svc-p898g
Jan  5 07:24:12.410: INFO: Got endpoints: latency-svc-p898g [372.84394ms]
Jan  5 07:24:12.544: INFO: Created: latency-svc-vzgz9
Jan  5 07:24:12.571: INFO: Got endpoints: latency-svc-vzgz9 [534.296932ms]
Jan  5 07:24:12.579: INFO: Created: latency-svc-8rfqw
Jan  5 07:24:12.587: INFO: Got endpoints: latency-svc-8rfqw [508.65862ms]
Jan  5 07:24:12.588: INFO: Created: latency-svc-bxbzh
Jan  5 07:24:12.710: INFO: Got endpoints: latency-svc-bxbzh [611.184015ms]
Jan  5 07:24:12.712: INFO: Created: latency-svc-jk7q9
Jan  5 07:24:12.724: INFO: Got endpoints: latency-svc-jk7q9 [613.284603ms]
Jan  5 07:24:12.732: INFO: Created: latency-svc-gncmt
Jan  5 07:24:12.744: INFO: Got endpoints: latency-svc-gncmt [609.344223ms]
Jan  5 07:24:12.744: INFO: Created: latency-svc-dz9dw
Jan  5 07:24:12.760: INFO: Got endpoints: latency-svc-dz9dw [601.57365ms]
Jan  5 07:24:12.773: INFO: Created: latency-svc-67lgk
Jan  5 07:24:12.781: INFO: Got endpoints: latency-svc-67lgk [612.67594ms]
Jan  5 07:24:12.807: INFO: Created: latency-svc-kgqpr
Jan  5 07:24:12.822: INFO: Got endpoints: latency-svc-kgqpr [605.433235ms]
Jan  5 07:24:12.843: INFO: Created: latency-svc-jkwc8
Jan  5 07:24:12.849: INFO: Got endpoints: latency-svc-jkwc8 [630.950552ms]
Jan  5 07:24:12.855: INFO: Created: latency-svc-mb22x
Jan  5 07:24:12.921: INFO: Created: latency-svc-rt8rl
Jan  5 07:24:12.922: INFO: Got endpoints: latency-svc-mb22x [685.871362ms]
Jan  5 07:24:12.941: INFO: Got endpoints: latency-svc-rt8rl [683.504133ms]
Jan  5 07:24:12.953: INFO: Created: latency-svc-gkcbp
Jan  5 07:24:12.975: INFO: Got endpoints: latency-svc-gkcbp [706.422708ms]
Jan  5 07:24:12.985: INFO: Created: latency-svc-nwnm9
Jan  5 07:24:12.987: INFO: Created: latency-svc-q4c9q
Jan  5 07:24:12.992: INFO: Got endpoints: latency-svc-nwnm9 [704.131206ms]
Jan  5 07:24:12.993: INFO: Got endpoints: latency-svc-q4c9q [956.349325ms]
Jan  5 07:24:12.996: INFO: Created: latency-svc-vdrjs
Jan  5 07:24:13.002: INFO: Got endpoints: latency-svc-vdrjs [592.070886ms]
Jan  5 07:24:13.012: INFO: Created: latency-svc-9k7qv
Jan  5 07:24:13.021: INFO: Got endpoints: latency-svc-9k7qv [449.274052ms]
Jan  5 07:24:13.021: INFO: Created: latency-svc-lmb79
Jan  5 07:24:13.028: INFO: Got endpoints: latency-svc-lmb79 [440.625879ms]
Jan  5 07:24:13.114: INFO: Created: latency-svc-gkfd5
Jan  5 07:24:13.137: INFO: Got endpoints: latency-svc-gkfd5 [426.937621ms]
Jan  5 07:24:13.139: INFO: Created: latency-svc-hgvn5
Jan  5 07:24:13.148: INFO: Got endpoints: latency-svc-hgvn5 [423.705543ms]
Jan  5 07:24:13.176: INFO: Created: latency-svc-s9lfd
Jan  5 07:24:13.204: INFO: Got endpoints: latency-svc-s9lfd [460.176347ms]
Jan  5 07:24:13.204: INFO: Created: latency-svc-l9m4z
Jan  5 07:24:13.231: INFO: Got endpoints: latency-svc-l9m4z [470.423566ms]
Jan  5 07:24:13.235: INFO: Created: latency-svc-p7lcg
Jan  5 07:24:13.242: INFO: Got endpoints: latency-svc-p7lcg [460.936659ms]
Jan  5 07:24:13.252: INFO: Created: latency-svc-rrgrk
Jan  5 07:24:13.265: INFO: Got endpoints: latency-svc-rrgrk [443.203446ms]
Jan  5 07:24:13.267: INFO: Created: latency-svc-jntnq
Jan  5 07:24:13.274: INFO: Got endpoints: latency-svc-jntnq [425.259945ms]
Jan  5 07:24:13.282: INFO: Created: latency-svc-5nqsv
Jan  5 07:24:13.290: INFO: Got endpoints: latency-svc-5nqsv [367.550347ms]
Jan  5 07:24:13.301: INFO: Created: latency-svc-b9dvf
Jan  5 07:24:13.311: INFO: Got endpoints: latency-svc-b9dvf [370.22622ms]
Jan  5 07:24:13.313: INFO: Created: latency-svc-n4txv
Jan  5 07:24:13.414: INFO: Created: latency-svc-s5npb
Jan  5 07:24:13.420: INFO: Got endpoints: latency-svc-n4txv [445.507031ms]
Jan  5 07:24:13.421: INFO: Got endpoints: latency-svc-s5npb [428.83409ms]
Jan  5 07:24:13.433: INFO: Created: latency-svc-j7v5z
Jan  5 07:24:13.444: INFO: Got endpoints: latency-svc-j7v5z [450.663713ms]
Jan  5 07:24:13.452: INFO: Created: latency-svc-85sfv
Jan  5 07:24:13.455: INFO: Got endpoints: latency-svc-85sfv [453.364493ms]
Jan  5 07:24:13.464: INFO: Created: latency-svc-fp6rf
Jan  5 07:24:13.473: INFO: Got endpoints: latency-svc-fp6rf [452.315531ms]
Jan  5 07:24:13.481: INFO: Created: latency-svc-fd7rq
Jan  5 07:24:13.487: INFO: Got endpoints: latency-svc-fd7rq [458.582135ms]
Jan  5 07:24:13.507: INFO: Created: latency-svc-qkvtz
Jan  5 07:24:13.510: INFO: Created: latency-svc-hjqk8
Jan  5 07:24:13.518: INFO: Got endpoints: latency-svc-qkvtz [381.54232ms]
Jan  5 07:24:13.519: INFO: Got endpoints: latency-svc-hjqk8 [371.193114ms]
Jan  5 07:24:13.528: INFO: Created: latency-svc-2wkbl
Jan  5 07:24:13.535: INFO: Got endpoints: latency-svc-2wkbl [331.326069ms]
Jan  5 07:24:13.538: INFO: Created: latency-svc-v57ps
Jan  5 07:24:13.544: INFO: Got endpoints: latency-svc-v57ps [313.021314ms]
Jan  5 07:24:13.553: INFO: Created: latency-svc-2cqfp
Jan  5 07:24:13.595: INFO: Got endpoints: latency-svc-2cqfp [352.997706ms]
Jan  5 07:24:13.595: INFO: Created: latency-svc-ctwrl
Jan  5 07:24:13.603: INFO: Got endpoints: latency-svc-ctwrl [337.458012ms]
Jan  5 07:24:13.632: INFO: Created: latency-svc-vqjlh
Jan  5 07:24:13.634: INFO: Got endpoints: latency-svc-vqjlh [360.062144ms]
Jan  5 07:24:13.647: INFO: Created: latency-svc-nld4s
Jan  5 07:24:13.654: INFO: Got endpoints: latency-svc-nld4s [364.306371ms]
Jan  5 07:24:13.656: INFO: Created: latency-svc-tb6q9
Jan  5 07:24:13.666: INFO: Got endpoints: latency-svc-tb6q9 [354.72753ms]
Jan  5 07:24:13.692: INFO: Created: latency-svc-4lzd2
Jan  5 07:24:13.695: INFO: Got endpoints: latency-svc-4lzd2 [274.041582ms]
Jan  5 07:24:13.704: INFO: Created: latency-svc-ph4j4
Jan  5 07:24:13.712: INFO: Got endpoints: latency-svc-ph4j4 [292.176763ms]
Jan  5 07:24:13.718: INFO: Created: latency-svc-7ns5v
Jan  5 07:24:13.725: INFO: Got endpoints: latency-svc-7ns5v [280.747962ms]
Jan  5 07:24:13.726: INFO: Created: latency-svc-njhcm
Jan  5 07:24:13.747: INFO: Got endpoints: latency-svc-njhcm [291.361714ms]
Jan  5 07:24:13.749: INFO: Created: latency-svc-hcf9x
Jan  5 07:24:13.758: INFO: Got endpoints: latency-svc-hcf9x [284.632056ms]
Jan  5 07:24:13.764: INFO: Created: latency-svc-q55hv
Jan  5 07:24:13.789: INFO: Got endpoints: latency-svc-q55hv [302.704169ms]
Jan  5 07:24:13.800: INFO: Created: latency-svc-d5chd
Jan  5 07:24:13.806: INFO: Got endpoints: latency-svc-d5chd [287.660495ms]
Jan  5 07:24:13.808: INFO: Created: latency-svc-v86zq
Jan  5 07:24:13.834: INFO: Got endpoints: latency-svc-v86zq [314.410843ms]
Jan  5 07:24:13.840: INFO: Created: latency-svc-vgmmq
Jan  5 07:24:13.847: INFO: Got endpoints: latency-svc-vgmmq [311.482458ms]
Jan  5 07:24:13.858: INFO: Created: latency-svc-7skqz
Jan  5 07:24:13.877: INFO: Got endpoints: latency-svc-7skqz [333.384722ms]
Jan  5 07:24:13.879: INFO: Created: latency-svc-864c5
Jan  5 07:24:13.890: INFO: Got endpoints: latency-svc-864c5 [294.997218ms]
Jan  5 07:24:13.895: INFO: Created: latency-svc-n9fqt
Jan  5 07:24:13.898: INFO: Got endpoints: latency-svc-n9fqt [295.732698ms]
Jan  5 07:24:13.908: INFO: Created: latency-svc-zkv76
Jan  5 07:24:13.919: INFO: Created: latency-svc-ng4bg
Jan  5 07:24:13.927: INFO: Created: latency-svc-2wz2b
Jan  5 07:24:13.936: INFO: Got endpoints: latency-svc-zkv76 [301.583564ms]
Jan  5 07:24:13.944: INFO: Created: latency-svc-gzx8w
Jan  5 07:24:13.953: INFO: Created: latency-svc-x46nx
Jan  5 07:24:13.967: INFO: Created: latency-svc-c7gd9
Jan  5 07:24:13.982: INFO: Created: latency-svc-p5pxq
Jan  5 07:24:13.994: INFO: Got endpoints: latency-svc-ng4bg [339.445476ms]
Jan  5 07:24:13.999: INFO: Created: latency-svc-phjn6
Jan  5 07:24:14.008: INFO: Created: latency-svc-m8nhp
Jan  5 07:24:14.015: INFO: Created: latency-svc-b7b7g
Jan  5 07:24:14.024: INFO: Created: latency-svc-pr5jw
Jan  5 07:24:14.037: INFO: Created: latency-svc-zbjfn
Jan  5 07:24:14.039: INFO: Got endpoints: latency-svc-2wz2b [372.952373ms]
Jan  5 07:24:14.047: INFO: Created: latency-svc-7c7lc
Jan  5 07:24:14.056: INFO: Created: latency-svc-4hrr5
Jan  5 07:24:14.065: INFO: Created: latency-svc-d8bfm
Jan  5 07:24:14.079: INFO: Created: latency-svc-6x2zh
Jan  5 07:24:14.090: INFO: Got endpoints: latency-svc-gzx8w [394.667541ms]
Jan  5 07:24:14.098: INFO: Created: latency-svc-hlm22
Jan  5 07:24:14.125: INFO: Created: latency-svc-bgz44
Jan  5 07:24:14.133: INFO: Created: latency-svc-kzbtk
Jan  5 07:24:14.277: INFO: Got endpoints: latency-svc-c7gd9 [551.947434ms]
Jan  5 07:24:14.277: INFO: Got endpoints: latency-svc-p5pxq [530.123218ms]
Jan  5 07:24:14.277: INFO: Got endpoints: latency-svc-x46nx [564.466632ms]
Jan  5 07:24:14.304: INFO: Got endpoints: latency-svc-phjn6 [545.950639ms]
Jan  5 07:24:14.334: INFO: Created: latency-svc-t6r8w
Jan  5 07:24:14.355: INFO: Got endpoints: latency-svc-m8nhp [565.485359ms]
Jan  5 07:24:14.363: INFO: Created: latency-svc-blzqn
Jan  5 07:24:14.406: INFO: Created: latency-svc-zpdqf
Jan  5 07:24:14.412: INFO: Got endpoints: latency-svc-b7b7g [605.683338ms]
Jan  5 07:24:14.433: INFO: Created: latency-svc-2vlzw
Jan  5 07:24:14.441: INFO: Got endpoints: latency-svc-pr5jw [607.570065ms]
Jan  5 07:24:14.446: INFO: Created: latency-svc-vtg94
Jan  5 07:24:14.460: INFO: Created: latency-svc-f5mw2
Jan  5 07:24:14.472: INFO: Created: latency-svc-v5x2k
Jan  5 07:24:14.496: INFO: Got endpoints: latency-svc-zbjfn [648.868827ms]
Jan  5 07:24:14.508: INFO: Created: latency-svc-h9tvq
Jan  5 07:24:14.542: INFO: Got endpoints: latency-svc-7c7lc [664.626334ms]
Jan  5 07:24:14.554: INFO: Created: latency-svc-xjbwk
Jan  5 07:24:14.587: INFO: Got endpoints: latency-svc-4hrr5 [697.156381ms]
Jan  5 07:24:14.606: INFO: Created: latency-svc-w7nfh
Jan  5 07:24:14.641: INFO: Got endpoints: latency-svc-d8bfm [742.347957ms]
Jan  5 07:24:14.654: INFO: Created: latency-svc-89rtc
Jan  5 07:24:14.691: INFO: Got endpoints: latency-svc-6x2zh [754.696653ms]
Jan  5 07:24:14.734: INFO: Created: latency-svc-dvvwh
Jan  5 07:24:14.736: INFO: Got endpoints: latency-svc-hlm22 [742.005877ms]
Jan  5 07:24:14.771: INFO: Created: latency-svc-tg9rn
Jan  5 07:24:14.786: INFO: Got endpoints: latency-svc-bgz44 [746.962713ms]
Jan  5 07:24:14.814: INFO: Created: latency-svc-b8cht
Jan  5 07:24:14.862: INFO: Got endpoints: latency-svc-kzbtk [772.057258ms]
Jan  5 07:24:14.893: INFO: Got endpoints: latency-svc-t6r8w [616.226902ms]
Jan  5 07:24:14.895: INFO: Created: latency-svc-jsmp4
Jan  5 07:24:14.909: INFO: Created: latency-svc-njgpj
Jan  5 07:24:14.950: INFO: Got endpoints: latency-svc-blzqn [673.334424ms]
Jan  5 07:24:15.029: INFO: Created: latency-svc-grrr9
Jan  5 07:24:15.030: INFO: Got endpoints: latency-svc-zpdqf [752.727425ms]
Jan  5 07:24:15.041: INFO: Got endpoints: latency-svc-2vlzw [737.79868ms]
Jan  5 07:24:15.048: INFO: Created: latency-svc-gvrnt
Jan  5 07:24:15.061: INFO: Created: latency-svc-cfg9n
Jan  5 07:24:15.092: INFO: Got endpoints: latency-svc-vtg94 [736.616942ms]
Jan  5 07:24:15.104: INFO: Created: latency-svc-zlj77
Jan  5 07:24:15.137: INFO: Got endpoints: latency-svc-f5mw2 [724.984399ms]
Jan  5 07:24:15.153: INFO: Created: latency-svc-tfhcm
Jan  5 07:24:15.192: INFO: Got endpoints: latency-svc-v5x2k [750.248018ms]
Jan  5 07:24:15.205: INFO: Created: latency-svc-jlb88
Jan  5 07:24:15.237: INFO: Got endpoints: latency-svc-h9tvq [741.175697ms]
Jan  5 07:24:15.268: INFO: Created: latency-svc-22sxw
Jan  5 07:24:15.301: INFO: Got endpoints: latency-svc-xjbwk [759.262615ms]
Jan  5 07:24:15.317: INFO: Created: latency-svc-bjdgn
Jan  5 07:24:15.345: INFO: Got endpoints: latency-svc-w7nfh [758.049119ms]
Jan  5 07:24:15.358: INFO: Created: latency-svc-v944s
Jan  5 07:24:15.391: INFO: Got endpoints: latency-svc-89rtc [750.04168ms]
Jan  5 07:24:15.419: INFO: Created: latency-svc-wqlcg
Jan  5 07:24:15.436: INFO: Got endpoints: latency-svc-dvvwh [745.373501ms]
Jan  5 07:24:15.453: INFO: Created: latency-svc-dbpdd
Jan  5 07:24:15.486: INFO: Got endpoints: latency-svc-tg9rn [750.328514ms]
Jan  5 07:24:15.504: INFO: Created: latency-svc-9mwlv
Jan  5 07:24:15.536: INFO: Got endpoints: latency-svc-b8cht [749.987222ms]
Jan  5 07:24:15.552: INFO: Created: latency-svc-xkspx
Jan  5 07:24:15.591: INFO: Got endpoints: latency-svc-jsmp4 [728.992632ms]
Jan  5 07:24:15.603: INFO: Created: latency-svc-tdj8r
Jan  5 07:24:15.641: INFO: Got endpoints: latency-svc-njgpj [748.1081ms]
Jan  5 07:24:15.672: INFO: Created: latency-svc-t6s4t
Jan  5 07:24:15.747: INFO: Got endpoints: latency-svc-gvrnt [717.575544ms]
Jan  5 07:24:15.747: INFO: Got endpoints: latency-svc-grrr9 [796.930831ms]
Jan  5 07:24:15.765: INFO: Created: latency-svc-tbmjq
Jan  5 07:24:15.774: INFO: Created: latency-svc-jvrp8
Jan  5 07:24:15.786: INFO: Got endpoints: latency-svc-cfg9n [744.275013ms]
Jan  5 07:24:15.803: INFO: Created: latency-svc-6sjfr
Jan  5 07:24:15.836: INFO: Got endpoints: latency-svc-zlj77 [744.139339ms]
Jan  5 07:24:15.866: INFO: Created: latency-svc-9b58p
Jan  5 07:24:15.886: INFO: Got endpoints: latency-svc-tfhcm [749.550435ms]
Jan  5 07:24:15.907: INFO: Created: latency-svc-cdnc7
Jan  5 07:24:15.941: INFO: Got endpoints: latency-svc-jlb88 [749.645048ms]
Jan  5 07:24:15.955: INFO: Created: latency-svc-2fpc2
Jan  5 07:24:15.991: INFO: Got endpoints: latency-svc-22sxw [754.45602ms]
Jan  5 07:24:16.004: INFO: Created: latency-svc-ksp82
Jan  5 07:24:16.037: INFO: Got endpoints: latency-svc-bjdgn [735.641788ms]
Jan  5 07:24:16.050: INFO: Created: latency-svc-r9jzv
Jan  5 07:24:16.086: INFO: Got endpoints: latency-svc-v944s [741.552873ms]
Jan  5 07:24:16.100: INFO: Created: latency-svc-xx5ln
Jan  5 07:24:16.141: INFO: Got endpoints: latency-svc-wqlcg [749.817748ms]
Jan  5 07:24:16.155: INFO: Created: latency-svc-kgxm9
Jan  5 07:24:16.191: INFO: Got endpoints: latency-svc-dbpdd [754.407461ms]
Jan  5 07:24:16.206: INFO: Created: latency-svc-w4p2v
Jan  5 07:24:16.237: INFO: Got endpoints: latency-svc-9mwlv [750.458869ms]
Jan  5 07:24:16.250: INFO: Created: latency-svc-r6q7w
Jan  5 07:24:16.298: INFO: Got endpoints: latency-svc-xkspx [761.823579ms]
Jan  5 07:24:16.317: INFO: Created: latency-svc-t6rrz
Jan  5 07:24:16.336: INFO: Got endpoints: latency-svc-tdj8r [745.659096ms]
Jan  5 07:24:16.352: INFO: Created: latency-svc-vftjd
Jan  5 07:24:16.392: INFO: Got endpoints: latency-svc-t6s4t [750.641329ms]
Jan  5 07:24:16.408: INFO: Created: latency-svc-4drt6
Jan  5 07:24:16.442: INFO: Got endpoints: latency-svc-tbmjq [694.375988ms]
Jan  5 07:24:16.456: INFO: Created: latency-svc-4xxkf
Jan  5 07:24:16.491: INFO: Got endpoints: latency-svc-jvrp8 [743.698589ms]
Jan  5 07:24:16.506: INFO: Created: latency-svc-p9vtz
Jan  5 07:24:16.537: INFO: Got endpoints: latency-svc-6sjfr [750.99097ms]
Jan  5 07:24:16.568: INFO: Created: latency-svc-4rdd7
Jan  5 07:24:16.592: INFO: Got endpoints: latency-svc-9b58p [756.288086ms]
Jan  5 07:24:16.606: INFO: Created: latency-svc-fpfjq
Jan  5 07:24:16.636: INFO: Got endpoints: latency-svc-cdnc7 [749.590314ms]
Jan  5 07:24:16.653: INFO: Created: latency-svc-679xn
Jan  5 07:24:16.690: INFO: Got endpoints: latency-svc-2fpc2 [748.826272ms]
Jan  5 07:24:16.702: INFO: Created: latency-svc-4929z
Jan  5 07:24:16.742: INFO: Got endpoints: latency-svc-ksp82 [750.294844ms]
Jan  5 07:24:16.754: INFO: Created: latency-svc-rxxbh
Jan  5 07:24:16.791: INFO: Got endpoints: latency-svc-r9jzv [754.089414ms]
Jan  5 07:24:16.803: INFO: Created: latency-svc-bcg88
Jan  5 07:24:16.884: INFO: Got endpoints: latency-svc-xx5ln [797.739418ms]
Jan  5 07:24:16.886: INFO: Got endpoints: latency-svc-kgxm9 [745.347693ms]
Jan  5 07:24:16.903: INFO: Created: latency-svc-vdgkz
Jan  5 07:24:17.084: INFO: Got endpoints: latency-svc-w4p2v [893.062893ms]
Jan  5 07:24:17.085: INFO: Got endpoints: latency-svc-t6rrz [787.556324ms]
Jan  5 07:24:17.085: INFO: Got endpoints: latency-svc-r6q7w [848.767727ms]
Jan  5 07:24:17.086: INFO: Created: latency-svc-hx5xh
Jan  5 07:24:17.092: INFO: Got endpoints: latency-svc-vftjd [755.005791ms]
Jan  5 07:24:17.104: INFO: Created: latency-svc-j4tbc
Jan  5 07:24:17.113: INFO: Created: latency-svc-69tws
Jan  5 07:24:17.121: INFO: Created: latency-svc-478nr
Jan  5 07:24:17.130: INFO: Created: latency-svc-4frwk
Jan  5 07:24:17.136: INFO: Got endpoints: latency-svc-4drt6 [743.893581ms]
Jan  5 07:24:17.152: INFO: Created: latency-svc-74bfw
Jan  5 07:24:17.199: INFO: Got endpoints: latency-svc-4xxkf [757.452663ms]
Jan  5 07:24:17.212: INFO: Created: latency-svc-pwlmh
Jan  5 07:24:17.235: INFO: Got endpoints: latency-svc-p9vtz [744.426503ms]
Jan  5 07:24:17.252: INFO: Created: latency-svc-mp5r7
Jan  5 07:24:17.292: INFO: Got endpoints: latency-svc-4rdd7 [754.703235ms]
Jan  5 07:24:17.311: INFO: Created: latency-svc-qg2cb
Jan  5 07:24:17.336: INFO: Got endpoints: latency-svc-fpfjq [743.347481ms]
Jan  5 07:24:17.352: INFO: Created: latency-svc-zlzds
Jan  5 07:24:17.387: INFO: Got endpoints: latency-svc-679xn [750.408588ms]
Jan  5 07:24:17.403: INFO: Created: latency-svc-rwsmg
Jan  5 07:24:17.436: INFO: Got endpoints: latency-svc-4929z [745.448032ms]
Jan  5 07:24:17.452: INFO: Created: latency-svc-j6tt2
Jan  5 07:24:17.486: INFO: Got endpoints: latency-svc-rxxbh [744.602882ms]
Jan  5 07:24:17.504: INFO: Created: latency-svc-rp7fh
Jan  5 07:24:17.535: INFO: Got endpoints: latency-svc-bcg88 [744.503663ms]
Jan  5 07:24:17.555: INFO: Created: latency-svc-7dwch
Jan  5 07:24:17.586: INFO: Got endpoints: latency-svc-vdgkz [701.523746ms]
Jan  5 07:24:17.605: INFO: Created: latency-svc-bc74m
Jan  5 07:24:17.642: INFO: Got endpoints: latency-svc-hx5xh [755.442134ms]
Jan  5 07:24:17.654: INFO: Created: latency-svc-nchqs
Jan  5 07:24:17.690: INFO: Got endpoints: latency-svc-j4tbc [605.750924ms]
Jan  5 07:24:17.702: INFO: Created: latency-svc-kk62d
Jan  5 07:24:17.754: INFO: Got endpoints: latency-svc-69tws [668.814482ms]
Jan  5 07:24:17.769: INFO: Created: latency-svc-7vgk8
Jan  5 07:24:17.786: INFO: Got endpoints: latency-svc-478nr [700.505088ms]
Jan  5 07:24:17.803: INFO: Created: latency-svc-6n5qn
Jan  5 07:24:17.837: INFO: Got endpoints: latency-svc-4frwk [745.233576ms]
Jan  5 07:24:17.848: INFO: Created: latency-svc-9zq2b
Jan  5 07:24:17.885: INFO: Got endpoints: latency-svc-74bfw [749.461208ms]
Jan  5 07:24:17.902: INFO: Created: latency-svc-9j4jp
Jan  5 07:24:17.936: INFO: Got endpoints: latency-svc-pwlmh [736.430148ms]
Jan  5 07:24:17.955: INFO: Created: latency-svc-z6rfh
Jan  5 07:24:17.986: INFO: Got endpoints: latency-svc-mp5r7 [750.824238ms]
Jan  5 07:24:17.998: INFO: Created: latency-svc-phz4b
Jan  5 07:24:18.042: INFO: Got endpoints: latency-svc-qg2cb [750.058052ms]
Jan  5 07:24:18.057: INFO: Created: latency-svc-k8vgm
Jan  5 07:24:18.095: INFO: Got endpoints: latency-svc-zlzds [759.474756ms]
Jan  5 07:24:18.109: INFO: Created: latency-svc-cm79z
Jan  5 07:24:18.140: INFO: Got endpoints: latency-svc-rwsmg [753.558738ms]
Jan  5 07:24:18.153: INFO: Created: latency-svc-qpgcm
Jan  5 07:24:18.199: INFO: Got endpoints: latency-svc-j6tt2 [763.199115ms]
Jan  5 07:24:18.211: INFO: Created: latency-svc-jgzxc
Jan  5 07:24:18.235: INFO: Got endpoints: latency-svc-rp7fh [748.999862ms]
Jan  5 07:24:18.254: INFO: Created: latency-svc-7b788
Jan  5 07:24:18.285: INFO: Got endpoints: latency-svc-7dwch [749.816839ms]
Jan  5 07:24:18.316: INFO: Created: latency-svc-jw2kb
Jan  5 07:24:18.336: INFO: Got endpoints: latency-svc-bc74m [750.267533ms]
Jan  5 07:24:18.355: INFO: Created: latency-svc-zg6k4
Jan  5 07:24:18.386: INFO: Got endpoints: latency-svc-nchqs [744.517932ms]
Jan  5 07:24:18.398: INFO: Created: latency-svc-6z2j6
Jan  5 07:24:18.436: INFO: Got endpoints: latency-svc-kk62d [746.628017ms]
Jan  5 07:24:18.451: INFO: Created: latency-svc-mq9m7
Jan  5 07:24:18.492: INFO: Got endpoints: latency-svc-7vgk8 [738.232342ms]
Jan  5 07:24:18.523: INFO: Created: latency-svc-tmn5v
Jan  5 07:24:18.565: INFO: Got endpoints: latency-svc-6n5qn [778.851959ms]
Jan  5 07:24:18.622: INFO: Created: latency-svc-k96sw
Jan  5 07:24:18.622: INFO: Got endpoints: latency-svc-9zq2b [785.568469ms]
Jan  5 07:24:18.653: INFO: Got endpoints: latency-svc-9j4jp [767.505082ms]
Jan  5 07:24:18.654: INFO: Created: latency-svc-hfs4h
Jan  5 07:24:18.671: INFO: Created: latency-svc-8fgdn
Jan  5 07:24:18.690: INFO: Got endpoints: latency-svc-z6rfh [754.743849ms]
Jan  5 07:24:18.703: INFO: Created: latency-svc-cmd92
Jan  5 07:24:18.735: INFO: Got endpoints: latency-svc-phz4b [749.108529ms]
Jan  5 07:24:18.767: INFO: Created: latency-svc-fbjgv
Jan  5 07:24:18.795: INFO: Got endpoints: latency-svc-k8vgm [753.149843ms]
Jan  5 07:24:18.807: INFO: Created: latency-svc-9nhpw
Jan  5 07:24:18.842: INFO: Got endpoints: latency-svc-cm79z [746.943718ms]
Jan  5 07:24:18.855: INFO: Created: latency-svc-rrzb8
Jan  5 07:24:18.886: INFO: Got endpoints: latency-svc-qpgcm [746.249239ms]
Jan  5 07:24:18.905: INFO: Created: latency-svc-xxqt2
Jan  5 07:24:18.942: INFO: Got endpoints: latency-svc-jgzxc [743.304266ms]
Jan  5 07:24:18.954: INFO: Created: latency-svc-5thsp
Jan  5 07:24:18.990: INFO: Got endpoints: latency-svc-7b788 [755.03841ms]
Jan  5 07:24:19.003: INFO: Created: latency-svc-q52pr
Jan  5 07:24:19.037: INFO: Got endpoints: latency-svc-jw2kb [751.317305ms]
Jan  5 07:24:19.050: INFO: Created: latency-svc-9brt5
Jan  5 07:24:19.091: INFO: Got endpoints: latency-svc-zg6k4 [754.889097ms]
Jan  5 07:24:19.106: INFO: Created: latency-svc-lvd96
Jan  5 07:24:19.143: INFO: Got endpoints: latency-svc-6z2j6 [756.705474ms]
Jan  5 07:24:19.155: INFO: Created: latency-svc-vt4wg
Jan  5 07:24:19.191: INFO: Got endpoints: latency-svc-mq9m7 [754.926792ms]
Jan  5 07:24:19.203: INFO: Created: latency-svc-rglg5
Jan  5 07:24:19.241: INFO: Got endpoints: latency-svc-tmn5v [748.299433ms]
Jan  5 07:24:19.253: INFO: Created: latency-svc-xb2lt
Jan  5 07:24:19.291: INFO: Got endpoints: latency-svc-k96sw [725.874963ms]
Jan  5 07:24:19.303: INFO: Created: latency-svc-hhxl9
Jan  5 07:24:19.345: INFO: Got endpoints: latency-svc-hfs4h [723.071151ms]
Jan  5 07:24:19.359: INFO: Created: latency-svc-2mlb8
Jan  5 07:24:19.392: INFO: Got endpoints: latency-svc-8fgdn [739.158685ms]
Jan  5 07:24:19.411: INFO: Created: latency-svc-d8l9b
Jan  5 07:24:19.441: INFO: Got endpoints: latency-svc-cmd92 [750.4799ms]
Jan  5 07:24:19.461: INFO: Created: latency-svc-dptq6
Jan  5 07:24:19.485: INFO: Got endpoints: latency-svc-fbjgv [750.006639ms]
Jan  5 07:24:19.502: INFO: Created: latency-svc-vn5k7
Jan  5 07:24:19.542: INFO: Got endpoints: latency-svc-9nhpw [746.892095ms]
Jan  5 07:24:19.566: INFO: Created: latency-svc-6h6xr
Jan  5 07:24:19.590: INFO: Got endpoints: latency-svc-rrzb8 [748.431297ms]
Jan  5 07:24:19.608: INFO: Created: latency-svc-mr6m5
Jan  5 07:24:19.642: INFO: Got endpoints: latency-svc-xxqt2 [755.572717ms]
Jan  5 07:24:19.658: INFO: Created: latency-svc-9rndj
Jan  5 07:24:19.692: INFO: Got endpoints: latency-svc-5thsp [750.052195ms]
Jan  5 07:24:19.718: INFO: Created: latency-svc-n7w9j
Jan  5 07:24:19.743: INFO: Got endpoints: latency-svc-q52pr [752.201021ms]
Jan  5 07:24:19.759: INFO: Created: latency-svc-2gpts
Jan  5 07:24:19.792: INFO: Got endpoints: latency-svc-9brt5 [755.339052ms]
Jan  5 07:24:19.808: INFO: Created: latency-svc-58rb4
Jan  5 07:24:19.836: INFO: Got endpoints: latency-svc-lvd96 [745.20239ms]
Jan  5 07:24:19.869: INFO: Created: latency-svc-9w7x8
Jan  5 07:24:19.911: INFO: Got endpoints: latency-svc-vt4wg [768.439214ms]
Jan  5 07:24:19.937: INFO: Got endpoints: latency-svc-rglg5 [745.258387ms]
Jan  5 07:24:19.987: INFO: Got endpoints: latency-svc-xb2lt [746.025217ms]
Jan  5 07:24:20.036: INFO: Got endpoints: latency-svc-hhxl9 [745.580837ms]
Jan  5 07:24:20.092: INFO: Got endpoints: latency-svc-2mlb8 [746.59441ms]
Jan  5 07:24:20.142: INFO: Got endpoints: latency-svc-d8l9b [750.000704ms]
Jan  5 07:24:20.186: INFO: Got endpoints: latency-svc-dptq6 [745.267322ms]
Jan  5 07:24:20.311: INFO: Got endpoints: latency-svc-vn5k7 [825.544397ms]
Jan  5 07:24:20.313: INFO: Got endpoints: latency-svc-6h6xr [771.101344ms]
Jan  5 07:24:20.348: INFO: Got endpoints: latency-svc-mr6m5 [757.798686ms]
Jan  5 07:24:20.391: INFO: Got endpoints: latency-svc-9rndj [749.119914ms]
Jan  5 07:24:20.441: INFO: Got endpoints: latency-svc-n7w9j [748.602724ms]
Jan  5 07:24:20.486: INFO: Got endpoints: latency-svc-2gpts [743.654224ms]
Jan  5 07:24:20.536: INFO: Got endpoints: latency-svc-58rb4 [744.215923ms]
Jan  5 07:24:20.592: INFO: Got endpoints: latency-svc-9w7x8 [755.333535ms]
Jan  5 07:24:20.592: INFO: Latencies: [41.968191ms 61.83692ms 73.981238ms 97.33964ms 121.67512ms 130.849586ms 179.530886ms 181.189397ms 199.394113ms 220.383824ms 231.372425ms 250.774109ms 274.041582ms 280.747962ms 284.632056ms 287.660495ms 291.361714ms 292.176763ms 294.997218ms 295.732698ms 301.583564ms 302.704169ms 311.482458ms 313.021314ms 314.410843ms 331.326069ms 333.384722ms 337.458012ms 339.445476ms 352.997706ms 354.72753ms 360.062144ms 364.306371ms 367.550347ms 370.22622ms 371.193114ms 372.84394ms 372.952373ms 381.54232ms 394.667541ms 423.705543ms 425.259945ms 426.937621ms 428.83409ms 440.625879ms 443.203446ms 445.507031ms 449.274052ms 450.663713ms 452.315531ms 453.364493ms 458.582135ms 460.176347ms 460.936659ms 470.423566ms 508.65862ms 530.123218ms 534.296932ms 545.950639ms 551.947434ms 564.466632ms 565.485359ms 592.070886ms 601.57365ms 605.433235ms 605.683338ms 605.750924ms 607.570065ms 609.344223ms 611.184015ms 612.67594ms 613.284603ms 616.226902ms 630.950552ms 648.868827ms 664.626334ms 668.814482ms 673.334424ms 683.504133ms 685.871362ms 694.375988ms 697.156381ms 700.505088ms 701.523746ms 704.131206ms 706.422708ms 717.575544ms 723.071151ms 724.984399ms 725.874963ms 728.992632ms 735.641788ms 736.430148ms 736.616942ms 737.79868ms 738.232342ms 739.158685ms 741.175697ms 741.552873ms 742.005877ms 742.347957ms 743.304266ms 743.347481ms 743.654224ms 743.698589ms 743.893581ms 744.139339ms 744.215923ms 744.275013ms 744.426503ms 744.503663ms 744.517932ms 744.602882ms 745.20239ms 745.233576ms 745.258387ms 745.267322ms 745.347693ms 745.373501ms 745.448032ms 745.580837ms 745.659096ms 746.025217ms 746.249239ms 746.59441ms 746.628017ms 746.892095ms 746.943718ms 746.962713ms 748.1081ms 748.299433ms 748.431297ms 748.602724ms 748.826272ms 748.999862ms 749.108529ms 749.119914ms 749.461208ms 749.550435ms 749.590314ms 749.645048ms 749.816839ms 749.817748ms 749.987222ms 750.000704ms 750.006639ms 750.04168ms 750.052195ms 750.058052ms 750.248018ms 750.267533ms 750.294844ms 750.328514ms 750.408588ms 750.458869ms 750.4799ms 750.641329ms 750.824238ms 750.99097ms 751.317305ms 752.201021ms 752.727425ms 753.149843ms 753.558738ms 754.089414ms 754.407461ms 754.45602ms 754.696653ms 754.703235ms 754.743849ms 754.889097ms 754.926792ms 755.005791ms 755.03841ms 755.333535ms 755.339052ms 755.442134ms 755.572717ms 756.288086ms 756.705474ms 757.452663ms 757.798686ms 758.049119ms 759.262615ms 759.474756ms 761.823579ms 763.199115ms 767.505082ms 768.439214ms 771.101344ms 772.057258ms 778.851959ms 785.568469ms 787.556324ms 796.930831ms 797.739418ms 825.544397ms 848.767727ms 893.062893ms 956.349325ms]
Jan  5 07:24:20.592: INFO: 50 %ile: 742.347957ms
Jan  5 07:24:20.592: INFO: 90 %ile: 757.452663ms
Jan  5 07:24:20.592: INFO: 99 %ile: 893.062893ms
Jan  5 07:24:20.592: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan  5 07:24:20.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6538" for this suite. 01/05/23 07:24:20.595
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":15,"skipped":346,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.771 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:24:07.827
    Jan  5 07:24:07.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename svc-latency 01/05/23 07:24:07.828
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:24:07.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:24:07.85
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan  5 07:24:07.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-6538 01/05/23 07:24:07.854
    I0105 07:24:07.863471      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6538, replica count: 1
    I0105 07:24:08.915220      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0105 07:24:09.915744      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0105 07:24:10.915950      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0105 07:24:11.916254      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 07:24:12.033: INFO: Created: latency-svc-79bsh
    Jan  5 07:24:12.037: INFO: Got endpoints: latency-svc-79bsh [20.113621ms]
    Jan  5 07:24:12.054: INFO: Created: latency-svc-lbdlt
    Jan  5 07:24:12.079: INFO: Got endpoints: latency-svc-lbdlt [41.968191ms]
    Jan  5 07:24:12.079: INFO: Created: latency-svc-7btfs
    Jan  5 07:24:12.099: INFO: Got endpoints: latency-svc-7btfs [61.83692ms]
    Jan  5 07:24:12.102: INFO: Created: latency-svc-sb4tv
    Jan  5 07:24:12.111: INFO: Got endpoints: latency-svc-sb4tv [73.981238ms]
    Jan  5 07:24:12.122: INFO: Created: latency-svc-968w2
    Jan  5 07:24:12.132: INFO: Created: latency-svc-x9p59
    Jan  5 07:24:12.134: INFO: Got endpoints: latency-svc-968w2 [97.33964ms]
    Jan  5 07:24:12.159: INFO: Got endpoints: latency-svc-x9p59 [121.67512ms]
    Jan  5 07:24:12.161: INFO: Created: latency-svc-vm8fm
    Jan  5 07:24:12.168: INFO: Got endpoints: latency-svc-vm8fm [130.849586ms]
    Jan  5 07:24:12.192: INFO: Created: latency-svc-t8vg8
    Jan  5 07:24:12.195: INFO: Created: latency-svc-dh87k
    Jan  5 07:24:12.216: INFO: Got endpoints: latency-svc-t8vg8 [179.530886ms]
    Jan  5 07:24:12.218: INFO: Got endpoints: latency-svc-dh87k [181.189397ms]
    Jan  5 07:24:12.225: INFO: Created: latency-svc-2rm59
    Jan  5 07:24:12.236: INFO: Got endpoints: latency-svc-2rm59 [199.394113ms]
    Jan  5 07:24:12.246: INFO: Created: latency-svc-t8gpg
    Jan  5 07:24:12.257: INFO: Got endpoints: latency-svc-t8gpg [220.383824ms]
    Jan  5 07:24:12.259: INFO: Created: latency-svc-l9j2j
    Jan  5 07:24:12.268: INFO: Got endpoints: latency-svc-l9j2j [231.372425ms]
    Jan  5 07:24:12.277: INFO: Created: latency-svc-kbbgr
    Jan  5 07:24:12.288: INFO: Got endpoints: latency-svc-kbbgr [250.774109ms]
    Jan  5 07:24:12.290: INFO: Created: latency-svc-p898g
    Jan  5 07:24:12.410: INFO: Got endpoints: latency-svc-p898g [372.84394ms]
    Jan  5 07:24:12.544: INFO: Created: latency-svc-vzgz9
    Jan  5 07:24:12.571: INFO: Got endpoints: latency-svc-vzgz9 [534.296932ms]
    Jan  5 07:24:12.579: INFO: Created: latency-svc-8rfqw
    Jan  5 07:24:12.587: INFO: Got endpoints: latency-svc-8rfqw [508.65862ms]
    Jan  5 07:24:12.588: INFO: Created: latency-svc-bxbzh
    Jan  5 07:24:12.710: INFO: Got endpoints: latency-svc-bxbzh [611.184015ms]
    Jan  5 07:24:12.712: INFO: Created: latency-svc-jk7q9
    Jan  5 07:24:12.724: INFO: Got endpoints: latency-svc-jk7q9 [613.284603ms]
    Jan  5 07:24:12.732: INFO: Created: latency-svc-gncmt
    Jan  5 07:24:12.744: INFO: Got endpoints: latency-svc-gncmt [609.344223ms]
    Jan  5 07:24:12.744: INFO: Created: latency-svc-dz9dw
    Jan  5 07:24:12.760: INFO: Got endpoints: latency-svc-dz9dw [601.57365ms]
    Jan  5 07:24:12.773: INFO: Created: latency-svc-67lgk
    Jan  5 07:24:12.781: INFO: Got endpoints: latency-svc-67lgk [612.67594ms]
    Jan  5 07:24:12.807: INFO: Created: latency-svc-kgqpr
    Jan  5 07:24:12.822: INFO: Got endpoints: latency-svc-kgqpr [605.433235ms]
    Jan  5 07:24:12.843: INFO: Created: latency-svc-jkwc8
    Jan  5 07:24:12.849: INFO: Got endpoints: latency-svc-jkwc8 [630.950552ms]
    Jan  5 07:24:12.855: INFO: Created: latency-svc-mb22x
    Jan  5 07:24:12.921: INFO: Created: latency-svc-rt8rl
    Jan  5 07:24:12.922: INFO: Got endpoints: latency-svc-mb22x [685.871362ms]
    Jan  5 07:24:12.941: INFO: Got endpoints: latency-svc-rt8rl [683.504133ms]
    Jan  5 07:24:12.953: INFO: Created: latency-svc-gkcbp
    Jan  5 07:24:12.975: INFO: Got endpoints: latency-svc-gkcbp [706.422708ms]
    Jan  5 07:24:12.985: INFO: Created: latency-svc-nwnm9
    Jan  5 07:24:12.987: INFO: Created: latency-svc-q4c9q
    Jan  5 07:24:12.992: INFO: Got endpoints: latency-svc-nwnm9 [704.131206ms]
    Jan  5 07:24:12.993: INFO: Got endpoints: latency-svc-q4c9q [956.349325ms]
    Jan  5 07:24:12.996: INFO: Created: latency-svc-vdrjs
    Jan  5 07:24:13.002: INFO: Got endpoints: latency-svc-vdrjs [592.070886ms]
    Jan  5 07:24:13.012: INFO: Created: latency-svc-9k7qv
    Jan  5 07:24:13.021: INFO: Got endpoints: latency-svc-9k7qv [449.274052ms]
    Jan  5 07:24:13.021: INFO: Created: latency-svc-lmb79
    Jan  5 07:24:13.028: INFO: Got endpoints: latency-svc-lmb79 [440.625879ms]
    Jan  5 07:24:13.114: INFO: Created: latency-svc-gkfd5
    Jan  5 07:24:13.137: INFO: Got endpoints: latency-svc-gkfd5 [426.937621ms]
    Jan  5 07:24:13.139: INFO: Created: latency-svc-hgvn5
    Jan  5 07:24:13.148: INFO: Got endpoints: latency-svc-hgvn5 [423.705543ms]
    Jan  5 07:24:13.176: INFO: Created: latency-svc-s9lfd
    Jan  5 07:24:13.204: INFO: Got endpoints: latency-svc-s9lfd [460.176347ms]
    Jan  5 07:24:13.204: INFO: Created: latency-svc-l9m4z
    Jan  5 07:24:13.231: INFO: Got endpoints: latency-svc-l9m4z [470.423566ms]
    Jan  5 07:24:13.235: INFO: Created: latency-svc-p7lcg
    Jan  5 07:24:13.242: INFO: Got endpoints: latency-svc-p7lcg [460.936659ms]
    Jan  5 07:24:13.252: INFO: Created: latency-svc-rrgrk
    Jan  5 07:24:13.265: INFO: Got endpoints: latency-svc-rrgrk [443.203446ms]
    Jan  5 07:24:13.267: INFO: Created: latency-svc-jntnq
    Jan  5 07:24:13.274: INFO: Got endpoints: latency-svc-jntnq [425.259945ms]
    Jan  5 07:24:13.282: INFO: Created: latency-svc-5nqsv
    Jan  5 07:24:13.290: INFO: Got endpoints: latency-svc-5nqsv [367.550347ms]
    Jan  5 07:24:13.301: INFO: Created: latency-svc-b9dvf
    Jan  5 07:24:13.311: INFO: Got endpoints: latency-svc-b9dvf [370.22622ms]
    Jan  5 07:24:13.313: INFO: Created: latency-svc-n4txv
    Jan  5 07:24:13.414: INFO: Created: latency-svc-s5npb
    Jan  5 07:24:13.420: INFO: Got endpoints: latency-svc-n4txv [445.507031ms]
    Jan  5 07:24:13.421: INFO: Got endpoints: latency-svc-s5npb [428.83409ms]
    Jan  5 07:24:13.433: INFO: Created: latency-svc-j7v5z
    Jan  5 07:24:13.444: INFO: Got endpoints: latency-svc-j7v5z [450.663713ms]
    Jan  5 07:24:13.452: INFO: Created: latency-svc-85sfv
    Jan  5 07:24:13.455: INFO: Got endpoints: latency-svc-85sfv [453.364493ms]
    Jan  5 07:24:13.464: INFO: Created: latency-svc-fp6rf
    Jan  5 07:24:13.473: INFO: Got endpoints: latency-svc-fp6rf [452.315531ms]
    Jan  5 07:24:13.481: INFO: Created: latency-svc-fd7rq
    Jan  5 07:24:13.487: INFO: Got endpoints: latency-svc-fd7rq [458.582135ms]
    Jan  5 07:24:13.507: INFO: Created: latency-svc-qkvtz
    Jan  5 07:24:13.510: INFO: Created: latency-svc-hjqk8
    Jan  5 07:24:13.518: INFO: Got endpoints: latency-svc-qkvtz [381.54232ms]
    Jan  5 07:24:13.519: INFO: Got endpoints: latency-svc-hjqk8 [371.193114ms]
    Jan  5 07:24:13.528: INFO: Created: latency-svc-2wkbl
    Jan  5 07:24:13.535: INFO: Got endpoints: latency-svc-2wkbl [331.326069ms]
    Jan  5 07:24:13.538: INFO: Created: latency-svc-v57ps
    Jan  5 07:24:13.544: INFO: Got endpoints: latency-svc-v57ps [313.021314ms]
    Jan  5 07:24:13.553: INFO: Created: latency-svc-2cqfp
    Jan  5 07:24:13.595: INFO: Got endpoints: latency-svc-2cqfp [352.997706ms]
    Jan  5 07:24:13.595: INFO: Created: latency-svc-ctwrl
    Jan  5 07:24:13.603: INFO: Got endpoints: latency-svc-ctwrl [337.458012ms]
    Jan  5 07:24:13.632: INFO: Created: latency-svc-vqjlh
    Jan  5 07:24:13.634: INFO: Got endpoints: latency-svc-vqjlh [360.062144ms]
    Jan  5 07:24:13.647: INFO: Created: latency-svc-nld4s
    Jan  5 07:24:13.654: INFO: Got endpoints: latency-svc-nld4s [364.306371ms]
    Jan  5 07:24:13.656: INFO: Created: latency-svc-tb6q9
    Jan  5 07:24:13.666: INFO: Got endpoints: latency-svc-tb6q9 [354.72753ms]
    Jan  5 07:24:13.692: INFO: Created: latency-svc-4lzd2
    Jan  5 07:24:13.695: INFO: Got endpoints: latency-svc-4lzd2 [274.041582ms]
    Jan  5 07:24:13.704: INFO: Created: latency-svc-ph4j4
    Jan  5 07:24:13.712: INFO: Got endpoints: latency-svc-ph4j4 [292.176763ms]
    Jan  5 07:24:13.718: INFO: Created: latency-svc-7ns5v
    Jan  5 07:24:13.725: INFO: Got endpoints: latency-svc-7ns5v [280.747962ms]
    Jan  5 07:24:13.726: INFO: Created: latency-svc-njhcm
    Jan  5 07:24:13.747: INFO: Got endpoints: latency-svc-njhcm [291.361714ms]
    Jan  5 07:24:13.749: INFO: Created: latency-svc-hcf9x
    Jan  5 07:24:13.758: INFO: Got endpoints: latency-svc-hcf9x [284.632056ms]
    Jan  5 07:24:13.764: INFO: Created: latency-svc-q55hv
    Jan  5 07:24:13.789: INFO: Got endpoints: latency-svc-q55hv [302.704169ms]
    Jan  5 07:24:13.800: INFO: Created: latency-svc-d5chd
    Jan  5 07:24:13.806: INFO: Got endpoints: latency-svc-d5chd [287.660495ms]
    Jan  5 07:24:13.808: INFO: Created: latency-svc-v86zq
    Jan  5 07:24:13.834: INFO: Got endpoints: latency-svc-v86zq [314.410843ms]
    Jan  5 07:24:13.840: INFO: Created: latency-svc-vgmmq
    Jan  5 07:24:13.847: INFO: Got endpoints: latency-svc-vgmmq [311.482458ms]
    Jan  5 07:24:13.858: INFO: Created: latency-svc-7skqz
    Jan  5 07:24:13.877: INFO: Got endpoints: latency-svc-7skqz [333.384722ms]
    Jan  5 07:24:13.879: INFO: Created: latency-svc-864c5
    Jan  5 07:24:13.890: INFO: Got endpoints: latency-svc-864c5 [294.997218ms]
    Jan  5 07:24:13.895: INFO: Created: latency-svc-n9fqt
    Jan  5 07:24:13.898: INFO: Got endpoints: latency-svc-n9fqt [295.732698ms]
    Jan  5 07:24:13.908: INFO: Created: latency-svc-zkv76
    Jan  5 07:24:13.919: INFO: Created: latency-svc-ng4bg
    Jan  5 07:24:13.927: INFO: Created: latency-svc-2wz2b
    Jan  5 07:24:13.936: INFO: Got endpoints: latency-svc-zkv76 [301.583564ms]
    Jan  5 07:24:13.944: INFO: Created: latency-svc-gzx8w
    Jan  5 07:24:13.953: INFO: Created: latency-svc-x46nx
    Jan  5 07:24:13.967: INFO: Created: latency-svc-c7gd9
    Jan  5 07:24:13.982: INFO: Created: latency-svc-p5pxq
    Jan  5 07:24:13.994: INFO: Got endpoints: latency-svc-ng4bg [339.445476ms]
    Jan  5 07:24:13.999: INFO: Created: latency-svc-phjn6
    Jan  5 07:24:14.008: INFO: Created: latency-svc-m8nhp
    Jan  5 07:24:14.015: INFO: Created: latency-svc-b7b7g
    Jan  5 07:24:14.024: INFO: Created: latency-svc-pr5jw
    Jan  5 07:24:14.037: INFO: Created: latency-svc-zbjfn
    Jan  5 07:24:14.039: INFO: Got endpoints: latency-svc-2wz2b [372.952373ms]
    Jan  5 07:24:14.047: INFO: Created: latency-svc-7c7lc
    Jan  5 07:24:14.056: INFO: Created: latency-svc-4hrr5
    Jan  5 07:24:14.065: INFO: Created: latency-svc-d8bfm
    Jan  5 07:24:14.079: INFO: Created: latency-svc-6x2zh
    Jan  5 07:24:14.090: INFO: Got endpoints: latency-svc-gzx8w [394.667541ms]
    Jan  5 07:24:14.098: INFO: Created: latency-svc-hlm22
    Jan  5 07:24:14.125: INFO: Created: latency-svc-bgz44
    Jan  5 07:24:14.133: INFO: Created: latency-svc-kzbtk
    Jan  5 07:24:14.277: INFO: Got endpoints: latency-svc-c7gd9 [551.947434ms]
    Jan  5 07:24:14.277: INFO: Got endpoints: latency-svc-p5pxq [530.123218ms]
    Jan  5 07:24:14.277: INFO: Got endpoints: latency-svc-x46nx [564.466632ms]
    Jan  5 07:24:14.304: INFO: Got endpoints: latency-svc-phjn6 [545.950639ms]
    Jan  5 07:24:14.334: INFO: Created: latency-svc-t6r8w
    Jan  5 07:24:14.355: INFO: Got endpoints: latency-svc-m8nhp [565.485359ms]
    Jan  5 07:24:14.363: INFO: Created: latency-svc-blzqn
    Jan  5 07:24:14.406: INFO: Created: latency-svc-zpdqf
    Jan  5 07:24:14.412: INFO: Got endpoints: latency-svc-b7b7g [605.683338ms]
    Jan  5 07:24:14.433: INFO: Created: latency-svc-2vlzw
    Jan  5 07:24:14.441: INFO: Got endpoints: latency-svc-pr5jw [607.570065ms]
    Jan  5 07:24:14.446: INFO: Created: latency-svc-vtg94
    Jan  5 07:24:14.460: INFO: Created: latency-svc-f5mw2
    Jan  5 07:24:14.472: INFO: Created: latency-svc-v5x2k
    Jan  5 07:24:14.496: INFO: Got endpoints: latency-svc-zbjfn [648.868827ms]
    Jan  5 07:24:14.508: INFO: Created: latency-svc-h9tvq
    Jan  5 07:24:14.542: INFO: Got endpoints: latency-svc-7c7lc [664.626334ms]
    Jan  5 07:24:14.554: INFO: Created: latency-svc-xjbwk
    Jan  5 07:24:14.587: INFO: Got endpoints: latency-svc-4hrr5 [697.156381ms]
    Jan  5 07:24:14.606: INFO: Created: latency-svc-w7nfh
    Jan  5 07:24:14.641: INFO: Got endpoints: latency-svc-d8bfm [742.347957ms]
    Jan  5 07:24:14.654: INFO: Created: latency-svc-89rtc
    Jan  5 07:24:14.691: INFO: Got endpoints: latency-svc-6x2zh [754.696653ms]
    Jan  5 07:24:14.734: INFO: Created: latency-svc-dvvwh
    Jan  5 07:24:14.736: INFO: Got endpoints: latency-svc-hlm22 [742.005877ms]
    Jan  5 07:24:14.771: INFO: Created: latency-svc-tg9rn
    Jan  5 07:24:14.786: INFO: Got endpoints: latency-svc-bgz44 [746.962713ms]
    Jan  5 07:24:14.814: INFO: Created: latency-svc-b8cht
    Jan  5 07:24:14.862: INFO: Got endpoints: latency-svc-kzbtk [772.057258ms]
    Jan  5 07:24:14.893: INFO: Got endpoints: latency-svc-t6r8w [616.226902ms]
    Jan  5 07:24:14.895: INFO: Created: latency-svc-jsmp4
    Jan  5 07:24:14.909: INFO: Created: latency-svc-njgpj
    Jan  5 07:24:14.950: INFO: Got endpoints: latency-svc-blzqn [673.334424ms]
    Jan  5 07:24:15.029: INFO: Created: latency-svc-grrr9
    Jan  5 07:24:15.030: INFO: Got endpoints: latency-svc-zpdqf [752.727425ms]
    Jan  5 07:24:15.041: INFO: Got endpoints: latency-svc-2vlzw [737.79868ms]
    Jan  5 07:24:15.048: INFO: Created: latency-svc-gvrnt
    Jan  5 07:24:15.061: INFO: Created: latency-svc-cfg9n
    Jan  5 07:24:15.092: INFO: Got endpoints: latency-svc-vtg94 [736.616942ms]
    Jan  5 07:24:15.104: INFO: Created: latency-svc-zlj77
    Jan  5 07:24:15.137: INFO: Got endpoints: latency-svc-f5mw2 [724.984399ms]
    Jan  5 07:24:15.153: INFO: Created: latency-svc-tfhcm
    Jan  5 07:24:15.192: INFO: Got endpoints: latency-svc-v5x2k [750.248018ms]
    Jan  5 07:24:15.205: INFO: Created: latency-svc-jlb88
    Jan  5 07:24:15.237: INFO: Got endpoints: latency-svc-h9tvq [741.175697ms]
    Jan  5 07:24:15.268: INFO: Created: latency-svc-22sxw
    Jan  5 07:24:15.301: INFO: Got endpoints: latency-svc-xjbwk [759.262615ms]
    Jan  5 07:24:15.317: INFO: Created: latency-svc-bjdgn
    Jan  5 07:24:15.345: INFO: Got endpoints: latency-svc-w7nfh [758.049119ms]
    Jan  5 07:24:15.358: INFO: Created: latency-svc-v944s
    Jan  5 07:24:15.391: INFO: Got endpoints: latency-svc-89rtc [750.04168ms]
    Jan  5 07:24:15.419: INFO: Created: latency-svc-wqlcg
    Jan  5 07:24:15.436: INFO: Got endpoints: latency-svc-dvvwh [745.373501ms]
    Jan  5 07:24:15.453: INFO: Created: latency-svc-dbpdd
    Jan  5 07:24:15.486: INFO: Got endpoints: latency-svc-tg9rn [750.328514ms]
    Jan  5 07:24:15.504: INFO: Created: latency-svc-9mwlv
    Jan  5 07:24:15.536: INFO: Got endpoints: latency-svc-b8cht [749.987222ms]
    Jan  5 07:24:15.552: INFO: Created: latency-svc-xkspx
    Jan  5 07:24:15.591: INFO: Got endpoints: latency-svc-jsmp4 [728.992632ms]
    Jan  5 07:24:15.603: INFO: Created: latency-svc-tdj8r
    Jan  5 07:24:15.641: INFO: Got endpoints: latency-svc-njgpj [748.1081ms]
    Jan  5 07:24:15.672: INFO: Created: latency-svc-t6s4t
    Jan  5 07:24:15.747: INFO: Got endpoints: latency-svc-gvrnt [717.575544ms]
    Jan  5 07:24:15.747: INFO: Got endpoints: latency-svc-grrr9 [796.930831ms]
    Jan  5 07:24:15.765: INFO: Created: latency-svc-tbmjq
    Jan  5 07:24:15.774: INFO: Created: latency-svc-jvrp8
    Jan  5 07:24:15.786: INFO: Got endpoints: latency-svc-cfg9n [744.275013ms]
    Jan  5 07:24:15.803: INFO: Created: latency-svc-6sjfr
    Jan  5 07:24:15.836: INFO: Got endpoints: latency-svc-zlj77 [744.139339ms]
    Jan  5 07:24:15.866: INFO: Created: latency-svc-9b58p
    Jan  5 07:24:15.886: INFO: Got endpoints: latency-svc-tfhcm [749.550435ms]
    Jan  5 07:24:15.907: INFO: Created: latency-svc-cdnc7
    Jan  5 07:24:15.941: INFO: Got endpoints: latency-svc-jlb88 [749.645048ms]
    Jan  5 07:24:15.955: INFO: Created: latency-svc-2fpc2
    Jan  5 07:24:15.991: INFO: Got endpoints: latency-svc-22sxw [754.45602ms]
    Jan  5 07:24:16.004: INFO: Created: latency-svc-ksp82
    Jan  5 07:24:16.037: INFO: Got endpoints: latency-svc-bjdgn [735.641788ms]
    Jan  5 07:24:16.050: INFO: Created: latency-svc-r9jzv
    Jan  5 07:24:16.086: INFO: Got endpoints: latency-svc-v944s [741.552873ms]
    Jan  5 07:24:16.100: INFO: Created: latency-svc-xx5ln
    Jan  5 07:24:16.141: INFO: Got endpoints: latency-svc-wqlcg [749.817748ms]
    Jan  5 07:24:16.155: INFO: Created: latency-svc-kgxm9
    Jan  5 07:24:16.191: INFO: Got endpoints: latency-svc-dbpdd [754.407461ms]
    Jan  5 07:24:16.206: INFO: Created: latency-svc-w4p2v
    Jan  5 07:24:16.237: INFO: Got endpoints: latency-svc-9mwlv [750.458869ms]
    Jan  5 07:24:16.250: INFO: Created: latency-svc-r6q7w
    Jan  5 07:24:16.298: INFO: Got endpoints: latency-svc-xkspx [761.823579ms]
    Jan  5 07:24:16.317: INFO: Created: latency-svc-t6rrz
    Jan  5 07:24:16.336: INFO: Got endpoints: latency-svc-tdj8r [745.659096ms]
    Jan  5 07:24:16.352: INFO: Created: latency-svc-vftjd
    Jan  5 07:24:16.392: INFO: Got endpoints: latency-svc-t6s4t [750.641329ms]
    Jan  5 07:24:16.408: INFO: Created: latency-svc-4drt6
    Jan  5 07:24:16.442: INFO: Got endpoints: latency-svc-tbmjq [694.375988ms]
    Jan  5 07:24:16.456: INFO: Created: latency-svc-4xxkf
    Jan  5 07:24:16.491: INFO: Got endpoints: latency-svc-jvrp8 [743.698589ms]
    Jan  5 07:24:16.506: INFO: Created: latency-svc-p9vtz
    Jan  5 07:24:16.537: INFO: Got endpoints: latency-svc-6sjfr [750.99097ms]
    Jan  5 07:24:16.568: INFO: Created: latency-svc-4rdd7
    Jan  5 07:24:16.592: INFO: Got endpoints: latency-svc-9b58p [756.288086ms]
    Jan  5 07:24:16.606: INFO: Created: latency-svc-fpfjq
    Jan  5 07:24:16.636: INFO: Got endpoints: latency-svc-cdnc7 [749.590314ms]
    Jan  5 07:24:16.653: INFO: Created: latency-svc-679xn
    Jan  5 07:24:16.690: INFO: Got endpoints: latency-svc-2fpc2 [748.826272ms]
    Jan  5 07:24:16.702: INFO: Created: latency-svc-4929z
    Jan  5 07:24:16.742: INFO: Got endpoints: latency-svc-ksp82 [750.294844ms]
    Jan  5 07:24:16.754: INFO: Created: latency-svc-rxxbh
    Jan  5 07:24:16.791: INFO: Got endpoints: latency-svc-r9jzv [754.089414ms]
    Jan  5 07:24:16.803: INFO: Created: latency-svc-bcg88
    Jan  5 07:24:16.884: INFO: Got endpoints: latency-svc-xx5ln [797.739418ms]
    Jan  5 07:24:16.886: INFO: Got endpoints: latency-svc-kgxm9 [745.347693ms]
    Jan  5 07:24:16.903: INFO: Created: latency-svc-vdgkz
    Jan  5 07:24:17.084: INFO: Got endpoints: latency-svc-w4p2v [893.062893ms]
    Jan  5 07:24:17.085: INFO: Got endpoints: latency-svc-t6rrz [787.556324ms]
    Jan  5 07:24:17.085: INFO: Got endpoints: latency-svc-r6q7w [848.767727ms]
    Jan  5 07:24:17.086: INFO: Created: latency-svc-hx5xh
    Jan  5 07:24:17.092: INFO: Got endpoints: latency-svc-vftjd [755.005791ms]
    Jan  5 07:24:17.104: INFO: Created: latency-svc-j4tbc
    Jan  5 07:24:17.113: INFO: Created: latency-svc-69tws
    Jan  5 07:24:17.121: INFO: Created: latency-svc-478nr
    Jan  5 07:24:17.130: INFO: Created: latency-svc-4frwk
    Jan  5 07:24:17.136: INFO: Got endpoints: latency-svc-4drt6 [743.893581ms]
    Jan  5 07:24:17.152: INFO: Created: latency-svc-74bfw
    Jan  5 07:24:17.199: INFO: Got endpoints: latency-svc-4xxkf [757.452663ms]
    Jan  5 07:24:17.212: INFO: Created: latency-svc-pwlmh
    Jan  5 07:24:17.235: INFO: Got endpoints: latency-svc-p9vtz [744.426503ms]
    Jan  5 07:24:17.252: INFO: Created: latency-svc-mp5r7
    Jan  5 07:24:17.292: INFO: Got endpoints: latency-svc-4rdd7 [754.703235ms]
    Jan  5 07:24:17.311: INFO: Created: latency-svc-qg2cb
    Jan  5 07:24:17.336: INFO: Got endpoints: latency-svc-fpfjq [743.347481ms]
    Jan  5 07:24:17.352: INFO: Created: latency-svc-zlzds
    Jan  5 07:24:17.387: INFO: Got endpoints: latency-svc-679xn [750.408588ms]
    Jan  5 07:24:17.403: INFO: Created: latency-svc-rwsmg
    Jan  5 07:24:17.436: INFO: Got endpoints: latency-svc-4929z [745.448032ms]
    Jan  5 07:24:17.452: INFO: Created: latency-svc-j6tt2
    Jan  5 07:24:17.486: INFO: Got endpoints: latency-svc-rxxbh [744.602882ms]
    Jan  5 07:24:17.504: INFO: Created: latency-svc-rp7fh
    Jan  5 07:24:17.535: INFO: Got endpoints: latency-svc-bcg88 [744.503663ms]
    Jan  5 07:24:17.555: INFO: Created: latency-svc-7dwch
    Jan  5 07:24:17.586: INFO: Got endpoints: latency-svc-vdgkz [701.523746ms]
    Jan  5 07:24:17.605: INFO: Created: latency-svc-bc74m
    Jan  5 07:24:17.642: INFO: Got endpoints: latency-svc-hx5xh [755.442134ms]
    Jan  5 07:24:17.654: INFO: Created: latency-svc-nchqs
    Jan  5 07:24:17.690: INFO: Got endpoints: latency-svc-j4tbc [605.750924ms]
    Jan  5 07:24:17.702: INFO: Created: latency-svc-kk62d
    Jan  5 07:24:17.754: INFO: Got endpoints: latency-svc-69tws [668.814482ms]
    Jan  5 07:24:17.769: INFO: Created: latency-svc-7vgk8
    Jan  5 07:24:17.786: INFO: Got endpoints: latency-svc-478nr [700.505088ms]
    Jan  5 07:24:17.803: INFO: Created: latency-svc-6n5qn
    Jan  5 07:24:17.837: INFO: Got endpoints: latency-svc-4frwk [745.233576ms]
    Jan  5 07:24:17.848: INFO: Created: latency-svc-9zq2b
    Jan  5 07:24:17.885: INFO: Got endpoints: latency-svc-74bfw [749.461208ms]
    Jan  5 07:24:17.902: INFO: Created: latency-svc-9j4jp
    Jan  5 07:24:17.936: INFO: Got endpoints: latency-svc-pwlmh [736.430148ms]
    Jan  5 07:24:17.955: INFO: Created: latency-svc-z6rfh
    Jan  5 07:24:17.986: INFO: Got endpoints: latency-svc-mp5r7 [750.824238ms]
    Jan  5 07:24:17.998: INFO: Created: latency-svc-phz4b
    Jan  5 07:24:18.042: INFO: Got endpoints: latency-svc-qg2cb [750.058052ms]
    Jan  5 07:24:18.057: INFO: Created: latency-svc-k8vgm
    Jan  5 07:24:18.095: INFO: Got endpoints: latency-svc-zlzds [759.474756ms]
    Jan  5 07:24:18.109: INFO: Created: latency-svc-cm79z
    Jan  5 07:24:18.140: INFO: Got endpoints: latency-svc-rwsmg [753.558738ms]
    Jan  5 07:24:18.153: INFO: Created: latency-svc-qpgcm
    Jan  5 07:24:18.199: INFO: Got endpoints: latency-svc-j6tt2 [763.199115ms]
    Jan  5 07:24:18.211: INFO: Created: latency-svc-jgzxc
    Jan  5 07:24:18.235: INFO: Got endpoints: latency-svc-rp7fh [748.999862ms]
    Jan  5 07:24:18.254: INFO: Created: latency-svc-7b788
    Jan  5 07:24:18.285: INFO: Got endpoints: latency-svc-7dwch [749.816839ms]
    Jan  5 07:24:18.316: INFO: Created: latency-svc-jw2kb
    Jan  5 07:24:18.336: INFO: Got endpoints: latency-svc-bc74m [750.267533ms]
    Jan  5 07:24:18.355: INFO: Created: latency-svc-zg6k4
    Jan  5 07:24:18.386: INFO: Got endpoints: latency-svc-nchqs [744.517932ms]
    Jan  5 07:24:18.398: INFO: Created: latency-svc-6z2j6
    Jan  5 07:24:18.436: INFO: Got endpoints: latency-svc-kk62d [746.628017ms]
    Jan  5 07:24:18.451: INFO: Created: latency-svc-mq9m7
    Jan  5 07:24:18.492: INFO: Got endpoints: latency-svc-7vgk8 [738.232342ms]
    Jan  5 07:24:18.523: INFO: Created: latency-svc-tmn5v
    Jan  5 07:24:18.565: INFO: Got endpoints: latency-svc-6n5qn [778.851959ms]
    Jan  5 07:24:18.622: INFO: Created: latency-svc-k96sw
    Jan  5 07:24:18.622: INFO: Got endpoints: latency-svc-9zq2b [785.568469ms]
    Jan  5 07:24:18.653: INFO: Got endpoints: latency-svc-9j4jp [767.505082ms]
    Jan  5 07:24:18.654: INFO: Created: latency-svc-hfs4h
    Jan  5 07:24:18.671: INFO: Created: latency-svc-8fgdn
    Jan  5 07:24:18.690: INFO: Got endpoints: latency-svc-z6rfh [754.743849ms]
    Jan  5 07:24:18.703: INFO: Created: latency-svc-cmd92
    Jan  5 07:24:18.735: INFO: Got endpoints: latency-svc-phz4b [749.108529ms]
    Jan  5 07:24:18.767: INFO: Created: latency-svc-fbjgv
    Jan  5 07:24:18.795: INFO: Got endpoints: latency-svc-k8vgm [753.149843ms]
    Jan  5 07:24:18.807: INFO: Created: latency-svc-9nhpw
    Jan  5 07:24:18.842: INFO: Got endpoints: latency-svc-cm79z [746.943718ms]
    Jan  5 07:24:18.855: INFO: Created: latency-svc-rrzb8
    Jan  5 07:24:18.886: INFO: Got endpoints: latency-svc-qpgcm [746.249239ms]
    Jan  5 07:24:18.905: INFO: Created: latency-svc-xxqt2
    Jan  5 07:24:18.942: INFO: Got endpoints: latency-svc-jgzxc [743.304266ms]
    Jan  5 07:24:18.954: INFO: Created: latency-svc-5thsp
    Jan  5 07:24:18.990: INFO: Got endpoints: latency-svc-7b788 [755.03841ms]
    Jan  5 07:24:19.003: INFO: Created: latency-svc-q52pr
    Jan  5 07:24:19.037: INFO: Got endpoints: latency-svc-jw2kb [751.317305ms]
    Jan  5 07:24:19.050: INFO: Created: latency-svc-9brt5
    Jan  5 07:24:19.091: INFO: Got endpoints: latency-svc-zg6k4 [754.889097ms]
    Jan  5 07:24:19.106: INFO: Created: latency-svc-lvd96
    Jan  5 07:24:19.143: INFO: Got endpoints: latency-svc-6z2j6 [756.705474ms]
    Jan  5 07:24:19.155: INFO: Created: latency-svc-vt4wg
    Jan  5 07:24:19.191: INFO: Got endpoints: latency-svc-mq9m7 [754.926792ms]
    Jan  5 07:24:19.203: INFO: Created: latency-svc-rglg5
    Jan  5 07:24:19.241: INFO: Got endpoints: latency-svc-tmn5v [748.299433ms]
    Jan  5 07:24:19.253: INFO: Created: latency-svc-xb2lt
    Jan  5 07:24:19.291: INFO: Got endpoints: latency-svc-k96sw [725.874963ms]
    Jan  5 07:24:19.303: INFO: Created: latency-svc-hhxl9
    Jan  5 07:24:19.345: INFO: Got endpoints: latency-svc-hfs4h [723.071151ms]
    Jan  5 07:24:19.359: INFO: Created: latency-svc-2mlb8
    Jan  5 07:24:19.392: INFO: Got endpoints: latency-svc-8fgdn [739.158685ms]
    Jan  5 07:24:19.411: INFO: Created: latency-svc-d8l9b
    Jan  5 07:24:19.441: INFO: Got endpoints: latency-svc-cmd92 [750.4799ms]
    Jan  5 07:24:19.461: INFO: Created: latency-svc-dptq6
    Jan  5 07:24:19.485: INFO: Got endpoints: latency-svc-fbjgv [750.006639ms]
    Jan  5 07:24:19.502: INFO: Created: latency-svc-vn5k7
    Jan  5 07:24:19.542: INFO: Got endpoints: latency-svc-9nhpw [746.892095ms]
    Jan  5 07:24:19.566: INFO: Created: latency-svc-6h6xr
    Jan  5 07:24:19.590: INFO: Got endpoints: latency-svc-rrzb8 [748.431297ms]
    Jan  5 07:24:19.608: INFO: Created: latency-svc-mr6m5
    Jan  5 07:24:19.642: INFO: Got endpoints: latency-svc-xxqt2 [755.572717ms]
    Jan  5 07:24:19.658: INFO: Created: latency-svc-9rndj
    Jan  5 07:24:19.692: INFO: Got endpoints: latency-svc-5thsp [750.052195ms]
    Jan  5 07:24:19.718: INFO: Created: latency-svc-n7w9j
    Jan  5 07:24:19.743: INFO: Got endpoints: latency-svc-q52pr [752.201021ms]
    Jan  5 07:24:19.759: INFO: Created: latency-svc-2gpts
    Jan  5 07:24:19.792: INFO: Got endpoints: latency-svc-9brt5 [755.339052ms]
    Jan  5 07:24:19.808: INFO: Created: latency-svc-58rb4
    Jan  5 07:24:19.836: INFO: Got endpoints: latency-svc-lvd96 [745.20239ms]
    Jan  5 07:24:19.869: INFO: Created: latency-svc-9w7x8
    Jan  5 07:24:19.911: INFO: Got endpoints: latency-svc-vt4wg [768.439214ms]
    Jan  5 07:24:19.937: INFO: Got endpoints: latency-svc-rglg5 [745.258387ms]
    Jan  5 07:24:19.987: INFO: Got endpoints: latency-svc-xb2lt [746.025217ms]
    Jan  5 07:24:20.036: INFO: Got endpoints: latency-svc-hhxl9 [745.580837ms]
    Jan  5 07:24:20.092: INFO: Got endpoints: latency-svc-2mlb8 [746.59441ms]
    Jan  5 07:24:20.142: INFO: Got endpoints: latency-svc-d8l9b [750.000704ms]
    Jan  5 07:24:20.186: INFO: Got endpoints: latency-svc-dptq6 [745.267322ms]
    Jan  5 07:24:20.311: INFO: Got endpoints: latency-svc-vn5k7 [825.544397ms]
    Jan  5 07:24:20.313: INFO: Got endpoints: latency-svc-6h6xr [771.101344ms]
    Jan  5 07:24:20.348: INFO: Got endpoints: latency-svc-mr6m5 [757.798686ms]
    Jan  5 07:24:20.391: INFO: Got endpoints: latency-svc-9rndj [749.119914ms]
    Jan  5 07:24:20.441: INFO: Got endpoints: latency-svc-n7w9j [748.602724ms]
    Jan  5 07:24:20.486: INFO: Got endpoints: latency-svc-2gpts [743.654224ms]
    Jan  5 07:24:20.536: INFO: Got endpoints: latency-svc-58rb4 [744.215923ms]
    Jan  5 07:24:20.592: INFO: Got endpoints: latency-svc-9w7x8 [755.333535ms]
    Jan  5 07:24:20.592: INFO: Latencies: [41.968191ms 61.83692ms 73.981238ms 97.33964ms 121.67512ms 130.849586ms 179.530886ms 181.189397ms 199.394113ms 220.383824ms 231.372425ms 250.774109ms 274.041582ms 280.747962ms 284.632056ms 287.660495ms 291.361714ms 292.176763ms 294.997218ms 295.732698ms 301.583564ms 302.704169ms 311.482458ms 313.021314ms 314.410843ms 331.326069ms 333.384722ms 337.458012ms 339.445476ms 352.997706ms 354.72753ms 360.062144ms 364.306371ms 367.550347ms 370.22622ms 371.193114ms 372.84394ms 372.952373ms 381.54232ms 394.667541ms 423.705543ms 425.259945ms 426.937621ms 428.83409ms 440.625879ms 443.203446ms 445.507031ms 449.274052ms 450.663713ms 452.315531ms 453.364493ms 458.582135ms 460.176347ms 460.936659ms 470.423566ms 508.65862ms 530.123218ms 534.296932ms 545.950639ms 551.947434ms 564.466632ms 565.485359ms 592.070886ms 601.57365ms 605.433235ms 605.683338ms 605.750924ms 607.570065ms 609.344223ms 611.184015ms 612.67594ms 613.284603ms 616.226902ms 630.950552ms 648.868827ms 664.626334ms 668.814482ms 673.334424ms 683.504133ms 685.871362ms 694.375988ms 697.156381ms 700.505088ms 701.523746ms 704.131206ms 706.422708ms 717.575544ms 723.071151ms 724.984399ms 725.874963ms 728.992632ms 735.641788ms 736.430148ms 736.616942ms 737.79868ms 738.232342ms 739.158685ms 741.175697ms 741.552873ms 742.005877ms 742.347957ms 743.304266ms 743.347481ms 743.654224ms 743.698589ms 743.893581ms 744.139339ms 744.215923ms 744.275013ms 744.426503ms 744.503663ms 744.517932ms 744.602882ms 745.20239ms 745.233576ms 745.258387ms 745.267322ms 745.347693ms 745.373501ms 745.448032ms 745.580837ms 745.659096ms 746.025217ms 746.249239ms 746.59441ms 746.628017ms 746.892095ms 746.943718ms 746.962713ms 748.1081ms 748.299433ms 748.431297ms 748.602724ms 748.826272ms 748.999862ms 749.108529ms 749.119914ms 749.461208ms 749.550435ms 749.590314ms 749.645048ms 749.816839ms 749.817748ms 749.987222ms 750.000704ms 750.006639ms 750.04168ms 750.052195ms 750.058052ms 750.248018ms 750.267533ms 750.294844ms 750.328514ms 750.408588ms 750.458869ms 750.4799ms 750.641329ms 750.824238ms 750.99097ms 751.317305ms 752.201021ms 752.727425ms 753.149843ms 753.558738ms 754.089414ms 754.407461ms 754.45602ms 754.696653ms 754.703235ms 754.743849ms 754.889097ms 754.926792ms 755.005791ms 755.03841ms 755.333535ms 755.339052ms 755.442134ms 755.572717ms 756.288086ms 756.705474ms 757.452663ms 757.798686ms 758.049119ms 759.262615ms 759.474756ms 761.823579ms 763.199115ms 767.505082ms 768.439214ms 771.101344ms 772.057258ms 778.851959ms 785.568469ms 787.556324ms 796.930831ms 797.739418ms 825.544397ms 848.767727ms 893.062893ms 956.349325ms]
    Jan  5 07:24:20.592: INFO: 50 %ile: 742.347957ms
    Jan  5 07:24:20.592: INFO: 90 %ile: 757.452663ms
    Jan  5 07:24:20.592: INFO: 99 %ile: 893.062893ms
    Jan  5 07:24:20.592: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan  5 07:24:20.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-6538" for this suite. 01/05/23 07:24:20.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:24:20.599
Jan  5 07:24:20.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 07:24:20.6
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:24:20.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:24:20.613
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-2294 01/05/23 07:24:20.617
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2294 to expose endpoints map[] 01/05/23 07:24:20.629
Jan  5 07:24:20.631: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan  5 07:24:21.636: INFO: successfully validated that service endpoint-test2 in namespace services-2294 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2294 01/05/23 07:24:21.636
Jan  5 07:24:21.640: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2294" to be "running and ready"
Jan  5 07:24:21.642: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015065ms
Jan  5 07:24:21.642: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:24:23.645: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005339459s
Jan  5 07:24:23.645: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  5 07:24:23.646: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2294 to expose endpoints map[pod1:[80]] 01/05/23 07:24:23.647
Jan  5 07:24:23.651: INFO: successfully validated that service endpoint-test2 in namespace services-2294 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/05/23 07:24:23.652
Jan  5 07:24:23.652: INFO: Creating new exec pod
Jan  5 07:24:23.655: INFO: Waiting up to 5m0s for pod "execpod9bthm" in namespace "services-2294" to be "running"
Jan  5 07:24:23.656: INFO: Pod "execpod9bthm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.412635ms
Jan  5 07:24:25.658: INFO: Pod "execpod9bthm": Phase="Running", Reason="", readiness=true. Elapsed: 2.003200184s
Jan  5 07:24:25.658: INFO: Pod "execpod9bthm" satisfied condition "running"
Jan  5 07:24:26.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan  5 07:24:26.770: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  5 07:24:26.770: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 07:24:26.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.201.86 80'
Jan  5 07:24:26.966: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.201.86 80\nConnection to 10.108.201.86 80 port [tcp/http] succeeded!\n"
Jan  5 07:24:26.966: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-2294 01/05/23 07:24:26.966
Jan  5 07:24:26.970: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2294" to be "running and ready"
Jan  5 07:24:26.978: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.733684ms
Jan  5 07:24:26.978: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:24:28.984: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013766209s
Jan  5 07:24:28.984: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  5 07:24:28.984: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2294 to expose endpoints map[pod1:[80] pod2:[80]] 01/05/23 07:24:28.993
Jan  5 07:24:33.006: INFO: Unexpected endpoints: found map[0e6bb1ca-3b24-4388-97e6-a4379bb7258a:[80]], expected map[pod1:[80] pod2:[80]], will retry
Jan  5 07:24:38.009: INFO: successfully validated that service endpoint-test2 in namespace services-2294 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/05/23 07:24:38.009
Jan  5 07:24:39.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan  5 07:24:39.212: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  5 07:24:39.212: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 07:24:39.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.201.86 80'
Jan  5 07:24:39.323: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.201.86 80\nConnection to 10.108.201.86 80 port [tcp/http] succeeded!\n"
Jan  5 07:24:39.323: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2294 01/05/23 07:24:39.323
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2294 to expose endpoints map[pod2:[80]] 01/05/23 07:24:39.347
Jan  5 07:24:39.353: INFO: successfully validated that service endpoint-test2 in namespace services-2294 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/05/23 07:24:39.353
Jan  5 07:24:40.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan  5 07:24:40.478: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  5 07:24:40.478: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 07:24:40.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.201.86 80'
Jan  5 07:24:40.608: INFO: stderr: "+ + nc -v -t -w 2 10.108.201.86 80echo\n hostName\nConnection to 10.108.201.86 80 port [tcp/http] succeeded!\n"
Jan  5 07:24:40.608: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-2294 01/05/23 07:24:40.608
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2294 to expose endpoints map[] 01/05/23 07:24:40.622
Jan  5 07:24:40.632: INFO: successfully validated that service endpoint-test2 in namespace services-2294 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 07:24:40.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2294" for this suite. 01/05/23 07:24:40.662
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":16,"skipped":353,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.068 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:24:20.599
    Jan  5 07:24:20.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 07:24:20.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:24:20.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:24:20.613
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-2294 01/05/23 07:24:20.617
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2294 to expose endpoints map[] 01/05/23 07:24:20.629
    Jan  5 07:24:20.631: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan  5 07:24:21.636: INFO: successfully validated that service endpoint-test2 in namespace services-2294 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2294 01/05/23 07:24:21.636
    Jan  5 07:24:21.640: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2294" to be "running and ready"
    Jan  5 07:24:21.642: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015065ms
    Jan  5 07:24:21.642: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:24:23.645: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005339459s
    Jan  5 07:24:23.645: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  5 07:24:23.646: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2294 to expose endpoints map[pod1:[80]] 01/05/23 07:24:23.647
    Jan  5 07:24:23.651: INFO: successfully validated that service endpoint-test2 in namespace services-2294 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/05/23 07:24:23.652
    Jan  5 07:24:23.652: INFO: Creating new exec pod
    Jan  5 07:24:23.655: INFO: Waiting up to 5m0s for pod "execpod9bthm" in namespace "services-2294" to be "running"
    Jan  5 07:24:23.656: INFO: Pod "execpod9bthm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.412635ms
    Jan  5 07:24:25.658: INFO: Pod "execpod9bthm": Phase="Running", Reason="", readiness=true. Elapsed: 2.003200184s
    Jan  5 07:24:25.658: INFO: Pod "execpod9bthm" satisfied condition "running"
    Jan  5 07:24:26.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan  5 07:24:26.770: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  5 07:24:26.770: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 07:24:26.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.201.86 80'
    Jan  5 07:24:26.966: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.201.86 80\nConnection to 10.108.201.86 80 port [tcp/http] succeeded!\n"
    Jan  5 07:24:26.966: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-2294 01/05/23 07:24:26.966
    Jan  5 07:24:26.970: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2294" to be "running and ready"
    Jan  5 07:24:26.978: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.733684ms
    Jan  5 07:24:26.978: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:24:28.984: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013766209s
    Jan  5 07:24:28.984: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  5 07:24:28.984: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2294 to expose endpoints map[pod1:[80] pod2:[80]] 01/05/23 07:24:28.993
    Jan  5 07:24:33.006: INFO: Unexpected endpoints: found map[0e6bb1ca-3b24-4388-97e6-a4379bb7258a:[80]], expected map[pod1:[80] pod2:[80]], will retry
    Jan  5 07:24:38.009: INFO: successfully validated that service endpoint-test2 in namespace services-2294 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/05/23 07:24:38.009
    Jan  5 07:24:39.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan  5 07:24:39.212: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  5 07:24:39.212: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 07:24:39.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.201.86 80'
    Jan  5 07:24:39.323: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.201.86 80\nConnection to 10.108.201.86 80 port [tcp/http] succeeded!\n"
    Jan  5 07:24:39.323: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-2294 01/05/23 07:24:39.323
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2294 to expose endpoints map[pod2:[80]] 01/05/23 07:24:39.347
    Jan  5 07:24:39.353: INFO: successfully validated that service endpoint-test2 in namespace services-2294 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/05/23 07:24:39.353
    Jan  5 07:24:40.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan  5 07:24:40.478: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  5 07:24:40.478: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 07:24:40.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2294 exec execpod9bthm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.201.86 80'
    Jan  5 07:24:40.608: INFO: stderr: "+ + nc -v -t -w 2 10.108.201.86 80echo\n hostName\nConnection to 10.108.201.86 80 port [tcp/http] succeeded!\n"
    Jan  5 07:24:40.608: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-2294 01/05/23 07:24:40.608
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2294 to expose endpoints map[] 01/05/23 07:24:40.622
    Jan  5 07:24:40.632: INFO: successfully validated that service endpoint-test2 in namespace services-2294 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 07:24:40.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2294" for this suite. 01/05/23 07:24:40.662
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:24:40.669
Jan  5 07:24:40.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-runtime 01/05/23 07:24:40.67
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:24:40.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:24:40.688
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/05/23 07:24:40.7
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/05/23 07:24:58.761
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/05/23 07:24:58.764
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/05/23 07:24:58.768
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/05/23 07:24:58.768
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/05/23 07:24:58.818
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/05/23 07:25:01.83
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/05/23 07:25:03.838
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/05/23 07:25:03.841
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/05/23 07:25:03.841
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/05/23 07:25:03.869
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/05/23 07:25:04.875
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/05/23 07:25:07.885
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/05/23 07:25:07.888
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/05/23 07:25:07.888
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  5 07:25:07.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6609" for this suite. 01/05/23 07:25:07.915
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":17,"skipped":401,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.262 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:24:40.669
    Jan  5 07:24:40.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-runtime 01/05/23 07:24:40.67
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:24:40.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:24:40.688
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/05/23 07:24:40.7
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/05/23 07:24:58.761
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/05/23 07:24:58.764
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/05/23 07:24:58.768
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/05/23 07:24:58.768
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/05/23 07:24:58.818
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/05/23 07:25:01.83
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/05/23 07:25:03.838
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/05/23 07:25:03.841
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/05/23 07:25:03.841
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/05/23 07:25:03.869
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/05/23 07:25:04.875
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/05/23 07:25:07.885
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/05/23 07:25:07.888
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/05/23 07:25:07.888
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  5 07:25:07.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6609" for this suite. 01/05/23 07:25:07.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:25:07.932
Jan  5 07:25:07.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename cronjob 01/05/23 07:25:07.933
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:25:07.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:25:07.946
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/05/23 07:25:07.948
STEP: Ensuring more than one job is running at a time 01/05/23 07:25:07.961
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/05/23 07:27:01.965
STEP: Removing cronjob 01/05/23 07:27:01.968
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  5 07:27:01.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6887" for this suite. 01/05/23 07:27:01.975
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":18,"skipped":407,"failed":0}
------------------------------
â€¢ [SLOW TEST] [114.053 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:25:07.932
    Jan  5 07:25:07.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename cronjob 01/05/23 07:25:07.933
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:25:07.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:25:07.946
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/05/23 07:25:07.948
    STEP: Ensuring more than one job is running at a time 01/05/23 07:25:07.961
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/05/23 07:27:01.965
    STEP: Removing cronjob 01/05/23 07:27:01.968
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  5 07:27:01.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6887" for this suite. 01/05/23 07:27:01.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:27:01.986
Jan  5 07:27:01.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename namespaces 01/05/23 07:27:01.986
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:27:02.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:27:02.027
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/05/23 07:27:02.029
Jan  5 07:27:02.035: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/05/23 07:27:02.035
Jan  5 07:27:02.047: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/05/23 07:27:02.047
Jan  5 07:27:02.060: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  5 07:27:02.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6351" for this suite. 01/05/23 07:27:02.062
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":19,"skipped":424,"failed":0}
------------------------------
â€¢ [0.085 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:27:01.986
    Jan  5 07:27:01.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename namespaces 01/05/23 07:27:01.986
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:27:02.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:27:02.027
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/05/23 07:27:02.029
    Jan  5 07:27:02.035: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/05/23 07:27:02.035
    Jan  5 07:27:02.047: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/05/23 07:27:02.047
    Jan  5 07:27:02.060: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 07:27:02.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6351" for this suite. 01/05/23 07:27:02.062
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:27:02.071
Jan  5 07:27:02.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:27:02.072
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:27:02.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:27:02.086
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/05/23 07:27:02.093
Jan  5 07:27:02.100: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66" in namespace "projected-1486" to be "Succeeded or Failed"
Jan  5 07:27:02.102: INFO: Pod "downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66": Phase="Pending", Reason="", readiness=false. Elapsed: 1.860972ms
Jan  5 07:27:04.106: INFO: Pod "downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005856525s
Jan  5 07:27:06.106: INFO: Pod "downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005704695s
STEP: Saw pod success 01/05/23 07:27:06.106
Jan  5 07:27:06.106: INFO: Pod "downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66" satisfied condition "Succeeded or Failed"
Jan  5 07:27:06.108: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66 container client-container: <nil>
STEP: delete the pod 01/05/23 07:27:06.118
Jan  5 07:27:06.132: INFO: Waiting for pod downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66 to disappear
Jan  5 07:27:06.134: INFO: Pod downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 07:27:06.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1486" for this suite. 01/05/23 07:27:06.136
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":20,"skipped":425,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:27:02.071
    Jan  5 07:27:02.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:27:02.072
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:27:02.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:27:02.086
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/05/23 07:27:02.093
    Jan  5 07:27:02.100: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66" in namespace "projected-1486" to be "Succeeded or Failed"
    Jan  5 07:27:02.102: INFO: Pod "downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66": Phase="Pending", Reason="", readiness=false. Elapsed: 1.860972ms
    Jan  5 07:27:04.106: INFO: Pod "downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005856525s
    Jan  5 07:27:06.106: INFO: Pod "downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005704695s
    STEP: Saw pod success 01/05/23 07:27:06.106
    Jan  5 07:27:06.106: INFO: Pod "downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66" satisfied condition "Succeeded or Failed"
    Jan  5 07:27:06.108: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66 container client-container: <nil>
    STEP: delete the pod 01/05/23 07:27:06.118
    Jan  5 07:27:06.132: INFO: Waiting for pod downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66 to disappear
    Jan  5 07:27:06.134: INFO: Pod downwardapi-volume-1b1586ca-6637-4d20-847d-b3bdff3a7a66 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 07:27:06.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1486" for this suite. 01/05/23 07:27:06.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:27:06.141
Jan  5 07:27:06.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename watch 01/05/23 07:27:06.142
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:27:06.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:27:06.16
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/05/23 07:27:06.162
STEP: creating a new configmap 01/05/23 07:27:06.163
STEP: modifying the configmap once 01/05/23 07:27:06.166
STEP: closing the watch once it receives two notifications 01/05/23 07:27:06.175
Jan  5 07:27:06.175: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7590  d9c95bf9-8566-45c2-a6e2-7086cfa0612b 5330 0 2023-01-05 07:27:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 07:27:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 07:27:06.176: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7590  d9c95bf9-8566-45c2-a6e2-7086cfa0612b 5331 0 2023-01-05 07:27:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 07:27:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/05/23 07:27:06.176
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/05/23 07:27:06.182
STEP: deleting the configmap 01/05/23 07:27:06.183
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/05/23 07:27:06.196
Jan  5 07:27:06.196: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7590  d9c95bf9-8566-45c2-a6e2-7086cfa0612b 5332 0 2023-01-05 07:27:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 07:27:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 07:27:06.196: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7590  d9c95bf9-8566-45c2-a6e2-7086cfa0612b 5333 0 2023-01-05 07:27:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 07:27:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  5 07:27:06.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7590" for this suite. 01/05/23 07:27:06.199
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":21,"skipped":451,"failed":0}
------------------------------
â€¢ [0.061 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:27:06.141
    Jan  5 07:27:06.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename watch 01/05/23 07:27:06.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:27:06.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:27:06.16
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/05/23 07:27:06.162
    STEP: creating a new configmap 01/05/23 07:27:06.163
    STEP: modifying the configmap once 01/05/23 07:27:06.166
    STEP: closing the watch once it receives two notifications 01/05/23 07:27:06.175
    Jan  5 07:27:06.175: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7590  d9c95bf9-8566-45c2-a6e2-7086cfa0612b 5330 0 2023-01-05 07:27:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 07:27:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 07:27:06.176: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7590  d9c95bf9-8566-45c2-a6e2-7086cfa0612b 5331 0 2023-01-05 07:27:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 07:27:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/05/23 07:27:06.176
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/05/23 07:27:06.182
    STEP: deleting the configmap 01/05/23 07:27:06.183
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/05/23 07:27:06.196
    Jan  5 07:27:06.196: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7590  d9c95bf9-8566-45c2-a6e2-7086cfa0612b 5332 0 2023-01-05 07:27:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 07:27:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 07:27:06.196: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7590  d9c95bf9-8566-45c2-a6e2-7086cfa0612b 5333 0 2023-01-05 07:27:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 07:27:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  5 07:27:06.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7590" for this suite. 01/05/23 07:27:06.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:27:06.205
Jan  5 07:27:06.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename cronjob 01/05/23 07:27:06.206
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:27:06.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:27:06.228
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/05/23 07:27:06.23
STEP: Ensuring a job is scheduled 01/05/23 07:27:06.24
STEP: Ensuring exactly one is scheduled 01/05/23 07:28:00.244
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 07:28:00.246
STEP: Ensuring no more jobs are scheduled 01/05/23 07:28:00.247
STEP: Removing cronjob 01/05/23 07:33:00.253
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  5 07:33:00.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7129" for this suite. 01/05/23 07:33:00.26
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":22,"skipped":468,"failed":0}
------------------------------
â€¢ [SLOW TEST] [354.072 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:27:06.205
    Jan  5 07:27:06.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename cronjob 01/05/23 07:27:06.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:27:06.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:27:06.228
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/05/23 07:27:06.23
    STEP: Ensuring a job is scheduled 01/05/23 07:27:06.24
    STEP: Ensuring exactly one is scheduled 01/05/23 07:28:00.244
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 07:28:00.246
    STEP: Ensuring no more jobs are scheduled 01/05/23 07:28:00.247
    STEP: Removing cronjob 01/05/23 07:33:00.253
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  5 07:33:00.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7129" for this suite. 01/05/23 07:33:00.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:33:00.278
Jan  5 07:33:00.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename statefulset 01/05/23 07:33:00.279
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:33:00.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:33:00.308
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7526 01/05/23 07:33:00.311
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-7526 01/05/23 07:33:00.318
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7526 01/05/23 07:33:00.329
Jan  5 07:33:00.331: INFO: Found 0 stateful pods, waiting for 1
Jan  5 07:33:10.334: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/05/23 07:33:10.334
Jan  5 07:33:10.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 07:33:10.486: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 07:33:10.486: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 07:33:10.486: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 07:33:10.489: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  5 07:33:20.493: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 07:33:20.493: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 07:33:20.505: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jan  5 07:33:20.505: INFO: ss-0  mip-bd-vm724.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  }]
Jan  5 07:33:20.505: INFO: 
Jan  5 07:33:20.505: INFO: StatefulSet ss has not reached scale 3, at 1
Jan  5 07:33:21.510: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9975451s
Jan  5 07:33:22.513: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992807177s
Jan  5 07:33:23.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989089619s
Jan  5 07:33:24.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985420606s
Jan  5 07:33:25.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982036063s
Jan  5 07:33:26.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978787361s
Jan  5 07:33:27.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.975489114s
Jan  5 07:33:28.534: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.971839949s
Jan  5 07:33:29.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 968.315459ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7526 01/05/23 07:33:30.537
Jan  5 07:33:30.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 07:33:30.658: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 07:33:30.658: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 07:33:30.658: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 07:33:30.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 07:33:30.785: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  5 07:33:30.785: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 07:33:30.785: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 07:33:30.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 07:33:30.907: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  5 07:33:30.907: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 07:33:30.907: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 07:33:30.910: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 07:33:30.910: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 07:33:30.910: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/05/23 07:33:30.91
Jan  5 07:33:30.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 07:33:31.039: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 07:33:31.039: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 07:33:31.039: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 07:33:31.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 07:33:31.165: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 07:33:31.165: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 07:33:31.165: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 07:33:31.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 07:33:31.307: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 07:33:31.307: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 07:33:31.307: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 07:33:31.307: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 07:33:31.310: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan  5 07:33:41.318: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 07:33:41.318: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 07:33:41.318: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 07:33:41.329: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jan  5 07:33:41.329: INFO: ss-0  mip-bd-vm724.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  }]
Jan  5 07:33:41.329: INFO: ss-1  mip-bd-vm722.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  }]
Jan  5 07:33:41.329: INFO: ss-2  mip-bd-vm724.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  }]
Jan  5 07:33:41.329: INFO: 
Jan  5 07:33:41.329: INFO: StatefulSet ss has not reached scale 0, at 3
Jan  5 07:33:42.333: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jan  5 07:33:42.333: INFO: ss-0  mip-bd-vm724.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  }]
Jan  5 07:33:42.333: INFO: ss-2  mip-bd-vm724.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  }]
Jan  5 07:33:42.333: INFO: 
Jan  5 07:33:42.333: INFO: StatefulSet ss has not reached scale 0, at 2
Jan  5 07:33:43.335: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993239627s
Jan  5 07:33:44.338: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.990920478s
Jan  5 07:33:45.342: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98763394s
Jan  5 07:33:46.344: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.984477944s
Jan  5 07:33:47.347: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.981699245s
Jan  5 07:33:48.350: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.979074442s
Jan  5 07:33:49.353: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.976202179s
Jan  5 07:33:50.356: INFO: Verifying statefulset ss doesn't scale past 0 for another 972.99166ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7526 01/05/23 07:33:51.356
Jan  5 07:33:51.359: INFO: Scaling statefulset ss to 0
Jan  5 07:33:51.366: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 07:33:51.369: INFO: Deleting all statefulset in ns statefulset-7526
Jan  5 07:33:51.371: INFO: Scaling statefulset ss to 0
Jan  5 07:33:51.378: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 07:33:51.379: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 07:33:51.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7526" for this suite. 01/05/23 07:33:51.388
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":23,"skipped":499,"failed":0}
------------------------------
â€¢ [SLOW TEST] [51.120 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:33:00.278
    Jan  5 07:33:00.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename statefulset 01/05/23 07:33:00.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:33:00.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:33:00.308
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7526 01/05/23 07:33:00.311
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-7526 01/05/23 07:33:00.318
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7526 01/05/23 07:33:00.329
    Jan  5 07:33:00.331: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 07:33:10.334: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/05/23 07:33:10.334
    Jan  5 07:33:10.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 07:33:10.486: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 07:33:10.486: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 07:33:10.486: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 07:33:10.489: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  5 07:33:20.493: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 07:33:20.493: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 07:33:20.505: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
    Jan  5 07:33:20.505: INFO: ss-0  mip-bd-vm724.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  }]
    Jan  5 07:33:20.505: INFO: 
    Jan  5 07:33:20.505: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan  5 07:33:21.510: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9975451s
    Jan  5 07:33:22.513: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992807177s
    Jan  5 07:33:23.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989089619s
    Jan  5 07:33:24.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985420606s
    Jan  5 07:33:25.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982036063s
    Jan  5 07:33:26.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978787361s
    Jan  5 07:33:27.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.975489114s
    Jan  5 07:33:28.534: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.971839949s
    Jan  5 07:33:29.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 968.315459ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7526 01/05/23 07:33:30.537
    Jan  5 07:33:30.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 07:33:30.658: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 07:33:30.658: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 07:33:30.658: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 07:33:30.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 07:33:30.785: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  5 07:33:30.785: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 07:33:30.785: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 07:33:30.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 07:33:30.907: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  5 07:33:30.907: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 07:33:30.907: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 07:33:30.910: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 07:33:30.910: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 07:33:30.910: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/05/23 07:33:30.91
    Jan  5 07:33:30.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 07:33:31.039: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 07:33:31.039: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 07:33:31.039: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 07:33:31.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 07:33:31.165: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 07:33:31.165: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 07:33:31.165: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 07:33:31.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-7526 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 07:33:31.307: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 07:33:31.307: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 07:33:31.307: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 07:33:31.307: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 07:33:31.310: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan  5 07:33:41.318: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 07:33:41.318: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 07:33:41.318: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 07:33:41.329: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
    Jan  5 07:33:41.329: INFO: ss-0  mip-bd-vm724.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  }]
    Jan  5 07:33:41.329: INFO: ss-1  mip-bd-vm722.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  }]
    Jan  5 07:33:41.329: INFO: ss-2  mip-bd-vm724.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  }]
    Jan  5 07:33:41.329: INFO: 
    Jan  5 07:33:41.329: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan  5 07:33:42.333: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
    Jan  5 07:33:42.333: INFO: ss-0  mip-bd-vm724.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:00 +0000 UTC  }]
    Jan  5 07:33:42.333: INFO: ss-2  mip-bd-vm724.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:33:20 +0000 UTC  }]
    Jan  5 07:33:42.333: INFO: 
    Jan  5 07:33:42.333: INFO: StatefulSet ss has not reached scale 0, at 2
    Jan  5 07:33:43.335: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993239627s
    Jan  5 07:33:44.338: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.990920478s
    Jan  5 07:33:45.342: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98763394s
    Jan  5 07:33:46.344: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.984477944s
    Jan  5 07:33:47.347: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.981699245s
    Jan  5 07:33:48.350: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.979074442s
    Jan  5 07:33:49.353: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.976202179s
    Jan  5 07:33:50.356: INFO: Verifying statefulset ss doesn't scale past 0 for another 972.99166ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7526 01/05/23 07:33:51.356
    Jan  5 07:33:51.359: INFO: Scaling statefulset ss to 0
    Jan  5 07:33:51.366: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 07:33:51.369: INFO: Deleting all statefulset in ns statefulset-7526
    Jan  5 07:33:51.371: INFO: Scaling statefulset ss to 0
    Jan  5 07:33:51.378: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 07:33:51.379: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 07:33:51.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7526" for this suite. 01/05/23 07:33:51.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:33:51.398
Jan  5 07:33:51.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 07:33:51.399
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:33:51.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:33:51.414
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8640 01/05/23 07:33:51.421
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 07:33:51.436
STEP: creating service externalsvc in namespace services-8640 01/05/23 07:33:51.436
STEP: creating replication controller externalsvc in namespace services-8640 01/05/23 07:33:51.457
I0105 07:33:51.472877      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8640, replica count: 2
I0105 07:33:54.523896      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/05/23 07:33:54.526
Jan  5 07:33:54.548: INFO: Creating new exec pod
Jan  5 07:33:54.560: INFO: Waiting up to 5m0s for pod "execpodtqmh7" in namespace "services-8640" to be "running"
Jan  5 07:33:54.562: INFO: Pod "execpodtqmh7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.911137ms
Jan  5 07:33:56.575: INFO: Pod "execpodtqmh7": Phase="Running", Reason="", readiness=true. Elapsed: 2.014846294s
Jan  5 07:33:56.575: INFO: Pod "execpodtqmh7" satisfied condition "running"
Jan  5 07:33:56.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8640 exec execpodtqmh7 -- /bin/sh -x -c nslookup nodeport-service.services-8640.svc.cluster.local'
Jan  5 07:33:56.734: INFO: stderr: "+ nslookup nodeport-service.services-8640.svc.cluster.local\n"
Jan  5 07:33:56.734: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8640.svc.cluster.local\tcanonical name = externalsvc.services-8640.svc.cluster.local.\nName:\texternalsvc.services-8640.svc.cluster.local\nAddress: 10.99.165.69\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8640, will wait for the garbage collector to delete the pods 01/05/23 07:33:56.734
Jan  5 07:33:56.800: INFO: Deleting ReplicationController externalsvc took: 13.600923ms
Jan  5 07:33:56.901: INFO: Terminating ReplicationController externalsvc pods took: 101.058559ms
Jan  5 07:33:59.019: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 07:33:59.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8640" for this suite. 01/05/23 07:33:59.066
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":24,"skipped":510,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.676 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:33:51.398
    Jan  5 07:33:51.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 07:33:51.399
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:33:51.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:33:51.414
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8640 01/05/23 07:33:51.421
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 07:33:51.436
    STEP: creating service externalsvc in namespace services-8640 01/05/23 07:33:51.436
    STEP: creating replication controller externalsvc in namespace services-8640 01/05/23 07:33:51.457
    I0105 07:33:51.472877      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8640, replica count: 2
    I0105 07:33:54.523896      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/05/23 07:33:54.526
    Jan  5 07:33:54.548: INFO: Creating new exec pod
    Jan  5 07:33:54.560: INFO: Waiting up to 5m0s for pod "execpodtqmh7" in namespace "services-8640" to be "running"
    Jan  5 07:33:54.562: INFO: Pod "execpodtqmh7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.911137ms
    Jan  5 07:33:56.575: INFO: Pod "execpodtqmh7": Phase="Running", Reason="", readiness=true. Elapsed: 2.014846294s
    Jan  5 07:33:56.575: INFO: Pod "execpodtqmh7" satisfied condition "running"
    Jan  5 07:33:56.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8640 exec execpodtqmh7 -- /bin/sh -x -c nslookup nodeport-service.services-8640.svc.cluster.local'
    Jan  5 07:33:56.734: INFO: stderr: "+ nslookup nodeport-service.services-8640.svc.cluster.local\n"
    Jan  5 07:33:56.734: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8640.svc.cluster.local\tcanonical name = externalsvc.services-8640.svc.cluster.local.\nName:\texternalsvc.services-8640.svc.cluster.local\nAddress: 10.99.165.69\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8640, will wait for the garbage collector to delete the pods 01/05/23 07:33:56.734
    Jan  5 07:33:56.800: INFO: Deleting ReplicationController externalsvc took: 13.600923ms
    Jan  5 07:33:56.901: INFO: Terminating ReplicationController externalsvc pods took: 101.058559ms
    Jan  5 07:33:59.019: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 07:33:59.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8640" for this suite. 01/05/23 07:33:59.066
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:33:59.076
Jan  5 07:33:59.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename daemonsets 01/05/23 07:33:59.077
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:33:59.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:33:59.092
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan  5 07:33:59.110: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 07:33:59.115
Jan  5 07:33:59.117: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:33:59.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 07:33:59.123: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:34:00.126: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:34:00.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 07:34:00.128: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:34:01.127: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:34:01.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 07:34:01.129: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/05/23 07:34:01.136
STEP: Check that daemon pods images are updated. 01/05/23 07:34:01.146
Jan  5 07:34:01.152: INFO: Wrong image for pod: daemon-set-md7rz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 07:34:01.152: INFO: Wrong image for pod: daemon-set-q8r8v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 07:34:01.156: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:34:02.160: INFO: Wrong image for pod: daemon-set-q8r8v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 07:34:02.162: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:34:03.160: INFO: Wrong image for pod: daemon-set-q8r8v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 07:34:03.163: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:34:04.158: INFO: Pod daemon-set-8qccm is not available
Jan  5 07:34:04.158: INFO: Wrong image for pod: daemon-set-q8r8v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 07:34:04.161: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:34:05.159: INFO: Pod daemon-set-8qccm is not available
Jan  5 07:34:05.159: INFO: Wrong image for pod: daemon-set-q8r8v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  5 07:34:05.161: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:34:06.163: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:34:07.158: INFO: Pod daemon-set-fghfz is not available
Jan  5 07:34:07.161: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 01/05/23 07:34:07.161
Jan  5 07:34:07.162: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:34:07.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 07:34:07.165: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:34:08.170: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:34:08.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 07:34:08.172: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:34:09.167: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:34:09.169: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 07:34:09.169: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 07:34:09.178
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5958, will wait for the garbage collector to delete the pods 01/05/23 07:34:09.178
Jan  5 07:34:09.236: INFO: Deleting DaemonSet.extensions daemon-set took: 5.404844ms
Jan  5 07:34:09.336: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.546128ms
Jan  5 07:34:11.639: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 07:34:11.639: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 07:34:11.640: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6514"},"items":null}

Jan  5 07:34:11.642: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6514"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 07:34:11.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5958" for this suite. 01/05/23 07:34:11.649
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":25,"skipped":535,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.577 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:33:59.076
    Jan  5 07:33:59.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename daemonsets 01/05/23 07:33:59.077
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:33:59.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:33:59.092
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan  5 07:33:59.110: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 07:33:59.115
    Jan  5 07:33:59.117: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:33:59.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 07:33:59.123: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:34:00.126: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:34:00.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 07:34:00.128: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:34:01.127: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:34:01.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 07:34:01.129: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/05/23 07:34:01.136
    STEP: Check that daemon pods images are updated. 01/05/23 07:34:01.146
    Jan  5 07:34:01.152: INFO: Wrong image for pod: daemon-set-md7rz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 07:34:01.152: INFO: Wrong image for pod: daemon-set-q8r8v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 07:34:01.156: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:34:02.160: INFO: Wrong image for pod: daemon-set-q8r8v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 07:34:02.162: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:34:03.160: INFO: Wrong image for pod: daemon-set-q8r8v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 07:34:03.163: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:34:04.158: INFO: Pod daemon-set-8qccm is not available
    Jan  5 07:34:04.158: INFO: Wrong image for pod: daemon-set-q8r8v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 07:34:04.161: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:34:05.159: INFO: Pod daemon-set-8qccm is not available
    Jan  5 07:34:05.159: INFO: Wrong image for pod: daemon-set-q8r8v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  5 07:34:05.161: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:34:06.163: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:34:07.158: INFO: Pod daemon-set-fghfz is not available
    Jan  5 07:34:07.161: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 01/05/23 07:34:07.161
    Jan  5 07:34:07.162: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:34:07.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 07:34:07.165: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:34:08.170: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:34:08.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 07:34:08.172: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:34:09.167: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:34:09.169: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 07:34:09.169: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 07:34:09.178
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5958, will wait for the garbage collector to delete the pods 01/05/23 07:34:09.178
    Jan  5 07:34:09.236: INFO: Deleting DaemonSet.extensions daemon-set took: 5.404844ms
    Jan  5 07:34:09.336: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.546128ms
    Jan  5 07:34:11.639: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 07:34:11.639: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 07:34:11.640: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6514"},"items":null}

    Jan  5 07:34:11.642: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6514"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 07:34:11.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5958" for this suite. 01/05/23 07:34:11.649
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:34:11.653
Jan  5 07:34:11.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:34:11.654
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:34:11.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:34:11.74
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-f868f75e-9aa3-4531-8b97-2464438fea94 01/05/23 07:34:11.743
STEP: Creating a pod to test consume configMaps 01/05/23 07:34:11.746
Jan  5 07:34:11.756: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70" in namespace "projected-3672" to be "Succeeded or Failed"
Jan  5 07:34:11.758: INFO: Pod "pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297561ms
Jan  5 07:34:13.764: INFO: Pod "pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007539766s
Jan  5 07:34:15.762: INFO: Pod "pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005846498s
STEP: Saw pod success 01/05/23 07:34:15.762
Jan  5 07:34:15.762: INFO: Pod "pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70" satisfied condition "Succeeded or Failed"
Jan  5 07:34:15.764: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 07:34:15.781
Jan  5 07:34:15.799: INFO: Waiting for pod pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70 to disappear
Jan  5 07:34:15.801: INFO: Pod pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 07:34:15.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3672" for this suite. 01/05/23 07:34:15.808
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":26,"skipped":536,"failed":0}
------------------------------
â€¢ [4.159 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:34:11.653
    Jan  5 07:34:11.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:34:11.654
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:34:11.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:34:11.74
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-f868f75e-9aa3-4531-8b97-2464438fea94 01/05/23 07:34:11.743
    STEP: Creating a pod to test consume configMaps 01/05/23 07:34:11.746
    Jan  5 07:34:11.756: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70" in namespace "projected-3672" to be "Succeeded or Failed"
    Jan  5 07:34:11.758: INFO: Pod "pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297561ms
    Jan  5 07:34:13.764: INFO: Pod "pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007539766s
    Jan  5 07:34:15.762: INFO: Pod "pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005846498s
    STEP: Saw pod success 01/05/23 07:34:15.762
    Jan  5 07:34:15.762: INFO: Pod "pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70" satisfied condition "Succeeded or Failed"
    Jan  5 07:34:15.764: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 07:34:15.781
    Jan  5 07:34:15.799: INFO: Waiting for pod pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70 to disappear
    Jan  5 07:34:15.801: INFO: Pod pod-projected-configmaps-5d45382c-0afa-4e51-9d99-39658f580c70 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 07:34:15.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3672" for this suite. 01/05/23 07:34:15.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:34:15.815
Jan  5 07:34:15.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename conformance-tests 01/05/23 07:34:15.816
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:34:15.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:34:15.832
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/05/23 07:34:15.835
Jan  5 07:34:15.835: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan  5 07:34:15.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-610" for this suite. 01/05/23 07:34:15.84
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":27,"skipped":644,"failed":0}
------------------------------
â€¢ [0.029 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:34:15.815
    Jan  5 07:34:15.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename conformance-tests 01/05/23 07:34:15.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:34:15.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:34:15.832
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/05/23 07:34:15.835
    Jan  5 07:34:15.835: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan  5 07:34:15.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-610" for this suite. 01/05/23 07:34:15.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:34:15.844
Jan  5 07:34:15.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 07:34:15.845
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:34:15.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:34:15.865
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/05/23 07:34:15.87
STEP: fetching the ConfigMap 01/05/23 07:34:15.873
STEP: patching the ConfigMap 01/05/23 07:34:15.875
STEP: listing all ConfigMaps in all namespaces with a label selector 01/05/23 07:34:15.882
STEP: deleting the ConfigMap by collection with a label selector 01/05/23 07:34:15.883
STEP: listing all ConfigMaps in test namespace 01/05/23 07:34:15.888
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 07:34:15.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6069" for this suite. 01/05/23 07:34:15.891
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":28,"skipped":669,"failed":0}
------------------------------
â€¢ [0.055 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:34:15.844
    Jan  5 07:34:15.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 07:34:15.845
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:34:15.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:34:15.865
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/05/23 07:34:15.87
    STEP: fetching the ConfigMap 01/05/23 07:34:15.873
    STEP: patching the ConfigMap 01/05/23 07:34:15.875
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/05/23 07:34:15.882
    STEP: deleting the ConfigMap by collection with a label selector 01/05/23 07:34:15.883
    STEP: listing all ConfigMaps in test namespace 01/05/23 07:34:15.888
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 07:34:15.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6069" for this suite. 01/05/23 07:34:15.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:34:15.9
Jan  5 07:34:15.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename events 01/05/23 07:34:15.901
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:34:15.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:34:15.916
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/05/23 07:34:15.917
STEP: get a list of Events with a label in the current namespace 01/05/23 07:34:15.935
STEP: delete a list of events 01/05/23 07:34:15.937
Jan  5 07:34:15.937: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/05/23 07:34:15.95
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan  5 07:34:15.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4730" for this suite. 01/05/23 07:34:15.954
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":29,"skipped":693,"failed":0}
------------------------------
â€¢ [0.069 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:34:15.9
    Jan  5 07:34:15.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename events 01/05/23 07:34:15.901
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:34:15.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:34:15.916
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/05/23 07:34:15.917
    STEP: get a list of Events with a label in the current namespace 01/05/23 07:34:15.935
    STEP: delete a list of events 01/05/23 07:34:15.937
    Jan  5 07:34:15.937: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/05/23 07:34:15.95
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan  5 07:34:15.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4730" for this suite. 01/05/23 07:34:15.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:34:15.972
Jan  5 07:34:15.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename cronjob 01/05/23 07:34:15.973
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:34:15.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:34:15.987
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/05/23 07:34:15.991
STEP: Ensuring no jobs are scheduled 01/05/23 07:34:15.994
STEP: Ensuring no job exists by listing jobs explicitly 01/05/23 07:39:15.999
STEP: Removing cronjob 01/05/23 07:39:16.001
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  5 07:39:16.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1138" for this suite. 01/05/23 07:39:16.007
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":30,"skipped":761,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.044 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:34:15.972
    Jan  5 07:34:15.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename cronjob 01/05/23 07:34:15.973
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:34:15.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:34:15.987
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/05/23 07:34:15.991
    STEP: Ensuring no jobs are scheduled 01/05/23 07:34:15.994
    STEP: Ensuring no job exists by listing jobs explicitly 01/05/23 07:39:15.999
    STEP: Removing cronjob 01/05/23 07:39:16.001
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  5 07:39:16.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1138" for this suite. 01/05/23 07:39:16.007
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:39:16.016
Jan  5 07:39:16.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename security-context-test 01/05/23 07:39:16.017
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:39:16.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:39:16.034
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan  5 07:39:16.050: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e" in namespace "security-context-test-352" to be "Succeeded or Failed"
Jan  5 07:39:16.052: INFO: Pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.771202ms
Jan  5 07:39:18.056: INFO: Pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005558692s
Jan  5 07:39:20.055: INFO: Pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004389337s
Jan  5 07:39:22.054: INFO: Pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004151075s
Jan  5 07:39:22.054: INFO: Pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e" satisfied condition "Succeeded or Failed"
Jan  5 07:39:22.066: INFO: Got logs for pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 07:39:22.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-352" for this suite. 01/05/23 07:39:22.069
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":31,"skipped":761,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.057 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:39:16.016
    Jan  5 07:39:16.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename security-context-test 01/05/23 07:39:16.017
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:39:16.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:39:16.034
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan  5 07:39:16.050: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e" in namespace "security-context-test-352" to be "Succeeded or Failed"
    Jan  5 07:39:16.052: INFO: Pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.771202ms
    Jan  5 07:39:18.056: INFO: Pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005558692s
    Jan  5 07:39:20.055: INFO: Pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004389337s
    Jan  5 07:39:22.054: INFO: Pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004151075s
    Jan  5 07:39:22.054: INFO: Pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e" satisfied condition "Succeeded or Failed"
    Jan  5 07:39:22.066: INFO: Got logs for pod "busybox-privileged-false-803188d9-c00b-49f5-b967-6e99c7aca42e": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 07:39:22.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-352" for this suite. 01/05/23 07:39:22.069
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:39:22.073
Jan  5 07:39:22.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename gc 01/05/23 07:39:22.074
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:39:22.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:39:22.092
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/05/23 07:39:22.096
STEP: delete the rc 01/05/23 07:39:27.148
STEP: wait for all pods to be garbage collected 01/05/23 07:39:27.168
STEP: Gathering metrics 01/05/23 07:39:32.174
W0105 07:39:32.178615      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 07:39:32.178: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 07:39:32.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8007" for this suite. 01/05/23 07:39:32.18
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":32,"skipped":763,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.112 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:39:22.073
    Jan  5 07:39:22.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename gc 01/05/23 07:39:22.074
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:39:22.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:39:22.092
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/05/23 07:39:22.096
    STEP: delete the rc 01/05/23 07:39:27.148
    STEP: wait for all pods to be garbage collected 01/05/23 07:39:27.168
    STEP: Gathering metrics 01/05/23 07:39:32.174
    W0105 07:39:32.178615      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 07:39:32.178: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 07:39:32.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8007" for this suite. 01/05/23 07:39:32.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:39:32.185
Jan  5 07:39:32.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pod-network-test 01/05/23 07:39:32.186
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:39:32.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:39:32.202
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5787 01/05/23 07:39:32.205
STEP: creating a selector 01/05/23 07:39:32.205
STEP: Creating the service pods in kubernetes 01/05/23 07:39:32.205
Jan  5 07:39:32.205: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 07:39:32.227: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5787" to be "running and ready"
Jan  5 07:39:32.234: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.623201ms
Jan  5 07:39:32.234: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:39:34.237: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010479985s
Jan  5 07:39:34.237: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:39:36.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01091322s
Jan  5 07:39:36.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:39:38.239: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.0122376s
Jan  5 07:39:38.239: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:39:40.237: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009862279s
Jan  5 07:39:40.237: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:39:42.237: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010095863s
Jan  5 07:39:42.237: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:39:44.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010810491s
Jan  5 07:39:44.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:39:46.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010868847s
Jan  5 07:39:46.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:39:48.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01078052s
Jan  5 07:39:48.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:39:50.237: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010017243s
Jan  5 07:39:50.237: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:39:52.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011409171s
Jan  5 07:39:52.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:39:54.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010886311s
Jan  5 07:39:54.238: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 07:39:54.238: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 07:39:54.240: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5787" to be "running and ready"
Jan  5 07:39:54.241: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.463905ms
Jan  5 07:39:54.241: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 07:39:54.241: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 07:39:54.243
Jan  5 07:39:54.254: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5787" to be "running"
Jan  5 07:39:54.256: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282988ms
Jan  5 07:39:56.260: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006016787s
Jan  5 07:39:58.261: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007310905s
Jan  5 07:39:58.261: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 07:39:58.263: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5787" to be "running"
Jan  5 07:39:58.265: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.617453ms
Jan  5 07:39:58.265: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  5 07:39:58.266: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan  5 07:39:58.266: INFO: Going to poll 10.244.0.140 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan  5 07:39:58.268: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.140 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 07:39:58.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:39:58.269: INFO: ExecWithOptions: Clientset creation
Jan  5 07:39:58.269: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.140+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 07:39:59.335: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  5 07:39:59.335: INFO: Going to poll 10.244.1.36 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan  5 07:39:59.337: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.36 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 07:39:59.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:39:59.338: INFO: ExecWithOptions: Clientset creation
Jan  5 07:39:59.338: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.36+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 07:40:00.406: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  5 07:40:00.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5787" for this suite. 01/05/23 07:40:00.41
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":33,"skipped":769,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.229 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:39:32.185
    Jan  5 07:39:32.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 07:39:32.186
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:39:32.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:39:32.202
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5787 01/05/23 07:39:32.205
    STEP: creating a selector 01/05/23 07:39:32.205
    STEP: Creating the service pods in kubernetes 01/05/23 07:39:32.205
    Jan  5 07:39:32.205: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 07:39:32.227: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5787" to be "running and ready"
    Jan  5 07:39:32.234: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.623201ms
    Jan  5 07:39:32.234: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:39:34.237: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010479985s
    Jan  5 07:39:34.237: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:39:36.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01091322s
    Jan  5 07:39:36.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:39:38.239: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.0122376s
    Jan  5 07:39:38.239: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:39:40.237: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009862279s
    Jan  5 07:39:40.237: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:39:42.237: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010095863s
    Jan  5 07:39:42.237: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:39:44.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010810491s
    Jan  5 07:39:44.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:39:46.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010868847s
    Jan  5 07:39:46.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:39:48.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01078052s
    Jan  5 07:39:48.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:39:50.237: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010017243s
    Jan  5 07:39:50.237: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:39:52.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011409171s
    Jan  5 07:39:52.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:39:54.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010886311s
    Jan  5 07:39:54.238: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 07:39:54.238: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 07:39:54.240: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5787" to be "running and ready"
    Jan  5 07:39:54.241: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.463905ms
    Jan  5 07:39:54.241: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 07:39:54.241: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 07:39:54.243
    Jan  5 07:39:54.254: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5787" to be "running"
    Jan  5 07:39:54.256: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282988ms
    Jan  5 07:39:56.260: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006016787s
    Jan  5 07:39:58.261: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007310905s
    Jan  5 07:39:58.261: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 07:39:58.263: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5787" to be "running"
    Jan  5 07:39:58.265: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.617453ms
    Jan  5 07:39:58.265: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  5 07:39:58.266: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan  5 07:39:58.266: INFO: Going to poll 10.244.0.140 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan  5 07:39:58.268: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.140 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 07:39:58.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:39:58.269: INFO: ExecWithOptions: Clientset creation
    Jan  5 07:39:58.269: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.140+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 07:39:59.335: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  5 07:39:59.335: INFO: Going to poll 10.244.1.36 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan  5 07:39:59.337: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.36 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 07:39:59.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:39:59.338: INFO: ExecWithOptions: Clientset creation
    Jan  5 07:39:59.338: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.36+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 07:40:00.406: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  5 07:40:00.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5787" for this suite. 01/05/23 07:40:00.41
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:40:00.415
Jan  5 07:40:00.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:40:00.416
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:00.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:00.46
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-8c0f4cad-ef92-4b22-96ab-7ab429ec96f2 01/05/23 07:40:00.468
STEP: Creating secret with name secret-projected-all-test-volume-12afe739-5c5f-4c93-baf5-fd17986be7ed 01/05/23 07:40:00.472
STEP: Creating a pod to test Check all projections for projected volume plugin 01/05/23 07:40:00.481
Jan  5 07:40:00.488: INFO: Waiting up to 5m0s for pod "projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4" in namespace "projected-5762" to be "Succeeded or Failed"
Jan  5 07:40:00.490: INFO: Pod "projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144318ms
Jan  5 07:40:02.493: INFO: Pod "projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005129426s
Jan  5 07:40:04.495: INFO: Pod "projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006918222s
STEP: Saw pod success 01/05/23 07:40:04.495
Jan  5 07:40:04.495: INFO: Pod "projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4" satisfied condition "Succeeded or Failed"
Jan  5 07:40:04.498: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4 container projected-all-volume-test: <nil>
STEP: delete the pod 01/05/23 07:40:04.501
Jan  5 07:40:04.513: INFO: Waiting for pod projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4 to disappear
Jan  5 07:40:04.515: INFO: Pod projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan  5 07:40:04.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5762" for this suite. 01/05/23 07:40:04.517
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":34,"skipped":769,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:40:00.415
    Jan  5 07:40:00.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:40:00.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:00.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:00.46
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-8c0f4cad-ef92-4b22-96ab-7ab429ec96f2 01/05/23 07:40:00.468
    STEP: Creating secret with name secret-projected-all-test-volume-12afe739-5c5f-4c93-baf5-fd17986be7ed 01/05/23 07:40:00.472
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/05/23 07:40:00.481
    Jan  5 07:40:00.488: INFO: Waiting up to 5m0s for pod "projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4" in namespace "projected-5762" to be "Succeeded or Failed"
    Jan  5 07:40:00.490: INFO: Pod "projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144318ms
    Jan  5 07:40:02.493: INFO: Pod "projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005129426s
    Jan  5 07:40:04.495: INFO: Pod "projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006918222s
    STEP: Saw pod success 01/05/23 07:40:04.495
    Jan  5 07:40:04.495: INFO: Pod "projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4" satisfied condition "Succeeded or Failed"
    Jan  5 07:40:04.498: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/05/23 07:40:04.501
    Jan  5 07:40:04.513: INFO: Waiting for pod projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4 to disappear
    Jan  5 07:40:04.515: INFO: Pod projected-volume-c13daa19-b7d4-4ef8-98b8-5721fe1d19c4 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan  5 07:40:04.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5762" for this suite. 01/05/23 07:40:04.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:40:04.529
Jan  5 07:40:04.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubelet-test 01/05/23 07:40:04.529
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:04.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:04.543
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan  5 07:40:04.560: INFO: Waiting up to 5m0s for pod "busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35" in namespace "kubelet-test-4894" to be "running and ready"
Jan  5 07:40:04.569: INFO: Pod "busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35": Phase="Pending", Reason="", readiness=false. Elapsed: 8.666846ms
Jan  5 07:40:04.569: INFO: The phase of Pod busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:40:06.572: INFO: Pod "busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35": Phase="Running", Reason="", readiness=true. Elapsed: 2.012198367s
Jan  5 07:40:06.572: INFO: The phase of Pod busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35 is Running (Ready = true)
Jan  5 07:40:06.572: INFO: Pod "busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  5 07:40:06.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4894" for this suite. 01/05/23 07:40:06.58
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":35,"skipped":832,"failed":0}
------------------------------
â€¢ [2.055 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:40:04.529
    Jan  5 07:40:04.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 07:40:04.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:04.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:04.543
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan  5 07:40:04.560: INFO: Waiting up to 5m0s for pod "busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35" in namespace "kubelet-test-4894" to be "running and ready"
    Jan  5 07:40:04.569: INFO: Pod "busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35": Phase="Pending", Reason="", readiness=false. Elapsed: 8.666846ms
    Jan  5 07:40:04.569: INFO: The phase of Pod busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:40:06.572: INFO: Pod "busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35": Phase="Running", Reason="", readiness=true. Elapsed: 2.012198367s
    Jan  5 07:40:06.572: INFO: The phase of Pod busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35 is Running (Ready = true)
    Jan  5 07:40:06.572: INFO: Pod "busybox-scheduling-3c7c67f1-87bb-473d-9ed0-c73c9f99dd35" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  5 07:40:06.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4894" for this suite. 01/05/23 07:40:06.58
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:40:06.584
Jan  5 07:40:06.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pod-network-test 01/05/23 07:40:06.585
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:06.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:06.601
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-5811 01/05/23 07:40:06.604
STEP: creating a selector 01/05/23 07:40:06.604
STEP: Creating the service pods in kubernetes 01/05/23 07:40:06.604
Jan  5 07:40:06.604: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 07:40:06.630: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5811" to be "running and ready"
Jan  5 07:40:06.632: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138244ms
Jan  5 07:40:06.632: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:40:08.634: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004368791s
Jan  5 07:40:08.634: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:40:10.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005582942s
Jan  5 07:40:10.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:40:12.636: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006402323s
Jan  5 07:40:12.636: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:40:14.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005021207s
Jan  5 07:40:14.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:40:16.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005405627s
Jan  5 07:40:16.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:40:18.636: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006624762s
Jan  5 07:40:18.636: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:40:20.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005418545s
Jan  5 07:40:20.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:40:22.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00538606s
Jan  5 07:40:22.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:40:24.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004989787s
Jan  5 07:40:24.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:40:26.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.005712125s
Jan  5 07:40:26.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 07:40:28.634: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004848998s
Jan  5 07:40:28.635: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 07:40:28.635: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 07:40:28.636: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5811" to be "running and ready"
Jan  5 07:40:28.638: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.693395ms
Jan  5 07:40:28.638: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 07:40:28.638: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 07:40:28.64
Jan  5 07:40:28.651: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5811" to be "running"
Jan  5 07:40:28.654: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.791786ms
Jan  5 07:40:30.658: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007087226s
Jan  5 07:40:32.659: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007765775s
Jan  5 07:40:32.659: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 07:40:32.661: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5811" to be "running"
Jan  5 07:40:32.663: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.633576ms
Jan  5 07:40:32.663: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  5 07:40:32.664: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan  5 07:40:32.664: INFO: Going to poll 10.244.0.141 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan  5 07:40:32.666: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.141:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5811 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 07:40:32.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:40:32.666: INFO: ExecWithOptions: Clientset creation
Jan  5 07:40:32.666: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5811/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.141%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 07:40:32.749: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  5 07:40:32.749: INFO: Going to poll 10.244.1.40 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan  5 07:40:32.751: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.40:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5811 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 07:40:32.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:40:32.752: INFO: ExecWithOptions: Clientset creation
Jan  5 07:40:32.752: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5811/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.40%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 07:40:32.826: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  5 07:40:32.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5811" for this suite. 01/05/23 07:40:32.829
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":36,"skipped":835,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.248 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:40:06.584
    Jan  5 07:40:06.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 07:40:06.585
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:06.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:06.601
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-5811 01/05/23 07:40:06.604
    STEP: creating a selector 01/05/23 07:40:06.604
    STEP: Creating the service pods in kubernetes 01/05/23 07:40:06.604
    Jan  5 07:40:06.604: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 07:40:06.630: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5811" to be "running and ready"
    Jan  5 07:40:06.632: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138244ms
    Jan  5 07:40:06.632: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:40:08.634: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004368791s
    Jan  5 07:40:08.634: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:40:10.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005582942s
    Jan  5 07:40:10.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:40:12.636: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006402323s
    Jan  5 07:40:12.636: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:40:14.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005021207s
    Jan  5 07:40:14.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:40:16.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005405627s
    Jan  5 07:40:16.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:40:18.636: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006624762s
    Jan  5 07:40:18.636: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:40:20.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005418545s
    Jan  5 07:40:20.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:40:22.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00538606s
    Jan  5 07:40:22.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:40:24.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004989787s
    Jan  5 07:40:24.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:40:26.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.005712125s
    Jan  5 07:40:26.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 07:40:28.634: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004848998s
    Jan  5 07:40:28.635: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 07:40:28.635: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 07:40:28.636: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5811" to be "running and ready"
    Jan  5 07:40:28.638: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.693395ms
    Jan  5 07:40:28.638: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 07:40:28.638: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 07:40:28.64
    Jan  5 07:40:28.651: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5811" to be "running"
    Jan  5 07:40:28.654: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.791786ms
    Jan  5 07:40:30.658: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007087226s
    Jan  5 07:40:32.659: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007765775s
    Jan  5 07:40:32.659: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 07:40:32.661: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5811" to be "running"
    Jan  5 07:40:32.663: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.633576ms
    Jan  5 07:40:32.663: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  5 07:40:32.664: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan  5 07:40:32.664: INFO: Going to poll 10.244.0.141 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan  5 07:40:32.666: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.141:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5811 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 07:40:32.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:40:32.666: INFO: ExecWithOptions: Clientset creation
    Jan  5 07:40:32.666: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5811/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.141%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 07:40:32.749: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  5 07:40:32.749: INFO: Going to poll 10.244.1.40 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan  5 07:40:32.751: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.40:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5811 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 07:40:32.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:40:32.752: INFO: ExecWithOptions: Clientset creation
    Jan  5 07:40:32.752: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5811/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.40%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 07:40:32.826: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  5 07:40:32.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5811" for this suite. 01/05/23 07:40:32.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:40:32.833
Jan  5 07:40:32.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename containers 01/05/23 07:40:32.834
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:32.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:32.857
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/05/23 07:40:32.859
Jan  5 07:40:32.886: INFO: Waiting up to 5m0s for pod "client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7" in namespace "containers-8850" to be "Succeeded or Failed"
Jan  5 07:40:32.894: INFO: Pod "client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.880794ms
Jan  5 07:40:34.896: INFO: Pod "client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010536264s
Jan  5 07:40:36.898: INFO: Pod "client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012124991s
STEP: Saw pod success 01/05/23 07:40:36.898
Jan  5 07:40:36.898: INFO: Pod "client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7" satisfied condition "Succeeded or Failed"
Jan  5 07:40:36.900: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 07:40:36.905
Jan  5 07:40:36.921: INFO: Waiting for pod client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7 to disappear
Jan  5 07:40:36.923: INFO: Pod client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  5 07:40:36.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8850" for this suite. 01/05/23 07:40:36.926
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":37,"skipped":863,"failed":0}
------------------------------
â€¢ [4.098 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:40:32.833
    Jan  5 07:40:32.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename containers 01/05/23 07:40:32.834
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:32.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:32.857
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/05/23 07:40:32.859
    Jan  5 07:40:32.886: INFO: Waiting up to 5m0s for pod "client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7" in namespace "containers-8850" to be "Succeeded or Failed"
    Jan  5 07:40:32.894: INFO: Pod "client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.880794ms
    Jan  5 07:40:34.896: INFO: Pod "client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010536264s
    Jan  5 07:40:36.898: INFO: Pod "client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012124991s
    STEP: Saw pod success 01/05/23 07:40:36.898
    Jan  5 07:40:36.898: INFO: Pod "client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7" satisfied condition "Succeeded or Failed"
    Jan  5 07:40:36.900: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 07:40:36.905
    Jan  5 07:40:36.921: INFO: Waiting for pod client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7 to disappear
    Jan  5 07:40:36.923: INFO: Pod client-containers-ca9579a8-2247-41a5-95cd-2239808e3ea7 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  5 07:40:36.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8850" for this suite. 01/05/23 07:40:36.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:40:36.931
Jan  5 07:40:36.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename endpointslice 01/05/23 07:40:36.932
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:36.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:36.949
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  5 07:40:37.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-728" for this suite. 01/05/23 07:40:37.04
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":38,"skipped":876,"failed":0}
------------------------------
â€¢ [0.113 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:40:36.931
    Jan  5 07:40:36.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename endpointslice 01/05/23 07:40:36.932
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:36.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:36.949
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  5 07:40:37.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-728" for this suite. 01/05/23 07:40:37.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:40:37.045
Jan  5 07:40:37.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename daemonsets 01/05/23 07:40:37.046
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:37.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:37.066
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/05/23 07:40:37.078
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 07:40:37.086
Jan  5 07:40:37.088: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:40:37.091: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 07:40:37.091: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:40:38.094: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:40:38.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 07:40:38.096: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:40:39.094: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:40:39.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 07:40:39.096: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:40:40.095: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:40:40.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 07:40:40.098: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/05/23 07:40:40.1
Jan  5 07:40:40.108: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:40:40.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 07:40:40.109: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:40:41.112: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:40:41.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 07:40:41.114: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:40:42.111: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:40:42.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 07:40:42.113: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:40:43.112: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:40:43.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 07:40:43.114: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:40:44.112: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:40:44.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 07:40:44.114: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 07:40:44.116
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3472, will wait for the garbage collector to delete the pods 01/05/23 07:40:44.116
Jan  5 07:40:44.172: INFO: Deleting DaemonSet.extensions daemon-set took: 4.326577ms
Jan  5 07:40:44.272: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.43922ms
Jan  5 07:40:46.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 07:40:46.775: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 07:40:46.776: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7684"},"items":null}

Jan  5 07:40:46.778: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7684"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 07:40:46.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3472" for this suite. 01/05/23 07:40:46.785
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":39,"skipped":897,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.748 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:40:37.045
    Jan  5 07:40:37.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename daemonsets 01/05/23 07:40:37.046
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:37.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:37.066
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/05/23 07:40:37.078
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 07:40:37.086
    Jan  5 07:40:37.088: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:40:37.091: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 07:40:37.091: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:40:38.094: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:40:38.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 07:40:38.096: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:40:39.094: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:40:39.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 07:40:39.096: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:40:40.095: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:40:40.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 07:40:40.098: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/05/23 07:40:40.1
    Jan  5 07:40:40.108: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:40:40.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 07:40:40.109: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:40:41.112: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:40:41.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 07:40:41.114: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:40:42.111: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:40:42.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 07:40:42.113: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:40:43.112: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:40:43.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 07:40:43.114: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:40:44.112: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:40:44.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 07:40:44.114: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 07:40:44.116
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3472, will wait for the garbage collector to delete the pods 01/05/23 07:40:44.116
    Jan  5 07:40:44.172: INFO: Deleting DaemonSet.extensions daemon-set took: 4.326577ms
    Jan  5 07:40:44.272: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.43922ms
    Jan  5 07:40:46.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 07:40:46.775: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 07:40:46.776: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7684"},"items":null}

    Jan  5 07:40:46.778: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7684"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 07:40:46.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3472" for this suite. 01/05/23 07:40:46.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:40:46.796
Jan  5 07:40:46.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename endpointslice 01/05/23 07:40:46.797
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:46.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:46.813
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/05/23 07:40:51.928
STEP: referencing matching pods with named port 01/05/23 07:40:56.936
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/05/23 07:41:01.944
STEP: recreating EndpointSlices after they've been deleted 01/05/23 07:41:06.949
Jan  5 07:41:06.971: INFO: EndpointSlice for Service endpointslice-5695/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  5 07:41:16.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5695" for this suite. 01/05/23 07:41:16.981
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":40,"skipped":946,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.190 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:40:46.796
    Jan  5 07:40:46.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename endpointslice 01/05/23 07:40:46.797
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:40:46.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:40:46.813
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/05/23 07:40:51.928
    STEP: referencing matching pods with named port 01/05/23 07:40:56.936
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/05/23 07:41:01.944
    STEP: recreating EndpointSlices after they've been deleted 01/05/23 07:41:06.949
    Jan  5 07:41:06.971: INFO: EndpointSlice for Service endpointslice-5695/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  5 07:41:16.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5695" for this suite. 01/05/23 07:41:16.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:41:16.988
Jan  5 07:41:16.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename csistoragecapacity 01/05/23 07:41:16.989
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:41:17.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:41:17.008
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/05/23 07:41:17.012
STEP: getting /apis/storage.k8s.io 01/05/23 07:41:17.013
STEP: getting /apis/storage.k8s.io/v1 01/05/23 07:41:17.014
STEP: creating 01/05/23 07:41:17.014
STEP: watching 01/05/23 07:41:17.036
Jan  5 07:41:17.036: INFO: starting watch
STEP: getting 01/05/23 07:41:17.042
STEP: listing in namespace 01/05/23 07:41:17.043
STEP: listing across namespaces 01/05/23 07:41:17.045
STEP: patching 01/05/23 07:41:17.046
STEP: updating 01/05/23 07:41:17.057
Jan  5 07:41:17.061: INFO: waiting for watch events with expected annotations in namespace
Jan  5 07:41:17.061: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/05/23 07:41:17.061
STEP: deleting a collection 01/05/23 07:41:17.073
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan  5 07:41:17.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-41" for this suite. 01/05/23 07:41:17.091
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":41,"skipped":966,"failed":0}
------------------------------
â€¢ [0.107 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:41:16.988
    Jan  5 07:41:16.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename csistoragecapacity 01/05/23 07:41:16.989
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:41:17.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:41:17.008
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/05/23 07:41:17.012
    STEP: getting /apis/storage.k8s.io 01/05/23 07:41:17.013
    STEP: getting /apis/storage.k8s.io/v1 01/05/23 07:41:17.014
    STEP: creating 01/05/23 07:41:17.014
    STEP: watching 01/05/23 07:41:17.036
    Jan  5 07:41:17.036: INFO: starting watch
    STEP: getting 01/05/23 07:41:17.042
    STEP: listing in namespace 01/05/23 07:41:17.043
    STEP: listing across namespaces 01/05/23 07:41:17.045
    STEP: patching 01/05/23 07:41:17.046
    STEP: updating 01/05/23 07:41:17.057
    Jan  5 07:41:17.061: INFO: waiting for watch events with expected annotations in namespace
    Jan  5 07:41:17.061: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/05/23 07:41:17.061
    STEP: deleting a collection 01/05/23 07:41:17.073
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan  5 07:41:17.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-41" for this suite. 01/05/23 07:41:17.091
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:41:17.094
Jan  5 07:41:17.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 07:41:17.095
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:41:17.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:41:17.119
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 07:41:17.12
Jan  5 07:41:17.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5074 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  5 07:41:17.192: INFO: stderr: ""
Jan  5 07:41:17.192: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/05/23 07:41:17.192
STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 07:41:22.244
Jan  5 07:41:22.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5074 get pod e2e-test-httpd-pod -o json'
Jan  5 07:41:22.309: INFO: stderr: ""
Jan  5 07:41:22.309: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"f49bf83e9d24c77a9508f491de8591c5036ed1b1d1bbe6acc4fabb1989a7aa06\",\n            \"cni.projectcalico.org/podIP\": \"10.244.1.46/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.1.46/32\"\n        },\n        \"creationTimestamp\": \"2023-01-05T07:41:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5074\",\n        \"resourceVersion\": \"7837\",\n        \"uid\": \"2eb58e25-8642-4674-9d14-10a50611f6d8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qcjc9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"mip-bd-vm724.mip.storage.hpecorp.net\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qcjc9\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T07:41:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T07:41:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T07:41:19Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T07:41:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://33c538996846ee3364ac1c07dd29d93cb00beea1dcd29239895729bb61e8b584\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-05T07:41:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"16.0.14.214\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.46\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.46\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-05T07:41:17Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/05/23 07:41:22.309
Jan  5 07:41:22.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5074 replace -f -'
Jan  5 07:41:22.527: INFO: stderr: ""
Jan  5 07:41:22.527: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/05/23 07:41:22.527
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan  5 07:41:22.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5074 delete pods e2e-test-httpd-pod'
Jan  5 07:41:25.490: INFO: stderr: ""
Jan  5 07:41:25.490: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 07:41:25.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5074" for this suite. 01/05/23 07:41:25.493
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":42,"skipped":969,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.410 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:41:17.094
    Jan  5 07:41:17.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 07:41:17.095
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:41:17.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:41:17.119
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 07:41:17.12
    Jan  5 07:41:17.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5074 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  5 07:41:17.192: INFO: stderr: ""
    Jan  5 07:41:17.192: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/05/23 07:41:17.192
    STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 07:41:22.244
    Jan  5 07:41:22.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5074 get pod e2e-test-httpd-pod -o json'
    Jan  5 07:41:22.309: INFO: stderr: ""
    Jan  5 07:41:22.309: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"f49bf83e9d24c77a9508f491de8591c5036ed1b1d1bbe6acc4fabb1989a7aa06\",\n            \"cni.projectcalico.org/podIP\": \"10.244.1.46/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.1.46/32\"\n        },\n        \"creationTimestamp\": \"2023-01-05T07:41:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5074\",\n        \"resourceVersion\": \"7837\",\n        \"uid\": \"2eb58e25-8642-4674-9d14-10a50611f6d8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qcjc9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"mip-bd-vm724.mip.storage.hpecorp.net\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qcjc9\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T07:41:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T07:41:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T07:41:19Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T07:41:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://33c538996846ee3364ac1c07dd29d93cb00beea1dcd29239895729bb61e8b584\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-05T07:41:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"16.0.14.214\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.46\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.46\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-05T07:41:17Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/05/23 07:41:22.309
    Jan  5 07:41:22.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5074 replace -f -'
    Jan  5 07:41:22.527: INFO: stderr: ""
    Jan  5 07:41:22.527: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/05/23 07:41:22.527
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan  5 07:41:22.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5074 delete pods e2e-test-httpd-pod'
    Jan  5 07:41:25.490: INFO: stderr: ""
    Jan  5 07:41:25.490: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 07:41:25.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5074" for this suite. 01/05/23 07:41:25.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:41:25.506
Jan  5 07:41:25.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:41:25.509
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:41:25.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:41:25.525
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-d646e56a-67bb-455e-853c-841b52620ae2 01/05/23 07:41:25.534
STEP: Creating secret with name s-test-opt-upd-05635e79-54b7-4b81-abb7-ba376435f091 01/05/23 07:41:25.541
STEP: Creating the pod 01/05/23 07:41:25.563
Jan  5 07:41:25.571: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d" in namespace "projected-5378" to be "running and ready"
Jan  5 07:41:25.572: INFO: Pod "pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.679494ms
Jan  5 07:41:25.572: INFO: The phase of Pod pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:41:27.575: INFO: Pod "pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004219534s
Jan  5 07:41:27.575: INFO: The phase of Pod pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:41:29.581: INFO: Pod "pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d": Phase="Running", Reason="", readiness=true. Elapsed: 4.009735882s
Jan  5 07:41:29.581: INFO: The phase of Pod pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d is Running (Ready = true)
Jan  5 07:41:29.581: INFO: Pod "pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-d646e56a-67bb-455e-853c-841b52620ae2 01/05/23 07:41:29.594
STEP: Updating secret s-test-opt-upd-05635e79-54b7-4b81-abb7-ba376435f091 01/05/23 07:41:29.602
STEP: Creating secret with name s-test-opt-create-0340095f-3769-47b4-8df2-d6119dfdcdb5 01/05/23 07:41:29.606
STEP: waiting to observe update in volume 01/05/23 07:41:29.616
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 07:42:47.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5378" for this suite. 01/05/23 07:42:47.873
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":43,"skipped":994,"failed":0}
------------------------------
â€¢ [SLOW TEST] [82.371 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:41:25.506
    Jan  5 07:41:25.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:41:25.509
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:41:25.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:41:25.525
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-d646e56a-67bb-455e-853c-841b52620ae2 01/05/23 07:41:25.534
    STEP: Creating secret with name s-test-opt-upd-05635e79-54b7-4b81-abb7-ba376435f091 01/05/23 07:41:25.541
    STEP: Creating the pod 01/05/23 07:41:25.563
    Jan  5 07:41:25.571: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d" in namespace "projected-5378" to be "running and ready"
    Jan  5 07:41:25.572: INFO: Pod "pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.679494ms
    Jan  5 07:41:25.572: INFO: The phase of Pod pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:41:27.575: INFO: Pod "pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004219534s
    Jan  5 07:41:27.575: INFO: The phase of Pod pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:41:29.581: INFO: Pod "pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d": Phase="Running", Reason="", readiness=true. Elapsed: 4.009735882s
    Jan  5 07:41:29.581: INFO: The phase of Pod pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d is Running (Ready = true)
    Jan  5 07:41:29.581: INFO: Pod "pod-projected-secrets-d6ad6914-7e14-4a03-bf0e-fe07ee76983d" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-d646e56a-67bb-455e-853c-841b52620ae2 01/05/23 07:41:29.594
    STEP: Updating secret s-test-opt-upd-05635e79-54b7-4b81-abb7-ba376435f091 01/05/23 07:41:29.602
    STEP: Creating secret with name s-test-opt-create-0340095f-3769-47b4-8df2-d6119dfdcdb5 01/05/23 07:41:29.606
    STEP: waiting to observe update in volume 01/05/23 07:41:29.616
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 07:42:47.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5378" for this suite. 01/05/23 07:42:47.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:42:47.88
Jan  5 07:42:47.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 07:42:47.881
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:42:47.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:42:47.911
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/05/23 07:42:47.913
Jan  5 07:42:47.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/05/23 07:42:57.389
Jan  5 07:42:57.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:42:59.427: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 07:43:09.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-873" for this suite. 01/05/23 07:43:09.399
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":44,"skipped":1039,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.525 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:42:47.88
    Jan  5 07:42:47.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 07:42:47.881
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:42:47.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:42:47.911
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/05/23 07:42:47.913
    Jan  5 07:42:47.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/05/23 07:42:57.389
    Jan  5 07:42:57.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:42:59.427: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 07:43:09.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-873" for this suite. 01/05/23 07:43:09.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:43:09.408
Jan  5 07:43:09.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename job 01/05/23 07:43:09.41
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:43:09.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:43:09.426
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/05/23 07:43:09.43
STEP: Patching the Job 01/05/23 07:43:09.443
STEP: Watching for Job to be patched 01/05/23 07:43:09.468
Jan  5 07:43:09.469: INFO: Event ADDED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan  5 07:43:09.469: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan  5 07:43:09.469: INFO: Event MODIFIED found for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/05/23 07:43:09.469
STEP: Watching for Job to be updated 01/05/23 07:43:09.475
Jan  5 07:43:09.476: INFO: Event MODIFIED found for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 07:43:09.476: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/05/23 07:43:09.476
Jan  5 07:43:09.479: INFO: Job: e2e-7wxs5 as labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5]
STEP: Waiting for job to complete 01/05/23 07:43:09.479
STEP: Delete a job collection with a labelselector 01/05/23 07:43:19.483
STEP: Watching for Job to be deleted 01/05/23 07:43:19.489
Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 07:43:19.491: INFO: Event DELETED found for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/05/23 07:43:19.491
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 07:43:19.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1032" for this suite. 01/05/23 07:43:19.497
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":45,"skipped":1080,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.096 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:43:09.408
    Jan  5 07:43:09.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename job 01/05/23 07:43:09.41
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:43:09.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:43:09.426
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/05/23 07:43:09.43
    STEP: Patching the Job 01/05/23 07:43:09.443
    STEP: Watching for Job to be patched 01/05/23 07:43:09.468
    Jan  5 07:43:09.469: INFO: Event ADDED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan  5 07:43:09.469: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan  5 07:43:09.469: INFO: Event MODIFIED found for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/05/23 07:43:09.469
    STEP: Watching for Job to be updated 01/05/23 07:43:09.475
    Jan  5 07:43:09.476: INFO: Event MODIFIED found for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 07:43:09.476: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/05/23 07:43:09.476
    Jan  5 07:43:09.479: INFO: Job: e2e-7wxs5 as labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5]
    STEP: Waiting for job to complete 01/05/23 07:43:09.479
    STEP: Delete a job collection with a labelselector 01/05/23 07:43:19.483
    STEP: Watching for Job to be deleted 01/05/23 07:43:19.489
    Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 07:43:19.491: INFO: Event MODIFIED observed for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 07:43:19.491: INFO: Event DELETED found for Job e2e-7wxs5 in namespace job-1032 with labels: map[e2e-7wxs5:patched e2e-job-label:e2e-7wxs5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/05/23 07:43:19.491
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 07:43:19.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1032" for this suite. 01/05/23 07:43:19.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:43:19.505
Jan  5 07:43:19.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename subpath 01/05/23 07:43:19.506
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:43:19.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:43:19.543
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 07:43:19.545
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-jnt7 01/05/23 07:43:19.556
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 07:43:19.556
Jan  5 07:43:19.579: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jnt7" in namespace "subpath-6127" to be "Succeeded or Failed"
Jan  5 07:43:19.581: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.738406ms
Jan  5 07:43:21.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005745231s
Jan  5 07:43:23.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 4.00571152s
Jan  5 07:43:25.584: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 6.005054775s
Jan  5 07:43:27.587: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 8.007318906s
Jan  5 07:43:29.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 10.005817522s
Jan  5 07:43:31.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 12.005684791s
Jan  5 07:43:33.586: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 14.006411545s
Jan  5 07:43:35.587: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 16.007556881s
Jan  5 07:43:37.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 18.005697343s
Jan  5 07:43:39.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 20.005639842s
Jan  5 07:43:41.586: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 22.006294967s
Jan  5 07:43:43.586: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=false. Elapsed: 24.0061461s
Jan  5 07:43:45.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.005284216s
STEP: Saw pod success 01/05/23 07:43:45.585
Jan  5 07:43:45.585: INFO: Pod "pod-subpath-test-configmap-jnt7" satisfied condition "Succeeded or Failed"
Jan  5 07:43:45.587: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-subpath-test-configmap-jnt7 container test-container-subpath-configmap-jnt7: <nil>
STEP: delete the pod 01/05/23 07:43:45.593
Jan  5 07:43:45.615: INFO: Waiting for pod pod-subpath-test-configmap-jnt7 to disappear
Jan  5 07:43:45.618: INFO: Pod pod-subpath-test-configmap-jnt7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jnt7 01/05/23 07:43:45.618
Jan  5 07:43:45.618: INFO: Deleting pod "pod-subpath-test-configmap-jnt7" in namespace "subpath-6127"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  5 07:43:45.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6127" for this suite. 01/05/23 07:43:45.625
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":46,"skipped":1119,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.271 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:43:19.505
    Jan  5 07:43:19.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename subpath 01/05/23 07:43:19.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:43:19.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:43:19.543
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 07:43:19.545
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-jnt7 01/05/23 07:43:19.556
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 07:43:19.556
    Jan  5 07:43:19.579: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jnt7" in namespace "subpath-6127" to be "Succeeded or Failed"
    Jan  5 07:43:19.581: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.738406ms
    Jan  5 07:43:21.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005745231s
    Jan  5 07:43:23.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 4.00571152s
    Jan  5 07:43:25.584: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 6.005054775s
    Jan  5 07:43:27.587: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 8.007318906s
    Jan  5 07:43:29.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 10.005817522s
    Jan  5 07:43:31.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 12.005684791s
    Jan  5 07:43:33.586: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 14.006411545s
    Jan  5 07:43:35.587: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 16.007556881s
    Jan  5 07:43:37.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 18.005697343s
    Jan  5 07:43:39.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 20.005639842s
    Jan  5 07:43:41.586: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=true. Elapsed: 22.006294967s
    Jan  5 07:43:43.586: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Running", Reason="", readiness=false. Elapsed: 24.0061461s
    Jan  5 07:43:45.585: INFO: Pod "pod-subpath-test-configmap-jnt7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.005284216s
    STEP: Saw pod success 01/05/23 07:43:45.585
    Jan  5 07:43:45.585: INFO: Pod "pod-subpath-test-configmap-jnt7" satisfied condition "Succeeded or Failed"
    Jan  5 07:43:45.587: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-subpath-test-configmap-jnt7 container test-container-subpath-configmap-jnt7: <nil>
    STEP: delete the pod 01/05/23 07:43:45.593
    Jan  5 07:43:45.615: INFO: Waiting for pod pod-subpath-test-configmap-jnt7 to disappear
    Jan  5 07:43:45.618: INFO: Pod pod-subpath-test-configmap-jnt7 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-jnt7 01/05/23 07:43:45.618
    Jan  5 07:43:45.618: INFO: Deleting pod "pod-subpath-test-configmap-jnt7" in namespace "subpath-6127"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  5 07:43:45.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6127" for this suite. 01/05/23 07:43:45.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:43:45.777
Jan  5 07:43:45.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 07:43:45.777
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:43:45.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:43:45.794
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 07:43:45.809
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 07:43:46.413
STEP: Deploying the webhook pod 01/05/23 07:43:46.419
STEP: Wait for the deployment to be ready 01/05/23 07:43:46.431
Jan  5 07:43:46.434: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan  5 07:43:48.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 7, 43, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 7, 43, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 7, 43, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 7, 43, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/05/23 07:43:50.448
STEP: Verifying the service has paired with the endpoint 01/05/23 07:43:50.466
Jan  5 07:43:51.466: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/05/23 07:43:51.47
STEP: Creating a custom resource definition that should be denied by the webhook 01/05/23 07:43:51.487
Jan  5 07:43:51.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 07:43:51.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2393" for this suite. 01/05/23 07:43:51.506
STEP: Destroying namespace "webhook-2393-markers" for this suite. 01/05/23 07:43:51.519
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":47,"skipped":1130,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.791 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:43:45.777
    Jan  5 07:43:45.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 07:43:45.777
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:43:45.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:43:45.794
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 07:43:45.809
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 07:43:46.413
    STEP: Deploying the webhook pod 01/05/23 07:43:46.419
    STEP: Wait for the deployment to be ready 01/05/23 07:43:46.431
    Jan  5 07:43:46.434: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan  5 07:43:48.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 7, 43, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 7, 43, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 7, 43, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 7, 43, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/05/23 07:43:50.448
    STEP: Verifying the service has paired with the endpoint 01/05/23 07:43:50.466
    Jan  5 07:43:51.466: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/05/23 07:43:51.47
    STEP: Creating a custom resource definition that should be denied by the webhook 01/05/23 07:43:51.487
    Jan  5 07:43:51.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 07:43:51.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2393" for this suite. 01/05/23 07:43:51.506
    STEP: Destroying namespace "webhook-2393-markers" for this suite. 01/05/23 07:43:51.519
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:43:51.568
Jan  5 07:43:51.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 07:43:51.569
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:43:51.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:43:51.587
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/05/23 07:43:51.589
Jan  5 07:43:51.601: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0" in namespace "downward-api-7258" to be "Succeeded or Failed"
Jan  5 07:43:51.603: INFO: Pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.661149ms
Jan  5 07:43:53.606: INFO: Pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00451371s
Jan  5 07:43:55.606: INFO: Pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00497498s
Jan  5 07:43:57.607: INFO: Pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006176752s
STEP: Saw pod success 01/05/23 07:43:57.607
Jan  5 07:43:57.607: INFO: Pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0" satisfied condition "Succeeded or Failed"
Jan  5 07:43:57.610: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0 container client-container: <nil>
STEP: delete the pod 01/05/23 07:43:57.613
Jan  5 07:43:57.633: INFO: Waiting for pod downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0 to disappear
Jan  5 07:43:57.635: INFO: Pod downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 07:43:57.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7258" for this suite. 01/05/23 07:43:57.639
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":48,"skipped":1135,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.080 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:43:51.568
    Jan  5 07:43:51.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 07:43:51.569
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:43:51.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:43:51.587
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/05/23 07:43:51.589
    Jan  5 07:43:51.601: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0" in namespace "downward-api-7258" to be "Succeeded or Failed"
    Jan  5 07:43:51.603: INFO: Pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.661149ms
    Jan  5 07:43:53.606: INFO: Pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00451371s
    Jan  5 07:43:55.606: INFO: Pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00497498s
    Jan  5 07:43:57.607: INFO: Pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006176752s
    STEP: Saw pod success 01/05/23 07:43:57.607
    Jan  5 07:43:57.607: INFO: Pod "downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0" satisfied condition "Succeeded or Failed"
    Jan  5 07:43:57.610: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0 container client-container: <nil>
    STEP: delete the pod 01/05/23 07:43:57.613
    Jan  5 07:43:57.633: INFO: Waiting for pod downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0 to disappear
    Jan  5 07:43:57.635: INFO: Pod downwardapi-volume-f16ebd5f-5dfc-47aa-aa49-e2c0dcc15dc0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 07:43:57.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7258" for this suite. 01/05/23 07:43:57.639
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:43:57.648
Jan  5 07:43:57.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:43:57.65
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:43:57.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:43:57.68
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/05/23 07:43:57.683
Jan  5 07:43:57.693: INFO: Waiting up to 5m0s for pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77" in namespace "projected-819" to be "running and ready"
Jan  5 07:43:57.696: INFO: Pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.882402ms
Jan  5 07:43:57.696: INFO: The phase of Pod labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:43:59.699: INFO: Pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006218167s
Jan  5 07:43:59.699: INFO: The phase of Pod labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:44:01.699: INFO: Pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77": Phase="Running", Reason="", readiness=true. Elapsed: 4.00640508s
Jan  5 07:44:01.699: INFO: The phase of Pod labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77 is Running (Ready = true)
Jan  5 07:44:01.699: INFO: Pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77" satisfied condition "running and ready"
Jan  5 07:44:02.216: INFO: Successfully updated pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 07:44:04.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-819" for this suite. 01/05/23 07:44:04.229
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":49,"skipped":1135,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.594 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:43:57.648
    Jan  5 07:43:57.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:43:57.65
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:43:57.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:43:57.68
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/05/23 07:43:57.683
    Jan  5 07:43:57.693: INFO: Waiting up to 5m0s for pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77" in namespace "projected-819" to be "running and ready"
    Jan  5 07:43:57.696: INFO: Pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.882402ms
    Jan  5 07:43:57.696: INFO: The phase of Pod labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:43:59.699: INFO: Pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006218167s
    Jan  5 07:43:59.699: INFO: The phase of Pod labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:44:01.699: INFO: Pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77": Phase="Running", Reason="", readiness=true. Elapsed: 4.00640508s
    Jan  5 07:44:01.699: INFO: The phase of Pod labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77 is Running (Ready = true)
    Jan  5 07:44:01.699: INFO: Pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77" satisfied condition "running and ready"
    Jan  5 07:44:02.216: INFO: Successfully updated pod "labelsupdateb8ba7b64-b78d-491f-92a6-cd84cb91ef77"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 07:44:04.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-819" for this suite. 01/05/23 07:44:04.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:44:04.242
Jan  5 07:44:04.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename replication-controller 01/05/23 07:44:04.243
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:04.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:04.258
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/05/23 07:44:04.26
Jan  5 07:44:04.276: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7682" to be "running and ready"
Jan  5 07:44:04.278: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.803722ms
Jan  5 07:44:04.278: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:44:06.287: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.011460796s
Jan  5 07:44:06.287: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan  5 07:44:06.287: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/05/23 07:44:06.29
STEP: Then the orphan pod is adopted 01/05/23 07:44:06.293
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  5 07:44:07.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7682" for this suite. 01/05/23 07:44:07.302
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":50,"skipped":1151,"failed":0}
------------------------------
â€¢ [3.065 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:44:04.242
    Jan  5 07:44:04.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename replication-controller 01/05/23 07:44:04.243
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:04.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:04.258
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/05/23 07:44:04.26
    Jan  5 07:44:04.276: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7682" to be "running and ready"
    Jan  5 07:44:04.278: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.803722ms
    Jan  5 07:44:04.278: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:44:06.287: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.011460796s
    Jan  5 07:44:06.287: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan  5 07:44:06.287: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/05/23 07:44:06.29
    STEP: Then the orphan pod is adopted 01/05/23 07:44:06.293
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  5 07:44:07.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7682" for this suite. 01/05/23 07:44:07.302
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:44:07.308
Jan  5 07:44:07.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename subpath 01/05/23 07:44:07.309
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:07.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:07.333
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 07:44:07.338
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-c9bx 01/05/23 07:44:07.348
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 07:44:07.348
Jan  5 07:44:07.356: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c9bx" in namespace "subpath-7668" to be "Succeeded or Failed"
Jan  5 07:44:07.358: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.417442ms
Jan  5 07:44:09.361: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 2.005059398s
Jan  5 07:44:11.362: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 4.006691799s
Jan  5 07:44:13.363: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 6.00682517s
Jan  5 07:44:15.362: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 8.006187256s
Jan  5 07:44:17.361: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 10.004890691s
Jan  5 07:44:19.362: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 12.005921727s
Jan  5 07:44:21.364: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 14.008124979s
Jan  5 07:44:23.363: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 16.006968546s
Jan  5 07:44:25.361: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 18.005354374s
Jan  5 07:44:27.363: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 20.007074697s
Jan  5 07:44:29.362: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=false. Elapsed: 22.006171525s
Jan  5 07:44:31.362: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006195932s
STEP: Saw pod success 01/05/23 07:44:31.362
Jan  5 07:44:31.362: INFO: Pod "pod-subpath-test-configmap-c9bx" satisfied condition "Succeeded or Failed"
Jan  5 07:44:31.364: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-subpath-test-configmap-c9bx container test-container-subpath-configmap-c9bx: <nil>
STEP: delete the pod 01/05/23 07:44:31.37
Jan  5 07:44:31.384: INFO: Waiting for pod pod-subpath-test-configmap-c9bx to disappear
Jan  5 07:44:31.386: INFO: Pod pod-subpath-test-configmap-c9bx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c9bx 01/05/23 07:44:31.386
Jan  5 07:44:31.386: INFO: Deleting pod "pod-subpath-test-configmap-c9bx" in namespace "subpath-7668"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  5 07:44:31.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7668" for this suite. 01/05/23 07:44:31.389
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":51,"skipped":1155,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.092 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:44:07.308
    Jan  5 07:44:07.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename subpath 01/05/23 07:44:07.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:07.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:07.333
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 07:44:07.338
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-c9bx 01/05/23 07:44:07.348
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 07:44:07.348
    Jan  5 07:44:07.356: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c9bx" in namespace "subpath-7668" to be "Succeeded or Failed"
    Jan  5 07:44:07.358: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.417442ms
    Jan  5 07:44:09.361: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 2.005059398s
    Jan  5 07:44:11.362: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 4.006691799s
    Jan  5 07:44:13.363: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 6.00682517s
    Jan  5 07:44:15.362: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 8.006187256s
    Jan  5 07:44:17.361: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 10.004890691s
    Jan  5 07:44:19.362: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 12.005921727s
    Jan  5 07:44:21.364: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 14.008124979s
    Jan  5 07:44:23.363: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 16.006968546s
    Jan  5 07:44:25.361: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 18.005354374s
    Jan  5 07:44:27.363: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=true. Elapsed: 20.007074697s
    Jan  5 07:44:29.362: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Running", Reason="", readiness=false. Elapsed: 22.006171525s
    Jan  5 07:44:31.362: INFO: Pod "pod-subpath-test-configmap-c9bx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006195932s
    STEP: Saw pod success 01/05/23 07:44:31.362
    Jan  5 07:44:31.362: INFO: Pod "pod-subpath-test-configmap-c9bx" satisfied condition "Succeeded or Failed"
    Jan  5 07:44:31.364: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-subpath-test-configmap-c9bx container test-container-subpath-configmap-c9bx: <nil>
    STEP: delete the pod 01/05/23 07:44:31.37
    Jan  5 07:44:31.384: INFO: Waiting for pod pod-subpath-test-configmap-c9bx to disappear
    Jan  5 07:44:31.386: INFO: Pod pod-subpath-test-configmap-c9bx no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-c9bx 01/05/23 07:44:31.386
    Jan  5 07:44:31.386: INFO: Deleting pod "pod-subpath-test-configmap-c9bx" in namespace "subpath-7668"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  5 07:44:31.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7668" for this suite. 01/05/23 07:44:31.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:44:31.402
Jan  5 07:44:31.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 07:44:31.403
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:31.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:31.418
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-15818465-cf7b-457d-abfe-14e962372267 01/05/23 07:44:31.43
STEP: Creating configMap with name cm-test-opt-upd-674db689-4106-4f75-81a9-f460bb0f9e79 01/05/23 07:44:31.434
STEP: Creating the pod 01/05/23 07:44:31.441
Jan  5 07:44:31.449: INFO: Waiting up to 5m0s for pod "pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491" in namespace "configmap-6363" to be "running and ready"
Jan  5 07:44:31.451: INFO: Pod "pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.213879ms
Jan  5 07:44:31.451: INFO: The phase of Pod pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:44:33.455: INFO: Pod "pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006606519s
Jan  5 07:44:33.455: INFO: The phase of Pod pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:44:35.454: INFO: Pod "pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491": Phase="Running", Reason="", readiness=true. Elapsed: 4.005152828s
Jan  5 07:44:35.454: INFO: The phase of Pod pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491 is Running (Ready = true)
Jan  5 07:44:35.454: INFO: Pod "pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-15818465-cf7b-457d-abfe-14e962372267 01/05/23 07:44:35.466
STEP: Updating configmap cm-test-opt-upd-674db689-4106-4f75-81a9-f460bb0f9e79 01/05/23 07:44:35.47
STEP: Creating configMap with name cm-test-opt-create-80e64dbe-5a6a-4b9a-8db8-a8c902c9a8c8 01/05/23 07:44:35.491
STEP: waiting to observe update in volume 01/05/23 07:44:35.512
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 07:44:37.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6363" for this suite. 01/05/23 07:44:37.533
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":52,"skipped":1216,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.135 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:44:31.402
    Jan  5 07:44:31.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 07:44:31.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:31.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:31.418
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-15818465-cf7b-457d-abfe-14e962372267 01/05/23 07:44:31.43
    STEP: Creating configMap with name cm-test-opt-upd-674db689-4106-4f75-81a9-f460bb0f9e79 01/05/23 07:44:31.434
    STEP: Creating the pod 01/05/23 07:44:31.441
    Jan  5 07:44:31.449: INFO: Waiting up to 5m0s for pod "pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491" in namespace "configmap-6363" to be "running and ready"
    Jan  5 07:44:31.451: INFO: Pod "pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.213879ms
    Jan  5 07:44:31.451: INFO: The phase of Pod pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:44:33.455: INFO: Pod "pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006606519s
    Jan  5 07:44:33.455: INFO: The phase of Pod pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:44:35.454: INFO: Pod "pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491": Phase="Running", Reason="", readiness=true. Elapsed: 4.005152828s
    Jan  5 07:44:35.454: INFO: The phase of Pod pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491 is Running (Ready = true)
    Jan  5 07:44:35.454: INFO: Pod "pod-configmaps-a00eef3e-22d7-4751-a3be-a56c97237491" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-15818465-cf7b-457d-abfe-14e962372267 01/05/23 07:44:35.466
    STEP: Updating configmap cm-test-opt-upd-674db689-4106-4f75-81a9-f460bb0f9e79 01/05/23 07:44:35.47
    STEP: Creating configMap with name cm-test-opt-create-80e64dbe-5a6a-4b9a-8db8-a8c902c9a8c8 01/05/23 07:44:35.491
    STEP: waiting to observe update in volume 01/05/23 07:44:35.512
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 07:44:37.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6363" for this suite. 01/05/23 07:44:37.533
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:44:37.538
Jan  5 07:44:37.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 07:44:37.538
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:37.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:37.559
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 07:44:37.562
Jan  5 07:44:37.574: INFO: Waiting up to 5m0s for pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa" in namespace "emptydir-9352" to be "Succeeded or Failed"
Jan  5 07:44:37.576: INFO: Pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.891397ms
Jan  5 07:44:39.579: INFO: Pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa": Phase="Running", Reason="", readiness=true. Elapsed: 2.005096031s
Jan  5 07:44:41.579: INFO: Pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa": Phase="Running", Reason="", readiness=false. Elapsed: 4.005005226s
Jan  5 07:44:43.581: INFO: Pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007196013s
STEP: Saw pod success 01/05/23 07:44:43.581
Jan  5 07:44:43.581: INFO: Pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa" satisfied condition "Succeeded or Failed"
Jan  5 07:44:43.583: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-e016ccc6-1275-4cb4-9275-924306f53ffa container test-container: <nil>
STEP: delete the pod 01/05/23 07:44:43.587
Jan  5 07:44:43.600: INFO: Waiting for pod pod-e016ccc6-1275-4cb4-9275-924306f53ffa to disappear
Jan  5 07:44:43.602: INFO: Pod pod-e016ccc6-1275-4cb4-9275-924306f53ffa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 07:44:43.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9352" for this suite. 01/05/23 07:44:43.605
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":53,"skipped":1220,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.075 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:44:37.538
    Jan  5 07:44:37.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 07:44:37.538
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:37.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:37.559
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 07:44:37.562
    Jan  5 07:44:37.574: INFO: Waiting up to 5m0s for pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa" in namespace "emptydir-9352" to be "Succeeded or Failed"
    Jan  5 07:44:37.576: INFO: Pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.891397ms
    Jan  5 07:44:39.579: INFO: Pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa": Phase="Running", Reason="", readiness=true. Elapsed: 2.005096031s
    Jan  5 07:44:41.579: INFO: Pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa": Phase="Running", Reason="", readiness=false. Elapsed: 4.005005226s
    Jan  5 07:44:43.581: INFO: Pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007196013s
    STEP: Saw pod success 01/05/23 07:44:43.581
    Jan  5 07:44:43.581: INFO: Pod "pod-e016ccc6-1275-4cb4-9275-924306f53ffa" satisfied condition "Succeeded or Failed"
    Jan  5 07:44:43.583: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-e016ccc6-1275-4cb4-9275-924306f53ffa container test-container: <nil>
    STEP: delete the pod 01/05/23 07:44:43.587
    Jan  5 07:44:43.600: INFO: Waiting for pod pod-e016ccc6-1275-4cb4-9275-924306f53ffa to disappear
    Jan  5 07:44:43.602: INFO: Pod pod-e016ccc6-1275-4cb4-9275-924306f53ffa no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 07:44:43.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9352" for this suite. 01/05/23 07:44:43.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:44:43.613
Jan  5 07:44:43.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 07:44:43.614
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:43.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:43.681
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 07:44:43.683
Jan  5 07:44:43.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4706 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  5 07:44:43.757: INFO: stderr: ""
Jan  5 07:44:43.757: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/05/23 07:44:43.757
Jan  5 07:44:43.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4706 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan  5 07:44:44.223: INFO: stderr: ""
Jan  5 07:44:44.223: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 07:44:44.223
Jan  5 07:44:44.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4706 delete pods e2e-test-httpd-pod'
Jan  5 07:44:46.934: INFO: stderr: ""
Jan  5 07:44:46.934: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 07:44:46.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4706" for this suite. 01/05/23 07:44:46.936
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":54,"skipped":1238,"failed":0}
------------------------------
â€¢ [3.328 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:44:43.613
    Jan  5 07:44:43.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 07:44:43.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:43.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:43.681
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 07:44:43.683
    Jan  5 07:44:43.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4706 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  5 07:44:43.757: INFO: stderr: ""
    Jan  5 07:44:43.757: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/05/23 07:44:43.757
    Jan  5 07:44:43.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4706 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan  5 07:44:44.223: INFO: stderr: ""
    Jan  5 07:44:44.223: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 07:44:44.223
    Jan  5 07:44:44.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4706 delete pods e2e-test-httpd-pod'
    Jan  5 07:44:46.934: INFO: stderr: ""
    Jan  5 07:44:46.934: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 07:44:46.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4706" for this suite. 01/05/23 07:44:46.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:44:46.942
Jan  5 07:44:46.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 07:44:46.942
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:46.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:46.966
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 07:44:46.967
Jan  5 07:44:46.972: INFO: Waiting up to 5m0s for pod "pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c" in namespace "emptydir-3355" to be "Succeeded or Failed"
Jan  5 07:44:46.974: INFO: Pod "pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.484075ms
Jan  5 07:44:48.979: INFO: Pod "pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006092864s
Jan  5 07:44:50.977: INFO: Pod "pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004871149s
STEP: Saw pod success 01/05/23 07:44:50.977
Jan  5 07:44:50.978: INFO: Pod "pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c" satisfied condition "Succeeded or Failed"
Jan  5 07:44:50.980: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c container test-container: <nil>
STEP: delete the pod 01/05/23 07:44:50.984
Jan  5 07:44:50.998: INFO: Waiting for pod pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c to disappear
Jan  5 07:44:51.000: INFO: Pod pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 07:44:51.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3355" for this suite. 01/05/23 07:44:51.002
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":55,"skipped":1263,"failed":0}
------------------------------
â€¢ [4.071 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:44:46.942
    Jan  5 07:44:46.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 07:44:46.942
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:46.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:46.966
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 07:44:46.967
    Jan  5 07:44:46.972: INFO: Waiting up to 5m0s for pod "pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c" in namespace "emptydir-3355" to be "Succeeded or Failed"
    Jan  5 07:44:46.974: INFO: Pod "pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.484075ms
    Jan  5 07:44:48.979: INFO: Pod "pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006092864s
    Jan  5 07:44:50.977: INFO: Pod "pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004871149s
    STEP: Saw pod success 01/05/23 07:44:50.977
    Jan  5 07:44:50.978: INFO: Pod "pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c" satisfied condition "Succeeded or Failed"
    Jan  5 07:44:50.980: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c container test-container: <nil>
    STEP: delete the pod 01/05/23 07:44:50.984
    Jan  5 07:44:50.998: INFO: Waiting for pod pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c to disappear
    Jan  5 07:44:51.000: INFO: Pod pod-a3ad2260-5396-4a1c-9d43-2c2a41b06e2c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 07:44:51.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3355" for this suite. 01/05/23 07:44:51.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:44:51.013
Jan  5 07:44:51.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename daemonsets 01/05/23 07:44:51.014
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:51.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:51.058
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/05/23 07:44:51.073
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 07:44:51.081
Jan  5 07:44:51.082: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:44:51.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 07:44:51.084: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:44:52.087: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:44:52.089: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 07:44:52.089: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:44:53.087: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:44:53.089: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 07:44:53.089: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:44:54.088: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:44:54.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 07:44:54.090: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/05/23 07:44:54.092
Jan  5 07:44:54.105: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:44:54.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 07:44:54.113: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:44:55.118: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:44:55.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 07:44:55.120: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:44:56.115: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:44:56.117: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 07:44:56.117: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 07:44:57.116: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 07:44:57.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 07:44:57.119: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/05/23 07:44:57.119
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 07:44:57.122
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7193, will wait for the garbage collector to delete the pods 01/05/23 07:44:57.123
Jan  5 07:44:57.180: INFO: Deleting DaemonSet.extensions daemon-set took: 5.160469ms
Jan  5 07:44:57.281: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.642008ms
Jan  5 07:44:59.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 07:44:59.388: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 07:44:59.390: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8898"},"items":null}

Jan  5 07:44:59.391: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8898"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 07:44:59.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7193" for this suite. 01/05/23 07:44:59.398
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":56,"skipped":1270,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.389 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:44:51.013
    Jan  5 07:44:51.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename daemonsets 01/05/23 07:44:51.014
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:51.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:51.058
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/05/23 07:44:51.073
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 07:44:51.081
    Jan  5 07:44:51.082: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:44:51.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 07:44:51.084: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:44:52.087: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:44:52.089: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 07:44:52.089: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:44:53.087: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:44:53.089: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 07:44:53.089: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:44:54.088: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:44:54.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 07:44:54.090: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/05/23 07:44:54.092
    Jan  5 07:44:54.105: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:44:54.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 07:44:54.113: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:44:55.118: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:44:55.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 07:44:55.120: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:44:56.115: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:44:56.117: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 07:44:56.117: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 07:44:57.116: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 07:44:57.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 07:44:57.119: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/05/23 07:44:57.119
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 07:44:57.122
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7193, will wait for the garbage collector to delete the pods 01/05/23 07:44:57.123
    Jan  5 07:44:57.180: INFO: Deleting DaemonSet.extensions daemon-set took: 5.160469ms
    Jan  5 07:44:57.281: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.642008ms
    Jan  5 07:44:59.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 07:44:59.388: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 07:44:59.390: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8898"},"items":null}

    Jan  5 07:44:59.391: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8898"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 07:44:59.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7193" for this suite. 01/05/23 07:44:59.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:44:59.403
Jan  5 07:44:59.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sched-pred 01/05/23 07:44:59.404
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:59.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:59.419
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  5 07:44:59.421: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 07:44:59.425: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 07:44:59.426: INFO: 
Logging pods the apiserver thinks is on node mip-bd-vm722.mip.storage.hpecorp.net before test
Jan  5 07:44:59.429: INFO: csi-hostpathplugin-0 from default started at 2023-01-05 07:12:21 +0000 UTC (8 container statuses recorded)
Jan  5 07:44:59.429: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  5 07:44:59.429: INFO: 	Container csi-external-health-monitor-controller ready: true, restart count 1
Jan  5 07:44:59.429: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  5 07:44:59.429: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  5 07:44:59.429: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  5 07:44:59.429: INFO: 	Container hostpath ready: true, restart count 0
Jan  5 07:44:59.429: INFO: 	Container liveness-probe ready: true, restart count 0
Jan  5 07:44:59.429: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan  5 07:44:59.429: INFO: canal-6z7xb from kube-system started at 2023-01-05 07:12:02 +0000 UTC (2 container statuses recorded)
Jan  5 07:44:59.429: INFO: 	Container calico-node ready: true, restart count 0
Jan  5 07:44:59.429: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  5 07:44:59.429: INFO: coredns-564fd8c776-nwmff from kube-system started at 2023-01-05 07:12:21 +0000 UTC (1 container statuses recorded)
Jan  5 07:44:59.429: INFO: 	Container coredns ready: true, restart count 0
Jan  5 07:44:59.429: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2p88t from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
Jan  5 07:44:59.429: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 07:44:59.429: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 07:44:59.429: INFO: 
Logging pods the apiserver thinks is on node mip-bd-vm724.mip.storage.hpecorp.net before test
Jan  5 07:44:59.431: INFO: canal-6sfgp from kube-system started at 2023-01-05 07:21:40 +0000 UTC (2 container statuses recorded)
Jan  5 07:44:59.431: INFO: 	Container calico-node ready: true, restart count 0
Jan  5 07:44:59.431: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  5 07:44:59.431: INFO: sonobuoy from sonobuoy started at 2023-01-05 07:22:09 +0000 UTC (1 container statuses recorded)
Jan  5 07:44:59.431: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 07:44:59.431: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2qcx8 from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
Jan  5 07:44:59.431: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 07:44:59.431: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/05/23 07:44:59.431
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173759f5f15f4fac], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 01/05/23 07:44:59.455
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  5 07:45:00.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6738" for this suite. 01/05/23 07:45:00.448
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":57,"skipped":1306,"failed":0}
------------------------------
â€¢ [1.068 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:44:59.403
    Jan  5 07:44:59.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sched-pred 01/05/23 07:44:59.404
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:44:59.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:44:59.419
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  5 07:44:59.421: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 07:44:59.425: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 07:44:59.426: INFO: 
    Logging pods the apiserver thinks is on node mip-bd-vm722.mip.storage.hpecorp.net before test
    Jan  5 07:44:59.429: INFO: csi-hostpathplugin-0 from default started at 2023-01-05 07:12:21 +0000 UTC (8 container statuses recorded)
    Jan  5 07:44:59.429: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: 	Container csi-external-health-monitor-controller ready: true, restart count 1
    Jan  5 07:44:59.429: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: 	Container hostpath ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: canal-6z7xb from kube-system started at 2023-01-05 07:12:02 +0000 UTC (2 container statuses recorded)
    Jan  5 07:44:59.429: INFO: 	Container calico-node ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: coredns-564fd8c776-nwmff from kube-system started at 2023-01-05 07:12:21 +0000 UTC (1 container statuses recorded)
    Jan  5 07:44:59.429: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2p88t from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
    Jan  5 07:44:59.429: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 07:44:59.429: INFO: 
    Logging pods the apiserver thinks is on node mip-bd-vm724.mip.storage.hpecorp.net before test
    Jan  5 07:44:59.431: INFO: canal-6sfgp from kube-system started at 2023-01-05 07:21:40 +0000 UTC (2 container statuses recorded)
    Jan  5 07:44:59.431: INFO: 	Container calico-node ready: true, restart count 0
    Jan  5 07:44:59.431: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  5 07:44:59.431: INFO: sonobuoy from sonobuoy started at 2023-01-05 07:22:09 +0000 UTC (1 container statuses recorded)
    Jan  5 07:44:59.431: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 07:44:59.431: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2qcx8 from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
    Jan  5 07:44:59.431: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 07:44:59.431: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/05/23 07:44:59.431
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173759f5f15f4fac], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 01/05/23 07:44:59.455
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 07:45:00.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6738" for this suite. 01/05/23 07:45:00.448
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:45:00.472
Jan  5 07:45:00.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename containers 01/05/23 07:45:00.472
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:45:00.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:45:00.491
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/05/23 07:45:00.493
Jan  5 07:45:00.502: INFO: Waiting up to 5m0s for pod "client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3" in namespace "containers-1684" to be "Succeeded or Failed"
Jan  5 07:45:00.505: INFO: Pod "client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.615376ms
Jan  5 07:45:02.509: INFO: Pod "client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00635051s
Jan  5 07:45:04.512: INFO: Pod "client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00961291s
STEP: Saw pod success 01/05/23 07:45:04.512
Jan  5 07:45:04.512: INFO: Pod "client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3" satisfied condition "Succeeded or Failed"
Jan  5 07:45:04.514: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 07:45:04.517
Jan  5 07:45:04.570: INFO: Waiting for pod client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3 to disappear
Jan  5 07:45:04.577: INFO: Pod client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  5 07:45:04.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1684" for this suite. 01/05/23 07:45:04.579
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":58,"skipped":1308,"failed":0}
------------------------------
â€¢ [4.133 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:45:00.472
    Jan  5 07:45:00.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename containers 01/05/23 07:45:00.472
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:45:00.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:45:00.491
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/05/23 07:45:00.493
    Jan  5 07:45:00.502: INFO: Waiting up to 5m0s for pod "client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3" in namespace "containers-1684" to be "Succeeded or Failed"
    Jan  5 07:45:00.505: INFO: Pod "client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.615376ms
    Jan  5 07:45:02.509: INFO: Pod "client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00635051s
    Jan  5 07:45:04.512: INFO: Pod "client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00961291s
    STEP: Saw pod success 01/05/23 07:45:04.512
    Jan  5 07:45:04.512: INFO: Pod "client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3" satisfied condition "Succeeded or Failed"
    Jan  5 07:45:04.514: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 07:45:04.517
    Jan  5 07:45:04.570: INFO: Waiting for pod client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3 to disappear
    Jan  5 07:45:04.577: INFO: Pod client-containers-248beee5-3d95-4838-a5de-be76a9c03aa3 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  5 07:45:04.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1684" for this suite. 01/05/23 07:45:04.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:45:04.605
Jan  5 07:45:04.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubelet-test 01/05/23 07:45:04.605
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:45:04.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:45:04.644
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/05/23 07:45:04.677
Jan  5 07:45:04.677: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7" in namespace "kubelet-test-2648" to be "completed"
Jan  5 07:45:04.682: INFO: Pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59079ms
Jan  5 07:45:06.684: INFO: Pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007236325s
Jan  5 07:45:08.686: INFO: Pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7": Phase="Running", Reason="", readiness=false. Elapsed: 4.008981867s
Jan  5 07:45:10.685: INFO: Pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007734726s
Jan  5 07:45:10.685: INFO: Pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  5 07:45:10.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2648" for this suite. 01/05/23 07:45:10.691
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":59,"skipped":1315,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.090 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:45:04.605
    Jan  5 07:45:04.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 07:45:04.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:45:04.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:45:04.644
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/05/23 07:45:04.677
    Jan  5 07:45:04.677: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7" in namespace "kubelet-test-2648" to be "completed"
    Jan  5 07:45:04.682: INFO: Pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59079ms
    Jan  5 07:45:06.684: INFO: Pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007236325s
    Jan  5 07:45:08.686: INFO: Pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7": Phase="Running", Reason="", readiness=false. Elapsed: 4.008981867s
    Jan  5 07:45:10.685: INFO: Pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007734726s
    Jan  5 07:45:10.685: INFO: Pod "agnhost-host-aliases78420aed-1499-4e93-b290-57806c5989e7" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  5 07:45:10.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2648" for this suite. 01/05/23 07:45:10.691
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:45:10.695
Jan  5 07:45:10.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:45:10.695
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:45:10.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:45:10.73
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/05/23 07:45:10.735
Jan  5 07:45:10.740: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb" in namespace "projected-8560" to be "Succeeded or Failed"
Jan  5 07:45:10.742: INFO: Pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.588879ms
Jan  5 07:45:12.745: INFO: Pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004479487s
Jan  5 07:45:14.745: INFO: Pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005023873s
Jan  5 07:45:16.746: INFO: Pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005455507s
STEP: Saw pod success 01/05/23 07:45:16.746
Jan  5 07:45:16.746: INFO: Pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb" satisfied condition "Succeeded or Failed"
Jan  5 07:45:16.749: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb container client-container: <nil>
STEP: delete the pod 01/05/23 07:45:16.757
Jan  5 07:45:16.769: INFO: Waiting for pod downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb to disappear
Jan  5 07:45:16.772: INFO: Pod downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 07:45:16.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8560" for this suite. 01/05/23 07:45:16.774
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":60,"skipped":1318,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.086 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:45:10.695
    Jan  5 07:45:10.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:45:10.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:45:10.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:45:10.73
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/05/23 07:45:10.735
    Jan  5 07:45:10.740: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb" in namespace "projected-8560" to be "Succeeded or Failed"
    Jan  5 07:45:10.742: INFO: Pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.588879ms
    Jan  5 07:45:12.745: INFO: Pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004479487s
    Jan  5 07:45:14.745: INFO: Pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005023873s
    Jan  5 07:45:16.746: INFO: Pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005455507s
    STEP: Saw pod success 01/05/23 07:45:16.746
    Jan  5 07:45:16.746: INFO: Pod "downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb" satisfied condition "Succeeded or Failed"
    Jan  5 07:45:16.749: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb container client-container: <nil>
    STEP: delete the pod 01/05/23 07:45:16.757
    Jan  5 07:45:16.769: INFO: Waiting for pod downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb to disappear
    Jan  5 07:45:16.772: INFO: Pod downwardapi-volume-42af3541-2118-449e-bd0d-72fa28fa04bb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 07:45:16.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8560" for this suite. 01/05/23 07:45:16.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:45:16.782
Jan  5 07:45:16.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename ingress 01/05/23 07:45:16.783
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:45:16.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:45:16.799
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/05/23 07:45:16.801
STEP: getting /apis/networking.k8s.io 01/05/23 07:45:16.802
STEP: getting /apis/networking.k8s.iov1 01/05/23 07:45:16.803
STEP: creating 01/05/23 07:45:16.803
STEP: getting 01/05/23 07:45:16.826
STEP: listing 01/05/23 07:45:16.828
STEP: watching 01/05/23 07:45:16.829
Jan  5 07:45:16.829: INFO: starting watch
STEP: cluster-wide listing 01/05/23 07:45:16.83
STEP: cluster-wide watching 01/05/23 07:45:16.831
Jan  5 07:45:16.831: INFO: starting watch
STEP: patching 01/05/23 07:45:16.832
STEP: updating 01/05/23 07:45:16.836
Jan  5 07:45:16.849: INFO: waiting for watch events with expected annotations
Jan  5 07:45:16.849: INFO: saw patched and updated annotations
STEP: patching /status 01/05/23 07:45:16.849
STEP: updating /status 01/05/23 07:45:16.852
STEP: get /status 01/05/23 07:45:16.861
STEP: deleting 01/05/23 07:45:16.863
STEP: deleting a collection 01/05/23 07:45:16.869
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan  5 07:45:16.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3275" for this suite. 01/05/23 07:45:16.886
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":61,"skipped":1336,"failed":0}
------------------------------
â€¢ [0.114 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:45:16.782
    Jan  5 07:45:16.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename ingress 01/05/23 07:45:16.783
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:45:16.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:45:16.799
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/05/23 07:45:16.801
    STEP: getting /apis/networking.k8s.io 01/05/23 07:45:16.802
    STEP: getting /apis/networking.k8s.iov1 01/05/23 07:45:16.803
    STEP: creating 01/05/23 07:45:16.803
    STEP: getting 01/05/23 07:45:16.826
    STEP: listing 01/05/23 07:45:16.828
    STEP: watching 01/05/23 07:45:16.829
    Jan  5 07:45:16.829: INFO: starting watch
    STEP: cluster-wide listing 01/05/23 07:45:16.83
    STEP: cluster-wide watching 01/05/23 07:45:16.831
    Jan  5 07:45:16.831: INFO: starting watch
    STEP: patching 01/05/23 07:45:16.832
    STEP: updating 01/05/23 07:45:16.836
    Jan  5 07:45:16.849: INFO: waiting for watch events with expected annotations
    Jan  5 07:45:16.849: INFO: saw patched and updated annotations
    STEP: patching /status 01/05/23 07:45:16.849
    STEP: updating /status 01/05/23 07:45:16.852
    STEP: get /status 01/05/23 07:45:16.861
    STEP: deleting 01/05/23 07:45:16.863
    STEP: deleting a collection 01/05/23 07:45:16.869
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan  5 07:45:16.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-3275" for this suite. 01/05/23 07:45:16.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:45:16.897
Jan  5 07:45:16.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename taint-multiple-pods 01/05/23 07:45:16.897
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:45:16.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:45:16.919
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan  5 07:45:16.921: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 07:46:16.935: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jan  5 07:46:16.937: INFO: Starting informer...
STEP: Starting pods... 01/05/23 07:46:16.937
Jan  5 07:46:17.153: INFO: Pod1 is running on mip-bd-vm724.mip.storage.hpecorp.net. Tainting Node
Jan  5 07:46:17.360: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-283" to be "running"
Jan  5 07:46:17.363: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155761ms
Jan  5 07:46:19.366: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005975946s
Jan  5 07:46:19.366: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan  5 07:46:19.366: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-283" to be "running"
Jan  5 07:46:19.368: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.844037ms
Jan  5 07:46:19.368: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan  5 07:46:19.368: INFO: Pod2 is running on mip-bd-vm724.mip.storage.hpecorp.net. Tainting Node
STEP: Trying to apply a taint on the Node 01/05/23 07:46:19.368
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 07:46:19.377
STEP: Waiting for Pod1 and Pod2 to be deleted 01/05/23 07:46:19.38
Jan  5 07:46:25.146: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan  5 07:46:45.186: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 07:46:45.195
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jan  5 07:46:45.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-283" for this suite. 01/05/23 07:46:45.2
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":62,"skipped":1356,"failed":0}
------------------------------
â€¢ [SLOW TEST] [88.318 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:45:16.897
    Jan  5 07:45:16.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename taint-multiple-pods 01/05/23 07:45:16.897
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:45:16.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:45:16.919
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jan  5 07:45:16.921: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 07:46:16.935: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jan  5 07:46:16.937: INFO: Starting informer...
    STEP: Starting pods... 01/05/23 07:46:16.937
    Jan  5 07:46:17.153: INFO: Pod1 is running on mip-bd-vm724.mip.storage.hpecorp.net. Tainting Node
    Jan  5 07:46:17.360: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-283" to be "running"
    Jan  5 07:46:17.363: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155761ms
    Jan  5 07:46:19.366: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005975946s
    Jan  5 07:46:19.366: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan  5 07:46:19.366: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-283" to be "running"
    Jan  5 07:46:19.368: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.844037ms
    Jan  5 07:46:19.368: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan  5 07:46:19.368: INFO: Pod2 is running on mip-bd-vm724.mip.storage.hpecorp.net. Tainting Node
    STEP: Trying to apply a taint on the Node 01/05/23 07:46:19.368
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 07:46:19.377
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/05/23 07:46:19.38
    Jan  5 07:46:25.146: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan  5 07:46:45.186: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 07:46:45.195
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 07:46:45.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-283" for this suite. 01/05/23 07:46:45.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:46:45.216
Jan  5 07:46:45.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 07:46:45.217
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:46:45.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:46:45.237
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 07:46:45.24
Jan  5 07:46:45.250: INFO: Waiting up to 5m0s for pod "pod-01a59e6d-eca6-4a99-8469-584af49c5a32" in namespace "emptydir-9284" to be "Succeeded or Failed"
Jan  5 07:46:45.253: INFO: Pod "pod-01a59e6d-eca6-4a99-8469-584af49c5a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.203469ms
Jan  5 07:46:47.257: INFO: Pod "pod-01a59e6d-eca6-4a99-8469-584af49c5a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006172087s
Jan  5 07:46:49.256: INFO: Pod "pod-01a59e6d-eca6-4a99-8469-584af49c5a32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005584525s
STEP: Saw pod success 01/05/23 07:46:49.256
Jan  5 07:46:49.256: INFO: Pod "pod-01a59e6d-eca6-4a99-8469-584af49c5a32" satisfied condition "Succeeded or Failed"
Jan  5 07:46:49.258: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-01a59e6d-eca6-4a99-8469-584af49c5a32 container test-container: <nil>
STEP: delete the pod 01/05/23 07:46:49.272
Jan  5 07:46:49.286: INFO: Waiting for pod pod-01a59e6d-eca6-4a99-8469-584af49c5a32 to disappear
Jan  5 07:46:49.288: INFO: Pod pod-01a59e6d-eca6-4a99-8469-584af49c5a32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 07:46:49.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9284" for this suite. 01/05/23 07:46:49.291
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":63,"skipped":1383,"failed":0}
------------------------------
â€¢ [4.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:46:45.216
    Jan  5 07:46:45.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 07:46:45.217
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:46:45.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:46:45.237
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 07:46:45.24
    Jan  5 07:46:45.250: INFO: Waiting up to 5m0s for pod "pod-01a59e6d-eca6-4a99-8469-584af49c5a32" in namespace "emptydir-9284" to be "Succeeded or Failed"
    Jan  5 07:46:45.253: INFO: Pod "pod-01a59e6d-eca6-4a99-8469-584af49c5a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.203469ms
    Jan  5 07:46:47.257: INFO: Pod "pod-01a59e6d-eca6-4a99-8469-584af49c5a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006172087s
    Jan  5 07:46:49.256: INFO: Pod "pod-01a59e6d-eca6-4a99-8469-584af49c5a32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005584525s
    STEP: Saw pod success 01/05/23 07:46:49.256
    Jan  5 07:46:49.256: INFO: Pod "pod-01a59e6d-eca6-4a99-8469-584af49c5a32" satisfied condition "Succeeded or Failed"
    Jan  5 07:46:49.258: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-01a59e6d-eca6-4a99-8469-584af49c5a32 container test-container: <nil>
    STEP: delete the pod 01/05/23 07:46:49.272
    Jan  5 07:46:49.286: INFO: Waiting for pod pod-01a59e6d-eca6-4a99-8469-584af49c5a32 to disappear
    Jan  5 07:46:49.288: INFO: Pod pod-01a59e6d-eca6-4a99-8469-584af49c5a32 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 07:46:49.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9284" for this suite. 01/05/23 07:46:49.291
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:46:49.297
Jan  5 07:46:49.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:46:49.298
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:46:49.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:46:49.34
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-6495b148-0c73-413d-afff-c3df149e81b5 01/05/23 07:46:49.343
STEP: Creating a pod to test consume secrets 01/05/23 07:46:49.348
Jan  5 07:46:49.361: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea" in namespace "projected-3527" to be "Succeeded or Failed"
Jan  5 07:46:49.364: INFO: Pod "pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.715146ms
Jan  5 07:46:51.368: INFO: Pod "pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006873518s
Jan  5 07:46:53.367: INFO: Pod "pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005711681s
STEP: Saw pod success 01/05/23 07:46:53.367
Jan  5 07:46:53.367: INFO: Pod "pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea" satisfied condition "Succeeded or Failed"
Jan  5 07:46:53.369: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 07:46:53.373
Jan  5 07:46:53.399: INFO: Waiting for pod pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea to disappear
Jan  5 07:46:53.401: INFO: Pod pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 07:46:53.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3527" for this suite. 01/05/23 07:46:53.403
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":64,"skipped":1383,"failed":0}
------------------------------
â€¢ [4.117 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:46:49.297
    Jan  5 07:46:49.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:46:49.298
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:46:49.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:46:49.34
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-6495b148-0c73-413d-afff-c3df149e81b5 01/05/23 07:46:49.343
    STEP: Creating a pod to test consume secrets 01/05/23 07:46:49.348
    Jan  5 07:46:49.361: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea" in namespace "projected-3527" to be "Succeeded or Failed"
    Jan  5 07:46:49.364: INFO: Pod "pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.715146ms
    Jan  5 07:46:51.368: INFO: Pod "pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006873518s
    Jan  5 07:46:53.367: INFO: Pod "pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005711681s
    STEP: Saw pod success 01/05/23 07:46:53.367
    Jan  5 07:46:53.367: INFO: Pod "pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea" satisfied condition "Succeeded or Failed"
    Jan  5 07:46:53.369: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 07:46:53.373
    Jan  5 07:46:53.399: INFO: Waiting for pod pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea to disappear
    Jan  5 07:46:53.401: INFO: Pod pod-projected-secrets-540823ea-09f9-438f-a1b0-743bd3b952ea no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 07:46:53.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3527" for this suite. 01/05/23 07:46:53.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:46:53.414
Jan  5 07:46:53.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 07:46:53.415
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:46:53.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:46:53.433
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-2326 01/05/23 07:46:53.434
STEP: creating service affinity-clusterip-transition in namespace services-2326 01/05/23 07:46:53.434
STEP: creating replication controller affinity-clusterip-transition in namespace services-2326 01/05/23 07:46:53.449
I0105 07:46:53.460289      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2326, replica count: 3
I0105 07:46:56.512518      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 07:46:56.516: INFO: Creating new exec pod
Jan  5 07:46:56.519: INFO: Waiting up to 5m0s for pod "execpod-affinityd2ldx" in namespace "services-2326" to be "running"
Jan  5 07:46:56.521: INFO: Pod "execpod-affinityd2ldx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.438889ms
Jan  5 07:46:58.526: INFO: Pod "execpod-affinityd2ldx": Phase="Running", Reason="", readiness=true. Elapsed: 2.007078394s
Jan  5 07:46:58.526: INFO: Pod "execpod-affinityd2ldx" satisfied condition "running"
Jan  5 07:46:59.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2326 exec execpod-affinityd2ldx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan  5 07:46:59.659: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan  5 07:46:59.659: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 07:46:59.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2326 exec execpod-affinityd2ldx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.193.5 80'
Jan  5 07:46:59.773: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.193.5 80\nConnection to 10.97.193.5 80 port [tcp/http] succeeded!\n"
Jan  5 07:46:59.773: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 07:46:59.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2326 exec execpod-affinityd2ldx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.193.5:80/ ; done'
Jan  5 07:47:00.061: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n"
Jan  5 07:47:00.061: INFO: stdout: "\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-fl9w6\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-fl9w6\naffinity-clusterip-transition-fl9w6\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-fl9w6\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-tt9f4"
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-fl9w6
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-fl9w6
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-fl9w6
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-fl9w6
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
Jan  5 07:47:00.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2326 exec execpod-affinityd2ldx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.193.5:80/ ; done'
Jan  5 07:47:00.329: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n"
Jan  5 07:47:00.329: INFO: stdout: "\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc"
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
Jan  5 07:47:00.329: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2326, will wait for the garbage collector to delete the pods 01/05/23 07:47:00.348
Jan  5 07:47:00.407: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.022482ms
Jan  5 07:47:00.508: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.869085ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 07:47:02.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2326" for this suite. 01/05/23 07:47:02.625
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":65,"skipped":1398,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.236 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:46:53.414
    Jan  5 07:46:53.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 07:46:53.415
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:46:53.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:46:53.433
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-2326 01/05/23 07:46:53.434
    STEP: creating service affinity-clusterip-transition in namespace services-2326 01/05/23 07:46:53.434
    STEP: creating replication controller affinity-clusterip-transition in namespace services-2326 01/05/23 07:46:53.449
    I0105 07:46:53.460289      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2326, replica count: 3
    I0105 07:46:56.512518      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 07:46:56.516: INFO: Creating new exec pod
    Jan  5 07:46:56.519: INFO: Waiting up to 5m0s for pod "execpod-affinityd2ldx" in namespace "services-2326" to be "running"
    Jan  5 07:46:56.521: INFO: Pod "execpod-affinityd2ldx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.438889ms
    Jan  5 07:46:58.526: INFO: Pod "execpod-affinityd2ldx": Phase="Running", Reason="", readiness=true. Elapsed: 2.007078394s
    Jan  5 07:46:58.526: INFO: Pod "execpod-affinityd2ldx" satisfied condition "running"
    Jan  5 07:46:59.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2326 exec execpod-affinityd2ldx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan  5 07:46:59.659: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan  5 07:46:59.659: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 07:46:59.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2326 exec execpod-affinityd2ldx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.193.5 80'
    Jan  5 07:46:59.773: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.193.5 80\nConnection to 10.97.193.5 80 port [tcp/http] succeeded!\n"
    Jan  5 07:46:59.773: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 07:46:59.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2326 exec execpod-affinityd2ldx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.193.5:80/ ; done'
    Jan  5 07:47:00.061: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n"
    Jan  5 07:47:00.061: INFO: stdout: "\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-fl9w6\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-fl9w6\naffinity-clusterip-transition-fl9w6\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-fl9w6\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-tt9f4\naffinity-clusterip-transition-tt9f4"
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-fl9w6
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-fl9w6
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-fl9w6
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-fl9w6
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
    Jan  5 07:47:00.061: INFO: Received response from host: affinity-clusterip-transition-tt9f4
    Jan  5 07:47:00.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-2326 exec execpod-affinityd2ldx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.193.5:80/ ; done'
    Jan  5 07:47:00.329: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.193.5:80/\n"
    Jan  5 07:47:00.329: INFO: stdout: "\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc\naffinity-clusterip-transition-r7mpc"
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Received response from host: affinity-clusterip-transition-r7mpc
    Jan  5 07:47:00.329: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2326, will wait for the garbage collector to delete the pods 01/05/23 07:47:00.348
    Jan  5 07:47:00.407: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.022482ms
    Jan  5 07:47:00.508: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.869085ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 07:47:02.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2326" for this suite. 01/05/23 07:47:02.625
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:47:02.651
Jan  5 07:47:02.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 07:47:02.652
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:47:02.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:47:02.667
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan  5 07:47:02.709: INFO: Waiting up to 5m0s for pod "pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216" in namespace "emptydir-wrapper-5944" to be "running and ready"
Jan  5 07:47:02.711: INFO: Pod "pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216": Phase="Pending", Reason="", readiness=false. Elapsed: 1.926182ms
Jan  5 07:47:02.711: INFO: The phase of Pod pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:47:04.713: INFO: Pod "pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216": Phase="Running", Reason="", readiness=true. Elapsed: 2.004692155s
Jan  5 07:47:04.713: INFO: The phase of Pod pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216 is Running (Ready = true)
Jan  5 07:47:04.713: INFO: Pod "pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/05/23 07:47:04.716
STEP: Cleaning up the configmap 01/05/23 07:47:04.72
STEP: Cleaning up the pod 01/05/23 07:47:04.727
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan  5 07:47:04.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5944" for this suite. 01/05/23 07:47:04.739
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":66,"skipped":1402,"failed":0}
------------------------------
â€¢ [2.091 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:47:02.651
    Jan  5 07:47:02.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 07:47:02.652
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:47:02.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:47:02.667
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan  5 07:47:02.709: INFO: Waiting up to 5m0s for pod "pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216" in namespace "emptydir-wrapper-5944" to be "running and ready"
    Jan  5 07:47:02.711: INFO: Pod "pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216": Phase="Pending", Reason="", readiness=false. Elapsed: 1.926182ms
    Jan  5 07:47:02.711: INFO: The phase of Pod pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:47:04.713: INFO: Pod "pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216": Phase="Running", Reason="", readiness=true. Elapsed: 2.004692155s
    Jan  5 07:47:04.713: INFO: The phase of Pod pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216 is Running (Ready = true)
    Jan  5 07:47:04.713: INFO: Pod "pod-secrets-d3b7b365-d45b-4a7d-8c8e-11f5a9e14216" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/05/23 07:47:04.716
    STEP: Cleaning up the configmap 01/05/23 07:47:04.72
    STEP: Cleaning up the pod 01/05/23 07:47:04.727
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan  5 07:47:04.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-5944" for this suite. 01/05/23 07:47:04.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:47:04.743
Jan  5 07:47:04.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename subpath 01/05/23 07:47:04.743
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:47:04.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:47:04.762
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 07:47:04.766
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-tm46 01/05/23 07:47:04.775
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 07:47:04.775
Jan  5 07:47:04.781: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tm46" in namespace "subpath-8838" to be "Succeeded or Failed"
Jan  5 07:47:04.791: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Pending", Reason="", readiness=false. Elapsed: 9.7299ms
Jan  5 07:47:06.795: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 2.013905975s
Jan  5 07:47:08.794: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 4.013134876s
Jan  5 07:47:10.794: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 6.01295076s
Jan  5 07:47:12.794: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 8.012820804s
Jan  5 07:47:14.794: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 10.01303338s
Jan  5 07:47:16.795: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 12.013819602s
Jan  5 07:47:18.795: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 14.014075351s
Jan  5 07:47:20.794: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 16.012895521s
Jan  5 07:47:22.795: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 18.013998152s
Jan  5 07:47:24.795: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 20.014009779s
Jan  5 07:47:26.796: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=false. Elapsed: 22.014765975s
Jan  5 07:47:28.796: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014580161s
STEP: Saw pod success 01/05/23 07:47:28.796
Jan  5 07:47:28.796: INFO: Pod "pod-subpath-test-downwardapi-tm46" satisfied condition "Succeeded or Failed"
Jan  5 07:47:28.798: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-subpath-test-downwardapi-tm46 container test-container-subpath-downwardapi-tm46: <nil>
STEP: delete the pod 01/05/23 07:47:28.804
Jan  5 07:47:28.820: INFO: Waiting for pod pod-subpath-test-downwardapi-tm46 to disappear
Jan  5 07:47:28.821: INFO: Pod pod-subpath-test-downwardapi-tm46 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-tm46 01/05/23 07:47:28.821
Jan  5 07:47:28.821: INFO: Deleting pod "pod-subpath-test-downwardapi-tm46" in namespace "subpath-8838"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  5 07:47:28.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8838" for this suite. 01/05/23 07:47:28.834
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":67,"skipped":1409,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.096 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:47:04.743
    Jan  5 07:47:04.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename subpath 01/05/23 07:47:04.743
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:47:04.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:47:04.762
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 07:47:04.766
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-tm46 01/05/23 07:47:04.775
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 07:47:04.775
    Jan  5 07:47:04.781: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tm46" in namespace "subpath-8838" to be "Succeeded or Failed"
    Jan  5 07:47:04.791: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Pending", Reason="", readiness=false. Elapsed: 9.7299ms
    Jan  5 07:47:06.795: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 2.013905975s
    Jan  5 07:47:08.794: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 4.013134876s
    Jan  5 07:47:10.794: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 6.01295076s
    Jan  5 07:47:12.794: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 8.012820804s
    Jan  5 07:47:14.794: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 10.01303338s
    Jan  5 07:47:16.795: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 12.013819602s
    Jan  5 07:47:18.795: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 14.014075351s
    Jan  5 07:47:20.794: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 16.012895521s
    Jan  5 07:47:22.795: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 18.013998152s
    Jan  5 07:47:24.795: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=true. Elapsed: 20.014009779s
    Jan  5 07:47:26.796: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Running", Reason="", readiness=false. Elapsed: 22.014765975s
    Jan  5 07:47:28.796: INFO: Pod "pod-subpath-test-downwardapi-tm46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014580161s
    STEP: Saw pod success 01/05/23 07:47:28.796
    Jan  5 07:47:28.796: INFO: Pod "pod-subpath-test-downwardapi-tm46" satisfied condition "Succeeded or Failed"
    Jan  5 07:47:28.798: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-subpath-test-downwardapi-tm46 container test-container-subpath-downwardapi-tm46: <nil>
    STEP: delete the pod 01/05/23 07:47:28.804
    Jan  5 07:47:28.820: INFO: Waiting for pod pod-subpath-test-downwardapi-tm46 to disappear
    Jan  5 07:47:28.821: INFO: Pod pod-subpath-test-downwardapi-tm46 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-tm46 01/05/23 07:47:28.821
    Jan  5 07:47:28.821: INFO: Deleting pod "pod-subpath-test-downwardapi-tm46" in namespace "subpath-8838"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  5 07:47:28.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8838" for this suite. 01/05/23 07:47:28.834
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:47:28.839
Jan  5 07:47:28.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 07:47:28.84
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:47:28.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:47:28.872
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-9968 01/05/23 07:47:28.875
Jan  5 07:47:28.886: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9968" to be "running and ready"
Jan  5 07:47:28.888: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055283ms
Jan  5 07:47:28.888: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:47:30.890: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.004596715s
Jan  5 07:47:30.890: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan  5 07:47:30.890: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan  5 07:47:30.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan  5 07:47:31.034: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan  5 07:47:31.034: INFO: stdout: "iptables"
Jan  5 07:47:31.034: INFO: proxyMode: iptables
Jan  5 07:47:31.053: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan  5 07:47:31.059: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9968 01/05/23 07:47:31.059
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9968 01/05/23 07:47:31.074
I0105 07:47:31.085876      23 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9968, replica count: 3
I0105 07:47:34.137576      23 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 07:47:34.140: INFO: Creating new exec pod
Jan  5 07:47:34.144: INFO: Waiting up to 5m0s for pod "execpod-affinityzlq89" in namespace "services-9968" to be "running"
Jan  5 07:47:34.146: INFO: Pod "execpod-affinityzlq89": Phase="Pending", Reason="", readiness=false. Elapsed: 1.912416ms
Jan  5 07:47:36.150: INFO: Pod "execpod-affinityzlq89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005244059s
Jan  5 07:47:38.149: INFO: Pod "execpod-affinityzlq89": Phase="Running", Reason="", readiness=true. Elapsed: 4.004661516s
Jan  5 07:47:38.149: INFO: Pod "execpod-affinityzlq89" satisfied condition "running"
Jan  5 07:47:39.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec execpod-affinityzlq89 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan  5 07:47:39.274: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan  5 07:47:39.274: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 07:47:39.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec execpod-affinityzlq89 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.201.69 80'
Jan  5 07:47:39.415: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.201.69 80\nConnection to 10.99.201.69 80 port [tcp/http] succeeded!\n"
Jan  5 07:47:39.415: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 07:47:39.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec execpod-affinityzlq89 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.201.69:80/ ; done'
Jan  5 07:47:39.637: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n"
Jan  5 07:47:39.637: INFO: stdout: "\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z"
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
Jan  5 07:47:39.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec execpod-affinityzlq89 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.99.201.69:80/'
Jan  5 07:47:39.758: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n"
Jan  5 07:47:39.758: INFO: stdout: "affinity-clusterip-timeout-z858z"
Jan  5 07:47:59.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec execpod-affinityzlq89 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.99.201.69:80/'
Jan  5 07:47:59.896: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n"
Jan  5 07:47:59.896: INFO: stdout: "affinity-clusterip-timeout-khlp4"
Jan  5 07:47:59.896: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9968, will wait for the garbage collector to delete the pods 01/05/23 07:47:59.921
Jan  5 07:47:59.988: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 13.510683ms
Jan  5 07:48:00.088: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.317909ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 07:48:01.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9968" for this suite. 01/05/23 07:48:01.716
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":68,"skipped":1409,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.884 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:47:28.839
    Jan  5 07:47:28.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 07:47:28.84
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:47:28.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:47:28.872
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-9968 01/05/23 07:47:28.875
    Jan  5 07:47:28.886: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9968" to be "running and ready"
    Jan  5 07:47:28.888: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055283ms
    Jan  5 07:47:28.888: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:47:30.890: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.004596715s
    Jan  5 07:47:30.890: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan  5 07:47:30.890: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan  5 07:47:30.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan  5 07:47:31.034: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan  5 07:47:31.034: INFO: stdout: "iptables"
    Jan  5 07:47:31.034: INFO: proxyMode: iptables
    Jan  5 07:47:31.053: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan  5 07:47:31.059: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-9968 01/05/23 07:47:31.059
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-9968 01/05/23 07:47:31.074
    I0105 07:47:31.085876      23 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9968, replica count: 3
    I0105 07:47:34.137576      23 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 07:47:34.140: INFO: Creating new exec pod
    Jan  5 07:47:34.144: INFO: Waiting up to 5m0s for pod "execpod-affinityzlq89" in namespace "services-9968" to be "running"
    Jan  5 07:47:34.146: INFO: Pod "execpod-affinityzlq89": Phase="Pending", Reason="", readiness=false. Elapsed: 1.912416ms
    Jan  5 07:47:36.150: INFO: Pod "execpod-affinityzlq89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005244059s
    Jan  5 07:47:38.149: INFO: Pod "execpod-affinityzlq89": Phase="Running", Reason="", readiness=true. Elapsed: 4.004661516s
    Jan  5 07:47:38.149: INFO: Pod "execpod-affinityzlq89" satisfied condition "running"
    Jan  5 07:47:39.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec execpod-affinityzlq89 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan  5 07:47:39.274: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan  5 07:47:39.274: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 07:47:39.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec execpod-affinityzlq89 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.201.69 80'
    Jan  5 07:47:39.415: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.201.69 80\nConnection to 10.99.201.69 80 port [tcp/http] succeeded!\n"
    Jan  5 07:47:39.415: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 07:47:39.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec execpod-affinityzlq89 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.201.69:80/ ; done'
    Jan  5 07:47:39.637: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n"
    Jan  5 07:47:39.637: INFO: stdout: "\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z\naffinity-clusterip-timeout-z858z"
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.637: INFO: Received response from host: affinity-clusterip-timeout-z858z
    Jan  5 07:47:39.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec execpod-affinityzlq89 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.99.201.69:80/'
    Jan  5 07:47:39.758: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n"
    Jan  5 07:47:39.758: INFO: stdout: "affinity-clusterip-timeout-z858z"
    Jan  5 07:47:59.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9968 exec execpod-affinityzlq89 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.99.201.69:80/'
    Jan  5 07:47:59.896: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.99.201.69:80/\n"
    Jan  5 07:47:59.896: INFO: stdout: "affinity-clusterip-timeout-khlp4"
    Jan  5 07:47:59.896: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9968, will wait for the garbage collector to delete the pods 01/05/23 07:47:59.921
    Jan  5 07:47:59.988: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 13.510683ms
    Jan  5 07:48:00.088: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.317909ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 07:48:01.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9968" for this suite. 01/05/23 07:48:01.716
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:48:01.724
Jan  5 07:48:01.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename deployment 01/05/23 07:48:01.725
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:01.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:01.743
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/05/23 07:48:01.747
STEP: waiting for Deployment to be created 01/05/23 07:48:01.764
STEP: waiting for all Replicas to be Ready 01/05/23 07:48:01.765
Jan  5 07:48:01.766: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 07:48:01.766: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 07:48:01.776: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 07:48:01.776: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 07:48:01.786: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 07:48:01.786: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 07:48:01.833: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 07:48:01.833: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 07:48:03.375: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  5 07:48:03.375: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  5 07:48:03.632: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/05/23 07:48:03.632
W0105 07:48:03.639913      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  5 07:48:03.640: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/05/23 07:48:03.641
Jan  5 07:48:03.642: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan  5 07:48:03.642: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan  5 07:48:03.642: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan  5 07:48:03.642: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:03.661: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:03.661: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:03.673: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:03.673: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:03.696: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan  5 07:48:03.697: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan  5 07:48:03.718: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan  5 07:48:03.718: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan  5 07:48:05.413: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:05.413: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:05.444: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
STEP: listing Deployments 01/05/23 07:48:05.444
Jan  5 07:48:05.446: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/05/23 07:48:05.446
Jan  5 07:48:05.453: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/05/23 07:48:05.453
Jan  5 07:48:05.457: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 07:48:05.474: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 07:48:05.501: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 07:48:05.518: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 07:48:05.542: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 07:48:07.390: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 07:48:07.412: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 07:48:07.421: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 07:48:07.443: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 07:48:14.670: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/05/23 07:48:14.714
STEP: fetching the DeploymentStatus 01/05/23 07:48:14.722
Jan  5 07:48:14.724: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan  5 07:48:14.724: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan  5 07:48:14.724: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 3
STEP: deleting the Deployment 01/05/23 07:48:14.725
Jan  5 07:48:14.729: INFO: observed event type MODIFIED
Jan  5 07:48:14.729: INFO: observed event type MODIFIED
Jan  5 07:48:14.729: INFO: observed event type MODIFIED
Jan  5 07:48:14.729: INFO: observed event type MODIFIED
Jan  5 07:48:14.729: INFO: observed event type MODIFIED
Jan  5 07:48:14.729: INFO: observed event type MODIFIED
Jan  5 07:48:14.729: INFO: observed event type MODIFIED
Jan  5 07:48:14.729: INFO: observed event type MODIFIED
Jan  5 07:48:14.729: INFO: observed event type MODIFIED
Jan  5 07:48:14.730: INFO: observed event type MODIFIED
Jan  5 07:48:14.730: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 07:48:14.731: INFO: Log out all the ReplicaSets if there is no deployment created
Jan  5 07:48:14.733: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-5772  6626cb66-ac7f-472d-b7ea-bc9fe0425403 9974 4 2023-01-05 07:48:03 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 28376c02-db65-4597-aebe-83a3695d4a5b 0xc0050c1557 0xc0050c1558}] [] [{kube-controller-manager Update apps/v1 2023-01-05 07:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28376c02-db65-4597-aebe-83a3695d4a5b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050c15e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan  5 07:48:14.740: INFO: pod: "test-deployment-54cc775c4b-8bkqd":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-8bkqd test-deployment-54cc775c4b- deployment-5772  c57be9dd-1d1b-48ca-a2fe-5985d458f942 9970 0 2023-01-05 07:48:03 +0000 UTC 2023-01-05 07:48:15 +0000 UTC 0xc0050c1a30 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:374323561b32324bb838929a8d26eba302e49536ff11a990d6060c70f4644a62 cni.projectcalico.org/podIP:10.244.1.79/32 cni.projectcalico.org/podIPs:10.244.1.79/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 6626cb66-ac7f-472d-b7ea-bc9fe0425403 0xc0050c1a87 0xc0050c1a88}] [] [{kube-controller-manager Update v1 2023-01-05 07:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6626cb66-ac7f-472d-b7ea-bc9fe0425403\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 07:48:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 07:48:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xt48f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xt48f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.79,StartTime:2023-01-05 07:48:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 07:48:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://506bbe9bcad28e0370d8632fc25291459eb9b4f23e51f7ab9bec9162410f7f94,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  5 07:48:14.740: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-5772  8b3f68f9-7687-4bc8-8052-6d17d746e285 9963 2 2023-01-05 07:48:05 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 28376c02-db65-4597-aebe-83a3695d4a5b 0xc0050c1647 0xc0050c1648}] [] [{kube-controller-manager Update apps/v1 2023-01-05 07:48:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28376c02-db65-4597-aebe-83a3695d4a5b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050c16d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan  5 07:48:14.742: INFO: pod: "test-deployment-7c7d8d58c8-pp82t":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-pp82t test-deployment-7c7d8d58c8- deployment-5772  a1449f97-4653-4aa6-8007-7e0fe582ba0a 9917 0 2023-01-05 07:48:05 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:f8f299f049da19edf741db9874342dde3e18735c5a4dceecedc94b2f51f39fed cni.projectcalico.org/podIP:10.244.1.80/32 cni.projectcalico.org/podIPs:10.244.1.80/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 8b3f68f9-7687-4bc8-8052-6d17d746e285 0xc0034ce6d7 0xc0034ce6d8}] [] [{kube-controller-manager Update v1 2023-01-05 07:48:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b3f68f9-7687-4bc8-8052-6d17d746e285\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 07:48:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 07:48:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lgw82,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lgw82,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.80,StartTime:2023-01-05 07:48:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 07:48:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3a55a54a7f283f8372dfc784f5b226a277a708141add916268d20b2b59c426a4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  5 07:48:14.743: INFO: pod: "test-deployment-7c7d8d58c8-tk5wv":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-tk5wv test-deployment-7c7d8d58c8- deployment-5772  1416dbad-1e34-4b0f-acf7-13a4851846c7 9962 0 2023-01-05 07:48:07 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:f3087a8eed7d3846cb63c5a2cd2bb1ebb842d4802c10f728c92a68514435fdbb cni.projectcalico.org/podIP:10.244.0.150/32 cni.projectcalico.org/podIPs:10.244.0.150/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 8b3f68f9-7687-4bc8-8052-6d17d746e285 0xc0034ce907 0xc0034ce908}] [] [{calico Update v1 2023-01-05 07:48:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-05 07:48:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b3f68f9-7687-4bc8-8052-6d17d746e285\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 07:48:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t27vs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t27vs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.150,StartTime:2023-01-05 07:48:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 07:48:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c3d54401208913378720fab358210a3f34d1f6ee8530ec53702f0587c2721d56,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  5 07:48:14.743: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-5772  234aaab4-a537-4efe-9bd3-d1e51453bd6f 9837 3 2023-01-05 07:48:01 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 28376c02-db65-4597-aebe-83a3695d4a5b 0xc0050c1737 0xc0050c1738}] [] [{kube-controller-manager Update apps/v1 2023-01-05 07:48:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28376c02-db65-4597-aebe-83a3695d4a5b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050c17c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 07:48:14.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5772" for this suite. 01/05/23 07:48:14.755
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":69,"skipped":1433,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.039 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:48:01.724
    Jan  5 07:48:01.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename deployment 01/05/23 07:48:01.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:01.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:01.743
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/05/23 07:48:01.747
    STEP: waiting for Deployment to be created 01/05/23 07:48:01.764
    STEP: waiting for all Replicas to be Ready 01/05/23 07:48:01.765
    Jan  5 07:48:01.766: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 07:48:01.766: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 07:48:01.776: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 07:48:01.776: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 07:48:01.786: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 07:48:01.786: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 07:48:01.833: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 07:48:01.833: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 07:48:03.375: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  5 07:48:03.375: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  5 07:48:03.632: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/05/23 07:48:03.632
    W0105 07:48:03.639913      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  5 07:48:03.640: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/05/23 07:48:03.641
    Jan  5 07:48:03.642: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan  5 07:48:03.642: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan  5 07:48:03.642: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan  5 07:48:03.642: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:03.643: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:03.661: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:03.661: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:03.673: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:03.673: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:03.696: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan  5 07:48:03.697: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan  5 07:48:03.718: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan  5 07:48:03.718: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan  5 07:48:05.413: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:05.413: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:05.444: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    STEP: listing Deployments 01/05/23 07:48:05.444
    Jan  5 07:48:05.446: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/05/23 07:48:05.446
    Jan  5 07:48:05.453: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/05/23 07:48:05.453
    Jan  5 07:48:05.457: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 07:48:05.474: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 07:48:05.501: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 07:48:05.518: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 07:48:05.542: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 07:48:07.390: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 07:48:07.412: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 07:48:07.421: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 07:48:07.443: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 07:48:14.670: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/05/23 07:48:14.714
    STEP: fetching the DeploymentStatus 01/05/23 07:48:14.722
    Jan  5 07:48:14.724: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan  5 07:48:14.724: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan  5 07:48:14.724: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan  5 07:48:14.725: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 3
    STEP: deleting the Deployment 01/05/23 07:48:14.725
    Jan  5 07:48:14.729: INFO: observed event type MODIFIED
    Jan  5 07:48:14.729: INFO: observed event type MODIFIED
    Jan  5 07:48:14.729: INFO: observed event type MODIFIED
    Jan  5 07:48:14.729: INFO: observed event type MODIFIED
    Jan  5 07:48:14.729: INFO: observed event type MODIFIED
    Jan  5 07:48:14.729: INFO: observed event type MODIFIED
    Jan  5 07:48:14.729: INFO: observed event type MODIFIED
    Jan  5 07:48:14.729: INFO: observed event type MODIFIED
    Jan  5 07:48:14.729: INFO: observed event type MODIFIED
    Jan  5 07:48:14.730: INFO: observed event type MODIFIED
    Jan  5 07:48:14.730: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 07:48:14.731: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan  5 07:48:14.733: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-5772  6626cb66-ac7f-472d-b7ea-bc9fe0425403 9974 4 2023-01-05 07:48:03 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 28376c02-db65-4597-aebe-83a3695d4a5b 0xc0050c1557 0xc0050c1558}] [] [{kube-controller-manager Update apps/v1 2023-01-05 07:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28376c02-db65-4597-aebe-83a3695d4a5b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050c15e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan  5 07:48:14.740: INFO: pod: "test-deployment-54cc775c4b-8bkqd":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-8bkqd test-deployment-54cc775c4b- deployment-5772  c57be9dd-1d1b-48ca-a2fe-5985d458f942 9970 0 2023-01-05 07:48:03 +0000 UTC 2023-01-05 07:48:15 +0000 UTC 0xc0050c1a30 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:374323561b32324bb838929a8d26eba302e49536ff11a990d6060c70f4644a62 cni.projectcalico.org/podIP:10.244.1.79/32 cni.projectcalico.org/podIPs:10.244.1.79/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 6626cb66-ac7f-472d-b7ea-bc9fe0425403 0xc0050c1a87 0xc0050c1a88}] [] [{kube-controller-manager Update v1 2023-01-05 07:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6626cb66-ac7f-472d-b7ea-bc9fe0425403\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 07:48:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 07:48:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xt48f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xt48f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.79,StartTime:2023-01-05 07:48:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 07:48:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://506bbe9bcad28e0370d8632fc25291459eb9b4f23e51f7ab9bec9162410f7f94,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  5 07:48:14.740: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-5772  8b3f68f9-7687-4bc8-8052-6d17d746e285 9963 2 2023-01-05 07:48:05 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 28376c02-db65-4597-aebe-83a3695d4a5b 0xc0050c1647 0xc0050c1648}] [] [{kube-controller-manager Update apps/v1 2023-01-05 07:48:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28376c02-db65-4597-aebe-83a3695d4a5b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050c16d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan  5 07:48:14.742: INFO: pod: "test-deployment-7c7d8d58c8-pp82t":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-pp82t test-deployment-7c7d8d58c8- deployment-5772  a1449f97-4653-4aa6-8007-7e0fe582ba0a 9917 0 2023-01-05 07:48:05 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:f8f299f049da19edf741db9874342dde3e18735c5a4dceecedc94b2f51f39fed cni.projectcalico.org/podIP:10.244.1.80/32 cni.projectcalico.org/podIPs:10.244.1.80/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 8b3f68f9-7687-4bc8-8052-6d17d746e285 0xc0034ce6d7 0xc0034ce6d8}] [] [{kube-controller-manager Update v1 2023-01-05 07:48:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b3f68f9-7687-4bc8-8052-6d17d746e285\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 07:48:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 07:48:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lgw82,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lgw82,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.80,StartTime:2023-01-05 07:48:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 07:48:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3a55a54a7f283f8372dfc784f5b226a277a708141add916268d20b2b59c426a4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  5 07:48:14.743: INFO: pod: "test-deployment-7c7d8d58c8-tk5wv":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-tk5wv test-deployment-7c7d8d58c8- deployment-5772  1416dbad-1e34-4b0f-acf7-13a4851846c7 9962 0 2023-01-05 07:48:07 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:f3087a8eed7d3846cb63c5a2cd2bb1ebb842d4802c10f728c92a68514435fdbb cni.projectcalico.org/podIP:10.244.0.150/32 cni.projectcalico.org/podIPs:10.244.0.150/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 8b3f68f9-7687-4bc8-8052-6d17d746e285 0xc0034ce907 0xc0034ce908}] [] [{calico Update v1 2023-01-05 07:48:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-05 07:48:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b3f68f9-7687-4bc8-8052-6d17d746e285\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 07:48:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t27vs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t27vs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.150,StartTime:2023-01-05 07:48:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 07:48:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c3d54401208913378720fab358210a3f34d1f6ee8530ec53702f0587c2721d56,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  5 07:48:14.743: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-5772  234aaab4-a537-4efe-9bd3-d1e51453bd6f 9837 3 2023-01-05 07:48:01 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 28376c02-db65-4597-aebe-83a3695d4a5b 0xc0050c1737 0xc0050c1738}] [] [{kube-controller-manager Update apps/v1 2023-01-05 07:48:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28376c02-db65-4597-aebe-83a3695d4a5b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050c17c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 07:48:14.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5772" for this suite. 01/05/23 07:48:14.755
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:48:14.763
Jan  5 07:48:14.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename deployment 01/05/23 07:48:14.764
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:14.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:14.78
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan  5 07:48:14.784: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan  5 07:48:14.795: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 07:48:19.797: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 07:48:19.797
Jan  5 07:48:19.797: INFO: Creating deployment "test-rolling-update-deployment"
Jan  5 07:48:19.804: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan  5 07:48:19.819: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan  5 07:48:21.825: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan  5 07:48:21.827: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 07:48:21.831: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9489  9387f2ea-c626-4467-ab2b-9c63a001f4ea 10114 1 2023-01-05 07:48:19 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-05 07:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fe5568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 07:48:19 +0000 UTC,LastTransitionTime:2023-01-05 07:48:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-05 07:48:21 +0000 UTC,LastTransitionTime:2023-01-05 07:48:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 07:48:21.833: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-9489  950165a5-54d2-4930-ac24-d96567e63acd 10104 1 2023-01-05 07:48:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9387f2ea-c626-4467-ab2b-9c63a001f4ea 0xc004239f37 0xc004239f38}] [] [{kube-controller-manager Update apps/v1 2023-01-05 07:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9387f2ea-c626-4467-ab2b-9c63a001f4ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004239fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 07:48:21.833: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan  5 07:48:21.833: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9489  b0653ce9-a100-4997-b37c-f25b881c47e4 10113 2 2023-01-05 07:48:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9387f2ea-c626-4467-ab2b-9c63a001f4ea 0xc004239e07 0xc004239e08}] [] [{e2e.test Update apps/v1 2023-01-05 07:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9387f2ea-c626-4467-ab2b-9c63a001f4ea\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004239ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 07:48:21.835: INFO: Pod "test-rolling-update-deployment-78f575d8ff-vc97z" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-vc97z test-rolling-update-deployment-78f575d8ff- deployment-9489  fdecfba0-2fbf-4e4a-b8f7-53b9984e80cf 10103 0 2023-01-05 07:48:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:fb36ea89c43c1ce563d888c7ae1edf9d4bd49d4745d6fb3fb5d9570538bf4cbe cni.projectcalico.org/podIP:10.244.1.82/32 cni.projectcalico.org/podIPs:10.244.1.82/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 950165a5-54d2-4930-ac24-d96567e63acd 0xc00351c667 0xc00351c668}] [] [{kube-controller-manager Update v1 2023-01-05 07:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"950165a5-54d2-4930-ac24-d96567e63acd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 07:48:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 07:48:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnb6c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnb6c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.82,StartTime:2023-01-05 07:48:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 07:48:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://efc218d0f6ba16867dcf34d86d54d045ebde3f2e9aebd6edcedfe9dcadc17946,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 07:48:21.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9489" for this suite. 01/05/23 07:48:21.837
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":70,"skipped":1433,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.077 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:48:14.763
    Jan  5 07:48:14.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename deployment 01/05/23 07:48:14.764
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:14.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:14.78
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan  5 07:48:14.784: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan  5 07:48:14.795: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 07:48:19.797: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 07:48:19.797
    Jan  5 07:48:19.797: INFO: Creating deployment "test-rolling-update-deployment"
    Jan  5 07:48:19.804: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan  5 07:48:19.819: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan  5 07:48:21.825: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan  5 07:48:21.827: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 07:48:21.831: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9489  9387f2ea-c626-4467-ab2b-9c63a001f4ea 10114 1 2023-01-05 07:48:19 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-05 07:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fe5568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 07:48:19 +0000 UTC,LastTransitionTime:2023-01-05 07:48:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-05 07:48:21 +0000 UTC,LastTransitionTime:2023-01-05 07:48:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 07:48:21.833: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-9489  950165a5-54d2-4930-ac24-d96567e63acd 10104 1 2023-01-05 07:48:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9387f2ea-c626-4467-ab2b-9c63a001f4ea 0xc004239f37 0xc004239f38}] [] [{kube-controller-manager Update apps/v1 2023-01-05 07:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9387f2ea-c626-4467-ab2b-9c63a001f4ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004239fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 07:48:21.833: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan  5 07:48:21.833: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9489  b0653ce9-a100-4997-b37c-f25b881c47e4 10113 2 2023-01-05 07:48:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9387f2ea-c626-4467-ab2b-9c63a001f4ea 0xc004239e07 0xc004239e08}] [] [{e2e.test Update apps/v1 2023-01-05 07:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9387f2ea-c626-4467-ab2b-9c63a001f4ea\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 07:48:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004239ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 07:48:21.835: INFO: Pod "test-rolling-update-deployment-78f575d8ff-vc97z" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-vc97z test-rolling-update-deployment-78f575d8ff- deployment-9489  fdecfba0-2fbf-4e4a-b8f7-53b9984e80cf 10103 0 2023-01-05 07:48:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:fb36ea89c43c1ce563d888c7ae1edf9d4bd49d4745d6fb3fb5d9570538bf4cbe cni.projectcalico.org/podIP:10.244.1.82/32 cni.projectcalico.org/podIPs:10.244.1.82/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 950165a5-54d2-4930-ac24-d96567e63acd 0xc00351c667 0xc00351c668}] [] [{kube-controller-manager Update v1 2023-01-05 07:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"950165a5-54d2-4930-ac24-d96567e63acd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 07:48:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 07:48:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnb6c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnb6c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 07:48:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.82,StartTime:2023-01-05 07:48:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 07:48:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://efc218d0f6ba16867dcf34d86d54d045ebde3f2e9aebd6edcedfe9dcadc17946,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 07:48:21.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9489" for this suite. 01/05/23 07:48:21.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:48:21.842
Jan  5 07:48:21.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 07:48:21.842
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:21.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:21.869
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/05/23 07:48:21.871
Jan  5 07:48:21.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: rename a version 01/05/23 07:48:28.899
STEP: check the new version name is served 01/05/23 07:48:28.911
STEP: check the old version name is removed 01/05/23 07:48:30.506
STEP: check the other version is not changed 01/05/23 07:48:31.821
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 07:48:37.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6877" for this suite. 01/05/23 07:48:37.694
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":71,"skipped":1473,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.859 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:48:21.842
    Jan  5 07:48:21.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 07:48:21.842
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:21.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:21.869
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/05/23 07:48:21.871
    Jan  5 07:48:21.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: rename a version 01/05/23 07:48:28.899
    STEP: check the new version name is served 01/05/23 07:48:28.911
    STEP: check the old version name is removed 01/05/23 07:48:30.506
    STEP: check the other version is not changed 01/05/23 07:48:31.821
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 07:48:37.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6877" for this suite. 01/05/23 07:48:37.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:48:37.702
Jan  5 07:48:37.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 07:48:37.703
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:37.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:37.725
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-2ffb4751-d67b-436e-b26e-c4254e0aa745 01/05/23 07:48:37.727
STEP: Creating a pod to test consume secrets 01/05/23 07:48:37.739
Jan  5 07:48:37.745: INFO: Waiting up to 5m0s for pod "pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f" in namespace "secrets-8053" to be "Succeeded or Failed"
Jan  5 07:48:37.747: INFO: Pod "pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047643ms
Jan  5 07:48:39.750: INFO: Pod "pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004936153s
Jan  5 07:48:41.751: INFO: Pod "pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006266476s
STEP: Saw pod success 01/05/23 07:48:41.751
Jan  5 07:48:41.751: INFO: Pod "pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f" satisfied condition "Succeeded or Failed"
Jan  5 07:48:41.753: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 07:48:41.757
Jan  5 07:48:41.788: INFO: Waiting for pod pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f to disappear
Jan  5 07:48:41.796: INFO: Pod pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 07:48:41.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8053" for this suite. 01/05/23 07:48:41.798
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":72,"skipped":1479,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:48:37.702
    Jan  5 07:48:37.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 07:48:37.703
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:37.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:37.725
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-2ffb4751-d67b-436e-b26e-c4254e0aa745 01/05/23 07:48:37.727
    STEP: Creating a pod to test consume secrets 01/05/23 07:48:37.739
    Jan  5 07:48:37.745: INFO: Waiting up to 5m0s for pod "pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f" in namespace "secrets-8053" to be "Succeeded or Failed"
    Jan  5 07:48:37.747: INFO: Pod "pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047643ms
    Jan  5 07:48:39.750: INFO: Pod "pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004936153s
    Jan  5 07:48:41.751: INFO: Pod "pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006266476s
    STEP: Saw pod success 01/05/23 07:48:41.751
    Jan  5 07:48:41.751: INFO: Pod "pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f" satisfied condition "Succeeded or Failed"
    Jan  5 07:48:41.753: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 07:48:41.757
    Jan  5 07:48:41.788: INFO: Waiting for pod pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f to disappear
    Jan  5 07:48:41.796: INFO: Pod pod-secrets-3fa07989-4e84-430b-9895-bf02836d6e8f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 07:48:41.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8053" for this suite. 01/05/23 07:48:41.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:48:41.804
Jan  5 07:48:41.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 07:48:41.805
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:41.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:41.823
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-ce98e3cf-26bc-426e-b371-6cd34f0f7ce8 01/05/23 07:48:41.825
STEP: Creating a pod to test consume configMaps 01/05/23 07:48:41.84
Jan  5 07:48:41.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf" in namespace "configmap-1667" to be "Succeeded or Failed"
Jan  5 07:48:41.848: INFO: Pod "pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.536467ms
Jan  5 07:48:43.851: INFO: Pod "pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005125243s
Jan  5 07:48:45.852: INFO: Pod "pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005769808s
STEP: Saw pod success 01/05/23 07:48:45.852
Jan  5 07:48:45.852: INFO: Pod "pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf" satisfied condition "Succeeded or Failed"
Jan  5 07:48:45.855: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf container configmap-volume-test: <nil>
STEP: delete the pod 01/05/23 07:48:45.859
Jan  5 07:48:45.875: INFO: Waiting for pod pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf to disappear
Jan  5 07:48:45.887: INFO: Pod pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 07:48:45.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1667" for this suite. 01/05/23 07:48:45.89
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":73,"skipped":1495,"failed":0}
------------------------------
â€¢ [4.091 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:48:41.804
    Jan  5 07:48:41.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 07:48:41.805
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:41.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:41.823
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-ce98e3cf-26bc-426e-b371-6cd34f0f7ce8 01/05/23 07:48:41.825
    STEP: Creating a pod to test consume configMaps 01/05/23 07:48:41.84
    Jan  5 07:48:41.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf" in namespace "configmap-1667" to be "Succeeded or Failed"
    Jan  5 07:48:41.848: INFO: Pod "pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.536467ms
    Jan  5 07:48:43.851: INFO: Pod "pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005125243s
    Jan  5 07:48:45.852: INFO: Pod "pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005769808s
    STEP: Saw pod success 01/05/23 07:48:45.852
    Jan  5 07:48:45.852: INFO: Pod "pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf" satisfied condition "Succeeded or Failed"
    Jan  5 07:48:45.855: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf container configmap-volume-test: <nil>
    STEP: delete the pod 01/05/23 07:48:45.859
    Jan  5 07:48:45.875: INFO: Waiting for pod pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf to disappear
    Jan  5 07:48:45.887: INFO: Pod pod-configmaps-a2e13a00-77d0-421c-81a0-67a5ce8f79cf no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 07:48:45.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1667" for this suite. 01/05/23 07:48:45.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:48:45.895
Jan  5 07:48:45.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename dns 01/05/23 07:48:45.896
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:45.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:45.911
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/05/23 07:48:45.916
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local;sleep 1; done
 01/05/23 07:48:45.921
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local;sleep 1; done
 01/05/23 07:48:45.921
STEP: creating a pod to probe DNS 01/05/23 07:48:45.921
STEP: submitting the pod to kubernetes 01/05/23 07:48:45.921
Jan  5 07:48:45.942: INFO: Waiting up to 15m0s for pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837" in namespace "dns-6571" to be "running"
Jan  5 07:48:45.944: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 1.757138ms
Jan  5 07:48:47.946: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004525269s
Jan  5 07:48:49.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005666317s
Jan  5 07:48:51.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005506505s
Jan  5 07:48:53.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005163783s
Jan  5 07:48:55.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005277103s
Jan  5 07:48:57.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004941643s
Jan  5 07:48:59.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004865944s
Jan  5 07:49:01.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Running", Reason="", readiness=true. Elapsed: 16.00502547s
Jan  5 07:49:01.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837" satisfied condition "running"
STEP: retrieving the pod 01/05/23 07:49:01.947
STEP: looking for the results for each expected name from probers 01/05/23 07:49:01.949
Jan  5 07:49:01.953: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
Jan  5 07:49:01.954: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
Jan  5 07:49:01.956: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
Jan  5 07:49:01.958: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
Jan  5 07:49:01.960: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
Jan  5 07:49:01.963: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
Jan  5 07:49:01.965: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
Jan  5 07:49:01.967: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
Jan  5 07:49:01.967: INFO: Lookups using dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

Jan  5 07:49:06.990: INFO: DNS probes using dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837 succeeded

STEP: deleting the pod 01/05/23 07:49:06.99
STEP: deleting the test headless service 01/05/23 07:49:07.018
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 07:49:07.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6571" for this suite. 01/05/23 07:49:07.051
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":74,"skipped":1514,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.167 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:48:45.895
    Jan  5 07:48:45.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename dns 01/05/23 07:48:45.896
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:48:45.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:48:45.911
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/05/23 07:48:45.916
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local;sleep 1; done
     01/05/23 07:48:45.921
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local;sleep 1; done
     01/05/23 07:48:45.921
    STEP: creating a pod to probe DNS 01/05/23 07:48:45.921
    STEP: submitting the pod to kubernetes 01/05/23 07:48:45.921
    Jan  5 07:48:45.942: INFO: Waiting up to 15m0s for pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837" in namespace "dns-6571" to be "running"
    Jan  5 07:48:45.944: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 1.757138ms
    Jan  5 07:48:47.946: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004525269s
    Jan  5 07:48:49.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005666317s
    Jan  5 07:48:51.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005506505s
    Jan  5 07:48:53.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005163783s
    Jan  5 07:48:55.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005277103s
    Jan  5 07:48:57.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004941643s
    Jan  5 07:48:59.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004865944s
    Jan  5 07:49:01.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837": Phase="Running", Reason="", readiness=true. Elapsed: 16.00502547s
    Jan  5 07:49:01.947: INFO: Pod "dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 07:49:01.947
    STEP: looking for the results for each expected name from probers 01/05/23 07:49:01.949
    Jan  5 07:49:01.953: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
    Jan  5 07:49:01.954: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
    Jan  5 07:49:01.956: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
    Jan  5 07:49:01.958: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
    Jan  5 07:49:01.960: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
    Jan  5 07:49:01.963: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
    Jan  5 07:49:01.965: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
    Jan  5 07:49:01.967: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837: the server could not find the requested resource (get pods dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837)
    Jan  5 07:49:01.967: INFO: Lookups using dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

    Jan  5 07:49:06.990: INFO: DNS probes using dns-6571/dns-test-56c15c2a-accb-4dd1-811a-0f499ec7a837 succeeded

    STEP: deleting the pod 01/05/23 07:49:06.99
    STEP: deleting the test headless service 01/05/23 07:49:07.018
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 07:49:07.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6571" for this suite. 01/05/23 07:49:07.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:49:07.064
Jan  5 07:49:07.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 07:49:07.066
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:49:07.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:49:07.083
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/05/23 07:49:07.086
Jan  5 07:49:07.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920" in namespace "downward-api-1258" to be "Succeeded or Failed"
Jan  5 07:49:07.104: INFO: Pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920": Phase="Pending", Reason="", readiness=false. Elapsed: 1.713725ms
Jan  5 07:49:09.108: INFO: Pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920": Phase="Running", Reason="", readiness=true. Elapsed: 2.005308626s
Jan  5 07:49:11.108: INFO: Pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920": Phase="Running", Reason="", readiness=false. Elapsed: 4.005821576s
Jan  5 07:49:13.108: INFO: Pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005886818s
STEP: Saw pod success 01/05/23 07:49:13.109
Jan  5 07:49:13.109: INFO: Pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920" satisfied condition "Succeeded or Failed"
Jan  5 07:49:13.112: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920 container client-container: <nil>
STEP: delete the pod 01/05/23 07:49:13.116
Jan  5 07:49:13.132: INFO: Waiting for pod downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920 to disappear
Jan  5 07:49:13.133: INFO: Pod downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 07:49:13.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1258" for this suite. 01/05/23 07:49:13.135
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":75,"skipped":1538,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.079 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:49:07.064
    Jan  5 07:49:07.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 07:49:07.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:49:07.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:49:07.083
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/05/23 07:49:07.086
    Jan  5 07:49:07.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920" in namespace "downward-api-1258" to be "Succeeded or Failed"
    Jan  5 07:49:07.104: INFO: Pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920": Phase="Pending", Reason="", readiness=false. Elapsed: 1.713725ms
    Jan  5 07:49:09.108: INFO: Pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920": Phase="Running", Reason="", readiness=true. Elapsed: 2.005308626s
    Jan  5 07:49:11.108: INFO: Pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920": Phase="Running", Reason="", readiness=false. Elapsed: 4.005821576s
    Jan  5 07:49:13.108: INFO: Pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005886818s
    STEP: Saw pod success 01/05/23 07:49:13.109
    Jan  5 07:49:13.109: INFO: Pod "downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920" satisfied condition "Succeeded or Failed"
    Jan  5 07:49:13.112: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920 container client-container: <nil>
    STEP: delete the pod 01/05/23 07:49:13.116
    Jan  5 07:49:13.132: INFO: Waiting for pod downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920 to disappear
    Jan  5 07:49:13.133: INFO: Pod downwardapi-volume-0f2dfb12-3368-4c48-96c0-643be1ea7920 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 07:49:13.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1258" for this suite. 01/05/23 07:49:13.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:49:13.143
Jan  5 07:49:13.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 07:49:13.144
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:49:13.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:49:13.16
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/05/23 07:49:13.162
Jan  5 07:49:13.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-7852 create -f -'
Jan  5 07:49:13.667: INFO: stderr: ""
Jan  5 07:49:13.667: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/05/23 07:49:13.667
Jan  5 07:49:13.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-7852 diff -f -'
Jan  5 07:49:13.869: INFO: rc: 1
Jan  5 07:49:13.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-7852 delete -f -'
Jan  5 07:49:13.934: INFO: stderr: ""
Jan  5 07:49:13.934: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 07:49:13.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7852" for this suite. 01/05/23 07:49:13.937
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":76,"skipped":1543,"failed":0}
------------------------------
â€¢ [0.814 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:49:13.143
    Jan  5 07:49:13.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 07:49:13.144
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:49:13.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:49:13.16
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/05/23 07:49:13.162
    Jan  5 07:49:13.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-7852 create -f -'
    Jan  5 07:49:13.667: INFO: stderr: ""
    Jan  5 07:49:13.667: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/05/23 07:49:13.667
    Jan  5 07:49:13.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-7852 diff -f -'
    Jan  5 07:49:13.869: INFO: rc: 1
    Jan  5 07:49:13.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-7852 delete -f -'
    Jan  5 07:49:13.934: INFO: stderr: ""
    Jan  5 07:49:13.934: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 07:49:13.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7852" for this suite. 01/05/23 07:49:13.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:49:13.958
Jan  5 07:49:13.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 07:49:13.959
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:49:13.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:49:13.989
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-5181a99f-0710-4b28-8437-73b4d0c90dcf 01/05/23 07:49:13.993
STEP: Creating the pod 01/05/23 07:49:14.001
Jan  5 07:49:14.022: INFO: Waiting up to 5m0s for pod "pod-configmaps-da173682-6548-4b02-88f9-d7ef58fc0d9f" in namespace "configmap-3650" to be "running"
Jan  5 07:49:14.024: INFO: Pod "pod-configmaps-da173682-6548-4b02-88f9-d7ef58fc0d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.230533ms
Jan  5 07:49:16.028: INFO: Pod "pod-configmaps-da173682-6548-4b02-88f9-d7ef58fc0d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005903803s
Jan  5 07:49:18.030: INFO: Pod "pod-configmaps-da173682-6548-4b02-88f9-d7ef58fc0d9f": Phase="Running", Reason="", readiness=false. Elapsed: 4.007454862s
Jan  5 07:49:18.030: INFO: Pod "pod-configmaps-da173682-6548-4b02-88f9-d7ef58fc0d9f" satisfied condition "running"
STEP: Waiting for pod with text data 01/05/23 07:49:18.03
STEP: Waiting for pod with binary data 01/05/23 07:49:18.036
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 07:49:18.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3650" for this suite. 01/05/23 07:49:18.041
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":77,"skipped":1571,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:49:13.958
    Jan  5 07:49:13.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 07:49:13.959
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:49:13.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:49:13.989
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-5181a99f-0710-4b28-8437-73b4d0c90dcf 01/05/23 07:49:13.993
    STEP: Creating the pod 01/05/23 07:49:14.001
    Jan  5 07:49:14.022: INFO: Waiting up to 5m0s for pod "pod-configmaps-da173682-6548-4b02-88f9-d7ef58fc0d9f" in namespace "configmap-3650" to be "running"
    Jan  5 07:49:14.024: INFO: Pod "pod-configmaps-da173682-6548-4b02-88f9-d7ef58fc0d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.230533ms
    Jan  5 07:49:16.028: INFO: Pod "pod-configmaps-da173682-6548-4b02-88f9-d7ef58fc0d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005903803s
    Jan  5 07:49:18.030: INFO: Pod "pod-configmaps-da173682-6548-4b02-88f9-d7ef58fc0d9f": Phase="Running", Reason="", readiness=false. Elapsed: 4.007454862s
    Jan  5 07:49:18.030: INFO: Pod "pod-configmaps-da173682-6548-4b02-88f9-d7ef58fc0d9f" satisfied condition "running"
    STEP: Waiting for pod with text data 01/05/23 07:49:18.03
    STEP: Waiting for pod with binary data 01/05/23 07:49:18.036
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 07:49:18.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3650" for this suite. 01/05/23 07:49:18.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:49:18.046
Jan  5 07:49:18.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pods 01/05/23 07:49:18.046
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:49:18.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:49:18.063
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/05/23 07:49:18.072
STEP: watching for Pod to be ready 01/05/23 07:49:18.079
Jan  5 07:49:18.081: INFO: observed Pod pod-test in namespace pods-2656 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan  5 07:49:18.088: INFO: observed Pod pod-test in namespace pods-2656 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  }]
Jan  5 07:49:18.105: INFO: observed Pod pod-test in namespace pods-2656 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  }]
Jan  5 07:49:18.553: INFO: observed Pod pod-test in namespace pods-2656 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  }]
Jan  5 07:49:19.558: INFO: Found Pod pod-test in namespace pods-2656 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/05/23 07:49:19.56
STEP: getting the Pod and ensuring that it's patched 01/05/23 07:49:19.567
STEP: replacing the Pod's status Ready condition to False 01/05/23 07:49:19.569
STEP: check the Pod again to ensure its Ready conditions are False 01/05/23 07:49:19.582
STEP: deleting the Pod via a Collection with a LabelSelector 01/05/23 07:49:19.583
STEP: watching for the Pod to be deleted 01/05/23 07:49:19.588
Jan  5 07:49:19.590: INFO: observed event type MODIFIED
Jan  5 07:49:21.562: INFO: observed event type MODIFIED
Jan  5 07:49:21.705: INFO: observed event type MODIFIED
Jan  5 07:49:22.564: INFO: observed event type MODIFIED
Jan  5 07:49:22.575: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 07:49:22.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2656" for this suite. 01/05/23 07:49:22.589
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":78,"skipped":1577,"failed":0}
------------------------------
â€¢ [4.551 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:49:18.046
    Jan  5 07:49:18.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pods 01/05/23 07:49:18.046
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:49:18.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:49:18.063
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/05/23 07:49:18.072
    STEP: watching for Pod to be ready 01/05/23 07:49:18.079
    Jan  5 07:49:18.081: INFO: observed Pod pod-test in namespace pods-2656 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan  5 07:49:18.088: INFO: observed Pod pod-test in namespace pods-2656 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  }]
    Jan  5 07:49:18.105: INFO: observed Pod pod-test in namespace pods-2656 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  }]
    Jan  5 07:49:18.553: INFO: observed Pod pod-test in namespace pods-2656 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  }]
    Jan  5 07:49:19.558: INFO: Found Pod pod-test in namespace pods-2656 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:49:18 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/05/23 07:49:19.56
    STEP: getting the Pod and ensuring that it's patched 01/05/23 07:49:19.567
    STEP: replacing the Pod's status Ready condition to False 01/05/23 07:49:19.569
    STEP: check the Pod again to ensure its Ready conditions are False 01/05/23 07:49:19.582
    STEP: deleting the Pod via a Collection with a LabelSelector 01/05/23 07:49:19.583
    STEP: watching for the Pod to be deleted 01/05/23 07:49:19.588
    Jan  5 07:49:19.590: INFO: observed event type MODIFIED
    Jan  5 07:49:21.562: INFO: observed event type MODIFIED
    Jan  5 07:49:21.705: INFO: observed event type MODIFIED
    Jan  5 07:49:22.564: INFO: observed event type MODIFIED
    Jan  5 07:49:22.575: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 07:49:22.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2656" for this suite. 01/05/23 07:49:22.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:49:22.598
Jan  5 07:49:22.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename statefulset 01/05/23 07:49:22.599
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:49:22.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:49:22.613
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3059 01/05/23 07:49:22.614
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/05/23 07:49:22.624
STEP: Creating stateful set ss in namespace statefulset-3059 01/05/23 07:49:22.626
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3059 01/05/23 07:49:22.635
Jan  5 07:49:22.637: INFO: Found 0 stateful pods, waiting for 1
Jan  5 07:49:32.640: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/05/23 07:49:32.64
Jan  5 07:49:32.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 07:49:32.777: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 07:49:32.777: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 07:49:32.777: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 07:49:32.780: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  5 07:49:42.787: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 07:49:42.787: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 07:49:42.798: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999843s
Jan  5 07:49:43.800: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996856848s
Jan  5 07:49:44.805: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993273462s
Jan  5 07:49:45.808: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989575813s
Jan  5 07:49:46.812: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985596769s
Jan  5 07:49:47.815: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982700841s
Jan  5 07:49:48.819: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.978776315s
Jan  5 07:49:49.823: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.974962242s
Jan  5 07:49:50.826: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.971193444s
Jan  5 07:49:51.830: INFO: Verifying statefulset ss doesn't scale past 1 for another 968.004789ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3059 01/05/23 07:49:52.83
Jan  5 07:49:52.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 07:49:52.966: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 07:49:52.966: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 07:49:52.966: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 07:49:52.969: INFO: Found 1 stateful pods, waiting for 3
Jan  5 07:50:02.975: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 07:50:02.975: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 07:50:02.975: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/05/23 07:50:02.975
STEP: Scale down will halt with unhealthy stateful pod 01/05/23 07:50:02.975
Jan  5 07:50:02.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 07:50:03.099: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 07:50:03.099: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 07:50:03.099: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 07:50:03.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 07:50:03.217: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 07:50:03.217: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 07:50:03.217: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 07:50:03.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 07:50:03.340: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 07:50:03.340: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 07:50:03.340: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 07:50:03.340: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 07:50:03.342: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan  5 07:50:13.352: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 07:50:13.352: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 07:50:13.352: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 07:50:13.361: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999722s
Jan  5 07:50:14.365: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997270904s
Jan  5 07:50:15.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992955669s
Jan  5 07:50:16.373: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989692494s
Jan  5 07:50:17.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986180425s
Jan  5 07:50:18.381: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981848456s
Jan  5 07:50:19.384: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978306033s
Jan  5 07:50:20.388: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974490513s
Jan  5 07:50:21.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970770419s
Jan  5 07:50:22.393: INFO: Verifying statefulset ss doesn't scale past 3 for another 968.386135ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3059 01/05/23 07:50:23.394
Jan  5 07:50:23.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 07:50:23.511: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 07:50:23.511: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 07:50:23.511: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 07:50:23.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 07:50:23.638: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 07:50:23.638: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 07:50:23.638: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 07:50:23.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 07:50:23.766: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 07:50:23.766: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 07:50:23.766: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 07:50:23.766: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/05/23 07:50:33.78
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 07:50:33.780: INFO: Deleting all statefulset in ns statefulset-3059
Jan  5 07:50:33.782: INFO: Scaling statefulset ss to 0
Jan  5 07:50:33.789: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 07:50:33.791: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 07:50:33.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3059" for this suite. 01/05/23 07:50:33.806
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":79,"skipped":1628,"failed":0}
------------------------------
â€¢ [SLOW TEST] [71.219 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:49:22.598
    Jan  5 07:49:22.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename statefulset 01/05/23 07:49:22.599
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:49:22.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:49:22.613
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3059 01/05/23 07:49:22.614
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/05/23 07:49:22.624
    STEP: Creating stateful set ss in namespace statefulset-3059 01/05/23 07:49:22.626
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3059 01/05/23 07:49:22.635
    Jan  5 07:49:22.637: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 07:49:32.640: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/05/23 07:49:32.64
    Jan  5 07:49:32.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 07:49:32.777: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 07:49:32.777: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 07:49:32.777: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 07:49:32.780: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  5 07:49:42.787: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 07:49:42.787: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 07:49:42.798: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999843s
    Jan  5 07:49:43.800: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996856848s
    Jan  5 07:49:44.805: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993273462s
    Jan  5 07:49:45.808: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989575813s
    Jan  5 07:49:46.812: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985596769s
    Jan  5 07:49:47.815: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982700841s
    Jan  5 07:49:48.819: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.978776315s
    Jan  5 07:49:49.823: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.974962242s
    Jan  5 07:49:50.826: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.971193444s
    Jan  5 07:49:51.830: INFO: Verifying statefulset ss doesn't scale past 1 for another 968.004789ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3059 01/05/23 07:49:52.83
    Jan  5 07:49:52.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 07:49:52.966: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 07:49:52.966: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 07:49:52.966: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 07:49:52.969: INFO: Found 1 stateful pods, waiting for 3
    Jan  5 07:50:02.975: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 07:50:02.975: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 07:50:02.975: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/05/23 07:50:02.975
    STEP: Scale down will halt with unhealthy stateful pod 01/05/23 07:50:02.975
    Jan  5 07:50:02.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 07:50:03.099: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 07:50:03.099: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 07:50:03.099: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 07:50:03.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 07:50:03.217: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 07:50:03.217: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 07:50:03.217: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 07:50:03.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 07:50:03.340: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 07:50:03.340: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 07:50:03.340: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 07:50:03.340: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 07:50:03.342: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan  5 07:50:13.352: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 07:50:13.352: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 07:50:13.352: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 07:50:13.361: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999722s
    Jan  5 07:50:14.365: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997270904s
    Jan  5 07:50:15.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992955669s
    Jan  5 07:50:16.373: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989692494s
    Jan  5 07:50:17.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986180425s
    Jan  5 07:50:18.381: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981848456s
    Jan  5 07:50:19.384: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978306033s
    Jan  5 07:50:20.388: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974490513s
    Jan  5 07:50:21.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970770419s
    Jan  5 07:50:22.393: INFO: Verifying statefulset ss doesn't scale past 3 for another 968.386135ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3059 01/05/23 07:50:23.394
    Jan  5 07:50:23.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 07:50:23.511: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 07:50:23.511: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 07:50:23.511: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 07:50:23.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 07:50:23.638: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 07:50:23.638: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 07:50:23.638: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 07:50:23.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-3059 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 07:50:23.766: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 07:50:23.766: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 07:50:23.766: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 07:50:23.766: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/05/23 07:50:33.78
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 07:50:33.780: INFO: Deleting all statefulset in ns statefulset-3059
    Jan  5 07:50:33.782: INFO: Scaling statefulset ss to 0
    Jan  5 07:50:33.789: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 07:50:33.791: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 07:50:33.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3059" for this suite. 01/05/23 07:50:33.806
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:50:33.818
Jan  5 07:50:33.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename disruption 01/05/23 07:50:33.819
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:50:33.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:50:33.847
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/05/23 07:50:33.851
STEP: Waiting for the pdb to be processed 01/05/23 07:50:33.873
STEP: First trying to evict a pod which shouldn't be evictable 01/05/23 07:50:35.893
STEP: Waiting for all pods to be running 01/05/23 07:50:35.893
Jan  5 07:50:35.896: INFO: pods: 0 < 3
Jan  5 07:50:37.901: INFO: running pods: 1 < 3
STEP: locating a running pod 01/05/23 07:50:39.901
STEP: Updating the pdb to allow a pod to be evicted 01/05/23 07:50:39.907
STEP: Waiting for the pdb to be processed 01/05/23 07:50:39.911
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 07:50:41.916
STEP: Waiting for all pods to be running 01/05/23 07:50:41.916
STEP: Waiting for the pdb to observed all healthy pods 01/05/23 07:50:41.918
STEP: Patching the pdb to disallow a pod to be evicted 01/05/23 07:50:41.943
STEP: Waiting for the pdb to be processed 01/05/23 07:50:41.958
STEP: Waiting for all pods to be running 01/05/23 07:50:43.962
STEP: locating a running pod 01/05/23 07:50:43.965
STEP: Deleting the pdb to allow a pod to be evicted 01/05/23 07:50:43.97
STEP: Waiting for the pdb to be deleted 01/05/23 07:50:43.973
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 07:50:43.975
STEP: Waiting for all pods to be running 01/05/23 07:50:43.975
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  5 07:50:43.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8692" for this suite. 01/05/23 07:50:43.994
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":80,"skipped":1628,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.188 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:50:33.818
    Jan  5 07:50:33.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename disruption 01/05/23 07:50:33.819
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:50:33.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:50:33.847
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/05/23 07:50:33.851
    STEP: Waiting for the pdb to be processed 01/05/23 07:50:33.873
    STEP: First trying to evict a pod which shouldn't be evictable 01/05/23 07:50:35.893
    STEP: Waiting for all pods to be running 01/05/23 07:50:35.893
    Jan  5 07:50:35.896: INFO: pods: 0 < 3
    Jan  5 07:50:37.901: INFO: running pods: 1 < 3
    STEP: locating a running pod 01/05/23 07:50:39.901
    STEP: Updating the pdb to allow a pod to be evicted 01/05/23 07:50:39.907
    STEP: Waiting for the pdb to be processed 01/05/23 07:50:39.911
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 07:50:41.916
    STEP: Waiting for all pods to be running 01/05/23 07:50:41.916
    STEP: Waiting for the pdb to observed all healthy pods 01/05/23 07:50:41.918
    STEP: Patching the pdb to disallow a pod to be evicted 01/05/23 07:50:41.943
    STEP: Waiting for the pdb to be processed 01/05/23 07:50:41.958
    STEP: Waiting for all pods to be running 01/05/23 07:50:43.962
    STEP: locating a running pod 01/05/23 07:50:43.965
    STEP: Deleting the pdb to allow a pod to be evicted 01/05/23 07:50:43.97
    STEP: Waiting for the pdb to be deleted 01/05/23 07:50:43.973
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 07:50:43.975
    STEP: Waiting for all pods to be running 01/05/23 07:50:43.975
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  5 07:50:43.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8692" for this suite. 01/05/23 07:50:43.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:50:44.008
Jan  5 07:50:44.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename dns 01/05/23 07:50:44.009
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:50:44.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:50:44.041
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/05/23 07:50:44.051
Jan  5 07:50:44.057: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8912  e3a9739b-70e1-4e25-8ee0-7eacb36508d8 10968 0 2023-01-05 07:50:44 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-05 07:50:44 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ldd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ldd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 07:50:44.057: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8912" to be "running and ready"
Jan  5 07:50:44.067: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 9.591183ms
Jan  5 07:50:44.067: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:50:46.071: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.013859366s
Jan  5 07:50:46.071: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan  5 07:50:46.071: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/05/23 07:50:46.071
Jan  5 07:50:46.071: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8912 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 07:50:46.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:50:46.073: INFO: ExecWithOptions: Clientset creation
Jan  5 07:50:46.073: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8912/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/05/23 07:50:46.144
Jan  5 07:50:46.144: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8912 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 07:50:46.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:50:46.145: INFO: ExecWithOptions: Clientset creation
Jan  5 07:50:46.145: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8912/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 07:50:46.208: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 07:50:46.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8912" for this suite. 01/05/23 07:50:46.232
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":81,"skipped":1692,"failed":0}
------------------------------
â€¢ [2.231 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:50:44.008
    Jan  5 07:50:44.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename dns 01/05/23 07:50:44.009
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:50:44.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:50:44.041
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/05/23 07:50:44.051
    Jan  5 07:50:44.057: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8912  e3a9739b-70e1-4e25-8ee0-7eacb36508d8 10968 0 2023-01-05 07:50:44 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-05 07:50:44 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ldd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ldd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 07:50:44.057: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8912" to be "running and ready"
    Jan  5 07:50:44.067: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 9.591183ms
    Jan  5 07:50:44.067: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:50:46.071: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.013859366s
    Jan  5 07:50:46.071: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan  5 07:50:46.071: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/05/23 07:50:46.071
    Jan  5 07:50:46.071: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8912 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 07:50:46.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:50:46.073: INFO: ExecWithOptions: Clientset creation
    Jan  5 07:50:46.073: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8912/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/05/23 07:50:46.144
    Jan  5 07:50:46.144: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8912 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 07:50:46.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:50:46.145: INFO: ExecWithOptions: Clientset creation
    Jan  5 07:50:46.145: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8912/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 07:50:46.208: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 07:50:46.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8912" for this suite. 01/05/23 07:50:46.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:50:46.24
Jan  5 07:50:46.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:50:46.241
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:50:46.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:50:46.27
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-9b7a859d-3558-4003-9e6e-e20d3485830d 01/05/23 07:50:46.272
STEP: Creating a pod to test consume configMaps 01/05/23 07:50:46.279
Jan  5 07:50:46.284: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab" in namespace "projected-7095" to be "Succeeded or Failed"
Jan  5 07:50:46.286: INFO: Pod "pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.417434ms
Jan  5 07:50:48.290: INFO: Pod "pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006191336s
Jan  5 07:50:50.289: INFO: Pod "pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005089852s
STEP: Saw pod success 01/05/23 07:50:50.289
Jan  5 07:50:50.289: INFO: Pod "pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab" satisfied condition "Succeeded or Failed"
Jan  5 07:50:50.291: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/05/23 07:50:50.302
Jan  5 07:50:50.315: INFO: Waiting for pod pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab to disappear
Jan  5 07:50:50.316: INFO: Pod pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 07:50:50.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7095" for this suite. 01/05/23 07:50:50.318
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":82,"skipped":1709,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:50:46.24
    Jan  5 07:50:46.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:50:46.241
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:50:46.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:50:46.27
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-9b7a859d-3558-4003-9e6e-e20d3485830d 01/05/23 07:50:46.272
    STEP: Creating a pod to test consume configMaps 01/05/23 07:50:46.279
    Jan  5 07:50:46.284: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab" in namespace "projected-7095" to be "Succeeded or Failed"
    Jan  5 07:50:46.286: INFO: Pod "pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.417434ms
    Jan  5 07:50:48.290: INFO: Pod "pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006191336s
    Jan  5 07:50:50.289: INFO: Pod "pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005089852s
    STEP: Saw pod success 01/05/23 07:50:50.289
    Jan  5 07:50:50.289: INFO: Pod "pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab" satisfied condition "Succeeded or Failed"
    Jan  5 07:50:50.291: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/05/23 07:50:50.302
    Jan  5 07:50:50.315: INFO: Waiting for pod pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab to disappear
    Jan  5 07:50:50.316: INFO: Pod pod-projected-configmaps-69b50597-2bdf-42ce-8106-510331830cab no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 07:50:50.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7095" for this suite. 01/05/23 07:50:50.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:50:50.33
Jan  5 07:50:50.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-webhook 01/05/23 07:50:50.331
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:50:50.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:50:50.345
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/05/23 07:50:50.347
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 07:50:51.099
STEP: Deploying the custom resource conversion webhook pod 01/05/23 07:50:51.106
STEP: Wait for the deployment to be ready 01/05/23 07:50:51.12
Jan  5 07:50:51.131: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 07:50:53.137
STEP: Verifying the service has paired with the endpoint 01/05/23 07:50:53.148
Jan  5 07:50:54.148: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan  5 07:50:54.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Creating a v1 custom resource 01/05/23 07:50:56.686
STEP: Create a v2 custom resource 01/05/23 07:50:56.701
STEP: List CRs in v1 01/05/23 07:50:56.746
STEP: List CRs in v2 01/05/23 07:50:56.748
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 07:50:57.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6763" for this suite. 01/05/23 07:50:57.266
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":83,"skipped":1753,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.995 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:50:50.33
    Jan  5 07:50:50.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-webhook 01/05/23 07:50:50.331
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:50:50.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:50:50.345
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/05/23 07:50:50.347
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 07:50:51.099
    STEP: Deploying the custom resource conversion webhook pod 01/05/23 07:50:51.106
    STEP: Wait for the deployment to be ready 01/05/23 07:50:51.12
    Jan  5 07:50:51.131: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 07:50:53.137
    STEP: Verifying the service has paired with the endpoint 01/05/23 07:50:53.148
    Jan  5 07:50:54.148: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan  5 07:50:54.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Creating a v1 custom resource 01/05/23 07:50:56.686
    STEP: Create a v2 custom resource 01/05/23 07:50:56.701
    STEP: List CRs in v1 01/05/23 07:50:56.746
    STEP: List CRs in v2 01/05/23 07:50:56.748
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 07:50:57.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-6763" for this suite. 01/05/23 07:50:57.266
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:50:57.328
Jan  5 07:50:57.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename statefulset 01/05/23 07:50:57.328
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:50:57.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:50:57.369
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8427 01/05/23 07:50:57.37
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/05/23 07:50:57.382
Jan  5 07:50:57.443: INFO: Found 0 stateful pods, waiting for 3
Jan  5 07:51:07.447: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 07:51:07.447: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 07:51:07.447: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/05/23 07:51:07.453
Jan  5 07:51:07.470: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/05/23 07:51:07.47
STEP: Not applying an update when the partition is greater than the number of replicas 01/05/23 07:51:17.486
STEP: Performing a canary update 01/05/23 07:51:17.486
Jan  5 07:51:17.502: INFO: Updating stateful set ss2
Jan  5 07:51:17.516: INFO: Waiting for Pod statefulset-8427/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/05/23 07:51:27.524
Jan  5 07:51:27.574: INFO: Found 2 stateful pods, waiting for 3
Jan  5 07:51:37.578: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 07:51:37.578: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 07:51:37.578: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/05/23 07:51:37.583
Jan  5 07:51:37.601: INFO: Updating stateful set ss2
Jan  5 07:51:37.605: INFO: Waiting for Pod statefulset-8427/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan  5 07:51:47.631: INFO: Updating stateful set ss2
Jan  5 07:51:47.635: INFO: Waiting for StatefulSet statefulset-8427/ss2 to complete update
Jan  5 07:51:47.635: INFO: Waiting for Pod statefulset-8427/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 07:51:57.643: INFO: Deleting all statefulset in ns statefulset-8427
Jan  5 07:51:57.645: INFO: Scaling statefulset ss2 to 0
Jan  5 07:52:07.666: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 07:52:07.670: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 07:52:07.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8427" for this suite. 01/05/23 07:52:07.68
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":84,"skipped":1817,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.364 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:50:57.328
    Jan  5 07:50:57.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename statefulset 01/05/23 07:50:57.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:50:57.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:50:57.369
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8427 01/05/23 07:50:57.37
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/05/23 07:50:57.382
    Jan  5 07:50:57.443: INFO: Found 0 stateful pods, waiting for 3
    Jan  5 07:51:07.447: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 07:51:07.447: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 07:51:07.447: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/05/23 07:51:07.453
    Jan  5 07:51:07.470: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/05/23 07:51:07.47
    STEP: Not applying an update when the partition is greater than the number of replicas 01/05/23 07:51:17.486
    STEP: Performing a canary update 01/05/23 07:51:17.486
    Jan  5 07:51:17.502: INFO: Updating stateful set ss2
    Jan  5 07:51:17.516: INFO: Waiting for Pod statefulset-8427/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/05/23 07:51:27.524
    Jan  5 07:51:27.574: INFO: Found 2 stateful pods, waiting for 3
    Jan  5 07:51:37.578: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 07:51:37.578: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 07:51:37.578: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/05/23 07:51:37.583
    Jan  5 07:51:37.601: INFO: Updating stateful set ss2
    Jan  5 07:51:37.605: INFO: Waiting for Pod statefulset-8427/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan  5 07:51:47.631: INFO: Updating stateful set ss2
    Jan  5 07:51:47.635: INFO: Waiting for StatefulSet statefulset-8427/ss2 to complete update
    Jan  5 07:51:47.635: INFO: Waiting for Pod statefulset-8427/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 07:51:57.643: INFO: Deleting all statefulset in ns statefulset-8427
    Jan  5 07:51:57.645: INFO: Scaling statefulset ss2 to 0
    Jan  5 07:52:07.666: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 07:52:07.670: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 07:52:07.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8427" for this suite. 01/05/23 07:52:07.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:52:07.693
Jan  5 07:52:07.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 07:52:07.694
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:07.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:07.721
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-eb563e9f-b3c2-42a0-80f0-99a2fa500e45 01/05/23 07:52:07.723
STEP: Creating a pod to test consume secrets 01/05/23 07:52:07.73
Jan  5 07:52:07.736: INFO: Waiting up to 5m0s for pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f" in namespace "secrets-2939" to be "Succeeded or Failed"
Jan  5 07:52:07.739: INFO: Pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980583ms
Jan  5 07:52:09.743: INFO: Pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006481575s
Jan  5 07:52:11.744: INFO: Pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007857944s
Jan  5 07:52:13.743: INFO: Pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006472364s
STEP: Saw pod success 01/05/23 07:52:13.743
Jan  5 07:52:13.743: INFO: Pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f" satisfied condition "Succeeded or Failed"
Jan  5 07:52:13.745: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 07:52:13.749
Jan  5 07:52:13.765: INFO: Waiting for pod pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f to disappear
Jan  5 07:52:13.766: INFO: Pod pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 07:52:13.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2939" for this suite. 01/05/23 07:52:13.769
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":85,"skipped":1830,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.086 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:52:07.693
    Jan  5 07:52:07.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 07:52:07.694
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:07.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:07.721
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-eb563e9f-b3c2-42a0-80f0-99a2fa500e45 01/05/23 07:52:07.723
    STEP: Creating a pod to test consume secrets 01/05/23 07:52:07.73
    Jan  5 07:52:07.736: INFO: Waiting up to 5m0s for pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f" in namespace "secrets-2939" to be "Succeeded or Failed"
    Jan  5 07:52:07.739: INFO: Pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980583ms
    Jan  5 07:52:09.743: INFO: Pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006481575s
    Jan  5 07:52:11.744: INFO: Pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007857944s
    Jan  5 07:52:13.743: INFO: Pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006472364s
    STEP: Saw pod success 01/05/23 07:52:13.743
    Jan  5 07:52:13.743: INFO: Pod "pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f" satisfied condition "Succeeded or Failed"
    Jan  5 07:52:13.745: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 07:52:13.749
    Jan  5 07:52:13.765: INFO: Waiting for pod pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f to disappear
    Jan  5 07:52:13.766: INFO: Pod pod-secrets-85f8a071-00d8-4e63-90f8-7352a5e1da1f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 07:52:13.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2939" for this suite. 01/05/23 07:52:13.769
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:52:13.779
Jan  5 07:52:13.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename hostport 01/05/23 07:52:13.78
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:13.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:13.802
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/05/23 07:52:13.806
Jan  5 07:52:13.816: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5220" to be "running and ready"
Jan  5 07:52:13.817: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.524622ms
Jan  5 07:52:13.817: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:52:15.821: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005356486s
Jan  5 07:52:15.821: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:52:17.821: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.005096687s
Jan  5 07:52:17.821: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  5 07:52:17.821: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 16.0.14.212 on the node which pod1 resides and expect scheduled 01/05/23 07:52:17.821
Jan  5 07:52:17.825: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5220" to be "running and ready"
Jan  5 07:52:17.828: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.189849ms
Jan  5 07:52:17.828: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:52:19.831: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006587119s
Jan  5 07:52:19.831: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  5 07:52:19.831: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 16.0.14.212 but use UDP protocol on the node which pod2 resides 01/05/23 07:52:19.831
Jan  5 07:52:19.836: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5220" to be "running and ready"
Jan  5 07:52:19.838: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.894117ms
Jan  5 07:52:19.838: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:52:21.842: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.006470748s
Jan  5 07:52:21.842: INFO: The phase of Pod pod3 is Running (Ready = false)
Jan  5 07:52:23.841: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.005248185s
Jan  5 07:52:23.841: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan  5 07:52:23.841: INFO: Pod "pod3" satisfied condition "running and ready"
Jan  5 07:52:23.845: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5220" to be "running and ready"
Jan  5 07:52:23.846: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455857ms
Jan  5 07:52:23.846: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:52:25.850: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005554521s
Jan  5 07:52:25.850: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan  5 07:52:25.850: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/05/23 07:52:25.853
Jan  5 07:52:25.853: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 16.0.14.212 http://127.0.0.1:54323/hostname] Namespace:hostport-5220 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 07:52:25.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:52:25.853: INFO: ExecWithOptions: Clientset creation
Jan  5 07:52:25.853: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5220/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+16.0.14.212+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 16.0.14.212, port: 54323 01/05/23 07:52:25.939
Jan  5 07:52:25.939: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://16.0.14.212:54323/hostname] Namespace:hostport-5220 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 07:52:25.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:52:25.940: INFO: ExecWithOptions: Clientset creation
Jan  5 07:52:25.940: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5220/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F16.0.14.212%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 16.0.14.212, port: 54323 UDP 01/05/23 07:52:26.021
Jan  5 07:52:26.021: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 16.0.14.212 54323] Namespace:hostport-5220 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 07:52:26.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:52:26.022: INFO: ExecWithOptions: Clientset creation
Jan  5 07:52:26.022: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5220/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+16.0.14.212+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan  5 07:52:31.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-5220" for this suite. 01/05/23 07:52:31.096
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":86,"skipped":1830,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.321 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:52:13.779
    Jan  5 07:52:13.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename hostport 01/05/23 07:52:13.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:13.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:13.802
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/05/23 07:52:13.806
    Jan  5 07:52:13.816: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5220" to be "running and ready"
    Jan  5 07:52:13.817: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.524622ms
    Jan  5 07:52:13.817: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:52:15.821: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005356486s
    Jan  5 07:52:15.821: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:52:17.821: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.005096687s
    Jan  5 07:52:17.821: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  5 07:52:17.821: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 16.0.14.212 on the node which pod1 resides and expect scheduled 01/05/23 07:52:17.821
    Jan  5 07:52:17.825: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5220" to be "running and ready"
    Jan  5 07:52:17.828: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.189849ms
    Jan  5 07:52:17.828: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:52:19.831: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006587119s
    Jan  5 07:52:19.831: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  5 07:52:19.831: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 16.0.14.212 but use UDP protocol on the node which pod2 resides 01/05/23 07:52:19.831
    Jan  5 07:52:19.836: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5220" to be "running and ready"
    Jan  5 07:52:19.838: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.894117ms
    Jan  5 07:52:19.838: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:52:21.842: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.006470748s
    Jan  5 07:52:21.842: INFO: The phase of Pod pod3 is Running (Ready = false)
    Jan  5 07:52:23.841: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.005248185s
    Jan  5 07:52:23.841: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan  5 07:52:23.841: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan  5 07:52:23.845: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5220" to be "running and ready"
    Jan  5 07:52:23.846: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455857ms
    Jan  5 07:52:23.846: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:52:25.850: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005554521s
    Jan  5 07:52:25.850: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan  5 07:52:25.850: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/05/23 07:52:25.853
    Jan  5 07:52:25.853: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 16.0.14.212 http://127.0.0.1:54323/hostname] Namespace:hostport-5220 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 07:52:25.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:52:25.853: INFO: ExecWithOptions: Clientset creation
    Jan  5 07:52:25.853: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5220/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+16.0.14.212+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 16.0.14.212, port: 54323 01/05/23 07:52:25.939
    Jan  5 07:52:25.939: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://16.0.14.212:54323/hostname] Namespace:hostport-5220 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 07:52:25.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:52:25.940: INFO: ExecWithOptions: Clientset creation
    Jan  5 07:52:25.940: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5220/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F16.0.14.212%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 16.0.14.212, port: 54323 UDP 01/05/23 07:52:26.021
    Jan  5 07:52:26.021: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 16.0.14.212 54323] Namespace:hostport-5220 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 07:52:26.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:52:26.022: INFO: ExecWithOptions: Clientset creation
    Jan  5 07:52:26.022: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5220/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+16.0.14.212+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan  5 07:52:31.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-5220" for this suite. 01/05/23 07:52:31.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:52:31.101
Jan  5 07:52:31.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 07:52:31.102
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:31.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:31.119
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/05/23 07:52:31.124
Jan  5 07:52:31.129: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051" in namespace "downward-api-2369" to be "Succeeded or Failed"
Jan  5 07:52:31.131: INFO: Pod "downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.760133ms
Jan  5 07:52:33.135: INFO: Pod "downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006468794s
Jan  5 07:52:35.135: INFO: Pod "downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006033536s
STEP: Saw pod success 01/05/23 07:52:35.135
Jan  5 07:52:35.135: INFO: Pod "downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051" satisfied condition "Succeeded or Failed"
Jan  5 07:52:35.137: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051 container client-container: <nil>
STEP: delete the pod 01/05/23 07:52:35.14
Jan  5 07:52:35.156: INFO: Waiting for pod downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051 to disappear
Jan  5 07:52:35.158: INFO: Pod downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 07:52:35.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2369" for this suite. 01/05/23 07:52:35.161
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":87,"skipped":1867,"failed":0}
------------------------------
â€¢ [4.071 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:52:31.101
    Jan  5 07:52:31.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 07:52:31.102
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:31.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:31.119
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/05/23 07:52:31.124
    Jan  5 07:52:31.129: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051" in namespace "downward-api-2369" to be "Succeeded or Failed"
    Jan  5 07:52:31.131: INFO: Pod "downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.760133ms
    Jan  5 07:52:33.135: INFO: Pod "downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006468794s
    Jan  5 07:52:35.135: INFO: Pod "downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006033536s
    STEP: Saw pod success 01/05/23 07:52:35.135
    Jan  5 07:52:35.135: INFO: Pod "downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051" satisfied condition "Succeeded or Failed"
    Jan  5 07:52:35.137: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051 container client-container: <nil>
    STEP: delete the pod 01/05/23 07:52:35.14
    Jan  5 07:52:35.156: INFO: Waiting for pod downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051 to disappear
    Jan  5 07:52:35.158: INFO: Pod downwardapi-volume-b8bb3273-bc40-40d6-90dc-68f4603be051 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 07:52:35.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2369" for this suite. 01/05/23 07:52:35.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:52:35.172
Jan  5 07:52:35.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename prestop 01/05/23 07:52:35.173
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:35.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:35.192
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-5990 01/05/23 07:52:35.195
STEP: Waiting for pods to come up. 01/05/23 07:52:35.204
Jan  5 07:52:35.204: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5990" to be "running"
Jan  5 07:52:35.206: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048881ms
Jan  5 07:52:37.214: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010405282s
Jan  5 07:52:37.214: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-5990 01/05/23 07:52:37.226
Jan  5 07:52:37.236: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5990" to be "running"
Jan  5 07:52:37.242: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110806ms
Jan  5 07:52:39.246: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.00995224s
Jan  5 07:52:39.246: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/05/23 07:52:39.246
Jan  5 07:52:44.255: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/05/23 07:52:44.255
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan  5 07:52:44.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5990" for this suite. 01/05/23 07:52:44.277
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":88,"skipped":1877,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.108 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:52:35.172
    Jan  5 07:52:35.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename prestop 01/05/23 07:52:35.173
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:35.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:35.192
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-5990 01/05/23 07:52:35.195
    STEP: Waiting for pods to come up. 01/05/23 07:52:35.204
    Jan  5 07:52:35.204: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5990" to be "running"
    Jan  5 07:52:35.206: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048881ms
    Jan  5 07:52:37.214: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010405282s
    Jan  5 07:52:37.214: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-5990 01/05/23 07:52:37.226
    Jan  5 07:52:37.236: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5990" to be "running"
    Jan  5 07:52:37.242: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110806ms
    Jan  5 07:52:39.246: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.00995224s
    Jan  5 07:52:39.246: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/05/23 07:52:39.246
    Jan  5 07:52:44.255: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/05/23 07:52:44.255
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan  5 07:52:44.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-5990" for this suite. 01/05/23 07:52:44.277
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:52:44.281
Jan  5 07:52:44.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 07:52:44.281
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:44.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:44.301
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan  5 07:52:44.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 07:52:50.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9518" for this suite. 01/05/23 07:52:50.526
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":89,"skipped":1881,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.249 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:52:44.281
    Jan  5 07:52:44.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 07:52:44.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:44.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:44.301
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan  5 07:52:44.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 07:52:50.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9518" for this suite. 01/05/23 07:52:50.526
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:52:50.53
Jan  5 07:52:50.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 07:52:50.531
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:50.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:50.549
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/05/23 07:52:50.551
Jan  5 07:52:50.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 create -f -'
Jan  5 07:52:51.294: INFO: stderr: ""
Jan  5 07:52:51.294: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 07:52:51.294
Jan  5 07:52:51.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 07:52:51.364: INFO: stderr: ""
Jan  5 07:52:51.364: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-m6sw8 "
Jan  5 07:52:51.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 07:52:51.438: INFO: stderr: ""
Jan  5 07:52:51.438: INFO: stdout: ""
Jan  5 07:52:51.438: INFO: update-demo-nautilus-6l6ht is created but not running
Jan  5 07:52:56.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 07:52:56.497: INFO: stderr: ""
Jan  5 07:52:56.497: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-m6sw8 "
Jan  5 07:52:56.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 07:52:56.550: INFO: stderr: ""
Jan  5 07:52:56.550: INFO: stdout: ""
Jan  5 07:52:56.550: INFO: update-demo-nautilus-6l6ht is created but not running
Jan  5 07:53:01.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 07:53:01.610: INFO: stderr: ""
Jan  5 07:53:01.610: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-m6sw8 "
Jan  5 07:53:01.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 07:53:01.670: INFO: stderr: ""
Jan  5 07:53:01.670: INFO: stdout: "true"
Jan  5 07:53:01.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 07:53:01.737: INFO: stderr: ""
Jan  5 07:53:01.737: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 07:53:01.737: INFO: validating pod update-demo-nautilus-6l6ht
Jan  5 07:53:01.740: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 07:53:01.740: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 07:53:01.740: INFO: update-demo-nautilus-6l6ht is verified up and running
Jan  5 07:53:01.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-m6sw8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 07:53:01.799: INFO: stderr: ""
Jan  5 07:53:01.799: INFO: stdout: "true"
Jan  5 07:53:01.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-m6sw8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 07:53:01.874: INFO: stderr: ""
Jan  5 07:53:01.874: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 07:53:01.874: INFO: validating pod update-demo-nautilus-m6sw8
Jan  5 07:53:01.877: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 07:53:01.877: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 07:53:01.877: INFO: update-demo-nautilus-m6sw8 is verified up and running
STEP: scaling down the replication controller 01/05/23 07:53:01.877
Jan  5 07:53:01.880: INFO: scanned /root for discovery docs: <nil>
Jan  5 07:53:01.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan  5 07:53:02.964: INFO: stderr: ""
Jan  5 07:53:02.964: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 07:53:02.964
Jan  5 07:53:02.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 07:53:03.030: INFO: stderr: ""
Jan  5 07:53:03.030: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-m6sw8 "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/05/23 07:53:03.03
Jan  5 07:53:08.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 07:53:08.110: INFO: stderr: ""
Jan  5 07:53:08.110: INFO: stdout: "update-demo-nautilus-6l6ht "
Jan  5 07:53:08.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 07:53:08.185: INFO: stderr: ""
Jan  5 07:53:08.185: INFO: stdout: "true"
Jan  5 07:53:08.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 07:53:08.256: INFO: stderr: ""
Jan  5 07:53:08.256: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 07:53:08.256: INFO: validating pod update-demo-nautilus-6l6ht
Jan  5 07:53:08.259: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 07:53:08.259: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 07:53:08.259: INFO: update-demo-nautilus-6l6ht is verified up and running
STEP: scaling up the replication controller 01/05/23 07:53:08.259
Jan  5 07:53:08.260: INFO: scanned /root for discovery docs: <nil>
Jan  5 07:53:08.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan  5 07:53:09.358: INFO: stderr: ""
Jan  5 07:53:09.358: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 07:53:09.358
Jan  5 07:53:09.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 07:53:09.418: INFO: stderr: ""
Jan  5 07:53:09.418: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-lxxpv "
Jan  5 07:53:09.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 07:53:09.481: INFO: stderr: ""
Jan  5 07:53:09.481: INFO: stdout: "true"
Jan  5 07:53:09.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 07:53:09.547: INFO: stderr: ""
Jan  5 07:53:09.547: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 07:53:09.547: INFO: validating pod update-demo-nautilus-6l6ht
Jan  5 07:53:09.549: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 07:53:09.549: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 07:53:09.549: INFO: update-demo-nautilus-6l6ht is verified up and running
Jan  5 07:53:09.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-lxxpv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 07:53:09.634: INFO: stderr: ""
Jan  5 07:53:09.634: INFO: stdout: ""
Jan  5 07:53:09.634: INFO: update-demo-nautilus-lxxpv is created but not running
Jan  5 07:53:14.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 07:53:14.703: INFO: stderr: ""
Jan  5 07:53:14.703: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-lxxpv "
Jan  5 07:53:14.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 07:53:14.763: INFO: stderr: ""
Jan  5 07:53:14.763: INFO: stdout: "true"
Jan  5 07:53:14.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 07:53:14.827: INFO: stderr: ""
Jan  5 07:53:14.827: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 07:53:14.827: INFO: validating pod update-demo-nautilus-6l6ht
Jan  5 07:53:14.831: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 07:53:14.831: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 07:53:14.831: INFO: update-demo-nautilus-6l6ht is verified up and running
Jan  5 07:53:14.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-lxxpv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 07:53:14.896: INFO: stderr: ""
Jan  5 07:53:14.896: INFO: stdout: "true"
Jan  5 07:53:14.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-lxxpv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 07:53:14.981: INFO: stderr: ""
Jan  5 07:53:14.981: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 07:53:14.981: INFO: validating pod update-demo-nautilus-lxxpv
Jan  5 07:53:14.984: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 07:53:14.984: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 07:53:14.984: INFO: update-demo-nautilus-lxxpv is verified up and running
STEP: using delete to clean up resources 01/05/23 07:53:14.984
Jan  5 07:53:14.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 delete --grace-period=0 --force -f -'
Jan  5 07:53:15.053: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 07:53:15.053: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  5 07:53:15.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get rc,svc -l name=update-demo --no-headers'
Jan  5 07:53:15.121: INFO: stderr: "No resources found in kubectl-5062 namespace.\n"
Jan  5 07:53:15.121: INFO: stdout: ""
Jan  5 07:53:15.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  5 07:53:15.187: INFO: stderr: ""
Jan  5 07:53:15.187: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 07:53:15.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5062" for this suite. 01/05/23 07:53:15.19
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":90,"skipped":1881,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.664 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:52:50.53
    Jan  5 07:52:50.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 07:52:50.531
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:52:50.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:52:50.549
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/05/23 07:52:50.551
    Jan  5 07:52:50.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 create -f -'
    Jan  5 07:52:51.294: INFO: stderr: ""
    Jan  5 07:52:51.294: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 07:52:51.294
    Jan  5 07:52:51.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 07:52:51.364: INFO: stderr: ""
    Jan  5 07:52:51.364: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-m6sw8 "
    Jan  5 07:52:51.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 07:52:51.438: INFO: stderr: ""
    Jan  5 07:52:51.438: INFO: stdout: ""
    Jan  5 07:52:51.438: INFO: update-demo-nautilus-6l6ht is created but not running
    Jan  5 07:52:56.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 07:52:56.497: INFO: stderr: ""
    Jan  5 07:52:56.497: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-m6sw8 "
    Jan  5 07:52:56.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 07:52:56.550: INFO: stderr: ""
    Jan  5 07:52:56.550: INFO: stdout: ""
    Jan  5 07:52:56.550: INFO: update-demo-nautilus-6l6ht is created but not running
    Jan  5 07:53:01.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 07:53:01.610: INFO: stderr: ""
    Jan  5 07:53:01.610: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-m6sw8 "
    Jan  5 07:53:01.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 07:53:01.670: INFO: stderr: ""
    Jan  5 07:53:01.670: INFO: stdout: "true"
    Jan  5 07:53:01.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 07:53:01.737: INFO: stderr: ""
    Jan  5 07:53:01.737: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 07:53:01.737: INFO: validating pod update-demo-nautilus-6l6ht
    Jan  5 07:53:01.740: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 07:53:01.740: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 07:53:01.740: INFO: update-demo-nautilus-6l6ht is verified up and running
    Jan  5 07:53:01.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-m6sw8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 07:53:01.799: INFO: stderr: ""
    Jan  5 07:53:01.799: INFO: stdout: "true"
    Jan  5 07:53:01.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-m6sw8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 07:53:01.874: INFO: stderr: ""
    Jan  5 07:53:01.874: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 07:53:01.874: INFO: validating pod update-demo-nautilus-m6sw8
    Jan  5 07:53:01.877: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 07:53:01.877: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 07:53:01.877: INFO: update-demo-nautilus-m6sw8 is verified up and running
    STEP: scaling down the replication controller 01/05/23 07:53:01.877
    Jan  5 07:53:01.880: INFO: scanned /root for discovery docs: <nil>
    Jan  5 07:53:01.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan  5 07:53:02.964: INFO: stderr: ""
    Jan  5 07:53:02.964: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 07:53:02.964
    Jan  5 07:53:02.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 07:53:03.030: INFO: stderr: ""
    Jan  5 07:53:03.030: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-m6sw8 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/05/23 07:53:03.03
    Jan  5 07:53:08.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 07:53:08.110: INFO: stderr: ""
    Jan  5 07:53:08.110: INFO: stdout: "update-demo-nautilus-6l6ht "
    Jan  5 07:53:08.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 07:53:08.185: INFO: stderr: ""
    Jan  5 07:53:08.185: INFO: stdout: "true"
    Jan  5 07:53:08.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 07:53:08.256: INFO: stderr: ""
    Jan  5 07:53:08.256: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 07:53:08.256: INFO: validating pod update-demo-nautilus-6l6ht
    Jan  5 07:53:08.259: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 07:53:08.259: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 07:53:08.259: INFO: update-demo-nautilus-6l6ht is verified up and running
    STEP: scaling up the replication controller 01/05/23 07:53:08.259
    Jan  5 07:53:08.260: INFO: scanned /root for discovery docs: <nil>
    Jan  5 07:53:08.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan  5 07:53:09.358: INFO: stderr: ""
    Jan  5 07:53:09.358: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 07:53:09.358
    Jan  5 07:53:09.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 07:53:09.418: INFO: stderr: ""
    Jan  5 07:53:09.418: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-lxxpv "
    Jan  5 07:53:09.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 07:53:09.481: INFO: stderr: ""
    Jan  5 07:53:09.481: INFO: stdout: "true"
    Jan  5 07:53:09.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 07:53:09.547: INFO: stderr: ""
    Jan  5 07:53:09.547: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 07:53:09.547: INFO: validating pod update-demo-nautilus-6l6ht
    Jan  5 07:53:09.549: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 07:53:09.549: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 07:53:09.549: INFO: update-demo-nautilus-6l6ht is verified up and running
    Jan  5 07:53:09.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-lxxpv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 07:53:09.634: INFO: stderr: ""
    Jan  5 07:53:09.634: INFO: stdout: ""
    Jan  5 07:53:09.634: INFO: update-demo-nautilus-lxxpv is created but not running
    Jan  5 07:53:14.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 07:53:14.703: INFO: stderr: ""
    Jan  5 07:53:14.703: INFO: stdout: "update-demo-nautilus-6l6ht update-demo-nautilus-lxxpv "
    Jan  5 07:53:14.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 07:53:14.763: INFO: stderr: ""
    Jan  5 07:53:14.763: INFO: stdout: "true"
    Jan  5 07:53:14.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-6l6ht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 07:53:14.827: INFO: stderr: ""
    Jan  5 07:53:14.827: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 07:53:14.827: INFO: validating pod update-demo-nautilus-6l6ht
    Jan  5 07:53:14.831: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 07:53:14.831: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 07:53:14.831: INFO: update-demo-nautilus-6l6ht is verified up and running
    Jan  5 07:53:14.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-lxxpv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 07:53:14.896: INFO: stderr: ""
    Jan  5 07:53:14.896: INFO: stdout: "true"
    Jan  5 07:53:14.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods update-demo-nautilus-lxxpv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 07:53:14.981: INFO: stderr: ""
    Jan  5 07:53:14.981: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 07:53:14.981: INFO: validating pod update-demo-nautilus-lxxpv
    Jan  5 07:53:14.984: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 07:53:14.984: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 07:53:14.984: INFO: update-demo-nautilus-lxxpv is verified up and running
    STEP: using delete to clean up resources 01/05/23 07:53:14.984
    Jan  5 07:53:14.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 delete --grace-period=0 --force -f -'
    Jan  5 07:53:15.053: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 07:53:15.053: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  5 07:53:15.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get rc,svc -l name=update-demo --no-headers'
    Jan  5 07:53:15.121: INFO: stderr: "No resources found in kubectl-5062 namespace.\n"
    Jan  5 07:53:15.121: INFO: stdout: ""
    Jan  5 07:53:15.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5062 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  5 07:53:15.187: INFO: stderr: ""
    Jan  5 07:53:15.187: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 07:53:15.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5062" for this suite. 01/05/23 07:53:15.19
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:53:15.195
Jan  5 07:53:15.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 07:53:15.196
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:15.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:15.211
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-c437587d-07ad-4f42-9741-917ccf46065d 01/05/23 07:53:15.223
STEP: Creating secret with name s-test-opt-upd-98542868-2f36-4b66-bbf7-13700d5ae40a 01/05/23 07:53:15.226
STEP: Creating the pod 01/05/23 07:53:15.236
Jan  5 07:53:15.268: INFO: Waiting up to 5m0s for pod "pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb" in namespace "secrets-8709" to be "running and ready"
Jan  5 07:53:15.280: INFO: Pod "pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.281404ms
Jan  5 07:53:15.280: INFO: The phase of Pod pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:53:17.282: INFO: Pod "pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014364965s
Jan  5 07:53:17.282: INFO: The phase of Pod pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:53:19.284: INFO: Pod "pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb": Phase="Running", Reason="", readiness=true. Elapsed: 4.01593008s
Jan  5 07:53:19.284: INFO: The phase of Pod pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb is Running (Ready = true)
Jan  5 07:53:19.284: INFO: Pod "pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-c437587d-07ad-4f42-9741-917ccf46065d 01/05/23 07:53:19.295
STEP: Updating secret s-test-opt-upd-98542868-2f36-4b66-bbf7-13700d5ae40a 01/05/23 07:53:19.299
STEP: Creating secret with name s-test-opt-create-50f0b028-8012-41a5-95b3-5fac5042c67f 01/05/23 07:53:19.308
STEP: waiting to observe update in volume 01/05/23 07:53:19.311
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 07:53:23.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8709" for this suite. 01/05/23 07:53:23.335
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":91,"skipped":1883,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.143 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:53:15.195
    Jan  5 07:53:15.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 07:53:15.196
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:15.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:15.211
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-c437587d-07ad-4f42-9741-917ccf46065d 01/05/23 07:53:15.223
    STEP: Creating secret with name s-test-opt-upd-98542868-2f36-4b66-bbf7-13700d5ae40a 01/05/23 07:53:15.226
    STEP: Creating the pod 01/05/23 07:53:15.236
    Jan  5 07:53:15.268: INFO: Waiting up to 5m0s for pod "pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb" in namespace "secrets-8709" to be "running and ready"
    Jan  5 07:53:15.280: INFO: Pod "pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.281404ms
    Jan  5 07:53:15.280: INFO: The phase of Pod pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:53:17.282: INFO: Pod "pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014364965s
    Jan  5 07:53:17.282: INFO: The phase of Pod pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:53:19.284: INFO: Pod "pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb": Phase="Running", Reason="", readiness=true. Elapsed: 4.01593008s
    Jan  5 07:53:19.284: INFO: The phase of Pod pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb is Running (Ready = true)
    Jan  5 07:53:19.284: INFO: Pod "pod-secrets-8cb41a81-f713-429c-800a-c85c6ccafdbb" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-c437587d-07ad-4f42-9741-917ccf46065d 01/05/23 07:53:19.295
    STEP: Updating secret s-test-opt-upd-98542868-2f36-4b66-bbf7-13700d5ae40a 01/05/23 07:53:19.299
    STEP: Creating secret with name s-test-opt-create-50f0b028-8012-41a5-95b3-5fac5042c67f 01/05/23 07:53:19.308
    STEP: waiting to observe update in volume 01/05/23 07:53:19.311
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 07:53:23.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8709" for this suite. 01/05/23 07:53:23.335
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:53:23.338
Jan  5 07:53:23.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 07:53:23.339
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:23.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:23.363
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/05/23 07:53:23.365
Jan  5 07:53:23.377: INFO: Waiting up to 5m0s for pod "labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d" in namespace "downward-api-540" to be "running and ready"
Jan  5 07:53:23.380: INFO: Pod "labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.890558ms
Jan  5 07:53:23.380: INFO: The phase of Pod labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d is Pending, waiting for it to be Running (with Ready = true)
Jan  5 07:53:25.383: INFO: Pod "labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006214129s
Jan  5 07:53:25.383: INFO: The phase of Pod labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d is Running (Ready = true)
Jan  5 07:53:25.383: INFO: Pod "labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d" satisfied condition "running and ready"
Jan  5 07:53:25.899: INFO: Successfully updated pod "labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 07:53:27.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-540" for this suite. 01/05/23 07:53:27.913
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":92,"skipped":1884,"failed":0}
------------------------------
â€¢ [4.580 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:53:23.338
    Jan  5 07:53:23.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 07:53:23.339
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:23.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:23.363
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/05/23 07:53:23.365
    Jan  5 07:53:23.377: INFO: Waiting up to 5m0s for pod "labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d" in namespace "downward-api-540" to be "running and ready"
    Jan  5 07:53:23.380: INFO: Pod "labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.890558ms
    Jan  5 07:53:23.380: INFO: The phase of Pod labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 07:53:25.383: INFO: Pod "labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006214129s
    Jan  5 07:53:25.383: INFO: The phase of Pod labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d is Running (Ready = true)
    Jan  5 07:53:25.383: INFO: Pod "labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d" satisfied condition "running and ready"
    Jan  5 07:53:25.899: INFO: Successfully updated pod "labelsupdate9a8806d8-06f9-4c78-9560-f8d3ff17ea8d"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 07:53:27.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-540" for this suite. 01/05/23 07:53:27.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:53:27.919
Jan  5 07:53:27.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 07:53:27.921
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:27.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:27.939
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-2b7bc3b3-f53e-4873-b192-41acf8c19f13 01/05/23 07:53:27.942
STEP: Creating a pod to test consume secrets 01/05/23 07:53:27.946
Jan  5 07:53:27.955: INFO: Waiting up to 5m0s for pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23" in namespace "secrets-5334" to be "Succeeded or Failed"
Jan  5 07:53:27.957: INFO: Pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331453ms
Jan  5 07:53:29.960: INFO: Pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005149535s
Jan  5 07:53:31.960: INFO: Pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005490902s
Jan  5 07:53:33.960: INFO: Pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005242493s
STEP: Saw pod success 01/05/23 07:53:33.96
Jan  5 07:53:33.960: INFO: Pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23" satisfied condition "Succeeded or Failed"
Jan  5 07:53:33.962: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 07:53:33.966
Jan  5 07:53:33.981: INFO: Waiting for pod pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23 to disappear
Jan  5 07:53:33.983: INFO: Pod pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 07:53:33.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5334" for this suite. 01/05/23 07:53:33.986
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":93,"skipped":1896,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.077 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:53:27.919
    Jan  5 07:53:27.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 07:53:27.921
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:27.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:27.939
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-2b7bc3b3-f53e-4873-b192-41acf8c19f13 01/05/23 07:53:27.942
    STEP: Creating a pod to test consume secrets 01/05/23 07:53:27.946
    Jan  5 07:53:27.955: INFO: Waiting up to 5m0s for pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23" in namespace "secrets-5334" to be "Succeeded or Failed"
    Jan  5 07:53:27.957: INFO: Pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331453ms
    Jan  5 07:53:29.960: INFO: Pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005149535s
    Jan  5 07:53:31.960: INFO: Pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005490902s
    Jan  5 07:53:33.960: INFO: Pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005242493s
    STEP: Saw pod success 01/05/23 07:53:33.96
    Jan  5 07:53:33.960: INFO: Pod "pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23" satisfied condition "Succeeded or Failed"
    Jan  5 07:53:33.962: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 07:53:33.966
    Jan  5 07:53:33.981: INFO: Waiting for pod pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23 to disappear
    Jan  5 07:53:33.983: INFO: Pod pod-secrets-f05dccce-b248-42c0-9289-a2de7ba47f23 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 07:53:33.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5334" for this suite. 01/05/23 07:53:33.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:53:33.997
Jan  5 07:53:33.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:53:33.997
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:34.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:34.011
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/05/23 07:53:34.013
Jan  5 07:53:34.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128" in namespace "projected-6648" to be "Succeeded or Failed"
Jan  5 07:53:34.024: INFO: Pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128": Phase="Pending", Reason="", readiness=false. Elapsed: 1.692157ms
Jan  5 07:53:36.027: INFO: Pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005056796s
Jan  5 07:53:38.027: INFO: Pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004646426s
Jan  5 07:53:40.028: INFO: Pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005500187s
STEP: Saw pod success 01/05/23 07:53:40.028
Jan  5 07:53:40.028: INFO: Pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128" satisfied condition "Succeeded or Failed"
Jan  5 07:53:40.085: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128 container client-container: <nil>
STEP: delete the pod 01/05/23 07:53:40.089
Jan  5 07:53:40.103: INFO: Waiting for pod downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128 to disappear
Jan  5 07:53:40.107: INFO: Pod downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 07:53:40.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6648" for this suite. 01/05/23 07:53:40.109
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":94,"skipped":1914,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.116 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:53:33.997
    Jan  5 07:53:33.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:53:33.997
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:34.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:34.011
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/05/23 07:53:34.013
    Jan  5 07:53:34.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128" in namespace "projected-6648" to be "Succeeded or Failed"
    Jan  5 07:53:34.024: INFO: Pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128": Phase="Pending", Reason="", readiness=false. Elapsed: 1.692157ms
    Jan  5 07:53:36.027: INFO: Pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005056796s
    Jan  5 07:53:38.027: INFO: Pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004646426s
    Jan  5 07:53:40.028: INFO: Pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005500187s
    STEP: Saw pod success 01/05/23 07:53:40.028
    Jan  5 07:53:40.028: INFO: Pod "downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128" satisfied condition "Succeeded or Failed"
    Jan  5 07:53:40.085: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128 container client-container: <nil>
    STEP: delete the pod 01/05/23 07:53:40.089
    Jan  5 07:53:40.103: INFO: Waiting for pod downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128 to disappear
    Jan  5 07:53:40.107: INFO: Pod downwardapi-volume-29b2f1e2-a900-4441-ba9b-7a662533a128 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 07:53:40.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6648" for this suite. 01/05/23 07:53:40.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:53:40.114
Jan  5 07:53:40.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubelet-test 01/05/23 07:53:40.115
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:40.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:40.129
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  5 07:53:40.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2559" for this suite. 01/05/23 07:53:40.161
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":95,"skipped":1961,"failed":0}
------------------------------
â€¢ [0.051 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:53:40.114
    Jan  5 07:53:40.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 07:53:40.115
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:40.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:40.129
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  5 07:53:40.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2559" for this suite. 01/05/23 07:53:40.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:53:40.166
Jan  5 07:53:40.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pods 01/05/23 07:53:40.167
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:40.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:40.18
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/05/23 07:53:40.196
Jan  5 07:53:40.207: INFO: Waiting up to 5m0s for pod "pod-n4g92" in namespace "pods-5900" to be "running"
Jan  5 07:53:40.209: INFO: Pod "pod-n4g92": Phase="Pending", Reason="", readiness=false. Elapsed: 1.722982ms
Jan  5 07:53:42.213: INFO: Pod "pod-n4g92": Phase="Running", Reason="", readiness=true. Elapsed: 2.006090591s
Jan  5 07:53:42.213: INFO: Pod "pod-n4g92" satisfied condition "running"
STEP: patching /status 01/05/23 07:53:42.213
Jan  5 07:53:42.226: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 07:53:42.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5900" for this suite. 01/05/23 07:53:42.228
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":96,"skipped":1988,"failed":0}
------------------------------
â€¢ [2.080 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:53:40.166
    Jan  5 07:53:40.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pods 01/05/23 07:53:40.167
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:40.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:40.18
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/05/23 07:53:40.196
    Jan  5 07:53:40.207: INFO: Waiting up to 5m0s for pod "pod-n4g92" in namespace "pods-5900" to be "running"
    Jan  5 07:53:40.209: INFO: Pod "pod-n4g92": Phase="Pending", Reason="", readiness=false. Elapsed: 1.722982ms
    Jan  5 07:53:42.213: INFO: Pod "pod-n4g92": Phase="Running", Reason="", readiness=true. Elapsed: 2.006090591s
    Jan  5 07:53:42.213: INFO: Pod "pod-n4g92" satisfied condition "running"
    STEP: patching /status 01/05/23 07:53:42.213
    Jan  5 07:53:42.226: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 07:53:42.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5900" for this suite. 01/05/23 07:53:42.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:53:42.247
Jan  5 07:53:42.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 07:53:42.247
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:42.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:42.277
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan  5 07:53:42.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/05/23 07:53:44.327
Jan  5 07:53:44.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 create -f -'
Jan  5 07:53:44.858: INFO: stderr: ""
Jan  5 07:53:44.858: INFO: stdout: "e2e-test-crd-publish-openapi-5786-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  5 07:53:44.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 delete e2e-test-crd-publish-openapi-5786-crds test-foo'
Jan  5 07:53:44.937: INFO: stderr: ""
Jan  5 07:53:44.937: INFO: stdout: "e2e-test-crd-publish-openapi-5786-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan  5 07:53:44.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 apply -f -'
Jan  5 07:53:45.153: INFO: stderr: ""
Jan  5 07:53:45.153: INFO: stdout: "e2e-test-crd-publish-openapi-5786-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  5 07:53:45.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 delete e2e-test-crd-publish-openapi-5786-crds test-foo'
Jan  5 07:53:45.221: INFO: stderr: ""
Jan  5 07:53:45.221: INFO: stdout: "e2e-test-crd-publish-openapi-5786-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/05/23 07:53:45.221
Jan  5 07:53:45.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 create -f -'
Jan  5 07:53:45.433: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/05/23 07:53:45.433
Jan  5 07:53:45.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 create -f -'
Jan  5 07:53:45.637: INFO: rc: 1
Jan  5 07:53:45.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 apply -f -'
Jan  5 07:53:45.823: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/05/23 07:53:45.823
Jan  5 07:53:45.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 create -f -'
Jan  5 07:53:46.005: INFO: rc: 1
Jan  5 07:53:46.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 apply -f -'
Jan  5 07:53:46.218: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/05/23 07:53:46.218
Jan  5 07:53:46.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 explain e2e-test-crd-publish-openapi-5786-crds'
Jan  5 07:53:46.395: INFO: stderr: ""
Jan  5 07:53:46.395: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/05/23 07:53:46.396
Jan  5 07:53:46.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 explain e2e-test-crd-publish-openapi-5786-crds.metadata'
Jan  5 07:53:46.576: INFO: stderr: ""
Jan  5 07:53:46.576: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan  5 07:53:46.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 explain e2e-test-crd-publish-openapi-5786-crds.spec'
Jan  5 07:53:46.763: INFO: stderr: ""
Jan  5 07:53:46.763: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan  5 07:53:46.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 explain e2e-test-crd-publish-openapi-5786-crds.spec.bars'
Jan  5 07:53:46.927: INFO: stderr: ""
Jan  5 07:53:46.927: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/05/23 07:53:46.927
Jan  5 07:53:46.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 explain e2e-test-crd-publish-openapi-5786-crds.spec.bars2'
Jan  5 07:53:47.107: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 07:53:49.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-838" for this suite. 01/05/23 07:53:49.157
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":97,"skipped":2002,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.932 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:53:42.247
    Jan  5 07:53:42.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 07:53:42.247
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:42.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:42.277
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan  5 07:53:42.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/05/23 07:53:44.327
    Jan  5 07:53:44.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 create -f -'
    Jan  5 07:53:44.858: INFO: stderr: ""
    Jan  5 07:53:44.858: INFO: stdout: "e2e-test-crd-publish-openapi-5786-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  5 07:53:44.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 delete e2e-test-crd-publish-openapi-5786-crds test-foo'
    Jan  5 07:53:44.937: INFO: stderr: ""
    Jan  5 07:53:44.937: INFO: stdout: "e2e-test-crd-publish-openapi-5786-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan  5 07:53:44.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 apply -f -'
    Jan  5 07:53:45.153: INFO: stderr: ""
    Jan  5 07:53:45.153: INFO: stdout: "e2e-test-crd-publish-openapi-5786-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  5 07:53:45.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 delete e2e-test-crd-publish-openapi-5786-crds test-foo'
    Jan  5 07:53:45.221: INFO: stderr: ""
    Jan  5 07:53:45.221: INFO: stdout: "e2e-test-crd-publish-openapi-5786-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/05/23 07:53:45.221
    Jan  5 07:53:45.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 create -f -'
    Jan  5 07:53:45.433: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/05/23 07:53:45.433
    Jan  5 07:53:45.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 create -f -'
    Jan  5 07:53:45.637: INFO: rc: 1
    Jan  5 07:53:45.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 apply -f -'
    Jan  5 07:53:45.823: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/05/23 07:53:45.823
    Jan  5 07:53:45.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 create -f -'
    Jan  5 07:53:46.005: INFO: rc: 1
    Jan  5 07:53:46.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 --namespace=crd-publish-openapi-838 apply -f -'
    Jan  5 07:53:46.218: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/05/23 07:53:46.218
    Jan  5 07:53:46.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 explain e2e-test-crd-publish-openapi-5786-crds'
    Jan  5 07:53:46.395: INFO: stderr: ""
    Jan  5 07:53:46.395: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/05/23 07:53:46.396
    Jan  5 07:53:46.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 explain e2e-test-crd-publish-openapi-5786-crds.metadata'
    Jan  5 07:53:46.576: INFO: stderr: ""
    Jan  5 07:53:46.576: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan  5 07:53:46.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 explain e2e-test-crd-publish-openapi-5786-crds.spec'
    Jan  5 07:53:46.763: INFO: stderr: ""
    Jan  5 07:53:46.763: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan  5 07:53:46.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 explain e2e-test-crd-publish-openapi-5786-crds.spec.bars'
    Jan  5 07:53:46.927: INFO: stderr: ""
    Jan  5 07:53:46.927: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/05/23 07:53:46.927
    Jan  5 07:53:46.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-838 explain e2e-test-crd-publish-openapi-5786-crds.spec.bars2'
    Jan  5 07:53:47.107: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 07:53:49.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-838" for this suite. 01/05/23 07:53:49.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:53:49.179
Jan  5 07:53:49.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 07:53:49.181
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:49.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:49.198
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 07:53:49.2
Jan  5 07:53:49.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4269 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan  5 07:53:49.266: INFO: stderr: ""
Jan  5 07:53:49.266: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 07:53:49.266
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan  5 07:53:49.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4269 delete pods e2e-test-httpd-pod'
Jan  5 07:53:52.220: INFO: stderr: ""
Jan  5 07:53:52.220: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 07:53:52.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4269" for this suite. 01/05/23 07:53:52.222
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":98,"skipped":2018,"failed":0}
------------------------------
â€¢ [3.046 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:53:49.179
    Jan  5 07:53:49.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 07:53:49.181
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:49.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:49.198
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/05/23 07:53:49.2
    Jan  5 07:53:49.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4269 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan  5 07:53:49.266: INFO: stderr: ""
    Jan  5 07:53:49.266: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 07:53:49.266
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan  5 07:53:49.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4269 delete pods e2e-test-httpd-pod'
    Jan  5 07:53:52.220: INFO: stderr: ""
    Jan  5 07:53:52.220: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 07:53:52.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4269" for this suite. 01/05/23 07:53:52.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:53:52.226
Jan  5 07:53:52.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 07:53:52.227
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:52.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:52.25
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 07:53:52.252
Jan  5 07:53:52.257: INFO: Waiting up to 5m0s for pod "pod-a53eaa04-287f-4c09-8c58-2c695af73c36" in namespace "emptydir-8309" to be "Succeeded or Failed"
Jan  5 07:53:52.259: INFO: Pod "pod-a53eaa04-287f-4c09-8c58-2c695af73c36": Phase="Pending", Reason="", readiness=false. Elapsed: 1.436305ms
Jan  5 07:53:54.262: INFO: Pod "pod-a53eaa04-287f-4c09-8c58-2c695af73c36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004409854s
Jan  5 07:53:56.261: INFO: Pod "pod-a53eaa04-287f-4c09-8c58-2c695af73c36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004084533s
STEP: Saw pod success 01/05/23 07:53:56.261
Jan  5 07:53:56.262: INFO: Pod "pod-a53eaa04-287f-4c09-8c58-2c695af73c36" satisfied condition "Succeeded or Failed"
Jan  5 07:53:56.263: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-a53eaa04-287f-4c09-8c58-2c695af73c36 container test-container: <nil>
STEP: delete the pod 01/05/23 07:53:56.266
Jan  5 07:53:56.282: INFO: Waiting for pod pod-a53eaa04-287f-4c09-8c58-2c695af73c36 to disappear
Jan  5 07:53:56.284: INFO: Pod pod-a53eaa04-287f-4c09-8c58-2c695af73c36 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 07:53:56.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8309" for this suite. 01/05/23 07:53:56.285
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":99,"skipped":2025,"failed":0}
------------------------------
â€¢ [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:53:52.226
    Jan  5 07:53:52.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 07:53:52.227
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:52.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:52.25
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 07:53:52.252
    Jan  5 07:53:52.257: INFO: Waiting up to 5m0s for pod "pod-a53eaa04-287f-4c09-8c58-2c695af73c36" in namespace "emptydir-8309" to be "Succeeded or Failed"
    Jan  5 07:53:52.259: INFO: Pod "pod-a53eaa04-287f-4c09-8c58-2c695af73c36": Phase="Pending", Reason="", readiness=false. Elapsed: 1.436305ms
    Jan  5 07:53:54.262: INFO: Pod "pod-a53eaa04-287f-4c09-8c58-2c695af73c36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004409854s
    Jan  5 07:53:56.261: INFO: Pod "pod-a53eaa04-287f-4c09-8c58-2c695af73c36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004084533s
    STEP: Saw pod success 01/05/23 07:53:56.261
    Jan  5 07:53:56.262: INFO: Pod "pod-a53eaa04-287f-4c09-8c58-2c695af73c36" satisfied condition "Succeeded or Failed"
    Jan  5 07:53:56.263: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-a53eaa04-287f-4c09-8c58-2c695af73c36 container test-container: <nil>
    STEP: delete the pod 01/05/23 07:53:56.266
    Jan  5 07:53:56.282: INFO: Waiting for pod pod-a53eaa04-287f-4c09-8c58-2c695af73c36 to disappear
    Jan  5 07:53:56.284: INFO: Pod pod-a53eaa04-287f-4c09-8c58-2c695af73c36 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 07:53:56.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8309" for this suite. 01/05/23 07:53:56.285
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:53:56.296
Jan  5 07:53:56.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 07:53:56.296
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:56.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:56.313
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/05/23 07:53:56.324
Jan  5 07:53:56.331: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c" in namespace "projected-6259" to be "Succeeded or Failed"
Jan  5 07:53:56.332: INFO: Pod "downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.687273ms
Jan  5 07:53:58.335: INFO: Pod "downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004626433s
Jan  5 07:54:00.336: INFO: Pod "downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005585154s
STEP: Saw pod success 01/05/23 07:54:00.336
Jan  5 07:54:00.336: INFO: Pod "downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c" satisfied condition "Succeeded or Failed"
Jan  5 07:54:00.338: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c container client-container: <nil>
STEP: delete the pod 01/05/23 07:54:00.341
Jan  5 07:54:00.352: INFO: Waiting for pod downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c to disappear
Jan  5 07:54:00.354: INFO: Pod downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 07:54:00.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6259" for this suite. 01/05/23 07:54:00.356
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":100,"skipped":2029,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:53:56.296
    Jan  5 07:53:56.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 07:53:56.296
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:53:56.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:53:56.313
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/05/23 07:53:56.324
    Jan  5 07:53:56.331: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c" in namespace "projected-6259" to be "Succeeded or Failed"
    Jan  5 07:53:56.332: INFO: Pod "downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.687273ms
    Jan  5 07:53:58.335: INFO: Pod "downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004626433s
    Jan  5 07:54:00.336: INFO: Pod "downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005585154s
    STEP: Saw pod success 01/05/23 07:54:00.336
    Jan  5 07:54:00.336: INFO: Pod "downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c" satisfied condition "Succeeded or Failed"
    Jan  5 07:54:00.338: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c container client-container: <nil>
    STEP: delete the pod 01/05/23 07:54:00.341
    Jan  5 07:54:00.352: INFO: Waiting for pod downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c to disappear
    Jan  5 07:54:00.354: INFO: Pod downwardapi-volume-56bc2a87-c35d-4864-9ec5-1c733440662c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 07:54:00.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6259" for this suite. 01/05/23 07:54:00.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:54:00.361
Jan  5 07:54:00.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pods 01/05/23 07:54:00.362
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:00.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:00.397
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/05/23 07:54:00.4
Jan  5 07:54:00.405: INFO: created test-pod-1
Jan  5 07:54:00.416: INFO: created test-pod-2
Jan  5 07:54:00.425: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/05/23 07:54:00.425
Jan  5 07:54:00.425: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7770' to be running and ready
Jan  5 07:54:00.444: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 07:54:00.444: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 07:54:00.444: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 07:54:00.444: INFO: 0 / 3 pods in namespace 'pods-7770' are running and ready (0 seconds elapsed)
Jan  5 07:54:00.444: INFO: expected 0 pod replicas in namespace 'pods-7770', 0 are Running and Ready.
Jan  5 07:54:00.444: INFO: POD         NODE                                  PHASE    GRACE  CONDITIONS
Jan  5 07:54:00.444: INFO: test-pod-1  mip-bd-vm724.mip.storage.hpecorp.net  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  }]
Jan  5 07:54:00.444: INFO: test-pod-2  mip-bd-vm724.mip.storage.hpecorp.net  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  }]
Jan  5 07:54:00.444: INFO: test-pod-3  mip-bd-vm724.mip.storage.hpecorp.net  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  }]
Jan  5 07:54:00.444: INFO: 
Jan  5 07:54:02.451: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 07:54:02.451: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 07:54:02.451: INFO: 1 / 3 pods in namespace 'pods-7770' are running and ready (2 seconds elapsed)
Jan  5 07:54:02.451: INFO: expected 0 pod replicas in namespace 'pods-7770', 0 are Running and Ready.
Jan  5 07:54:02.451: INFO: POD         NODE                                  PHASE    GRACE  CONDITIONS
Jan  5 07:54:02.451: INFO: test-pod-1  mip-bd-vm724.mip.storage.hpecorp.net  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  }]
Jan  5 07:54:02.451: INFO: test-pod-2  mip-bd-vm724.mip.storage.hpecorp.net  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  }]
Jan  5 07:54:02.451: INFO: 
Jan  5 07:54:04.452: INFO: 3 / 3 pods in namespace 'pods-7770' are running and ready (4 seconds elapsed)
Jan  5 07:54:04.452: INFO: expected 0 pod replicas in namespace 'pods-7770', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/05/23 07:54:04.47
Jan  5 07:54:04.472: INFO: Pod quantity 3 is different from expected quantity 0
Jan  5 07:54:05.496: INFO: Pod quantity 2 is different from expected quantity 0
Jan  5 07:54:06.475: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 07:54:07.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7770" for this suite. 01/05/23 07:54:07.479
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":101,"skipped":2056,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.123 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:54:00.361
    Jan  5 07:54:00.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pods 01/05/23 07:54:00.362
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:00.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:00.397
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/05/23 07:54:00.4
    Jan  5 07:54:00.405: INFO: created test-pod-1
    Jan  5 07:54:00.416: INFO: created test-pod-2
    Jan  5 07:54:00.425: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/05/23 07:54:00.425
    Jan  5 07:54:00.425: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7770' to be running and ready
    Jan  5 07:54:00.444: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 07:54:00.444: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 07:54:00.444: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 07:54:00.444: INFO: 0 / 3 pods in namespace 'pods-7770' are running and ready (0 seconds elapsed)
    Jan  5 07:54:00.444: INFO: expected 0 pod replicas in namespace 'pods-7770', 0 are Running and Ready.
    Jan  5 07:54:00.444: INFO: POD         NODE                                  PHASE    GRACE  CONDITIONS
    Jan  5 07:54:00.444: INFO: test-pod-1  mip-bd-vm724.mip.storage.hpecorp.net  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  }]
    Jan  5 07:54:00.444: INFO: test-pod-2  mip-bd-vm724.mip.storage.hpecorp.net  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  }]
    Jan  5 07:54:00.444: INFO: test-pod-3  mip-bd-vm724.mip.storage.hpecorp.net  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  }]
    Jan  5 07:54:00.444: INFO: 
    Jan  5 07:54:02.451: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 07:54:02.451: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 07:54:02.451: INFO: 1 / 3 pods in namespace 'pods-7770' are running and ready (2 seconds elapsed)
    Jan  5 07:54:02.451: INFO: expected 0 pod replicas in namespace 'pods-7770', 0 are Running and Ready.
    Jan  5 07:54:02.451: INFO: POD         NODE                                  PHASE    GRACE  CONDITIONS
    Jan  5 07:54:02.451: INFO: test-pod-1  mip-bd-vm724.mip.storage.hpecorp.net  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  }]
    Jan  5 07:54:02.451: INFO: test-pod-2  mip-bd-vm724.mip.storage.hpecorp.net  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 07:54:00 +0000 UTC  }]
    Jan  5 07:54:02.451: INFO: 
    Jan  5 07:54:04.452: INFO: 3 / 3 pods in namespace 'pods-7770' are running and ready (4 seconds elapsed)
    Jan  5 07:54:04.452: INFO: expected 0 pod replicas in namespace 'pods-7770', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/05/23 07:54:04.47
    Jan  5 07:54:04.472: INFO: Pod quantity 3 is different from expected quantity 0
    Jan  5 07:54:05.496: INFO: Pod quantity 2 is different from expected quantity 0
    Jan  5 07:54:06.475: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 07:54:07.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7770" for this suite. 01/05/23 07:54:07.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:54:07.488
Jan  5 07:54:07.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 07:54:07.489
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:07.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:07.506
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-b3d21126-7521-4dd6-ac60-42528c155e23 01/05/23 07:54:07.513
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  5 07:54:07.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1489" for this suite. 01/05/23 07:54:07.516
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":102,"skipped":2128,"failed":0}
------------------------------
â€¢ [0.032 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:54:07.488
    Jan  5 07:54:07.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 07:54:07.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:07.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:07.506
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-b3d21126-7521-4dd6-ac60-42528c155e23 01/05/23 07:54:07.513
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 07:54:07.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1489" for this suite. 01/05/23 07:54:07.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:54:07.521
Jan  5 07:54:07.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 07:54:07.521
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:07.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:07.535
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/05/23 07:54:07.539
Jan  5 07:54:07.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 07:54:09.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 07:54:19.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2007" for this suite. 01/05/23 07:54:19.365
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":103,"skipped":2138,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.850 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:54:07.521
    Jan  5 07:54:07.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 07:54:07.521
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:07.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:07.535
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/05/23 07:54:07.539
    Jan  5 07:54:07.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 07:54:09.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 07:54:19.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2007" for this suite. 01/05/23 07:54:19.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:54:19.371
Jan  5 07:54:19.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 07:54:19.371
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:19.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:19.389
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/05/23 07:54:19.394
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/05/23 07:54:19.395
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 07:54:19.395
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/05/23 07:54:19.395
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/05/23 07:54:19.396
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 07:54:19.396
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 07:54:19.397
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 07:54:19.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2717" for this suite. 01/05/23 07:54:19.398
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":104,"skipped":2143,"failed":0}
------------------------------
â€¢ [0.046 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:54:19.371
    Jan  5 07:54:19.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 07:54:19.371
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:19.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:19.389
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/05/23 07:54:19.394
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/05/23 07:54:19.395
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 07:54:19.395
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/05/23 07:54:19.395
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/05/23 07:54:19.396
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 07:54:19.396
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 07:54:19.397
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 07:54:19.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2717" for this suite. 01/05/23 07:54:19.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:54:19.417
Jan  5 07:54:19.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename namespaces 01/05/23 07:54:19.418
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:19.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:19.432
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/05/23 07:54:19.438
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:19.457
STEP: Creating a service in the namespace 01/05/23 07:54:19.458
STEP: Deleting the namespace 01/05/23 07:54:19.486
STEP: Waiting for the namespace to be removed. 01/05/23 07:54:19.5
STEP: Recreating the namespace 01/05/23 07:54:25.502
STEP: Verifying there is no service in the namespace 01/05/23 07:54:25.514
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  5 07:54:25.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8608" for this suite. 01/05/23 07:54:25.517
STEP: Destroying namespace "nsdeletetest-8617" for this suite. 01/05/23 07:54:25.525
Jan  5 07:54:25.526: INFO: Namespace nsdeletetest-8617 was already deleted
STEP: Destroying namespace "nsdeletetest-1225" for this suite. 01/05/23 07:54:25.526
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":105,"skipped":2159,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.112 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:54:19.417
    Jan  5 07:54:19.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename namespaces 01/05/23 07:54:19.418
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:19.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:19.432
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/05/23 07:54:19.438
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:19.457
    STEP: Creating a service in the namespace 01/05/23 07:54:19.458
    STEP: Deleting the namespace 01/05/23 07:54:19.486
    STEP: Waiting for the namespace to be removed. 01/05/23 07:54:19.5
    STEP: Recreating the namespace 01/05/23 07:54:25.502
    STEP: Verifying there is no service in the namespace 01/05/23 07:54:25.514
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 07:54:25.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8608" for this suite. 01/05/23 07:54:25.517
    STEP: Destroying namespace "nsdeletetest-8617" for this suite. 01/05/23 07:54:25.525
    Jan  5 07:54:25.526: INFO: Namespace nsdeletetest-8617 was already deleted
    STEP: Destroying namespace "nsdeletetest-1225" for this suite. 01/05/23 07:54:25.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:54:25.531
Jan  5 07:54:25.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 07:54:25.532
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:25.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:25.546
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/05/23 07:54:25.549
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 07:54:25.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4052" for this suite. 01/05/23 07:54:25.553
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":106,"skipped":2182,"failed":0}
------------------------------
â€¢ [0.025 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:54:25.531
    Jan  5 07:54:25.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 07:54:25.532
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:25.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:25.546
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/05/23 07:54:25.549
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 07:54:25.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4052" for this suite. 01/05/23 07:54:25.553
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:54:25.556
Jan  5 07:54:25.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename proxy 01/05/23 07:54:25.557
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:25.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:25.575
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan  5 07:54:25.577: INFO: Creating pod...
Jan  5 07:54:25.582: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4276" to be "running"
Jan  5 07:54:25.584: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366419ms
Jan  5 07:54:27.589: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.006888949s
Jan  5 07:54:27.589: INFO: Pod "agnhost" satisfied condition "running"
Jan  5 07:54:27.589: INFO: Creating service...
Jan  5 07:54:27.605: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=DELETE
Jan  5 07:54:27.608: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 07:54:27.608: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=OPTIONS
Jan  5 07:54:27.616: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 07:54:27.616: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=PATCH
Jan  5 07:54:27.619: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 07:54:27.619: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=POST
Jan  5 07:54:27.621: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 07:54:27.621: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=PUT
Jan  5 07:54:27.623: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  5 07:54:27.623: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=DELETE
Jan  5 07:54:27.624: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 07:54:27.624: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan  5 07:54:27.626: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 07:54:27.626: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=PATCH
Jan  5 07:54:27.628: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 07:54:27.628: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=POST
Jan  5 07:54:27.629: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 07:54:27.629: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=PUT
Jan  5 07:54:27.631: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  5 07:54:27.631: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=GET
Jan  5 07:54:27.632: INFO: http.Client request:GET StatusCode:301
Jan  5 07:54:27.632: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=GET
Jan  5 07:54:27.633: INFO: http.Client request:GET StatusCode:301
Jan  5 07:54:27.633: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=HEAD
Jan  5 07:54:27.634: INFO: http.Client request:HEAD StatusCode:301
Jan  5 07:54:27.634: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=HEAD
Jan  5 07:54:27.635: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan  5 07:54:27.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4276" for this suite. 01/05/23 07:54:27.636
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":107,"skipped":2193,"failed":0}
------------------------------
â€¢ [2.084 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:54:25.556
    Jan  5 07:54:25.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename proxy 01/05/23 07:54:25.557
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:25.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:25.575
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan  5 07:54:25.577: INFO: Creating pod...
    Jan  5 07:54:25.582: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4276" to be "running"
    Jan  5 07:54:25.584: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366419ms
    Jan  5 07:54:27.589: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.006888949s
    Jan  5 07:54:27.589: INFO: Pod "agnhost" satisfied condition "running"
    Jan  5 07:54:27.589: INFO: Creating service...
    Jan  5 07:54:27.605: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=DELETE
    Jan  5 07:54:27.608: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 07:54:27.608: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=OPTIONS
    Jan  5 07:54:27.616: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 07:54:27.616: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=PATCH
    Jan  5 07:54:27.619: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 07:54:27.619: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=POST
    Jan  5 07:54:27.621: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 07:54:27.621: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=PUT
    Jan  5 07:54:27.623: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  5 07:54:27.623: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan  5 07:54:27.624: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 07:54:27.624: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan  5 07:54:27.626: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 07:54:27.626: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan  5 07:54:27.628: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 07:54:27.628: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=POST
    Jan  5 07:54:27.629: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 07:54:27.629: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=PUT
    Jan  5 07:54:27.631: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  5 07:54:27.631: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=GET
    Jan  5 07:54:27.632: INFO: http.Client request:GET StatusCode:301
    Jan  5 07:54:27.632: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=GET
    Jan  5 07:54:27.633: INFO: http.Client request:GET StatusCode:301
    Jan  5 07:54:27.633: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/pods/agnhost/proxy?method=HEAD
    Jan  5 07:54:27.634: INFO: http.Client request:HEAD StatusCode:301
    Jan  5 07:54:27.634: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4276/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan  5 07:54:27.635: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan  5 07:54:27.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-4276" for this suite. 01/05/23 07:54:27.636
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:54:27.64
Jan  5 07:54:27.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-watch 01/05/23 07:54:27.641
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:27.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:27.653
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan  5 07:54:27.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Creating first CR  01/05/23 07:54:30.221
Jan  5 07:54:30.236: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:30Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:54:30Z]] name:name1 resourceVersion:12640 uid:821611ea-4549-479b-92a6-c2a84631d5bf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/05/23 07:54:40.237
Jan  5 07:54:40.241: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:40Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:54:40Z]] name:name2 resourceVersion:12688 uid:a2f3be15-973a-4620-936c-ade064d358cb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/05/23 07:54:50.242
Jan  5 07:54:50.248: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:54:50Z]] name:name1 resourceVersion:12705 uid:821611ea-4549-479b-92a6-c2a84631d5bf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/05/23 07:55:00.248
Jan  5 07:55:00.255: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:55:00Z]] name:name2 resourceVersion:12722 uid:a2f3be15-973a-4620-936c-ade064d358cb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/05/23 07:55:10.255
Jan  5 07:55:10.261: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:54:50Z]] name:name1 resourceVersion:12739 uid:821611ea-4549-479b-92a6-c2a84631d5bf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/05/23 07:55:20.261
Jan  5 07:55:20.268: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:55:00Z]] name:name2 resourceVersion:12756 uid:a2f3be15-973a-4620-936c-ade064d358cb] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 07:55:30.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2006" for this suite. 01/05/23 07:55:30.781
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":108,"skipped":2196,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.147 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:54:27.64
    Jan  5 07:54:27.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-watch 01/05/23 07:54:27.641
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:54:27.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:54:27.653
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan  5 07:54:27.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Creating first CR  01/05/23 07:54:30.221
    Jan  5 07:54:30.236: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:30Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:54:30Z]] name:name1 resourceVersion:12640 uid:821611ea-4549-479b-92a6-c2a84631d5bf] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/05/23 07:54:40.237
    Jan  5 07:54:40.241: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:40Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:54:40Z]] name:name2 resourceVersion:12688 uid:a2f3be15-973a-4620-936c-ade064d358cb] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/05/23 07:54:50.242
    Jan  5 07:54:50.248: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:54:50Z]] name:name1 resourceVersion:12705 uid:821611ea-4549-479b-92a6-c2a84631d5bf] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/05/23 07:55:00.248
    Jan  5 07:55:00.255: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:55:00Z]] name:name2 resourceVersion:12722 uid:a2f3be15-973a-4620-936c-ade064d358cb] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/05/23 07:55:10.255
    Jan  5 07:55:10.261: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:54:50Z]] name:name1 resourceVersion:12739 uid:821611ea-4549-479b-92a6-c2a84631d5bf] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/05/23 07:55:20.261
    Jan  5 07:55:20.268: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T07:54:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T07:55:00Z]] name:name2 resourceVersion:12756 uid:a2f3be15-973a-4620-936c-ade064d358cb] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 07:55:30.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-2006" for this suite. 01/05/23 07:55:30.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:55:30.788
Jan  5 07:55:30.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename gc 01/05/23 07:55:30.789
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:55:30.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:55:30.808
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/05/23 07:55:30.815
STEP: create the rc2 01/05/23 07:55:30.819
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/05/23 07:55:35.832
STEP: delete the rc simpletest-rc-to-be-deleted 01/05/23 07:55:36.227
STEP: wait for the rc to be deleted 01/05/23 07:55:36.231
Jan  5 07:55:41.240: INFO: 73 pods remaining
Jan  5 07:55:41.240: INFO: 73 pods has nil DeletionTimestamp
Jan  5 07:55:41.240: INFO: 
STEP: Gathering metrics 01/05/23 07:55:46.24
W0105 07:55:46.242930      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 07:55:46.242: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  5 07:55:46.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-29r4p" in namespace "gc-5126"
Jan  5 07:55:46.259: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bwtx" in namespace "gc-5126"
Jan  5 07:55:46.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-2c8x4" in namespace "gc-5126"
Jan  5 07:55:46.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mfzc" in namespace "gc-5126"
Jan  5 07:55:46.299: INFO: Deleting pod "simpletest-rc-to-be-deleted-49w9n" in namespace "gc-5126"
Jan  5 07:55:46.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bmfv" in namespace "gc-5126"
Jan  5 07:55:46.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hg69" in namespace "gc-5126"
Jan  5 07:55:46.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vvf4" in namespace "gc-5126"
Jan  5 07:55:46.346: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wbj6" in namespace "gc-5126"
Jan  5 07:55:46.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-6844p" in namespace "gc-5126"
Jan  5 07:55:46.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dw5m" in namespace "gc-5126"
Jan  5 07:55:46.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z9vb" in namespace "gc-5126"
Jan  5 07:55:46.454: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zwpm" in namespace "gc-5126"
Jan  5 07:55:46.464: INFO: Deleting pod "simpletest-rc-to-be-deleted-82kp6" in namespace "gc-5126"
Jan  5 07:55:46.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gskx" in namespace "gc-5126"
Jan  5 07:55:46.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mfjz" in namespace "gc-5126"
Jan  5 07:55:46.527: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mvwt" in namespace "gc-5126"
Jan  5 07:55:46.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-8smd7" in namespace "gc-5126"
Jan  5 07:55:46.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-92h9b" in namespace "gc-5126"
Jan  5 07:55:46.566: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kf5p" in namespace "gc-5126"
Jan  5 07:55:46.582: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lpd5" in namespace "gc-5126"
Jan  5 07:55:46.597: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2d6j" in namespace "gc-5126"
Jan  5 07:55:46.616: INFO: Deleting pod "simpletest-rc-to-be-deleted-b62g4" in namespace "gc-5126"
Jan  5 07:55:46.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9w6x" in namespace "gc-5126"
Jan  5 07:55:46.647: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxmfz" in namespace "gc-5126"
Jan  5 07:55:46.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzj5f" in namespace "gc-5126"
Jan  5 07:55:46.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5hs8" in namespace "gc-5126"
Jan  5 07:55:46.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8wdd" in namespace "gc-5126"
Jan  5 07:55:46.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-cttzb" in namespace "gc-5126"
Jan  5 07:55:46.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7xm7" in namespace "gc-5126"
Jan  5 07:55:46.763: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9g2x" in namespace "gc-5126"
Jan  5 07:55:46.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9pzk" in namespace "gc-5126"
Jan  5 07:55:46.823: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqcxc" in namespace "gc-5126"
Jan  5 07:55:46.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2wgl" in namespace "gc-5126"
Jan  5 07:55:46.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2znc" in namespace "gc-5126"
Jan  5 07:55:46.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4pt8" in namespace "gc-5126"
Jan  5 07:55:46.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-gccl7" in namespace "gc-5126"
Jan  5 07:55:46.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfjmf" in namespace "gc-5126"
Jan  5 07:55:46.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkbdc" in namespace "gc-5126"
Jan  5 07:55:46.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-grz8h" in namespace "gc-5126"
Jan  5 07:55:46.997: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxs7z" in namespace "gc-5126"
Jan  5 07:55:47.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-h54nf" in namespace "gc-5126"
Jan  5 07:55:47.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhpcf" in namespace "gc-5126"
Jan  5 07:55:47.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmq9k" in namespace "gc-5126"
Jan  5 07:55:47.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrk7d" in namespace "gc-5126"
Jan  5 07:55:47.161: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwgwq" in namespace "gc-5126"
Jan  5 07:55:47.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfpjc" in namespace "gc-5126"
Jan  5 07:55:47.194: INFO: Deleting pod "simpletest-rc-to-be-deleted-jht5f" in namespace "gc-5126"
Jan  5 07:55:47.214: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk52z" in namespace "gc-5126"
Jan  5 07:55:47.233: INFO: Deleting pod "simpletest-rc-to-be-deleted-jptsh" in namespace "gc-5126"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 07:55:47.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5126" for this suite. 01/05/23 07:55:47.255
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":109,"skipped":2223,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.475 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:55:30.788
    Jan  5 07:55:30.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename gc 01/05/23 07:55:30.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:55:30.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:55:30.808
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/05/23 07:55:30.815
    STEP: create the rc2 01/05/23 07:55:30.819
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/05/23 07:55:35.832
    STEP: delete the rc simpletest-rc-to-be-deleted 01/05/23 07:55:36.227
    STEP: wait for the rc to be deleted 01/05/23 07:55:36.231
    Jan  5 07:55:41.240: INFO: 73 pods remaining
    Jan  5 07:55:41.240: INFO: 73 pods has nil DeletionTimestamp
    Jan  5 07:55:41.240: INFO: 
    STEP: Gathering metrics 01/05/23 07:55:46.24
    W0105 07:55:46.242930      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 07:55:46.242: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  5 07:55:46.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-29r4p" in namespace "gc-5126"
    Jan  5 07:55:46.259: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bwtx" in namespace "gc-5126"
    Jan  5 07:55:46.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-2c8x4" in namespace "gc-5126"
    Jan  5 07:55:46.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mfzc" in namespace "gc-5126"
    Jan  5 07:55:46.299: INFO: Deleting pod "simpletest-rc-to-be-deleted-49w9n" in namespace "gc-5126"
    Jan  5 07:55:46.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bmfv" in namespace "gc-5126"
    Jan  5 07:55:46.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hg69" in namespace "gc-5126"
    Jan  5 07:55:46.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vvf4" in namespace "gc-5126"
    Jan  5 07:55:46.346: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wbj6" in namespace "gc-5126"
    Jan  5 07:55:46.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-6844p" in namespace "gc-5126"
    Jan  5 07:55:46.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dw5m" in namespace "gc-5126"
    Jan  5 07:55:46.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z9vb" in namespace "gc-5126"
    Jan  5 07:55:46.454: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zwpm" in namespace "gc-5126"
    Jan  5 07:55:46.464: INFO: Deleting pod "simpletest-rc-to-be-deleted-82kp6" in namespace "gc-5126"
    Jan  5 07:55:46.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gskx" in namespace "gc-5126"
    Jan  5 07:55:46.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mfjz" in namespace "gc-5126"
    Jan  5 07:55:46.527: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mvwt" in namespace "gc-5126"
    Jan  5 07:55:46.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-8smd7" in namespace "gc-5126"
    Jan  5 07:55:46.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-92h9b" in namespace "gc-5126"
    Jan  5 07:55:46.566: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kf5p" in namespace "gc-5126"
    Jan  5 07:55:46.582: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lpd5" in namespace "gc-5126"
    Jan  5 07:55:46.597: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2d6j" in namespace "gc-5126"
    Jan  5 07:55:46.616: INFO: Deleting pod "simpletest-rc-to-be-deleted-b62g4" in namespace "gc-5126"
    Jan  5 07:55:46.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9w6x" in namespace "gc-5126"
    Jan  5 07:55:46.647: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxmfz" in namespace "gc-5126"
    Jan  5 07:55:46.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzj5f" in namespace "gc-5126"
    Jan  5 07:55:46.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5hs8" in namespace "gc-5126"
    Jan  5 07:55:46.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8wdd" in namespace "gc-5126"
    Jan  5 07:55:46.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-cttzb" in namespace "gc-5126"
    Jan  5 07:55:46.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7xm7" in namespace "gc-5126"
    Jan  5 07:55:46.763: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9g2x" in namespace "gc-5126"
    Jan  5 07:55:46.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9pzk" in namespace "gc-5126"
    Jan  5 07:55:46.823: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqcxc" in namespace "gc-5126"
    Jan  5 07:55:46.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2wgl" in namespace "gc-5126"
    Jan  5 07:55:46.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2znc" in namespace "gc-5126"
    Jan  5 07:55:46.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4pt8" in namespace "gc-5126"
    Jan  5 07:55:46.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-gccl7" in namespace "gc-5126"
    Jan  5 07:55:46.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfjmf" in namespace "gc-5126"
    Jan  5 07:55:46.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkbdc" in namespace "gc-5126"
    Jan  5 07:55:46.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-grz8h" in namespace "gc-5126"
    Jan  5 07:55:46.997: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxs7z" in namespace "gc-5126"
    Jan  5 07:55:47.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-h54nf" in namespace "gc-5126"
    Jan  5 07:55:47.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhpcf" in namespace "gc-5126"
    Jan  5 07:55:47.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmq9k" in namespace "gc-5126"
    Jan  5 07:55:47.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrk7d" in namespace "gc-5126"
    Jan  5 07:55:47.161: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwgwq" in namespace "gc-5126"
    Jan  5 07:55:47.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfpjc" in namespace "gc-5126"
    Jan  5 07:55:47.194: INFO: Deleting pod "simpletest-rc-to-be-deleted-jht5f" in namespace "gc-5126"
    Jan  5 07:55:47.214: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk52z" in namespace "gc-5126"
    Jan  5 07:55:47.233: INFO: Deleting pod "simpletest-rc-to-be-deleted-jptsh" in namespace "gc-5126"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 07:55:47.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5126" for this suite. 01/05/23 07:55:47.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:55:47.263
Jan  5 07:55:47.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 07:55:47.264
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:55:47.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:55:47.281
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 07:55:47.283
Jan  5 07:55:47.292: INFO: Waiting up to 5m0s for pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684" in namespace "emptydir-9594" to be "Succeeded or Failed"
Jan  5 07:55:47.294: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 2.231319ms
Jan  5 07:55:49.297: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005424277s
Jan  5 07:55:51.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006474912s
Jan  5 07:55:53.313: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021065874s
Jan  5 07:55:55.297: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005310621s
Jan  5 07:55:57.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005908264s
Jan  5 07:55:59.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005925006s
Jan  5 07:56:01.297: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005613132s
Jan  5 07:56:03.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006096982s
Jan  5 07:56:05.297: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005189178s
Jan  5 07:56:07.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.006204175s
STEP: Saw pod success 01/05/23 07:56:07.298
Jan  5 07:56:07.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684" satisfied condition "Succeeded or Failed"
Jan  5 07:56:07.300: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-29f0e988-4315-4e89-a873-1d2c8afa3684 container test-container: <nil>
STEP: delete the pod 01/05/23 07:56:07.313
Jan  5 07:56:07.329: INFO: Waiting for pod pod-29f0e988-4315-4e89-a873-1d2c8afa3684 to disappear
Jan  5 07:56:07.330: INFO: Pod pod-29f0e988-4315-4e89-a873-1d2c8afa3684 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 07:56:07.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9594" for this suite. 01/05/23 07:56:07.332
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":110,"skipped":2235,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.078 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:55:47.263
    Jan  5 07:55:47.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 07:55:47.264
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:55:47.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:55:47.281
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 07:55:47.283
    Jan  5 07:55:47.292: INFO: Waiting up to 5m0s for pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684" in namespace "emptydir-9594" to be "Succeeded or Failed"
    Jan  5 07:55:47.294: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 2.231319ms
    Jan  5 07:55:49.297: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005424277s
    Jan  5 07:55:51.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006474912s
    Jan  5 07:55:53.313: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021065874s
    Jan  5 07:55:55.297: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005310621s
    Jan  5 07:55:57.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005908264s
    Jan  5 07:55:59.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005925006s
    Jan  5 07:56:01.297: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005613132s
    Jan  5 07:56:03.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006096982s
    Jan  5 07:56:05.297: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005189178s
    Jan  5 07:56:07.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.006204175s
    STEP: Saw pod success 01/05/23 07:56:07.298
    Jan  5 07:56:07.298: INFO: Pod "pod-29f0e988-4315-4e89-a873-1d2c8afa3684" satisfied condition "Succeeded or Failed"
    Jan  5 07:56:07.300: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-29f0e988-4315-4e89-a873-1d2c8afa3684 container test-container: <nil>
    STEP: delete the pod 01/05/23 07:56:07.313
    Jan  5 07:56:07.329: INFO: Waiting for pod pod-29f0e988-4315-4e89-a873-1d2c8afa3684 to disappear
    Jan  5 07:56:07.330: INFO: Pod pod-29f0e988-4315-4e89-a873-1d2c8afa3684 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 07:56:07.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9594" for this suite. 01/05/23 07:56:07.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:56:07.343
Jan  5 07:56:07.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename endpointslice 01/05/23 07:56:07.344
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:56:07.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:56:07.359
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan  5 07:56:07.364: INFO: Endpoints addresses: [16.0.14.211] , ports: [6443]
Jan  5 07:56:07.365: INFO: EndpointSlices addresses: [16.0.14.211] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  5 07:56:07.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3206" for this suite. 01/05/23 07:56:07.366
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":111,"skipped":2262,"failed":0}
------------------------------
â€¢ [0.045 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:56:07.343
    Jan  5 07:56:07.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename endpointslice 01/05/23 07:56:07.344
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:56:07.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:56:07.359
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan  5 07:56:07.364: INFO: Endpoints addresses: [16.0.14.211] , ports: [6443]
    Jan  5 07:56:07.365: INFO: EndpointSlices addresses: [16.0.14.211] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  5 07:56:07.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3206" for this suite. 01/05/23 07:56:07.366
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:56:07.388
Jan  5 07:56:07.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename var-expansion 01/05/23 07:56:07.389
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:56:07.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:56:07.405
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/05/23 07:56:07.407
Jan  5 07:56:07.419: INFO: Waiting up to 2m0s for pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501" in namespace "var-expansion-6160" to be "running"
Jan  5 07:56:07.421: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1.848433ms
Jan  5 07:56:09.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004652496s
Jan  5 07:56:11.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005169906s
Jan  5 07:56:13.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006103496s
Jan  5 07:56:15.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004734258s
Jan  5 07:56:17.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005144557s
Jan  5 07:56:19.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004870426s
Jan  5 07:56:21.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005346305s
Jan  5 07:56:23.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005850558s
Jan  5 07:56:25.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004998969s
Jan  5 07:56:27.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00455895s
Jan  5 07:56:29.423: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 22.003872773s
Jan  5 07:56:31.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004447496s
Jan  5 07:56:33.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006112897s
Jan  5 07:56:35.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004727861s
Jan  5 07:56:37.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005216627s
Jan  5 07:56:39.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004633088s
Jan  5 07:56:41.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004793472s
Jan  5 07:56:43.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005014722s
Jan  5 07:56:45.430: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010181052s
Jan  5 07:56:47.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005683768s
Jan  5 07:56:49.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005362526s
Jan  5 07:56:51.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006536701s
Jan  5 07:56:53.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004655546s
Jan  5 07:56:55.429: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009066666s
Jan  5 07:56:57.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004358234s
Jan  5 07:56:59.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004896058s
Jan  5 07:57:01.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005492547s
Jan  5 07:57:03.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006948454s
Jan  5 07:57:05.428: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008358612s
Jan  5 07:57:07.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005855088s
Jan  5 07:57:09.427: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007301748s
Jan  5 07:57:11.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006532959s
Jan  5 07:57:13.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006678117s
Jan  5 07:57:15.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005942785s
Jan  5 07:57:17.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006664849s
Jan  5 07:57:19.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006388012s
Jan  5 07:57:21.427: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007657618s
Jan  5 07:57:23.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005771919s
Jan  5 07:57:25.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005606933s
Jan  5 07:57:27.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006752076s
Jan  5 07:57:29.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005059273s
Jan  5 07:57:31.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006913552s
Jan  5 07:57:33.427: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007129384s
Jan  5 07:57:35.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004929148s
Jan  5 07:57:37.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004564838s
Jan  5 07:57:39.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006294159s
Jan  5 07:57:41.427: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007863403s
Jan  5 07:57:43.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005326736s
Jan  5 07:57:45.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00493813s
Jan  5 07:57:47.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006360376s
Jan  5 07:57:49.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006191766s
Jan  5 07:57:51.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005452519s
Jan  5 07:57:53.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005400034s
Jan  5 07:57:55.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005323617s
Jan  5 07:57:57.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00605267s
Jan  5 07:57:59.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005143132s
Jan  5 07:58:01.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005955573s
Jan  5 07:58:03.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006338908s
Jan  5 07:58:05.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005886757s
Jan  5 07:58:07.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005240083s
Jan  5 07:58:07.427: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007295631s
STEP: updating the pod 01/05/23 07:58:07.427
Jan  5 07:58:07.937: INFO: Successfully updated pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501"
STEP: waiting for pod running 01/05/23 07:58:07.937
Jan  5 07:58:07.937: INFO: Waiting up to 2m0s for pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501" in namespace "var-expansion-6160" to be "running"
Jan  5 07:58:07.941: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307926ms
Jan  5 07:58:09.944: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Running", Reason="", readiness=true. Elapsed: 2.007148237s
Jan  5 07:58:09.944: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501" satisfied condition "running"
STEP: deleting the pod gracefully 01/05/23 07:58:09.944
Jan  5 07:58:09.944: INFO: Deleting pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501" in namespace "var-expansion-6160"
Jan  5 07:58:09.949: INFO: Wait up to 5m0s for pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 07:58:41.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6160" for this suite. 01/05/23 07:58:41.957
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":112,"skipped":2265,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.574 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:56:07.388
    Jan  5 07:56:07.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename var-expansion 01/05/23 07:56:07.389
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:56:07.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:56:07.405
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/05/23 07:56:07.407
    Jan  5 07:56:07.419: INFO: Waiting up to 2m0s for pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501" in namespace "var-expansion-6160" to be "running"
    Jan  5 07:56:07.421: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1.848433ms
    Jan  5 07:56:09.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004652496s
    Jan  5 07:56:11.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005169906s
    Jan  5 07:56:13.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006103496s
    Jan  5 07:56:15.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004734258s
    Jan  5 07:56:17.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005144557s
    Jan  5 07:56:19.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004870426s
    Jan  5 07:56:21.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005346305s
    Jan  5 07:56:23.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005850558s
    Jan  5 07:56:25.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004998969s
    Jan  5 07:56:27.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00455895s
    Jan  5 07:56:29.423: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 22.003872773s
    Jan  5 07:56:31.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004447496s
    Jan  5 07:56:33.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006112897s
    Jan  5 07:56:35.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004727861s
    Jan  5 07:56:37.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005216627s
    Jan  5 07:56:39.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004633088s
    Jan  5 07:56:41.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004793472s
    Jan  5 07:56:43.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005014722s
    Jan  5 07:56:45.430: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010181052s
    Jan  5 07:56:47.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005683768s
    Jan  5 07:56:49.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005362526s
    Jan  5 07:56:51.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006536701s
    Jan  5 07:56:53.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004655546s
    Jan  5 07:56:55.429: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009066666s
    Jan  5 07:56:57.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004358234s
    Jan  5 07:56:59.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004896058s
    Jan  5 07:57:01.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005492547s
    Jan  5 07:57:03.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006948454s
    Jan  5 07:57:05.428: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008358612s
    Jan  5 07:57:07.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005855088s
    Jan  5 07:57:09.427: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007301748s
    Jan  5 07:57:11.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006532959s
    Jan  5 07:57:13.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006678117s
    Jan  5 07:57:15.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005942785s
    Jan  5 07:57:17.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006664849s
    Jan  5 07:57:19.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006388012s
    Jan  5 07:57:21.427: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007657618s
    Jan  5 07:57:23.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005771919s
    Jan  5 07:57:25.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005606933s
    Jan  5 07:57:27.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006752076s
    Jan  5 07:57:29.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005059273s
    Jan  5 07:57:31.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006913552s
    Jan  5 07:57:33.427: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007129384s
    Jan  5 07:57:35.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004929148s
    Jan  5 07:57:37.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004564838s
    Jan  5 07:57:39.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006294159s
    Jan  5 07:57:41.427: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007863403s
    Jan  5 07:57:43.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005326736s
    Jan  5 07:57:45.424: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00493813s
    Jan  5 07:57:47.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006360376s
    Jan  5 07:57:49.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006191766s
    Jan  5 07:57:51.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005452519s
    Jan  5 07:57:53.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005400034s
    Jan  5 07:57:55.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005323617s
    Jan  5 07:57:57.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00605267s
    Jan  5 07:57:59.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005143132s
    Jan  5 07:58:01.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005955573s
    Jan  5 07:58:03.426: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006338908s
    Jan  5 07:58:05.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005886757s
    Jan  5 07:58:07.425: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005240083s
    Jan  5 07:58:07.427: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007295631s
    STEP: updating the pod 01/05/23 07:58:07.427
    Jan  5 07:58:07.937: INFO: Successfully updated pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501"
    STEP: waiting for pod running 01/05/23 07:58:07.937
    Jan  5 07:58:07.937: INFO: Waiting up to 2m0s for pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501" in namespace "var-expansion-6160" to be "running"
    Jan  5 07:58:07.941: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307926ms
    Jan  5 07:58:09.944: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501": Phase="Running", Reason="", readiness=true. Elapsed: 2.007148237s
    Jan  5 07:58:09.944: INFO: Pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501" satisfied condition "running"
    STEP: deleting the pod gracefully 01/05/23 07:58:09.944
    Jan  5 07:58:09.944: INFO: Deleting pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501" in namespace "var-expansion-6160"
    Jan  5 07:58:09.949: INFO: Wait up to 5m0s for pod "var-expansion-33b7a06a-bb85-4988-aa2f-9b63ba91b501" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 07:58:41.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6160" for this suite. 01/05/23 07:58:41.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:58:41.963
Jan  5 07:58:41.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 07:58:41.963
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:58:41.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:58:41.991
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 07:58:42.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3573" for this suite. 01/05/23 07:58:42.027
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":113,"skipped":2278,"failed":0}
------------------------------
â€¢ [0.085 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:58:41.963
    Jan  5 07:58:41.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 07:58:41.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:58:41.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:58:41.991
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 07:58:42.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3573" for this suite. 01/05/23 07:58:42.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:58:42.049
Jan  5 07:58:42.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sched-preemption 01/05/23 07:58:42.05
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:58:42.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:58:42.071
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  5 07:58:42.095: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 07:59:42.112: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 07:59:42.114
Jan  5 07:59:42.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 07:59:42.114
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:59:42.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:59:42.133
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/05/23 07:59:42.136
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 07:59:42.137
Jan  5 07:59:42.147: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6472" to be "running"
Jan  5 07:59:42.149: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.964474ms
Jan  5 07:59:44.161: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014072424s
Jan  5 07:59:44.161: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 07:59:44.163
Jan  5 07:59:44.190: INFO: found a healthy node: mip-bd-vm724.mip.storage.hpecorp.net
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan  5 07:59:52.300: INFO: pods created so far: [1 1 1]
Jan  5 07:59:52.300: INFO: length of pods created so far: 3
Jan  5 07:59:54.311: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan  5 08:00:01.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6472" for this suite. 01/05/23 08:00:01.315
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:00:01.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7666" for this suite. 01/05/23 08:00:01.357
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":114,"skipped":2307,"failed":0}
------------------------------
â€¢ [SLOW TEST] [79.341 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:58:42.049
    Jan  5 07:58:42.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 07:58:42.05
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:58:42.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:58:42.071
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  5 07:58:42.095: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 07:59:42.112: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 07:59:42.114
    Jan  5 07:59:42.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 07:59:42.114
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 07:59:42.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 07:59:42.133
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/05/23 07:59:42.136
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 07:59:42.137
    Jan  5 07:59:42.147: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6472" to be "running"
    Jan  5 07:59:42.149: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.964474ms
    Jan  5 07:59:44.161: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014072424s
    Jan  5 07:59:44.161: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 07:59:44.163
    Jan  5 07:59:44.190: INFO: found a healthy node: mip-bd-vm724.mip.storage.hpecorp.net
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan  5 07:59:52.300: INFO: pods created so far: [1 1 1]
    Jan  5 07:59:52.300: INFO: length of pods created so far: 3
    Jan  5 07:59:54.311: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan  5 08:00:01.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-6472" for this suite. 01/05/23 08:00:01.315
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:00:01.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7666" for this suite. 01/05/23 08:00:01.357
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:00:01.39
Jan  5 08:00:01.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 08:00:01.39
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:00:01.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:00:01.42
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-cfad793f-f3c2-42af-aaf4-c3701623eb48 01/05/23 08:00:01.424
STEP: Creating the pod 01/05/23 08:00:01.428
Jan  5 08:00:01.455: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b" in namespace "configmap-8207" to be "running and ready"
Jan  5 08:00:01.457: INFO: Pod "pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.857543ms
Jan  5 08:00:01.457: INFO: The phase of Pod pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:00:03.460: INFO: Pod "pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004967357s
Jan  5 08:00:03.460: INFO: The phase of Pod pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:00:06.122: INFO: Pod "pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b": Phase="Running", Reason="", readiness=true. Elapsed: 4.667551181s
Jan  5 08:00:06.122: INFO: The phase of Pod pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b is Running (Ready = true)
Jan  5 08:00:06.122: INFO: Pod "pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-cfad793f-f3c2-42af-aaf4-c3701623eb48 01/05/23 08:00:06.144
STEP: waiting to observe update in volume 01/05/23 08:00:06.157
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 08:01:24.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8207" for this suite. 01/05/23 08:01:24.393
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":115,"skipped":2316,"failed":0}
------------------------------
â€¢ [SLOW TEST] [83.013 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:00:01.39
    Jan  5 08:00:01.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 08:00:01.39
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:00:01.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:00:01.42
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-cfad793f-f3c2-42af-aaf4-c3701623eb48 01/05/23 08:00:01.424
    STEP: Creating the pod 01/05/23 08:00:01.428
    Jan  5 08:00:01.455: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b" in namespace "configmap-8207" to be "running and ready"
    Jan  5 08:00:01.457: INFO: Pod "pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.857543ms
    Jan  5 08:00:01.457: INFO: The phase of Pod pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:00:03.460: INFO: Pod "pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004967357s
    Jan  5 08:00:03.460: INFO: The phase of Pod pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:00:06.122: INFO: Pod "pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b": Phase="Running", Reason="", readiness=true. Elapsed: 4.667551181s
    Jan  5 08:00:06.122: INFO: The phase of Pod pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b is Running (Ready = true)
    Jan  5 08:00:06.122: INFO: Pod "pod-configmaps-eb9f11f0-3e4b-4a60-ac5d-a859afff4d0b" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-cfad793f-f3c2-42af-aaf4-c3701623eb48 01/05/23 08:00:06.144
    STEP: waiting to observe update in volume 01/05/23 08:00:06.157
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 08:01:24.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8207" for this suite. 01/05/23 08:01:24.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:01:24.404
Jan  5 08:01:24.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubelet-test 01/05/23 08:01:24.405
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:01:24.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:01:24.42
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan  5 08:01:24.428: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3" in namespace "kubelet-test-2741" to be "running and ready"
Jan  5 08:01:24.430: INFO: Pod "busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.520514ms
Jan  5 08:01:24.430: INFO: The phase of Pod busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:01:26.434: INFO: Pod "busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005490988s
Jan  5 08:01:26.434: INFO: The phase of Pod busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3 is Running (Ready = true)
Jan  5 08:01:26.434: INFO: Pod "busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  5 08:01:26.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2741" for this suite. 01/05/23 08:01:26.445
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":116,"skipped":2327,"failed":0}
------------------------------
â€¢ [2.047 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:01:24.404
    Jan  5 08:01:24.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 08:01:24.405
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:01:24.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:01:24.42
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan  5 08:01:24.428: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3" in namespace "kubelet-test-2741" to be "running and ready"
    Jan  5 08:01:24.430: INFO: Pod "busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.520514ms
    Jan  5 08:01:24.430: INFO: The phase of Pod busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:01:26.434: INFO: Pod "busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005490988s
    Jan  5 08:01:26.434: INFO: The phase of Pod busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3 is Running (Ready = true)
    Jan  5 08:01:26.434: INFO: Pod "busybox-readonly-fs86ead7a3-3df0-4b51-9966-a6d67894b9d3" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  5 08:01:26.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2741" for this suite. 01/05/23 08:01:26.445
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:01:26.454
Jan  5 08:01:26.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 08:01:26.456
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:01:26.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:01:26.474
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/05/23 08:01:26.477
STEP: Creating RC which spawns configmap-volume pods 01/05/23 08:01:26.879
Jan  5 08:01:26.887: INFO: Pod name wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc: Found 0 pods out of 5
Jan  5 08:01:31.894: INFO: Pod name wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/05/23 08:01:31.894
Jan  5 08:01:31.894: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:01:31.896: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074435ms
Jan  5 08:01:33.901: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006569988s
Jan  5 08:01:35.901: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006997918s
Jan  5 08:01:37.900: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00630415s
Jan  5 08:01:39.901: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006721013s
Jan  5 08:01:41.900: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Running", Reason="", readiness=true. Elapsed: 10.005797022s
Jan  5 08:01:41.900: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5" satisfied condition "running"
Jan  5 08:01:41.900: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-dchdw" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:01:41.903: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-dchdw": Phase="Running", Reason="", readiness=true. Elapsed: 3.297464ms
Jan  5 08:01:41.903: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-dchdw" satisfied condition "running"
Jan  5 08:01:41.903: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-nxpmm" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:01:41.905: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-nxpmm": Phase="Running", Reason="", readiness=true. Elapsed: 2.070381ms
Jan  5 08:01:41.905: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-nxpmm" satisfied condition "running"
Jan  5 08:01:41.905: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pffcb" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:01:41.907: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pffcb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.518871ms
Jan  5 08:01:43.910: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pffcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005111314s
Jan  5 08:01:43.910: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pffcb" satisfied condition "running"
Jan  5 08:01:43.910: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pgmsc" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:01:43.913: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pgmsc": Phase="Running", Reason="", readiness=true. Elapsed: 2.797085ms
Jan  5 08:01:43.913: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pgmsc" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc in namespace emptydir-wrapper-7036, will wait for the garbage collector to delete the pods 01/05/23 08:01:43.913
Jan  5 08:01:43.972: INFO: Deleting ReplicationController wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc took: 5.813539ms
Jan  5 08:01:44.073: INFO: Terminating ReplicationController wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc pods took: 100.929124ms
STEP: Creating RC which spawns configmap-volume pods 01/05/23 08:01:46.978
Jan  5 08:01:46.989: INFO: Pod name wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8: Found 0 pods out of 5
Jan  5 08:01:51.996: INFO: Pod name wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/05/23 08:01:51.996
Jan  5 08:01:51.997: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:01:51.999: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.369152ms
Jan  5 08:01:54.008: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011426774s
Jan  5 08:01:56.003: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006788766s
Jan  5 08:01:58.003: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00680304s
Jan  5 08:02:00.004: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007660842s
Jan  5 08:02:02.004: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007717133s
Jan  5 08:02:04.004: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Running", Reason="", readiness=true. Elapsed: 12.007492154s
Jan  5 08:02:04.004: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6" satisfied condition "running"
Jan  5 08:02:04.004: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-b82wd" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:02:04.007: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-b82wd": Phase="Running", Reason="", readiness=true. Elapsed: 3.238699ms
Jan  5 08:02:04.008: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-b82wd" satisfied condition "running"
Jan  5 08:02:04.008: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-dz5mc" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:02:04.011: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-dz5mc": Phase="Running", Reason="", readiness=true. Elapsed: 2.974377ms
Jan  5 08:02:04.011: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-dz5mc" satisfied condition "running"
Jan  5 08:02:04.011: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-jrglf" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:02:04.013: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-jrglf": Phase="Running", Reason="", readiness=true. Elapsed: 2.649189ms
Jan  5 08:02:04.013: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-jrglf" satisfied condition "running"
Jan  5 08:02:04.014: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-z6ghw" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:02:04.016: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-z6ghw": Phase="Running", Reason="", readiness=true. Elapsed: 2.757867ms
Jan  5 08:02:04.016: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-z6ghw" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8 in namespace emptydir-wrapper-7036, will wait for the garbage collector to delete the pods 01/05/23 08:02:04.016
Jan  5 08:02:04.076: INFO: Deleting ReplicationController wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8 took: 6.185248ms
Jan  5 08:02:04.176: INFO: Terminating ReplicationController wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8 pods took: 100.834098ms
STEP: Creating RC which spawns configmap-volume pods 01/05/23 08:02:07.281
Jan  5 08:02:07.318: INFO: Pod name wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1: Found 0 pods out of 5
Jan  5 08:02:12.329: INFO: Pod name wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/05/23 08:02:12.329
Jan  5 08:02:12.329: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:02:12.331: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.516646ms
Jan  5 08:02:14.335: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006036314s
Jan  5 08:02:16.335: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006363082s
Jan  5 08:02:18.335: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006284067s
Jan  5 08:02:20.335: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006638566s
Jan  5 08:02:22.337: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Running", Reason="", readiness=true. Elapsed: 10.008196761s
Jan  5 08:02:22.337: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28" satisfied condition "running"
Jan  5 08:02:22.337: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-dt7r7" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:02:22.339: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-dt7r7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066141ms
Jan  5 08:02:24.344: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-dt7r7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006682251s
Jan  5 08:02:24.344: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-dt7r7" satisfied condition "running"
Jan  5 08:02:24.344: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-gn9cm" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:02:24.346: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-gn9cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.030446ms
Jan  5 08:02:24.346: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-gn9cm" satisfied condition "running"
Jan  5 08:02:24.346: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-skhd4" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:02:24.348: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-skhd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.195223ms
Jan  5 08:02:24.348: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-skhd4" satisfied condition "running"
Jan  5 08:02:24.348: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-x8x59" in namespace "emptydir-wrapper-7036" to be "running"
Jan  5 08:02:24.350: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-x8x59": Phase="Running", Reason="", readiness=true. Elapsed: 1.687353ms
Jan  5 08:02:24.350: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-x8x59" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1 in namespace emptydir-wrapper-7036, will wait for the garbage collector to delete the pods 01/05/23 08:02:24.35
Jan  5 08:02:24.407: INFO: Deleting ReplicationController wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1 took: 4.88191ms
Jan  5 08:02:24.507: INFO: Terminating ReplicationController wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1 pods took: 100.526724ms
STEP: Cleaning up the configMaps 01/05/23 08:02:27.508
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan  5 08:02:27.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7036" for this suite. 01/05/23 08:02:27.854
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":117,"skipped":2331,"failed":0}
------------------------------
â€¢ [SLOW TEST] [61.406 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:01:26.454
    Jan  5 08:01:26.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 08:01:26.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:01:26.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:01:26.474
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/05/23 08:01:26.477
    STEP: Creating RC which spawns configmap-volume pods 01/05/23 08:01:26.879
    Jan  5 08:01:26.887: INFO: Pod name wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc: Found 0 pods out of 5
    Jan  5 08:01:31.894: INFO: Pod name wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/05/23 08:01:31.894
    Jan  5 08:01:31.894: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:01:31.896: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074435ms
    Jan  5 08:01:33.901: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006569988s
    Jan  5 08:01:35.901: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006997918s
    Jan  5 08:01:37.900: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00630415s
    Jan  5 08:01:39.901: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006721013s
    Jan  5 08:01:41.900: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5": Phase="Running", Reason="", readiness=true. Elapsed: 10.005797022s
    Jan  5 08:01:41.900: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-8fxr5" satisfied condition "running"
    Jan  5 08:01:41.900: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-dchdw" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:01:41.903: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-dchdw": Phase="Running", Reason="", readiness=true. Elapsed: 3.297464ms
    Jan  5 08:01:41.903: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-dchdw" satisfied condition "running"
    Jan  5 08:01:41.903: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-nxpmm" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:01:41.905: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-nxpmm": Phase="Running", Reason="", readiness=true. Elapsed: 2.070381ms
    Jan  5 08:01:41.905: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-nxpmm" satisfied condition "running"
    Jan  5 08:01:41.905: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pffcb" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:01:41.907: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pffcb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.518871ms
    Jan  5 08:01:43.910: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pffcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005111314s
    Jan  5 08:01:43.910: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pffcb" satisfied condition "running"
    Jan  5 08:01:43.910: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pgmsc" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:01:43.913: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pgmsc": Phase="Running", Reason="", readiness=true. Elapsed: 2.797085ms
    Jan  5 08:01:43.913: INFO: Pod "wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc-pgmsc" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc in namespace emptydir-wrapper-7036, will wait for the garbage collector to delete the pods 01/05/23 08:01:43.913
    Jan  5 08:01:43.972: INFO: Deleting ReplicationController wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc took: 5.813539ms
    Jan  5 08:01:44.073: INFO: Terminating ReplicationController wrapped-volume-race-32e02520-5c6f-459d-af85-2e050abacdbc pods took: 100.929124ms
    STEP: Creating RC which spawns configmap-volume pods 01/05/23 08:01:46.978
    Jan  5 08:01:46.989: INFO: Pod name wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8: Found 0 pods out of 5
    Jan  5 08:01:51.996: INFO: Pod name wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/05/23 08:01:51.996
    Jan  5 08:01:51.997: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:01:51.999: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.369152ms
    Jan  5 08:01:54.008: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011426774s
    Jan  5 08:01:56.003: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006788766s
    Jan  5 08:01:58.003: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00680304s
    Jan  5 08:02:00.004: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007660842s
    Jan  5 08:02:02.004: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007717133s
    Jan  5 08:02:04.004: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6": Phase="Running", Reason="", readiness=true. Elapsed: 12.007492154s
    Jan  5 08:02:04.004: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-79jk6" satisfied condition "running"
    Jan  5 08:02:04.004: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-b82wd" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:02:04.007: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-b82wd": Phase="Running", Reason="", readiness=true. Elapsed: 3.238699ms
    Jan  5 08:02:04.008: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-b82wd" satisfied condition "running"
    Jan  5 08:02:04.008: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-dz5mc" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:02:04.011: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-dz5mc": Phase="Running", Reason="", readiness=true. Elapsed: 2.974377ms
    Jan  5 08:02:04.011: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-dz5mc" satisfied condition "running"
    Jan  5 08:02:04.011: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-jrglf" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:02:04.013: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-jrglf": Phase="Running", Reason="", readiness=true. Elapsed: 2.649189ms
    Jan  5 08:02:04.013: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-jrglf" satisfied condition "running"
    Jan  5 08:02:04.014: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-z6ghw" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:02:04.016: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-z6ghw": Phase="Running", Reason="", readiness=true. Elapsed: 2.757867ms
    Jan  5 08:02:04.016: INFO: Pod "wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8-z6ghw" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8 in namespace emptydir-wrapper-7036, will wait for the garbage collector to delete the pods 01/05/23 08:02:04.016
    Jan  5 08:02:04.076: INFO: Deleting ReplicationController wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8 took: 6.185248ms
    Jan  5 08:02:04.176: INFO: Terminating ReplicationController wrapped-volume-race-3f8cdc66-f54c-44f4-8fcd-09cc86bd95b8 pods took: 100.834098ms
    STEP: Creating RC which spawns configmap-volume pods 01/05/23 08:02:07.281
    Jan  5 08:02:07.318: INFO: Pod name wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1: Found 0 pods out of 5
    Jan  5 08:02:12.329: INFO: Pod name wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/05/23 08:02:12.329
    Jan  5 08:02:12.329: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:02:12.331: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.516646ms
    Jan  5 08:02:14.335: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006036314s
    Jan  5 08:02:16.335: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006363082s
    Jan  5 08:02:18.335: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006284067s
    Jan  5 08:02:20.335: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006638566s
    Jan  5 08:02:22.337: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28": Phase="Running", Reason="", readiness=true. Elapsed: 10.008196761s
    Jan  5 08:02:22.337: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-5rl28" satisfied condition "running"
    Jan  5 08:02:22.337: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-dt7r7" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:02:22.339: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-dt7r7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066141ms
    Jan  5 08:02:24.344: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-dt7r7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006682251s
    Jan  5 08:02:24.344: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-dt7r7" satisfied condition "running"
    Jan  5 08:02:24.344: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-gn9cm" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:02:24.346: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-gn9cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.030446ms
    Jan  5 08:02:24.346: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-gn9cm" satisfied condition "running"
    Jan  5 08:02:24.346: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-skhd4" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:02:24.348: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-skhd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.195223ms
    Jan  5 08:02:24.348: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-skhd4" satisfied condition "running"
    Jan  5 08:02:24.348: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-x8x59" in namespace "emptydir-wrapper-7036" to be "running"
    Jan  5 08:02:24.350: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-x8x59": Phase="Running", Reason="", readiness=true. Elapsed: 1.687353ms
    Jan  5 08:02:24.350: INFO: Pod "wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1-x8x59" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1 in namespace emptydir-wrapper-7036, will wait for the garbage collector to delete the pods 01/05/23 08:02:24.35
    Jan  5 08:02:24.407: INFO: Deleting ReplicationController wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1 took: 4.88191ms
    Jan  5 08:02:24.507: INFO: Terminating ReplicationController wrapped-volume-race-7043204d-c5b8-4664-b60d-ee6f1213f2c1 pods took: 100.526724ms
    STEP: Cleaning up the configMaps 01/05/23 08:02:27.508
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan  5 08:02:27.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-7036" for this suite. 01/05/23 08:02:27.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:02:27.862
Jan  5 08:02:27.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 08:02:27.863
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:02:27.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:02:27.877
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/05/23 08:02:27.878
Jan  5 08:02:27.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:02:29.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:02:38.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7159" for this suite. 01/05/23 08:02:38.398
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":118,"skipped":2352,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.540 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:02:27.862
    Jan  5 08:02:27.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 08:02:27.863
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:02:27.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:02:27.877
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/05/23 08:02:27.878
    Jan  5 08:02:27.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:02:29.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:02:38.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7159" for this suite. 01/05/23 08:02:38.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:02:38.402
Jan  5 08:02:38.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:02:38.403
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:02:38.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:02:38.425
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3115 01/05/23 08:02:38.426
STEP: changing the ExternalName service to type=NodePort 01/05/23 08:02:38.429
STEP: creating replication controller externalname-service in namespace services-3115 01/05/23 08:02:38.46
I0105 08:02:38.472299      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3115, replica count: 2
I0105 08:02:41.523506      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 08:02:41.523: INFO: Creating new exec pod
Jan  5 08:02:41.528: INFO: Waiting up to 5m0s for pod "execpodbvmgg" in namespace "services-3115" to be "running"
Jan  5 08:02:41.529: INFO: Pod "execpodbvmgg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.599159ms
Jan  5 08:02:43.533: INFO: Pod "execpodbvmgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004806905s
Jan  5 08:02:43.533: INFO: Pod "execpodbvmgg" satisfied condition "running"
Jan  5 08:02:44.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3115 exec execpodbvmgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan  5 08:02:44.660: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  5 08:02:44.660: INFO: stdout: ""
Jan  5 08:02:45.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3115 exec execpodbvmgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan  5 08:02:45.789: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  5 08:02:45.789: INFO: stdout: "externalname-service-wdmxb"
Jan  5 08:02:45.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3115 exec execpodbvmgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.249.161 80'
Jan  5 08:02:45.925: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.249.161 80\nConnection to 10.108.249.161 80 port [tcp/http] succeeded!\n"
Jan  5 08:02:45.925: INFO: stdout: "externalname-service-wdmxb"
Jan  5 08:02:45.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3115 exec execpodbvmgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.212 30614'
Jan  5 08:02:46.066: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.212 30614\nConnection to 16.0.14.212 30614 port [tcp/*] succeeded!\n"
Jan  5 08:02:46.066: INFO: stdout: "externalname-service-wdmxb"
Jan  5 08:02:46.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3115 exec execpodbvmgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.214 30614'
Jan  5 08:02:46.191: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.214 30614\nConnection to 16.0.14.214 30614 port [tcp/*] succeeded!\n"
Jan  5 08:02:46.191: INFO: stdout: "externalname-service-wdmxb"
Jan  5 08:02:46.191: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:02:46.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3115" for this suite. 01/05/23 08:02:46.225
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":119,"skipped":2382,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.827 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:02:38.402
    Jan  5 08:02:38.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:02:38.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:02:38.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:02:38.425
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3115 01/05/23 08:02:38.426
    STEP: changing the ExternalName service to type=NodePort 01/05/23 08:02:38.429
    STEP: creating replication controller externalname-service in namespace services-3115 01/05/23 08:02:38.46
    I0105 08:02:38.472299      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3115, replica count: 2
    I0105 08:02:41.523506      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 08:02:41.523: INFO: Creating new exec pod
    Jan  5 08:02:41.528: INFO: Waiting up to 5m0s for pod "execpodbvmgg" in namespace "services-3115" to be "running"
    Jan  5 08:02:41.529: INFO: Pod "execpodbvmgg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.599159ms
    Jan  5 08:02:43.533: INFO: Pod "execpodbvmgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004806905s
    Jan  5 08:02:43.533: INFO: Pod "execpodbvmgg" satisfied condition "running"
    Jan  5 08:02:44.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3115 exec execpodbvmgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan  5 08:02:44.660: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  5 08:02:44.660: INFO: stdout: ""
    Jan  5 08:02:45.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3115 exec execpodbvmgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan  5 08:02:45.789: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  5 08:02:45.789: INFO: stdout: "externalname-service-wdmxb"
    Jan  5 08:02:45.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3115 exec execpodbvmgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.249.161 80'
    Jan  5 08:02:45.925: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.249.161 80\nConnection to 10.108.249.161 80 port [tcp/http] succeeded!\n"
    Jan  5 08:02:45.925: INFO: stdout: "externalname-service-wdmxb"
    Jan  5 08:02:45.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3115 exec execpodbvmgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.212 30614'
    Jan  5 08:02:46.066: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.212 30614\nConnection to 16.0.14.212 30614 port [tcp/*] succeeded!\n"
    Jan  5 08:02:46.066: INFO: stdout: "externalname-service-wdmxb"
    Jan  5 08:02:46.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3115 exec execpodbvmgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.214 30614'
    Jan  5 08:02:46.191: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.214 30614\nConnection to 16.0.14.214 30614 port [tcp/*] succeeded!\n"
    Jan  5 08:02:46.191: INFO: stdout: "externalname-service-wdmxb"
    Jan  5 08:02:46.191: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:02:46.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3115" for this suite. 01/05/23 08:02:46.225
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:02:46.231
Jan  5 08:02:46.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 08:02:46.232
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:02:46.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:02:46.263
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/05/23 08:02:46.268
Jan  5 08:02:46.282: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2549" to be "running and ready"
Jan  5 08:02:46.288: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06986ms
Jan  5 08:02:46.288: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:02:48.291: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008779026s
Jan  5 08:02:48.291: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 08:02:48.291: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/05/23 08:02:48.293
Jan  5 08:02:48.297: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2549" to be "running and ready"
Jan  5 08:02:48.299: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.519831ms
Jan  5 08:02:48.299: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:02:50.302: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004617464s
Jan  5 08:02:50.302: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:02:52.302: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.00410153s
Jan  5 08:02:52.302: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan  5 08:02:52.302: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/05/23 08:02:52.303
STEP: delete the pod with lifecycle hook 01/05/23 08:02:52.307
Jan  5 08:02:52.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  5 08:02:52.312: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  5 08:02:54.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  5 08:02:54.314: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  5 08:02:54.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2549" for this suite. 01/05/23 08:02:54.316
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":120,"skipped":2443,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.089 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:02:46.231
    Jan  5 08:02:46.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 08:02:46.232
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:02:46.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:02:46.263
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 08:02:46.268
    Jan  5 08:02:46.282: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2549" to be "running and ready"
    Jan  5 08:02:46.288: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06986ms
    Jan  5 08:02:46.288: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:02:48.291: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008779026s
    Jan  5 08:02:48.291: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 08:02:48.291: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/05/23 08:02:48.293
    Jan  5 08:02:48.297: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2549" to be "running and ready"
    Jan  5 08:02:48.299: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.519831ms
    Jan  5 08:02:48.299: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:02:50.302: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004617464s
    Jan  5 08:02:50.302: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:02:52.302: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.00410153s
    Jan  5 08:02:52.302: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan  5 08:02:52.302: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/05/23 08:02:52.303
    STEP: delete the pod with lifecycle hook 01/05/23 08:02:52.307
    Jan  5 08:02:52.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  5 08:02:52.312: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan  5 08:02:54.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  5 08:02:54.314: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  5 08:02:54.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-2549" for this suite. 01/05/23 08:02:54.316
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:02:54.32
Jan  5 08:02:54.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 08:02:54.321
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:02:54.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:02:54.348
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-77a90810-65a1-4c87-9430-c7f609afcf3d 01/05/23 08:02:54.368
STEP: Creating a pod to test consume secrets 01/05/23 08:02:54.371
Jan  5 08:02:54.379: INFO: Waiting up to 5m0s for pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22" in namespace "secrets-7816" to be "Succeeded or Failed"
Jan  5 08:02:54.381: INFO: Pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22": Phase="Pending", Reason="", readiness=false. Elapsed: 1.278226ms
Jan  5 08:02:56.384: INFO: Pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004498015s
Jan  5 08:02:58.383: INFO: Pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004243949s
Jan  5 08:03:00.384: INFO: Pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004978748s
STEP: Saw pod success 01/05/23 08:03:00.384
Jan  5 08:03:00.384: INFO: Pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22" satisfied condition "Succeeded or Failed"
Jan  5 08:03:00.387: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 08:03:00.392
Jan  5 08:03:00.406: INFO: Waiting for pod pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22 to disappear
Jan  5 08:03:00.408: INFO: Pod pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 08:03:00.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7816" for this suite. 01/05/23 08:03:00.41
STEP: Destroying namespace "secret-namespace-5103" for this suite. 01/05/23 08:03:00.42
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":121,"skipped":2444,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.104 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:02:54.32
    Jan  5 08:02:54.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 08:02:54.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:02:54.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:02:54.348
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-77a90810-65a1-4c87-9430-c7f609afcf3d 01/05/23 08:02:54.368
    STEP: Creating a pod to test consume secrets 01/05/23 08:02:54.371
    Jan  5 08:02:54.379: INFO: Waiting up to 5m0s for pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22" in namespace "secrets-7816" to be "Succeeded or Failed"
    Jan  5 08:02:54.381: INFO: Pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22": Phase="Pending", Reason="", readiness=false. Elapsed: 1.278226ms
    Jan  5 08:02:56.384: INFO: Pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004498015s
    Jan  5 08:02:58.383: INFO: Pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004243949s
    Jan  5 08:03:00.384: INFO: Pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004978748s
    STEP: Saw pod success 01/05/23 08:03:00.384
    Jan  5 08:03:00.384: INFO: Pod "pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22" satisfied condition "Succeeded or Failed"
    Jan  5 08:03:00.387: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 08:03:00.392
    Jan  5 08:03:00.406: INFO: Waiting for pod pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22 to disappear
    Jan  5 08:03:00.408: INFO: Pod pod-secrets-5ec8bf58-8ca1-4a12-a68e-05bf9f73cb22 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 08:03:00.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7816" for this suite. 01/05/23 08:03:00.41
    STEP: Destroying namespace "secret-namespace-5103" for this suite. 01/05/23 08:03:00.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:03:00.428
Jan  5 08:03:00.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 08:03:00.43
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:00.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:00.463
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan  5 08:03:00.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3369 version'
Jan  5 08:03:00.523: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan  5 08:03:00.523: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25+\", GitVersion:\"v1.25.5-hpe1\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:06:36Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 08:03:00.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3369" for this suite. 01/05/23 08:03:00.525
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":122,"skipped":2480,"failed":0}
------------------------------
â€¢ [0.108 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:03:00.428
    Jan  5 08:03:00.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 08:03:00.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:00.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:00.463
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan  5 08:03:00.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3369 version'
    Jan  5 08:03:00.523: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan  5 08:03:00.523: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25+\", GitVersion:\"v1.25.5-hpe1\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:06:36Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 08:03:00.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3369" for this suite. 01/05/23 08:03:00.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:03:00.539
Jan  5 08:03:00.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename statefulset 01/05/23 08:03:00.54
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:00.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:00.561
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5491 01/05/23 08:03:00.563
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/05/23 08:03:00.585
STEP: Creating pod with conflicting port in namespace statefulset-5491 01/05/23 08:03:00.595
STEP: Waiting until pod test-pod will start running in namespace statefulset-5491 01/05/23 08:03:00.602
Jan  5 08:03:00.602: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5491" to be "running"
Jan  5 08:03:00.605: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068803ms
Jan  5 08:03:02.608: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005488146s
Jan  5 08:03:04.609: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006520265s
Jan  5 08:03:04.609: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-5491 01/05/23 08:03:04.609
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5491 01/05/23 08:03:04.613
Jan  5 08:03:04.626: INFO: Observed stateful pod in namespace: statefulset-5491, name: ss-0, uid: 7b476844-9449-442f-985c-c44dd2b99764, status phase: Pending. Waiting for statefulset controller to delete.
Jan  5 08:03:04.647: INFO: Observed stateful pod in namespace: statefulset-5491, name: ss-0, uid: 7b476844-9449-442f-985c-c44dd2b99764, status phase: Failed. Waiting for statefulset controller to delete.
Jan  5 08:03:04.671: INFO: Observed stateful pod in namespace: statefulset-5491, name: ss-0, uid: 7b476844-9449-442f-985c-c44dd2b99764, status phase: Failed. Waiting for statefulset controller to delete.
Jan  5 08:03:04.678: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5491
STEP: Removing pod with conflicting port in namespace statefulset-5491 01/05/23 08:03:04.678
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5491 and will be in running state 01/05/23 08:03:04.702
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 08:03:06.714: INFO: Deleting all statefulset in ns statefulset-5491
Jan  5 08:03:06.720: INFO: Scaling statefulset ss to 0
Jan  5 08:03:16.743: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 08:03:16.745: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 08:03:16.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5491" for this suite. 01/05/23 08:03:16.756
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":123,"skipped":2533,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.227 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:03:00.539
    Jan  5 08:03:00.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename statefulset 01/05/23 08:03:00.54
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:00.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:00.561
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5491 01/05/23 08:03:00.563
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/05/23 08:03:00.585
    STEP: Creating pod with conflicting port in namespace statefulset-5491 01/05/23 08:03:00.595
    STEP: Waiting until pod test-pod will start running in namespace statefulset-5491 01/05/23 08:03:00.602
    Jan  5 08:03:00.602: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5491" to be "running"
    Jan  5 08:03:00.605: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068803ms
    Jan  5 08:03:02.608: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005488146s
    Jan  5 08:03:04.609: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006520265s
    Jan  5 08:03:04.609: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-5491 01/05/23 08:03:04.609
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5491 01/05/23 08:03:04.613
    Jan  5 08:03:04.626: INFO: Observed stateful pod in namespace: statefulset-5491, name: ss-0, uid: 7b476844-9449-442f-985c-c44dd2b99764, status phase: Pending. Waiting for statefulset controller to delete.
    Jan  5 08:03:04.647: INFO: Observed stateful pod in namespace: statefulset-5491, name: ss-0, uid: 7b476844-9449-442f-985c-c44dd2b99764, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  5 08:03:04.671: INFO: Observed stateful pod in namespace: statefulset-5491, name: ss-0, uid: 7b476844-9449-442f-985c-c44dd2b99764, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  5 08:03:04.678: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5491
    STEP: Removing pod with conflicting port in namespace statefulset-5491 01/05/23 08:03:04.678
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5491 and will be in running state 01/05/23 08:03:04.702
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 08:03:06.714: INFO: Deleting all statefulset in ns statefulset-5491
    Jan  5 08:03:06.720: INFO: Scaling statefulset ss to 0
    Jan  5 08:03:16.743: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 08:03:16.745: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 08:03:16.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5491" for this suite. 01/05/23 08:03:16.756
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:03:16.766
Jan  5 08:03:16.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename events 01/05/23 08:03:16.766
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:16.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:16.79
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/05/23 08:03:16.792
STEP: listing events in all namespaces 01/05/23 08:03:16.796
STEP: listing events in test namespace 01/05/23 08:03:16.801
STEP: listing events with field selection filtering on source 01/05/23 08:03:16.802
STEP: listing events with field selection filtering on reportingController 01/05/23 08:03:16.804
STEP: getting the test event 01/05/23 08:03:16.805
STEP: patching the test event 01/05/23 08:03:16.807
STEP: getting the test event 01/05/23 08:03:16.819
STEP: updating the test event 01/05/23 08:03:16.82
STEP: getting the test event 01/05/23 08:03:16.824
STEP: deleting the test event 01/05/23 08:03:16.826
STEP: listing events in all namespaces 01/05/23 08:03:16.835
STEP: listing events in test namespace 01/05/23 08:03:16.842
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan  5 08:03:16.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2320" for this suite. 01/05/23 08:03:16.851
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":124,"skipped":2535,"failed":0}
------------------------------
â€¢ [0.090 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:03:16.766
    Jan  5 08:03:16.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename events 01/05/23 08:03:16.766
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:16.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:16.79
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/05/23 08:03:16.792
    STEP: listing events in all namespaces 01/05/23 08:03:16.796
    STEP: listing events in test namespace 01/05/23 08:03:16.801
    STEP: listing events with field selection filtering on source 01/05/23 08:03:16.802
    STEP: listing events with field selection filtering on reportingController 01/05/23 08:03:16.804
    STEP: getting the test event 01/05/23 08:03:16.805
    STEP: patching the test event 01/05/23 08:03:16.807
    STEP: getting the test event 01/05/23 08:03:16.819
    STEP: updating the test event 01/05/23 08:03:16.82
    STEP: getting the test event 01/05/23 08:03:16.824
    STEP: deleting the test event 01/05/23 08:03:16.826
    STEP: listing events in all namespaces 01/05/23 08:03:16.835
    STEP: listing events in test namespace 01/05/23 08:03:16.842
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan  5 08:03:16.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2320" for this suite. 01/05/23 08:03:16.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:03:16.856
Jan  5 08:03:16.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename var-expansion 01/05/23 08:03:16.857
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:16.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:16.875
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/05/23 08:03:16.876
Jan  5 08:03:16.881: INFO: Waiting up to 5m0s for pod "var-expansion-98d74a78-5ea4-4d64-af67-787d47712552" in namespace "var-expansion-8420" to be "Succeeded or Failed"
Jan  5 08:03:16.883: INFO: Pod "var-expansion-98d74a78-5ea4-4d64-af67-787d47712552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01059ms
Jan  5 08:03:18.888: INFO: Pod "var-expansion-98d74a78-5ea4-4d64-af67-787d47712552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006573394s
Jan  5 08:03:20.886: INFO: Pod "var-expansion-98d74a78-5ea4-4d64-af67-787d47712552": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004404797s
STEP: Saw pod success 01/05/23 08:03:20.886
Jan  5 08:03:20.886: INFO: Pod "var-expansion-98d74a78-5ea4-4d64-af67-787d47712552" satisfied condition "Succeeded or Failed"
Jan  5 08:03:20.888: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod var-expansion-98d74a78-5ea4-4d64-af67-787d47712552 container dapi-container: <nil>
STEP: delete the pod 01/05/23 08:03:20.891
Jan  5 08:03:20.927: INFO: Waiting for pod var-expansion-98d74a78-5ea4-4d64-af67-787d47712552 to disappear
Jan  5 08:03:20.930: INFO: Pod var-expansion-98d74a78-5ea4-4d64-af67-787d47712552 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 08:03:20.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8420" for this suite. 01/05/23 08:03:20.933
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":125,"skipped":2547,"failed":0}
------------------------------
â€¢ [4.086 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:03:16.856
    Jan  5 08:03:16.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename var-expansion 01/05/23 08:03:16.857
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:16.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:16.875
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/05/23 08:03:16.876
    Jan  5 08:03:16.881: INFO: Waiting up to 5m0s for pod "var-expansion-98d74a78-5ea4-4d64-af67-787d47712552" in namespace "var-expansion-8420" to be "Succeeded or Failed"
    Jan  5 08:03:16.883: INFO: Pod "var-expansion-98d74a78-5ea4-4d64-af67-787d47712552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01059ms
    Jan  5 08:03:18.888: INFO: Pod "var-expansion-98d74a78-5ea4-4d64-af67-787d47712552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006573394s
    Jan  5 08:03:20.886: INFO: Pod "var-expansion-98d74a78-5ea4-4d64-af67-787d47712552": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004404797s
    STEP: Saw pod success 01/05/23 08:03:20.886
    Jan  5 08:03:20.886: INFO: Pod "var-expansion-98d74a78-5ea4-4d64-af67-787d47712552" satisfied condition "Succeeded or Failed"
    Jan  5 08:03:20.888: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod var-expansion-98d74a78-5ea4-4d64-af67-787d47712552 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 08:03:20.891
    Jan  5 08:03:20.927: INFO: Waiting for pod var-expansion-98d74a78-5ea4-4d64-af67-787d47712552 to disappear
    Jan  5 08:03:20.930: INFO: Pod var-expansion-98d74a78-5ea4-4d64-af67-787d47712552 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 08:03:20.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8420" for this suite. 01/05/23 08:03:20.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:03:20.944
Jan  5 08:03:20.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 08:03:20.945
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:20.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:20.965
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-2450/secret-test-8f382ba4-5742-4c83-bf6e-63e88b0abdbf 01/05/23 08:03:20.967
STEP: Creating a pod to test consume secrets 01/05/23 08:03:20.971
Jan  5 08:03:20.985: INFO: Waiting up to 5m0s for pod "pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a" in namespace "secrets-2450" to be "Succeeded or Failed"
Jan  5 08:03:20.987: INFO: Pod "pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355923ms
Jan  5 08:03:22.991: INFO: Pod "pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006119358s
Jan  5 08:03:24.990: INFO: Pod "pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005675452s
STEP: Saw pod success 01/05/23 08:03:24.99
Jan  5 08:03:24.990: INFO: Pod "pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a" satisfied condition "Succeeded or Failed"
Jan  5 08:03:24.993: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a container env-test: <nil>
STEP: delete the pod 01/05/23 08:03:24.997
Jan  5 08:03:25.011: INFO: Waiting for pod pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a to disappear
Jan  5 08:03:25.013: INFO: Pod pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  5 08:03:25.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2450" for this suite. 01/05/23 08:03:25.015
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":126,"skipped":2576,"failed":0}
------------------------------
â€¢ [4.083 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:03:20.944
    Jan  5 08:03:20.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 08:03:20.945
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:20.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:20.965
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-2450/secret-test-8f382ba4-5742-4c83-bf6e-63e88b0abdbf 01/05/23 08:03:20.967
    STEP: Creating a pod to test consume secrets 01/05/23 08:03:20.971
    Jan  5 08:03:20.985: INFO: Waiting up to 5m0s for pod "pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a" in namespace "secrets-2450" to be "Succeeded or Failed"
    Jan  5 08:03:20.987: INFO: Pod "pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355923ms
    Jan  5 08:03:22.991: INFO: Pod "pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006119358s
    Jan  5 08:03:24.990: INFO: Pod "pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005675452s
    STEP: Saw pod success 01/05/23 08:03:24.99
    Jan  5 08:03:24.990: INFO: Pod "pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a" satisfied condition "Succeeded or Failed"
    Jan  5 08:03:24.993: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a container env-test: <nil>
    STEP: delete the pod 01/05/23 08:03:24.997
    Jan  5 08:03:25.011: INFO: Waiting for pod pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a to disappear
    Jan  5 08:03:25.013: INFO: Pod pod-configmaps-17ce0b01-bbc9-4763-9eed-f2ab59211c4a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 08:03:25.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2450" for this suite. 01/05/23 08:03:25.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:03:25.029
Jan  5 08:03:25.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubelet-test 01/05/23 08:03:25.03
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:25.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:25.047
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  5 08:03:29.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7811" for this suite. 01/05/23 08:03:29.066
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":127,"skipped":2598,"failed":0}
------------------------------
â€¢ [4.046 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:03:25.029
    Jan  5 08:03:25.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 08:03:25.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:25.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:25.047
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  5 08:03:29.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7811" for this suite. 01/05/23 08:03:29.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:03:29.076
Jan  5 08:03:29.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-probe 01/05/23 08:03:29.076
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:29.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:29.094
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf in namespace container-probe-9844 01/05/23 08:03:29.097
Jan  5 08:03:29.109: INFO: Waiting up to 5m0s for pod "test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf" in namespace "container-probe-9844" to be "not pending"
Jan  5 08:03:29.113: INFO: Pod "test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10692ms
Jan  5 08:03:31.116: INFO: Pod "test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007135531s
Jan  5 08:03:31.116: INFO: Pod "test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf" satisfied condition "not pending"
Jan  5 08:03:31.116: INFO: Started pod test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf in namespace container-probe-9844
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:03:31.116
Jan  5 08:03:31.118: INFO: Initial restart count of pod test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf is 0
STEP: deleting the pod 01/05/23 08:07:31.58
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 08:07:31.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9844" for this suite. 01/05/23 08:07:31.608
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":128,"skipped":2616,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.543 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:03:29.076
    Jan  5 08:03:29.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-probe 01/05/23 08:03:29.076
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:03:29.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:03:29.094
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf in namespace container-probe-9844 01/05/23 08:03:29.097
    Jan  5 08:03:29.109: INFO: Waiting up to 5m0s for pod "test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf" in namespace "container-probe-9844" to be "not pending"
    Jan  5 08:03:29.113: INFO: Pod "test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10692ms
    Jan  5 08:03:31.116: INFO: Pod "test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007135531s
    Jan  5 08:03:31.116: INFO: Pod "test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf" satisfied condition "not pending"
    Jan  5 08:03:31.116: INFO: Started pod test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf in namespace container-probe-9844
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:03:31.116
    Jan  5 08:03:31.118: INFO: Initial restart count of pod test-webserver-4868d9b1-0e48-4127-ac39-41cf5a39d5bf is 0
    STEP: deleting the pod 01/05/23 08:07:31.58
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 08:07:31.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9844" for this suite. 01/05/23 08:07:31.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:07:31.619
Jan  5 08:07:31.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename daemonsets 01/05/23 08:07:31.62
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:07:31.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:07:31.634
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/05/23 08:07:31.665
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 08:07:31.67
Jan  5 08:07:31.672: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:07:31.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:07:31.675: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:07:32.679: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:07:32.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:07:32.682: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:07:33.678: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:07:33.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 08:07:33.681: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/05/23 08:07:33.683
Jan  5 08:07:33.685: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/05/23 08:07:33.685
Jan  5 08:07:33.692: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/05/23 08:07:33.692
Jan  5 08:07:33.694: INFO: Observed &DaemonSet event: ADDED
Jan  5 08:07:33.694: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 08:07:33.694: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 08:07:33.694: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 08:07:33.694: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 08:07:33.694: INFO: Found daemon set daemon-set in namespace daemonsets-1497 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 08:07:33.694: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/05/23 08:07:33.694
STEP: watching for the daemon set status to be patched 01/05/23 08:07:33.707
Jan  5 08:07:33.708: INFO: Observed &DaemonSet event: ADDED
Jan  5 08:07:33.708: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 08:07:33.708: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 08:07:33.709: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 08:07:33.709: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 08:07:33.709: INFO: Observed daemon set daemon-set in namespace daemonsets-1497 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 08:07:33.709: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 08:07:33.709: INFO: Found daemon set daemon-set in namespace daemonsets-1497 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan  5 08:07:33.709: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 08:07:33.711
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1497, will wait for the garbage collector to delete the pods 01/05/23 08:07:33.711
Jan  5 08:07:33.771: INFO: Deleting DaemonSet.extensions daemon-set took: 5.608799ms
Jan  5 08:07:33.872: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.750913ms
Jan  5 08:07:36.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:07:36.375: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 08:07:36.377: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16953"},"items":null}

Jan  5 08:07:36.379: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16953"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:07:36.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1497" for this suite. 01/05/23 08:07:36.386
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":129,"skipped":2635,"failed":0}
------------------------------
â€¢ [4.770 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:07:31.619
    Jan  5 08:07:31.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename daemonsets 01/05/23 08:07:31.62
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:07:31.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:07:31.634
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/05/23 08:07:31.665
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 08:07:31.67
    Jan  5 08:07:31.672: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:07:31.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:07:31.675: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:07:32.679: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:07:32.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:07:32.682: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:07:33.678: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:07:33.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 08:07:33.681: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/05/23 08:07:33.683
    Jan  5 08:07:33.685: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/05/23 08:07:33.685
    Jan  5 08:07:33.692: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/05/23 08:07:33.692
    Jan  5 08:07:33.694: INFO: Observed &DaemonSet event: ADDED
    Jan  5 08:07:33.694: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 08:07:33.694: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 08:07:33.694: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 08:07:33.694: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 08:07:33.694: INFO: Found daemon set daemon-set in namespace daemonsets-1497 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 08:07:33.694: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/05/23 08:07:33.694
    STEP: watching for the daemon set status to be patched 01/05/23 08:07:33.707
    Jan  5 08:07:33.708: INFO: Observed &DaemonSet event: ADDED
    Jan  5 08:07:33.708: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 08:07:33.708: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 08:07:33.709: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 08:07:33.709: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 08:07:33.709: INFO: Observed daemon set daemon-set in namespace daemonsets-1497 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 08:07:33.709: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 08:07:33.709: INFO: Found daemon set daemon-set in namespace daemonsets-1497 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan  5 08:07:33.709: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 08:07:33.711
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1497, will wait for the garbage collector to delete the pods 01/05/23 08:07:33.711
    Jan  5 08:07:33.771: INFO: Deleting DaemonSet.extensions daemon-set took: 5.608799ms
    Jan  5 08:07:33.872: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.750913ms
    Jan  5 08:07:36.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:07:36.375: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 08:07:36.377: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16953"},"items":null}

    Jan  5 08:07:36.379: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16953"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:07:36.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1497" for this suite. 01/05/23 08:07:36.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:07:36.391
Jan  5 08:07:36.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename resourcequota 01/05/23 08:07:36.393
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:07:36.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:07:36.409
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/05/23 08:07:36.415
STEP: Getting a ResourceQuota 01/05/23 08:07:36.423
STEP: Listing all ResourceQuotas with LabelSelector 01/05/23 08:07:36.425
STEP: Patching the ResourceQuota 01/05/23 08:07:36.427
STEP: Deleting a Collection of ResourceQuotas 01/05/23 08:07:36.46
STEP: Verifying the deleted ResourceQuota 01/05/23 08:07:36.497
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 08:07:36.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3959" for this suite. 01/05/23 08:07:36.5
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":130,"skipped":2675,"failed":0}
------------------------------
â€¢ [0.114 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:07:36.391
    Jan  5 08:07:36.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename resourcequota 01/05/23 08:07:36.393
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:07:36.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:07:36.409
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/05/23 08:07:36.415
    STEP: Getting a ResourceQuota 01/05/23 08:07:36.423
    STEP: Listing all ResourceQuotas with LabelSelector 01/05/23 08:07:36.425
    STEP: Patching the ResourceQuota 01/05/23 08:07:36.427
    STEP: Deleting a Collection of ResourceQuotas 01/05/23 08:07:36.46
    STEP: Verifying the deleted ResourceQuota 01/05/23 08:07:36.497
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 08:07:36.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3959" for this suite. 01/05/23 08:07:36.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:07:36.507
Jan  5 08:07:36.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename tables 01/05/23 08:07:36.509
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:07:36.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:07:36.521
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan  5 08:07:36.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2300" for this suite. 01/05/23 08:07:36.531
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":131,"skipped":2682,"failed":0}
------------------------------
â€¢ [0.028 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:07:36.507
    Jan  5 08:07:36.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename tables 01/05/23 08:07:36.509
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:07:36.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:07:36.521
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan  5 08:07:36.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-2300" for this suite. 01/05/23 08:07:36.531
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:07:36.535
Jan  5 08:07:36.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-probe 01/05/23 08:07:36.536
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:07:36.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:07:36.55
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2 in namespace container-probe-8612 01/05/23 08:07:36.554
Jan  5 08:07:36.571: INFO: Waiting up to 5m0s for pod "busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2" in namespace "container-probe-8612" to be "not pending"
Jan  5 08:07:36.576: INFO: Pod "busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.595287ms
Jan  5 08:07:38.579: INFO: Pod "busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007784355s
Jan  5 08:07:38.579: INFO: Pod "busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2" satisfied condition "not pending"
Jan  5 08:07:38.579: INFO: Started pod busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2 in namespace container-probe-8612
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:07:38.579
Jan  5 08:07:38.581: INFO: Initial restart count of pod busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2 is 0
Jan  5 08:08:28.674: INFO: Restart count of pod container-probe-8612/busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2 is now 1 (50.093838651s elapsed)
STEP: deleting the pod 01/05/23 08:08:28.675
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 08:08:28.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8612" for this suite. 01/05/23 08:08:28.69
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":132,"skipped":2683,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.160 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:07:36.535
    Jan  5 08:07:36.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-probe 01/05/23 08:07:36.536
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:07:36.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:07:36.55
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2 in namespace container-probe-8612 01/05/23 08:07:36.554
    Jan  5 08:07:36.571: INFO: Waiting up to 5m0s for pod "busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2" in namespace "container-probe-8612" to be "not pending"
    Jan  5 08:07:36.576: INFO: Pod "busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.595287ms
    Jan  5 08:07:38.579: INFO: Pod "busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007784355s
    Jan  5 08:07:38.579: INFO: Pod "busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2" satisfied condition "not pending"
    Jan  5 08:07:38.579: INFO: Started pod busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2 in namespace container-probe-8612
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:07:38.579
    Jan  5 08:07:38.581: INFO: Initial restart count of pod busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2 is 0
    Jan  5 08:08:28.674: INFO: Restart count of pod container-probe-8612/busybox-f300cb6c-6829-4546-8118-92e4ce7e08e2 is now 1 (50.093838651s elapsed)
    STEP: deleting the pod 01/05/23 08:08:28.675
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 08:08:28.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8612" for this suite. 01/05/23 08:08:28.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:08:28.696
Jan  5 08:08:28.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:08:28.697
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:08:28.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:08:28.711
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-1182 01/05/23 08:08:28.716
Jan  5 08:08:28.722: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1182" to be "running and ready"
Jan  5 08:08:28.724: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074275ms
Jan  5 08:08:28.724: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:08:30.727: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.005084953s
Jan  5 08:08:30.727: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan  5 08:08:30.727: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan  5 08:08:30.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan  5 08:08:30.859: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan  5 08:08:30.859: INFO: stdout: "iptables"
Jan  5 08:08:30.859: INFO: proxyMode: iptables
Jan  5 08:08:30.872: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan  5 08:08:30.873: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1182 01/05/23 08:08:30.873
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1182 01/05/23 08:08:30.899
I0105 08:08:30.910205      23 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1182, replica count: 3
I0105 08:08:33.961591      23 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 08:08:33.967: INFO: Creating new exec pod
Jan  5 08:08:33.981: INFO: Waiting up to 5m0s for pod "execpod-affinitypsgsg" in namespace "services-1182" to be "running"
Jan  5 08:08:33.983: INFO: Pod "execpod-affinitypsgsg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.685133ms
Jan  5 08:08:35.986: INFO: Pod "execpod-affinitypsgsg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005439566s
Jan  5 08:08:37.988: INFO: Pod "execpod-affinitypsgsg": Phase="Running", Reason="", readiness=true. Elapsed: 4.006953712s
Jan  5 08:08:37.988: INFO: Pod "execpod-affinitypsgsg" satisfied condition "running"
Jan  5 08:08:38.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan  5 08:08:39.130: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan  5 08:08:39.130: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:08:39.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.236.1 80'
Jan  5 08:08:39.257: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.236.1 80\nConnection to 10.97.236.1 80 port [tcp/http] succeeded!\n"
Jan  5 08:08:39.257: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:08:39.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.212 31814'
Jan  5 08:08:39.392: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.212 31814\nConnection to 16.0.14.212 31814 port [tcp/*] succeeded!\n"
Jan  5 08:08:39.392: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:08:39.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.214 31814'
Jan  5 08:08:39.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.214 31814\nConnection to 16.0.14.214 31814 port [tcp/*] succeeded!\n"
Jan  5 08:08:39.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:08:39.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://16.0.14.212:31814/ ; done'
Jan  5 08:08:39.753: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n"
Jan  5 08:08:39.753: INFO: stdout: "\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4"
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
Jan  5 08:08:39.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://16.0.14.212:31814/'
Jan  5 08:08:39.889: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n"
Jan  5 08:08:39.889: INFO: stdout: "affinity-nodeport-timeout-8v7v4"
Jan  5 08:08:59.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://16.0.14.212:31814/'
Jan  5 08:09:00.017: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n"
Jan  5 08:09:00.017: INFO: stdout: "affinity-nodeport-timeout-8v7v4"
Jan  5 08:09:20.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://16.0.14.212:31814/'
Jan  5 08:09:20.149: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n"
Jan  5 08:09:20.149: INFO: stdout: "affinity-nodeport-timeout-8jr9z"
Jan  5 08:09:20.149: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1182, will wait for the garbage collector to delete the pods 01/05/23 08:09:20.172
Jan  5 08:09:20.231: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.765218ms
Jan  5 08:09:20.332: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.407586ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:09:22.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1182" for this suite. 01/05/23 08:09:22.559
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":133,"skipped":2717,"failed":0}
------------------------------
â€¢ [SLOW TEST] [53.871 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:08:28.696
    Jan  5 08:08:28.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:08:28.697
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:08:28.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:08:28.711
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-1182 01/05/23 08:08:28.716
    Jan  5 08:08:28.722: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1182" to be "running and ready"
    Jan  5 08:08:28.724: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074275ms
    Jan  5 08:08:28.724: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:08:30.727: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.005084953s
    Jan  5 08:08:30.727: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan  5 08:08:30.727: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan  5 08:08:30.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan  5 08:08:30.859: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan  5 08:08:30.859: INFO: stdout: "iptables"
    Jan  5 08:08:30.859: INFO: proxyMode: iptables
    Jan  5 08:08:30.872: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan  5 08:08:30.873: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-1182 01/05/23 08:08:30.873
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-1182 01/05/23 08:08:30.899
    I0105 08:08:30.910205      23 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1182, replica count: 3
    I0105 08:08:33.961591      23 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 08:08:33.967: INFO: Creating new exec pod
    Jan  5 08:08:33.981: INFO: Waiting up to 5m0s for pod "execpod-affinitypsgsg" in namespace "services-1182" to be "running"
    Jan  5 08:08:33.983: INFO: Pod "execpod-affinitypsgsg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.685133ms
    Jan  5 08:08:35.986: INFO: Pod "execpod-affinitypsgsg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005439566s
    Jan  5 08:08:37.988: INFO: Pod "execpod-affinitypsgsg": Phase="Running", Reason="", readiness=true. Elapsed: 4.006953712s
    Jan  5 08:08:37.988: INFO: Pod "execpod-affinitypsgsg" satisfied condition "running"
    Jan  5 08:08:38.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan  5 08:08:39.130: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan  5 08:08:39.130: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:08:39.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.236.1 80'
    Jan  5 08:08:39.257: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.236.1 80\nConnection to 10.97.236.1 80 port [tcp/http] succeeded!\n"
    Jan  5 08:08:39.257: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:08:39.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.212 31814'
    Jan  5 08:08:39.392: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.212 31814\nConnection to 16.0.14.212 31814 port [tcp/*] succeeded!\n"
    Jan  5 08:08:39.392: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:08:39.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.214 31814'
    Jan  5 08:08:39.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.214 31814\nConnection to 16.0.14.214 31814 port [tcp/*] succeeded!\n"
    Jan  5 08:08:39.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:08:39.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://16.0.14.212:31814/ ; done'
    Jan  5 08:08:39.753: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n"
    Jan  5 08:08:39.753: INFO: stdout: "\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4\naffinity-nodeport-timeout-8v7v4"
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Received response from host: affinity-nodeport-timeout-8v7v4
    Jan  5 08:08:39.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://16.0.14.212:31814/'
    Jan  5 08:08:39.889: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n"
    Jan  5 08:08:39.889: INFO: stdout: "affinity-nodeport-timeout-8v7v4"
    Jan  5 08:08:59.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://16.0.14.212:31814/'
    Jan  5 08:09:00.017: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n"
    Jan  5 08:09:00.017: INFO: stdout: "affinity-nodeport-timeout-8v7v4"
    Jan  5 08:09:20.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1182 exec execpod-affinitypsgsg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://16.0.14.212:31814/'
    Jan  5 08:09:20.149: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://16.0.14.212:31814/\n"
    Jan  5 08:09:20.149: INFO: stdout: "affinity-nodeport-timeout-8jr9z"
    Jan  5 08:09:20.149: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1182, will wait for the garbage collector to delete the pods 01/05/23 08:09:20.172
    Jan  5 08:09:20.231: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.765218ms
    Jan  5 08:09:20.332: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.407586ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:09:22.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1182" for this suite. 01/05/23 08:09:22.559
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:09:22.568
Jan  5 08:09:22.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename statefulset 01/05/23 08:09:22.569
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:09:22.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:09:22.585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9715 01/05/23 08:09:22.588
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-9715 01/05/23 08:09:22.598
Jan  5 08:09:22.607: INFO: Found 0 stateful pods, waiting for 1
Jan  5 08:09:32.611: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/05/23 08:09:32.614
STEP: updating a scale subresource 01/05/23 08:09:32.616
STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 08:09:32.619
STEP: Patch a scale subresource 01/05/23 08:09:32.621
STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 08:09:32.639
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 08:09:32.648: INFO: Deleting all statefulset in ns statefulset-9715
Jan  5 08:09:32.661: INFO: Scaling statefulset ss to 0
Jan  5 08:09:42.689: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 08:09:42.691: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 08:09:42.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9715" for this suite. 01/05/23 08:09:42.703
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":134,"skipped":2717,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.147 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:09:22.568
    Jan  5 08:09:22.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename statefulset 01/05/23 08:09:22.569
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:09:22.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:09:22.585
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9715 01/05/23 08:09:22.588
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-9715 01/05/23 08:09:22.598
    Jan  5 08:09:22.607: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 08:09:32.611: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/05/23 08:09:32.614
    STEP: updating a scale subresource 01/05/23 08:09:32.616
    STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 08:09:32.619
    STEP: Patch a scale subresource 01/05/23 08:09:32.621
    STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 08:09:32.639
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 08:09:32.648: INFO: Deleting all statefulset in ns statefulset-9715
    Jan  5 08:09:32.661: INFO: Scaling statefulset ss to 0
    Jan  5 08:09:42.689: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 08:09:42.691: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 08:09:42.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9715" for this suite. 01/05/23 08:09:42.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:09:42.716
Jan  5 08:09:42.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 08:09:42.718
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:09:42.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:09:42.74
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/05/23 08:09:42.743
Jan  5 08:09:42.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c" in namespace "downward-api-9278" to be "Succeeded or Failed"
Jan  5 08:09:42.751: INFO: Pod "downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.825276ms
Jan  5 08:09:44.754: INFO: Pod "downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004710683s
Jan  5 08:09:46.754: INFO: Pod "downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005531356s
STEP: Saw pod success 01/05/23 08:09:46.754
Jan  5 08:09:46.755: INFO: Pod "downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c" satisfied condition "Succeeded or Failed"
Jan  5 08:09:46.758: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c container client-container: <nil>
STEP: delete the pod 01/05/23 08:09:46.77
Jan  5 08:09:46.784: INFO: Waiting for pod downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c to disappear
Jan  5 08:09:46.786: INFO: Pod downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 08:09:46.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9278" for this suite. 01/05/23 08:09:46.788
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":135,"skipped":2755,"failed":0}
------------------------------
â€¢ [4.079 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:09:42.716
    Jan  5 08:09:42.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 08:09:42.718
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:09:42.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:09:42.74
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/05/23 08:09:42.743
    Jan  5 08:09:42.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c" in namespace "downward-api-9278" to be "Succeeded or Failed"
    Jan  5 08:09:42.751: INFO: Pod "downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.825276ms
    Jan  5 08:09:44.754: INFO: Pod "downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004710683s
    Jan  5 08:09:46.754: INFO: Pod "downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005531356s
    STEP: Saw pod success 01/05/23 08:09:46.754
    Jan  5 08:09:46.755: INFO: Pod "downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c" satisfied condition "Succeeded or Failed"
    Jan  5 08:09:46.758: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c container client-container: <nil>
    STEP: delete the pod 01/05/23 08:09:46.77
    Jan  5 08:09:46.784: INFO: Waiting for pod downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c to disappear
    Jan  5 08:09:46.786: INFO: Pod downwardapi-volume-1ddfda14-400b-4f95-974f-12b6abf0191c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 08:09:46.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9278" for this suite. 01/05/23 08:09:46.788
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:09:46.796
Jan  5 08:09:46.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:09:46.797
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:09:46.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:09:46.822
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/05/23 08:09:46.825
Jan  5 08:09:46.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1" in namespace "projected-860" to be "Succeeded or Failed"
Jan  5 08:09:46.834: INFO: Pod "downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.710794ms
Jan  5 08:09:48.837: INFO: Pod "downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004775111s
Jan  5 08:09:50.838: INFO: Pod "downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005250103s
STEP: Saw pod success 01/05/23 08:09:50.838
Jan  5 08:09:50.838: INFO: Pod "downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1" satisfied condition "Succeeded or Failed"
Jan  5 08:09:50.840: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1 container client-container: <nil>
STEP: delete the pod 01/05/23 08:09:50.843
Jan  5 08:09:50.859: INFO: Waiting for pod downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1 to disappear
Jan  5 08:09:50.862: INFO: Pod downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 08:09:50.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-860" for this suite. 01/05/23 08:09:50.864
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":136,"skipped":2756,"failed":0}
------------------------------
â€¢ [4.082 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:09:46.796
    Jan  5 08:09:46.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:09:46.797
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:09:46.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:09:46.822
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/05/23 08:09:46.825
    Jan  5 08:09:46.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1" in namespace "projected-860" to be "Succeeded or Failed"
    Jan  5 08:09:46.834: INFO: Pod "downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.710794ms
    Jan  5 08:09:48.837: INFO: Pod "downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004775111s
    Jan  5 08:09:50.838: INFO: Pod "downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005250103s
    STEP: Saw pod success 01/05/23 08:09:50.838
    Jan  5 08:09:50.838: INFO: Pod "downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1" satisfied condition "Succeeded or Failed"
    Jan  5 08:09:50.840: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1 container client-container: <nil>
    STEP: delete the pod 01/05/23 08:09:50.843
    Jan  5 08:09:50.859: INFO: Waiting for pod downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1 to disappear
    Jan  5 08:09:50.862: INFO: Pod downwardapi-volume-235c0e93-b015-41df-bc56-479ac8f8d3b1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 08:09:50.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-860" for this suite. 01/05/23 08:09:50.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:09:50.878
Jan  5 08:09:50.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 08:09:50.879
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:09:50.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:09:50.893
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/05/23 08:09:50.896
Jan  5 08:09:50.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 create -f -'
Jan  5 08:09:51.463: INFO: stderr: ""
Jan  5 08:09:51.463: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 08:09:51.463
Jan  5 08:09:51.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 08:09:51.521: INFO: stderr: ""
Jan  5 08:09:51.521: INFO: stdout: "update-demo-nautilus-g6cqr update-demo-nautilus-hbw4j "
Jan  5 08:09:51.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods update-demo-nautilus-g6cqr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 08:09:51.588: INFO: stderr: ""
Jan  5 08:09:51.588: INFO: stdout: ""
Jan  5 08:09:51.588: INFO: update-demo-nautilus-g6cqr is created but not running
Jan  5 08:09:56.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 08:09:56.650: INFO: stderr: ""
Jan  5 08:09:56.650: INFO: stdout: "update-demo-nautilus-g6cqr update-demo-nautilus-hbw4j "
Jan  5 08:09:56.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods update-demo-nautilus-g6cqr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 08:09:56.702: INFO: stderr: ""
Jan  5 08:09:56.702: INFO: stdout: "true"
Jan  5 08:09:56.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods update-demo-nautilus-g6cqr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 08:09:56.752: INFO: stderr: ""
Jan  5 08:09:56.752: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 08:09:56.752: INFO: validating pod update-demo-nautilus-g6cqr
Jan  5 08:09:56.757: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 08:09:56.757: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 08:09:56.757: INFO: update-demo-nautilus-g6cqr is verified up and running
Jan  5 08:09:56.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods update-demo-nautilus-hbw4j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 08:09:56.809: INFO: stderr: ""
Jan  5 08:09:56.809: INFO: stdout: "true"
Jan  5 08:09:56.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods update-demo-nautilus-hbw4j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 08:09:56.857: INFO: stderr: ""
Jan  5 08:09:56.857: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  5 08:09:56.857: INFO: validating pod update-demo-nautilus-hbw4j
Jan  5 08:09:56.860: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 08:09:56.860: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 08:09:56.860: INFO: update-demo-nautilus-hbw4j is verified up and running
STEP: using delete to clean up resources 01/05/23 08:09:56.86
Jan  5 08:09:56.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 delete --grace-period=0 --force -f -'
Jan  5 08:09:56.913: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 08:09:56.913: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  5 08:09:56.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get rc,svc -l name=update-demo --no-headers'
Jan  5 08:09:56.969: INFO: stderr: "No resources found in kubectl-8659 namespace.\n"
Jan  5 08:09:56.969: INFO: stdout: ""
Jan  5 08:09:56.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  5 08:09:57.027: INFO: stderr: ""
Jan  5 08:09:57.027: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 08:09:57.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8659" for this suite. 01/05/23 08:09:57.03
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":137,"skipped":2782,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.157 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:09:50.878
    Jan  5 08:09:50.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 08:09:50.879
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:09:50.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:09:50.893
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/05/23 08:09:50.896
    Jan  5 08:09:50.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 create -f -'
    Jan  5 08:09:51.463: INFO: stderr: ""
    Jan  5 08:09:51.463: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 08:09:51.463
    Jan  5 08:09:51.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 08:09:51.521: INFO: stderr: ""
    Jan  5 08:09:51.521: INFO: stdout: "update-demo-nautilus-g6cqr update-demo-nautilus-hbw4j "
    Jan  5 08:09:51.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods update-demo-nautilus-g6cqr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 08:09:51.588: INFO: stderr: ""
    Jan  5 08:09:51.588: INFO: stdout: ""
    Jan  5 08:09:51.588: INFO: update-demo-nautilus-g6cqr is created but not running
    Jan  5 08:09:56.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 08:09:56.650: INFO: stderr: ""
    Jan  5 08:09:56.650: INFO: stdout: "update-demo-nautilus-g6cqr update-demo-nautilus-hbw4j "
    Jan  5 08:09:56.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods update-demo-nautilus-g6cqr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 08:09:56.702: INFO: stderr: ""
    Jan  5 08:09:56.702: INFO: stdout: "true"
    Jan  5 08:09:56.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods update-demo-nautilus-g6cqr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 08:09:56.752: INFO: stderr: ""
    Jan  5 08:09:56.752: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 08:09:56.752: INFO: validating pod update-demo-nautilus-g6cqr
    Jan  5 08:09:56.757: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 08:09:56.757: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 08:09:56.757: INFO: update-demo-nautilus-g6cqr is verified up and running
    Jan  5 08:09:56.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods update-demo-nautilus-hbw4j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 08:09:56.809: INFO: stderr: ""
    Jan  5 08:09:56.809: INFO: stdout: "true"
    Jan  5 08:09:56.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods update-demo-nautilus-hbw4j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 08:09:56.857: INFO: stderr: ""
    Jan  5 08:09:56.857: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  5 08:09:56.857: INFO: validating pod update-demo-nautilus-hbw4j
    Jan  5 08:09:56.860: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 08:09:56.860: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 08:09:56.860: INFO: update-demo-nautilus-hbw4j is verified up and running
    STEP: using delete to clean up resources 01/05/23 08:09:56.86
    Jan  5 08:09:56.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 delete --grace-period=0 --force -f -'
    Jan  5 08:09:56.913: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 08:09:56.913: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  5 08:09:56.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get rc,svc -l name=update-demo --no-headers'
    Jan  5 08:09:56.969: INFO: stderr: "No resources found in kubectl-8659 namespace.\n"
    Jan  5 08:09:56.969: INFO: stdout: ""
    Jan  5 08:09:56.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-8659 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  5 08:09:57.027: INFO: stderr: ""
    Jan  5 08:09:57.027: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 08:09:57.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8659" for this suite. 01/05/23 08:09:57.03
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:09:57.036
Jan  5 08:09:57.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pods 01/05/23 08:09:57.036
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:09:57.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:09:57.057
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/05/23 08:09:57.059
STEP: submitting the pod to kubernetes 01/05/23 08:09:57.059
Jan  5 08:09:57.065: INFO: Waiting up to 5m0s for pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b" in namespace "pods-8016" to be "running and ready"
Jan  5 08:09:57.090: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.720943ms
Jan  5 08:09:57.090: INFO: The phase of Pod pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:09:59.093: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027898863s
Jan  5 08:09:59.093: INFO: The phase of Pod pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:10:01.093: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b": Phase="Running", Reason="", readiness=true. Elapsed: 4.02797178s
Jan  5 08:10:01.093: INFO: The phase of Pod pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b is Running (Ready = true)
Jan  5 08:10:01.093: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/05/23 08:10:01.095
STEP: updating the pod 01/05/23 08:10:01.097
Jan  5 08:10:01.607: INFO: Successfully updated pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b"
Jan  5 08:10:01.607: INFO: Waiting up to 5m0s for pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b" in namespace "pods-8016" to be "running"
Jan  5 08:10:01.608: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b": Phase="Running", Reason="", readiness=true. Elapsed: 1.794374ms
Jan  5 08:10:01.608: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/05/23 08:10:01.608
Jan  5 08:10:01.610: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 08:10:01.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8016" for this suite. 01/05/23 08:10:01.613
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":138,"skipped":2783,"failed":0}
------------------------------
â€¢ [4.586 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:09:57.036
    Jan  5 08:09:57.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pods 01/05/23 08:09:57.036
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:09:57.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:09:57.057
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/05/23 08:09:57.059
    STEP: submitting the pod to kubernetes 01/05/23 08:09:57.059
    Jan  5 08:09:57.065: INFO: Waiting up to 5m0s for pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b" in namespace "pods-8016" to be "running and ready"
    Jan  5 08:09:57.090: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.720943ms
    Jan  5 08:09:57.090: INFO: The phase of Pod pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:09:59.093: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027898863s
    Jan  5 08:09:59.093: INFO: The phase of Pod pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:10:01.093: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b": Phase="Running", Reason="", readiness=true. Elapsed: 4.02797178s
    Jan  5 08:10:01.093: INFO: The phase of Pod pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b is Running (Ready = true)
    Jan  5 08:10:01.093: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/05/23 08:10:01.095
    STEP: updating the pod 01/05/23 08:10:01.097
    Jan  5 08:10:01.607: INFO: Successfully updated pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b"
    Jan  5 08:10:01.607: INFO: Waiting up to 5m0s for pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b" in namespace "pods-8016" to be "running"
    Jan  5 08:10:01.608: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b": Phase="Running", Reason="", readiness=true. Elapsed: 1.794374ms
    Jan  5 08:10:01.608: INFO: Pod "pod-update-80d039ba-a6c6-4bab-b998-c5a8e4e5000b" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/05/23 08:10:01.608
    Jan  5 08:10:01.610: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 08:10:01.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8016" for this suite. 01/05/23 08:10:01.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:10:01.623
Jan  5 08:10:01.623: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename discovery 01/05/23 08:10:01.624
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:01.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:01.646
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/05/23 08:10:01.648
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan  5 08:10:02.771: INFO: Checking APIGroup: apiregistration.k8s.io
Jan  5 08:10:02.771: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan  5 08:10:02.772: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan  5 08:10:02.772: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan  5 08:10:02.772: INFO: Checking APIGroup: apps
Jan  5 08:10:02.772: INFO: PreferredVersion.GroupVersion: apps/v1
Jan  5 08:10:02.772: INFO: Versions found [{apps/v1 v1}]
Jan  5 08:10:02.772: INFO: apps/v1 matches apps/v1
Jan  5 08:10:02.772: INFO: Checking APIGroup: events.k8s.io
Jan  5 08:10:02.773: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan  5 08:10:02.773: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan  5 08:10:02.773: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan  5 08:10:02.773: INFO: Checking APIGroup: authentication.k8s.io
Jan  5 08:10:02.773: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan  5 08:10:02.773: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan  5 08:10:02.773: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan  5 08:10:02.773: INFO: Checking APIGroup: authorization.k8s.io
Jan  5 08:10:02.774: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan  5 08:10:02.774: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan  5 08:10:02.774: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan  5 08:10:02.774: INFO: Checking APIGroup: autoscaling
Jan  5 08:10:02.774: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan  5 08:10:02.774: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan  5 08:10:02.774: INFO: autoscaling/v2 matches autoscaling/v2
Jan  5 08:10:02.774: INFO: Checking APIGroup: batch
Jan  5 08:10:02.775: INFO: PreferredVersion.GroupVersion: batch/v1
Jan  5 08:10:02.775: INFO: Versions found [{batch/v1 v1}]
Jan  5 08:10:02.775: INFO: batch/v1 matches batch/v1
Jan  5 08:10:02.775: INFO: Checking APIGroup: certificates.k8s.io
Jan  5 08:10:02.776: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan  5 08:10:02.776: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan  5 08:10:02.776: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan  5 08:10:02.776: INFO: Checking APIGroup: networking.k8s.io
Jan  5 08:10:02.776: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan  5 08:10:02.776: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan  5 08:10:02.776: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan  5 08:10:02.776: INFO: Checking APIGroup: policy
Jan  5 08:10:02.777: INFO: PreferredVersion.GroupVersion: policy/v1
Jan  5 08:10:02.777: INFO: Versions found [{policy/v1 v1}]
Jan  5 08:10:02.777: INFO: policy/v1 matches policy/v1
Jan  5 08:10:02.777: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan  5 08:10:02.778: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan  5 08:10:02.778: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan  5 08:10:02.778: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan  5 08:10:02.778: INFO: Checking APIGroup: storage.k8s.io
Jan  5 08:10:02.778: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan  5 08:10:02.778: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan  5 08:10:02.778: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan  5 08:10:02.778: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan  5 08:10:02.779: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan  5 08:10:02.779: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan  5 08:10:02.779: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan  5 08:10:02.779: INFO: Checking APIGroup: apiextensions.k8s.io
Jan  5 08:10:02.780: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan  5 08:10:02.780: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan  5 08:10:02.780: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan  5 08:10:02.780: INFO: Checking APIGroup: scheduling.k8s.io
Jan  5 08:10:02.780: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan  5 08:10:02.780: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan  5 08:10:02.780: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan  5 08:10:02.780: INFO: Checking APIGroup: coordination.k8s.io
Jan  5 08:10:02.781: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan  5 08:10:02.781: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan  5 08:10:02.781: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan  5 08:10:02.781: INFO: Checking APIGroup: node.k8s.io
Jan  5 08:10:02.782: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan  5 08:10:02.782: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan  5 08:10:02.782: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan  5 08:10:02.782: INFO: Checking APIGroup: discovery.k8s.io
Jan  5 08:10:02.782: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan  5 08:10:02.782: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan  5 08:10:02.782: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan  5 08:10:02.782: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan  5 08:10:02.783: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan  5 08:10:02.783: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan  5 08:10:02.783: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan  5 08:10:02.783: INFO: Checking APIGroup: crd.projectcalico.org
Jan  5 08:10:02.783: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan  5 08:10:02.783: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan  5 08:10:02.783: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan  5 08:10:02.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6486" for this suite. 01/05/23 08:10:02.786
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":139,"skipped":2826,"failed":0}
------------------------------
â€¢ [1.167 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:10:01.623
    Jan  5 08:10:01.623: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename discovery 01/05/23 08:10:01.624
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:01.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:01.646
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/05/23 08:10:01.648
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan  5 08:10:02.771: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan  5 08:10:02.771: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan  5 08:10:02.772: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan  5 08:10:02.772: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan  5 08:10:02.772: INFO: Checking APIGroup: apps
    Jan  5 08:10:02.772: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan  5 08:10:02.772: INFO: Versions found [{apps/v1 v1}]
    Jan  5 08:10:02.772: INFO: apps/v1 matches apps/v1
    Jan  5 08:10:02.772: INFO: Checking APIGroup: events.k8s.io
    Jan  5 08:10:02.773: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan  5 08:10:02.773: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan  5 08:10:02.773: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan  5 08:10:02.773: INFO: Checking APIGroup: authentication.k8s.io
    Jan  5 08:10:02.773: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan  5 08:10:02.773: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan  5 08:10:02.773: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan  5 08:10:02.773: INFO: Checking APIGroup: authorization.k8s.io
    Jan  5 08:10:02.774: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan  5 08:10:02.774: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan  5 08:10:02.774: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan  5 08:10:02.774: INFO: Checking APIGroup: autoscaling
    Jan  5 08:10:02.774: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan  5 08:10:02.774: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan  5 08:10:02.774: INFO: autoscaling/v2 matches autoscaling/v2
    Jan  5 08:10:02.774: INFO: Checking APIGroup: batch
    Jan  5 08:10:02.775: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan  5 08:10:02.775: INFO: Versions found [{batch/v1 v1}]
    Jan  5 08:10:02.775: INFO: batch/v1 matches batch/v1
    Jan  5 08:10:02.775: INFO: Checking APIGroup: certificates.k8s.io
    Jan  5 08:10:02.776: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan  5 08:10:02.776: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan  5 08:10:02.776: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan  5 08:10:02.776: INFO: Checking APIGroup: networking.k8s.io
    Jan  5 08:10:02.776: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan  5 08:10:02.776: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan  5 08:10:02.776: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan  5 08:10:02.776: INFO: Checking APIGroup: policy
    Jan  5 08:10:02.777: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan  5 08:10:02.777: INFO: Versions found [{policy/v1 v1}]
    Jan  5 08:10:02.777: INFO: policy/v1 matches policy/v1
    Jan  5 08:10:02.777: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan  5 08:10:02.778: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan  5 08:10:02.778: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan  5 08:10:02.778: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan  5 08:10:02.778: INFO: Checking APIGroup: storage.k8s.io
    Jan  5 08:10:02.778: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan  5 08:10:02.778: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan  5 08:10:02.778: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan  5 08:10:02.778: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan  5 08:10:02.779: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan  5 08:10:02.779: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan  5 08:10:02.779: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan  5 08:10:02.779: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan  5 08:10:02.780: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan  5 08:10:02.780: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan  5 08:10:02.780: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan  5 08:10:02.780: INFO: Checking APIGroup: scheduling.k8s.io
    Jan  5 08:10:02.780: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan  5 08:10:02.780: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan  5 08:10:02.780: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan  5 08:10:02.780: INFO: Checking APIGroup: coordination.k8s.io
    Jan  5 08:10:02.781: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan  5 08:10:02.781: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan  5 08:10:02.781: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan  5 08:10:02.781: INFO: Checking APIGroup: node.k8s.io
    Jan  5 08:10:02.782: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan  5 08:10:02.782: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan  5 08:10:02.782: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan  5 08:10:02.782: INFO: Checking APIGroup: discovery.k8s.io
    Jan  5 08:10:02.782: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan  5 08:10:02.782: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan  5 08:10:02.782: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan  5 08:10:02.782: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan  5 08:10:02.783: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan  5 08:10:02.783: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan  5 08:10:02.783: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan  5 08:10:02.783: INFO: Checking APIGroup: crd.projectcalico.org
    Jan  5 08:10:02.783: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan  5 08:10:02.783: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan  5 08:10:02.783: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan  5 08:10:02.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-6486" for this suite. 01/05/23 08:10:02.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:10:02.792
Jan  5 08:10:02.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 08:10:02.793
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:02.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:02.814
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan  5 08:10:02.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 08:10:04.841
Jan  5 08:10:04.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-9384 --namespace=crd-publish-openapi-9384 create -f -'
Jan  5 08:10:05.452: INFO: stderr: ""
Jan  5 08:10:05.452: INFO: stdout: "e2e-test-crd-publish-openapi-9488-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  5 08:10:05.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-9384 --namespace=crd-publish-openapi-9384 delete e2e-test-crd-publish-openapi-9488-crds test-cr'
Jan  5 08:10:05.514: INFO: stderr: ""
Jan  5 08:10:05.514: INFO: stdout: "e2e-test-crd-publish-openapi-9488-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan  5 08:10:05.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-9384 --namespace=crd-publish-openapi-9384 apply -f -'
Jan  5 08:10:05.697: INFO: stderr: ""
Jan  5 08:10:05.697: INFO: stdout: "e2e-test-crd-publish-openapi-9488-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  5 08:10:05.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-9384 --namespace=crd-publish-openapi-9384 delete e2e-test-crd-publish-openapi-9488-crds test-cr'
Jan  5 08:10:05.776: INFO: stderr: ""
Jan  5 08:10:05.776: INFO: stdout: "e2e-test-crd-publish-openapi-9488-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/05/23 08:10:05.776
Jan  5 08:10:05.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-9384 explain e2e-test-crd-publish-openapi-9488-crds'
Jan  5 08:10:05.977: INFO: stderr: ""
Jan  5 08:10:05.977: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9488-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:10:07.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9384" for this suite. 01/05/23 08:10:07.987
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":140,"skipped":2873,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.199 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:10:02.792
    Jan  5 08:10:02.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 08:10:02.793
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:02.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:02.814
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan  5 08:10:02.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 08:10:04.841
    Jan  5 08:10:04.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-9384 --namespace=crd-publish-openapi-9384 create -f -'
    Jan  5 08:10:05.452: INFO: stderr: ""
    Jan  5 08:10:05.452: INFO: stdout: "e2e-test-crd-publish-openapi-9488-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  5 08:10:05.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-9384 --namespace=crd-publish-openapi-9384 delete e2e-test-crd-publish-openapi-9488-crds test-cr'
    Jan  5 08:10:05.514: INFO: stderr: ""
    Jan  5 08:10:05.514: INFO: stdout: "e2e-test-crd-publish-openapi-9488-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan  5 08:10:05.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-9384 --namespace=crd-publish-openapi-9384 apply -f -'
    Jan  5 08:10:05.697: INFO: stderr: ""
    Jan  5 08:10:05.697: INFO: stdout: "e2e-test-crd-publish-openapi-9488-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  5 08:10:05.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-9384 --namespace=crd-publish-openapi-9384 delete e2e-test-crd-publish-openapi-9488-crds test-cr'
    Jan  5 08:10:05.776: INFO: stderr: ""
    Jan  5 08:10:05.776: INFO: stdout: "e2e-test-crd-publish-openapi-9488-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/05/23 08:10:05.776
    Jan  5 08:10:05.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-9384 explain e2e-test-crd-publish-openapi-9488-crds'
    Jan  5 08:10:05.977: INFO: stderr: ""
    Jan  5 08:10:05.977: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9488-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:10:07.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9384" for this suite. 01/05/23 08:10:07.987
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:10:07.991
Jan  5 08:10:07.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:10:07.993
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:08.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:08.007
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:10:08.023
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:10:08.408
STEP: Deploying the webhook pod 01/05/23 08:10:08.413
STEP: Wait for the deployment to be ready 01/05/23 08:10:08.433
Jan  5 08:10:08.437: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 08:10:10.443
STEP: Verifying the service has paired with the endpoint 01/05/23 08:10:10.453
Jan  5 08:10:11.454: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan  5 08:10:11.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1199-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 08:10:11.965
STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 08:10:11.975
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:10:14.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2007" for this suite. 01/05/23 08:10:14.712
STEP: Destroying namespace "webhook-2007-markers" for this suite. 01/05/23 08:10:14.715
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":141,"skipped":2876,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.775 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:10:07.991
    Jan  5 08:10:07.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:10:07.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:08.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:08.007
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:10:08.023
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:10:08.408
    STEP: Deploying the webhook pod 01/05/23 08:10:08.413
    STEP: Wait for the deployment to be ready 01/05/23 08:10:08.433
    Jan  5 08:10:08.437: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 08:10:10.443
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:10:10.453
    Jan  5 08:10:11.454: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan  5 08:10:11.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1199-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 08:10:11.965
    STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 08:10:11.975
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:10:14.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2007" for this suite. 01/05/23 08:10:14.712
    STEP: Destroying namespace "webhook-2007-markers" for this suite. 01/05/23 08:10:14.715
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:10:14.768
Jan  5 08:10:14.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:10:14.769
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:14.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:14.798
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-4561 01/05/23 08:10:14.8
STEP: creating service affinity-clusterip in namespace services-4561 01/05/23 08:10:14.8
STEP: creating replication controller affinity-clusterip in namespace services-4561 01/05/23 08:10:14.813
I0105 08:10:14.820904      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4561, replica count: 3
I0105 08:10:17.871845      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 08:10:17.876: INFO: Creating new exec pod
Jan  5 08:10:17.879: INFO: Waiting up to 5m0s for pod "execpod-affinityd2dlr" in namespace "services-4561" to be "running"
Jan  5 08:10:17.881: INFO: Pod "execpod-affinityd2dlr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.740906ms
Jan  5 08:10:19.885: INFO: Pod "execpod-affinityd2dlr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005699793s
Jan  5 08:10:19.885: INFO: Pod "execpod-affinityd2dlr" satisfied condition "running"
Jan  5 08:10:20.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-4561 exec execpod-affinityd2dlr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan  5 08:10:20.994: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan  5 08:10:20.994: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:10:20.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-4561 exec execpod-affinityd2dlr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.189.210 80'
Jan  5 08:10:21.109: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.189.210 80\nConnection to 10.110.189.210 80 port [tcp/http] succeeded!\n"
Jan  5 08:10:21.109: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:10:21.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-4561 exec execpod-affinityd2dlr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.189.210:80/ ; done'
Jan  5 08:10:21.306: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n"
Jan  5 08:10:21.306: INFO: stdout: "\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4"
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
Jan  5 08:10:21.306: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4561, will wait for the garbage collector to delete the pods 01/05/23 08:10:21.341
Jan  5 08:10:21.398: INFO: Deleting ReplicationController affinity-clusterip took: 4.822174ms
Jan  5 08:10:21.499: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.641449ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:10:23.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4561" for this suite. 01/05/23 08:10:23.719
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":142,"skipped":2889,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.954 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:10:14.768
    Jan  5 08:10:14.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:10:14.769
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:14.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:14.798
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-4561 01/05/23 08:10:14.8
    STEP: creating service affinity-clusterip in namespace services-4561 01/05/23 08:10:14.8
    STEP: creating replication controller affinity-clusterip in namespace services-4561 01/05/23 08:10:14.813
    I0105 08:10:14.820904      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4561, replica count: 3
    I0105 08:10:17.871845      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 08:10:17.876: INFO: Creating new exec pod
    Jan  5 08:10:17.879: INFO: Waiting up to 5m0s for pod "execpod-affinityd2dlr" in namespace "services-4561" to be "running"
    Jan  5 08:10:17.881: INFO: Pod "execpod-affinityd2dlr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.740906ms
    Jan  5 08:10:19.885: INFO: Pod "execpod-affinityd2dlr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005699793s
    Jan  5 08:10:19.885: INFO: Pod "execpod-affinityd2dlr" satisfied condition "running"
    Jan  5 08:10:20.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-4561 exec execpod-affinityd2dlr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan  5 08:10:20.994: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan  5 08:10:20.994: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:10:20.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-4561 exec execpod-affinityd2dlr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.189.210 80'
    Jan  5 08:10:21.109: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.189.210 80\nConnection to 10.110.189.210 80 port [tcp/http] succeeded!\n"
    Jan  5 08:10:21.109: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:10:21.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-4561 exec execpod-affinityd2dlr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.189.210:80/ ; done'
    Jan  5 08:10:21.306: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.189.210:80/\n"
    Jan  5 08:10:21.306: INFO: stdout: "\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4\naffinity-clusterip-l7sp4"
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Received response from host: affinity-clusterip-l7sp4
    Jan  5 08:10:21.306: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-4561, will wait for the garbage collector to delete the pods 01/05/23 08:10:21.341
    Jan  5 08:10:21.398: INFO: Deleting ReplicationController affinity-clusterip took: 4.822174ms
    Jan  5 08:10:21.499: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.641449ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:10:23.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4561" for this suite. 01/05/23 08:10:23.719
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:10:23.722
Jan  5 08:10:23.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:10:23.723
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:23.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:23.74
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:10:23.756
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:10:24.499
STEP: Deploying the webhook pod 01/05/23 08:10:24.504
STEP: Wait for the deployment to be ready 01/05/23 08:10:24.521
Jan  5 08:10:24.529: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 08:10:26.536
STEP: Verifying the service has paired with the endpoint 01/05/23 08:10:26.567
Jan  5 08:10:27.567: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan  5 08:10:27.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/05/23 08:10:28.078
STEP: Creating a custom resource that should be denied by the webhook 01/05/23 08:10:28.088
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/05/23 08:10:30.127
STEP: Updating the custom resource with disallowed data should be denied 01/05/23 08:10:30.342
STEP: Deleting the custom resource should be denied 01/05/23 08:10:30.35
STEP: Remove the offending key and value from the custom resource data 01/05/23 08:10:30.354
STEP: Deleting the updated custom resource should be successful 01/05/23 08:10:30.367
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:10:30.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7167" for this suite. 01/05/23 08:10:30.889
STEP: Destroying namespace "webhook-7167-markers" for this suite. 01/05/23 08:10:30.894
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":143,"skipped":2892,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.291 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:10:23.722
    Jan  5 08:10:23.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:10:23.723
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:23.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:23.74
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:10:23.756
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:10:24.499
    STEP: Deploying the webhook pod 01/05/23 08:10:24.504
    STEP: Wait for the deployment to be ready 01/05/23 08:10:24.521
    Jan  5 08:10:24.529: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 08:10:26.536
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:10:26.567
    Jan  5 08:10:27.567: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan  5 08:10:27.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/05/23 08:10:28.078
    STEP: Creating a custom resource that should be denied by the webhook 01/05/23 08:10:28.088
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/05/23 08:10:30.127
    STEP: Updating the custom resource with disallowed data should be denied 01/05/23 08:10:30.342
    STEP: Deleting the custom resource should be denied 01/05/23 08:10:30.35
    STEP: Remove the offending key and value from the custom resource data 01/05/23 08:10:30.354
    STEP: Deleting the updated custom resource should be successful 01/05/23 08:10:30.367
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:10:30.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7167" for this suite. 01/05/23 08:10:30.889
    STEP: Destroying namespace "webhook-7167-markers" for this suite. 01/05/23 08:10:30.894
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:10:31.013
Jan  5 08:10:31.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 08:10:31.014
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:31.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:31.043
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-1679/configmap-test-b94856df-80bf-4df7-a6a8-dfc3070736b4 01/05/23 08:10:31.045
STEP: Creating a pod to test consume configMaps 01/05/23 08:10:31.066
Jan  5 08:10:31.071: INFO: Waiting up to 5m0s for pod "pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9" in namespace "configmap-1679" to be "Succeeded or Failed"
Jan  5 08:10:31.073: INFO: Pod "pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463731ms
Jan  5 08:10:33.075: INFO: Pod "pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003987116s
Jan  5 08:10:35.076: INFO: Pod "pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004974031s
STEP: Saw pod success 01/05/23 08:10:35.077
Jan  5 08:10:35.077: INFO: Pod "pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9" satisfied condition "Succeeded or Failed"
Jan  5 08:10:35.078: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9 container env-test: <nil>
STEP: delete the pod 01/05/23 08:10:35.082
Jan  5 08:10:35.096: INFO: Waiting for pod pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9 to disappear
Jan  5 08:10:35.097: INFO: Pod pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 08:10:35.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1679" for this suite. 01/05/23 08:10:35.099
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":144,"skipped":2898,"failed":0}
------------------------------
â€¢ [4.093 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:10:31.013
    Jan  5 08:10:31.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 08:10:31.014
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:31.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:31.043
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-1679/configmap-test-b94856df-80bf-4df7-a6a8-dfc3070736b4 01/05/23 08:10:31.045
    STEP: Creating a pod to test consume configMaps 01/05/23 08:10:31.066
    Jan  5 08:10:31.071: INFO: Waiting up to 5m0s for pod "pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9" in namespace "configmap-1679" to be "Succeeded or Failed"
    Jan  5 08:10:31.073: INFO: Pod "pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463731ms
    Jan  5 08:10:33.075: INFO: Pod "pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003987116s
    Jan  5 08:10:35.076: INFO: Pod "pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004974031s
    STEP: Saw pod success 01/05/23 08:10:35.077
    Jan  5 08:10:35.077: INFO: Pod "pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9" satisfied condition "Succeeded or Failed"
    Jan  5 08:10:35.078: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9 container env-test: <nil>
    STEP: delete the pod 01/05/23 08:10:35.082
    Jan  5 08:10:35.096: INFO: Waiting for pod pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9 to disappear
    Jan  5 08:10:35.097: INFO: Pod pod-configmaps-e3d950f4-39db-4a9a-bbb6-2b005c0eabf9 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 08:10:35.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1679" for this suite. 01/05/23 08:10:35.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:10:35.108
Jan  5 08:10:35.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:10:35.109
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:35.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:35.122
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:10:35.165
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:10:35.774
STEP: Deploying the webhook pod 01/05/23 08:10:35.778
STEP: Wait for the deployment to be ready 01/05/23 08:10:35.796
Jan  5 08:10:35.804: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 08:10:37.811
STEP: Verifying the service has paired with the endpoint 01/05/23 08:10:37.821
Jan  5 08:10:38.822: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan  5 08:10:38.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7412-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 08:10:39.332
STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 08:10:39.349
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:10:42.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6706" for this suite. 01/05/23 08:10:42.135
STEP: Destroying namespace "webhook-6706-markers" for this suite. 01/05/23 08:10:42.139
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":145,"skipped":2983,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.089 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:10:35.108
    Jan  5 08:10:35.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:10:35.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:35.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:35.122
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:10:35.165
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:10:35.774
    STEP: Deploying the webhook pod 01/05/23 08:10:35.778
    STEP: Wait for the deployment to be ready 01/05/23 08:10:35.796
    Jan  5 08:10:35.804: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 08:10:37.811
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:10:37.821
    Jan  5 08:10:38.822: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan  5 08:10:38.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7412-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 08:10:39.332
    STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 08:10:39.349
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:10:42.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6706" for this suite. 01/05/23 08:10:42.135
    STEP: Destroying namespace "webhook-6706-markers" for this suite. 01/05/23 08:10:42.139
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:10:42.2
Jan  5 08:10:42.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename replication-controller 01/05/23 08:10:42.201
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:42.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:42.24
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d 01/05/23 08:10:42.242
Jan  5 08:10:42.250: INFO: Pod name my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d: Found 0 pods out of 1
Jan  5 08:10:47.252: INFO: Pod name my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d: Found 1 pods out of 1
Jan  5 08:10:47.252: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d" are running
Jan  5 08:10:47.252: INFO: Waiting up to 5m0s for pod "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8" in namespace "replication-controller-2542" to be "running"
Jan  5 08:10:47.254: INFO: Pod "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8": Phase="Running", Reason="", readiness=true. Elapsed: 1.27906ms
Jan  5 08:10:47.254: INFO: Pod "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8" satisfied condition "running"
Jan  5 08:10:47.254: INFO: Pod "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:10:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:10:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:10:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:10:42 +0000 UTC Reason: Message:}])
Jan  5 08:10:47.254: INFO: Trying to dial the pod
Jan  5 08:10:52.262: INFO: Controller my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d: Got expected result from replica 1 [my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8]: "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  5 08:10:52.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2542" for this suite. 01/05/23 08:10:52.265
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":146,"skipped":3040,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.069 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:10:42.2
    Jan  5 08:10:42.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename replication-controller 01/05/23 08:10:42.201
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:42.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:42.24
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d 01/05/23 08:10:42.242
    Jan  5 08:10:42.250: INFO: Pod name my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d: Found 0 pods out of 1
    Jan  5 08:10:47.252: INFO: Pod name my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d: Found 1 pods out of 1
    Jan  5 08:10:47.252: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d" are running
    Jan  5 08:10:47.252: INFO: Waiting up to 5m0s for pod "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8" in namespace "replication-controller-2542" to be "running"
    Jan  5 08:10:47.254: INFO: Pod "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8": Phase="Running", Reason="", readiness=true. Elapsed: 1.27906ms
    Jan  5 08:10:47.254: INFO: Pod "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8" satisfied condition "running"
    Jan  5 08:10:47.254: INFO: Pod "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:10:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:10:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:10:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:10:42 +0000 UTC Reason: Message:}])
    Jan  5 08:10:47.254: INFO: Trying to dial the pod
    Jan  5 08:10:52.262: INFO: Controller my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d: Got expected result from replica 1 [my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8]: "my-hostname-basic-48af04be-c254-4038-a445-125734dacf5d-fmwv8", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  5 08:10:52.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2542" for this suite. 01/05/23 08:10:52.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:10:52.27
Jan  5 08:10:52.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-runtime 01/05/23 08:10:52.272
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:52.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:52.293
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/05/23 08:10:52.295
STEP: wait for the container to reach Succeeded 01/05/23 08:10:52.319
STEP: get the container status 01/05/23 08:10:57.35
STEP: the container should be terminated 01/05/23 08:10:57.352
STEP: the termination message should be set 01/05/23 08:10:57.352
Jan  5 08:10:57.352: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/05/23 08:10:57.352
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  5 08:10:57.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-585" for this suite. 01/05/23 08:10:57.379
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":147,"skipped":3047,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.113 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:10:52.27
    Jan  5 08:10:52.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-runtime 01/05/23 08:10:52.272
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:52.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:52.293
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/05/23 08:10:52.295
    STEP: wait for the container to reach Succeeded 01/05/23 08:10:52.319
    STEP: get the container status 01/05/23 08:10:57.35
    STEP: the container should be terminated 01/05/23 08:10:57.352
    STEP: the termination message should be set 01/05/23 08:10:57.352
    Jan  5 08:10:57.352: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/05/23 08:10:57.352
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  5 08:10:57.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-585" for this suite. 01/05/23 08:10:57.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:10:57.384
Jan  5 08:10:57.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename svcaccounts 01/05/23 08:10:57.384
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:57.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:57.402
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan  5 08:10:57.422: INFO: Waiting up to 5m0s for pod "pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b" in namespace "svcaccounts-2710" to be "running"
Jan  5 08:10:57.424: INFO: Pod "pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.665199ms
Jan  5 08:10:59.427: INFO: Pod "pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004798174s
Jan  5 08:11:01.429: INFO: Pod "pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b": Phase="Running", Reason="", readiness=true. Elapsed: 4.007461292s
Jan  5 08:11:01.429: INFO: Pod "pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b" satisfied condition "running"
STEP: reading a file in the container 01/05/23 08:11:01.429
Jan  5 08:11:01.430: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2710 pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/05/23 08:11:01.56
Jan  5 08:11:01.561: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2710 pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/05/23 08:11:01.675
Jan  5 08:11:01.675: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2710 pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan  5 08:11:01.792: INFO: Got root ca configmap in namespace "svcaccounts-2710"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 08:11:01.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2710" for this suite. 01/05/23 08:11:01.797
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":148,"skipped":3064,"failed":0}
------------------------------
â€¢ [4.417 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:10:57.384
    Jan  5 08:10:57.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 08:10:57.384
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:10:57.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:10:57.402
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan  5 08:10:57.422: INFO: Waiting up to 5m0s for pod "pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b" in namespace "svcaccounts-2710" to be "running"
    Jan  5 08:10:57.424: INFO: Pod "pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.665199ms
    Jan  5 08:10:59.427: INFO: Pod "pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004798174s
    Jan  5 08:11:01.429: INFO: Pod "pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b": Phase="Running", Reason="", readiness=true. Elapsed: 4.007461292s
    Jan  5 08:11:01.429: INFO: Pod "pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b" satisfied condition "running"
    STEP: reading a file in the container 01/05/23 08:11:01.429
    Jan  5 08:11:01.430: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2710 pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/05/23 08:11:01.56
    Jan  5 08:11:01.561: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2710 pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/05/23 08:11:01.675
    Jan  5 08:11:01.675: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2710 pod-service-account-24276fdb-2ba2-4fe5-9b7f-edc738bc0b2b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan  5 08:11:01.792: INFO: Got root ca configmap in namespace "svcaccounts-2710"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 08:11:01.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2710" for this suite. 01/05/23 08:11:01.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:11:01.801
Jan  5 08:11:01.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename gc 01/05/23 08:11:01.802
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:11:01.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:11:01.836
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/05/23 08:11:01.843
STEP: delete the rc 01/05/23 08:11:06.853
STEP: wait for the rc to be deleted 01/05/23 08:11:06.862
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/05/23 08:11:11.866
STEP: Gathering metrics 01/05/23 08:11:41.878
W0105 08:11:41.881772      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 08:11:41.881: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  5 08:11:41.881: INFO: Deleting pod "simpletest.rc-2gg4v" in namespace "gc-1380"
Jan  5 08:11:41.891: INFO: Deleting pod "simpletest.rc-2qplv" in namespace "gc-1380"
Jan  5 08:11:41.910: INFO: Deleting pod "simpletest.rc-2rttn" in namespace "gc-1380"
Jan  5 08:11:41.945: INFO: Deleting pod "simpletest.rc-452mn" in namespace "gc-1380"
Jan  5 08:11:41.957: INFO: Deleting pod "simpletest.rc-47mks" in namespace "gc-1380"
Jan  5 08:11:41.976: INFO: Deleting pod "simpletest.rc-49flg" in namespace "gc-1380"
Jan  5 08:11:41.991: INFO: Deleting pod "simpletest.rc-4jqt8" in namespace "gc-1380"
Jan  5 08:11:42.001: INFO: Deleting pod "simpletest.rc-4q67f" in namespace "gc-1380"
Jan  5 08:11:42.017: INFO: Deleting pod "simpletest.rc-55bkj" in namespace "gc-1380"
Jan  5 08:11:42.034: INFO: Deleting pod "simpletest.rc-55ln5" in namespace "gc-1380"
Jan  5 08:11:42.045: INFO: Deleting pod "simpletest.rc-5bpkt" in namespace "gc-1380"
Jan  5 08:11:42.059: INFO: Deleting pod "simpletest.rc-5jrsx" in namespace "gc-1380"
Jan  5 08:11:42.088: INFO: Deleting pod "simpletest.rc-5v6gz" in namespace "gc-1380"
Jan  5 08:11:42.105: INFO: Deleting pod "simpletest.rc-5xz7r" in namespace "gc-1380"
Jan  5 08:11:42.129: INFO: Deleting pod "simpletest.rc-6b6j4" in namespace "gc-1380"
Jan  5 08:11:42.146: INFO: Deleting pod "simpletest.rc-6mpzk" in namespace "gc-1380"
Jan  5 08:11:42.160: INFO: Deleting pod "simpletest.rc-6rjl7" in namespace "gc-1380"
Jan  5 08:11:42.172: INFO: Deleting pod "simpletest.rc-6w7kj" in namespace "gc-1380"
Jan  5 08:11:42.212: INFO: Deleting pod "simpletest.rc-75cnz" in namespace "gc-1380"
Jan  5 08:11:42.224: INFO: Deleting pod "simpletest.rc-7mhlc" in namespace "gc-1380"
Jan  5 08:11:42.242: INFO: Deleting pod "simpletest.rc-7tkp8" in namespace "gc-1380"
Jan  5 08:11:42.258: INFO: Deleting pod "simpletest.rc-7vg49" in namespace "gc-1380"
Jan  5 08:11:42.269: INFO: Deleting pod "simpletest.rc-8mld4" in namespace "gc-1380"
Jan  5 08:11:42.288: INFO: Deleting pod "simpletest.rc-8msqq" in namespace "gc-1380"
Jan  5 08:11:42.301: INFO: Deleting pod "simpletest.rc-8r2kg" in namespace "gc-1380"
Jan  5 08:11:42.320: INFO: Deleting pod "simpletest.rc-8zknk" in namespace "gc-1380"
Jan  5 08:11:42.333: INFO: Deleting pod "simpletest.rc-8zs5f" in namespace "gc-1380"
Jan  5 08:11:42.354: INFO: Deleting pod "simpletest.rc-9nhm4" in namespace "gc-1380"
Jan  5 08:11:42.366: INFO: Deleting pod "simpletest.rc-9nhzk" in namespace "gc-1380"
Jan  5 08:11:42.376: INFO: Deleting pod "simpletest.rc-9t5v2" in namespace "gc-1380"
Jan  5 08:11:42.393: INFO: Deleting pod "simpletest.rc-b86sj" in namespace "gc-1380"
Jan  5 08:11:42.404: INFO: Deleting pod "simpletest.rc-b8n9d" in namespace "gc-1380"
Jan  5 08:11:42.433: INFO: Deleting pod "simpletest.rc-bc9ts" in namespace "gc-1380"
Jan  5 08:11:42.452: INFO: Deleting pod "simpletest.rc-bqxgq" in namespace "gc-1380"
Jan  5 08:11:42.466: INFO: Deleting pod "simpletest.rc-brhpb" in namespace "gc-1380"
Jan  5 08:11:42.480: INFO: Deleting pod "simpletest.rc-btdxr" in namespace "gc-1380"
Jan  5 08:11:42.490: INFO: Deleting pod "simpletest.rc-c6hjg" in namespace "gc-1380"
Jan  5 08:11:42.503: INFO: Deleting pod "simpletest.rc-c8ch2" in namespace "gc-1380"
Jan  5 08:11:42.515: INFO: Deleting pod "simpletest.rc-csv9r" in namespace "gc-1380"
Jan  5 08:11:42.524: INFO: Deleting pod "simpletest.rc-cwz2h" in namespace "gc-1380"
Jan  5 08:11:42.550: INFO: Deleting pod "simpletest.rc-d2wrw" in namespace "gc-1380"
Jan  5 08:11:42.565: INFO: Deleting pod "simpletest.rc-dbjgc" in namespace "gc-1380"
Jan  5 08:11:42.576: INFO: Deleting pod "simpletest.rc-dd5r7" in namespace "gc-1380"
Jan  5 08:11:42.590: INFO: Deleting pod "simpletest.rc-dp75x" in namespace "gc-1380"
Jan  5 08:11:42.601: INFO: Deleting pod "simpletest.rc-dsd6h" in namespace "gc-1380"
Jan  5 08:11:42.617: INFO: Deleting pod "simpletest.rc-fpjhc" in namespace "gc-1380"
Jan  5 08:11:42.627: INFO: Deleting pod "simpletest.rc-g4nwt" in namespace "gc-1380"
Jan  5 08:11:42.639: INFO: Deleting pod "simpletest.rc-g5499" in namespace "gc-1380"
Jan  5 08:11:42.656: INFO: Deleting pod "simpletest.rc-g5b8h" in namespace "gc-1380"
Jan  5 08:11:42.666: INFO: Deleting pod "simpletest.rc-gdtzg" in namespace "gc-1380"
Jan  5 08:11:42.682: INFO: Deleting pod "simpletest.rc-gmkm7" in namespace "gc-1380"
Jan  5 08:11:42.698: INFO: Deleting pod "simpletest.rc-gn26x" in namespace "gc-1380"
Jan  5 08:11:42.709: INFO: Deleting pod "simpletest.rc-hr2ql" in namespace "gc-1380"
Jan  5 08:11:42.725: INFO: Deleting pod "simpletest.rc-hsgq4" in namespace "gc-1380"
Jan  5 08:11:42.736: INFO: Deleting pod "simpletest.rc-j7wzr" in namespace "gc-1380"
Jan  5 08:11:42.751: INFO: Deleting pod "simpletest.rc-l6str" in namespace "gc-1380"
Jan  5 08:11:42.771: INFO: Deleting pod "simpletest.rc-m8zl9" in namespace "gc-1380"
Jan  5 08:11:42.781: INFO: Deleting pod "simpletest.rc-mtn9s" in namespace "gc-1380"
Jan  5 08:11:42.791: INFO: Deleting pod "simpletest.rc-nfr2b" in namespace "gc-1380"
Jan  5 08:11:42.800: INFO: Deleting pod "simpletest.rc-nkdkm" in namespace "gc-1380"
Jan  5 08:11:42.809: INFO: Deleting pod "simpletest.rc-nmrt2" in namespace "gc-1380"
Jan  5 08:11:42.829: INFO: Deleting pod "simpletest.rc-nqctv" in namespace "gc-1380"
Jan  5 08:11:42.845: INFO: Deleting pod "simpletest.rc-nxdc5" in namespace "gc-1380"
Jan  5 08:11:42.856: INFO: Deleting pod "simpletest.rc-nxmd6" in namespace "gc-1380"
Jan  5 08:11:42.873: INFO: Deleting pod "simpletest.rc-pb2z6" in namespace "gc-1380"
Jan  5 08:11:42.889: INFO: Deleting pod "simpletest.rc-pc5wf" in namespace "gc-1380"
Jan  5 08:11:42.900: INFO: Deleting pod "simpletest.rc-pcdzr" in namespace "gc-1380"
Jan  5 08:11:42.916: INFO: Deleting pod "simpletest.rc-pgc2c" in namespace "gc-1380"
Jan  5 08:11:42.926: INFO: Deleting pod "simpletest.rc-ppl9k" in namespace "gc-1380"
Jan  5 08:11:42.937: INFO: Deleting pod "simpletest.rc-ptwdg" in namespace "gc-1380"
Jan  5 08:11:42.988: INFO: Deleting pod "simpletest.rc-pw7sg" in namespace "gc-1380"
Jan  5 08:11:43.034: INFO: Deleting pod "simpletest.rc-q7s6v" in namespace "gc-1380"
Jan  5 08:11:43.089: INFO: Deleting pod "simpletest.rc-qcbt4" in namespace "gc-1380"
Jan  5 08:11:43.130: INFO: Deleting pod "simpletest.rc-qcgsp" in namespace "gc-1380"
Jan  5 08:11:43.187: INFO: Deleting pod "simpletest.rc-qk676" in namespace "gc-1380"
Jan  5 08:11:43.230: INFO: Deleting pod "simpletest.rc-qqrtz" in namespace "gc-1380"
Jan  5 08:11:43.290: INFO: Deleting pod "simpletest.rc-r5rkg" in namespace "gc-1380"
Jan  5 08:11:43.333: INFO: Deleting pod "simpletest.rc-r9gt6" in namespace "gc-1380"
Jan  5 08:11:43.379: INFO: Deleting pod "simpletest.rc-s7hsc" in namespace "gc-1380"
Jan  5 08:11:43.433: INFO: Deleting pod "simpletest.rc-sbzf6" in namespace "gc-1380"
Jan  5 08:11:43.479: INFO: Deleting pod "simpletest.rc-sdxpb" in namespace "gc-1380"
Jan  5 08:11:43.534: INFO: Deleting pod "simpletest.rc-sh7gf" in namespace "gc-1380"
Jan  5 08:11:43.579: INFO: Deleting pod "simpletest.rc-sth75" in namespace "gc-1380"
Jan  5 08:11:43.635: INFO: Deleting pod "simpletest.rc-tk6kp" in namespace "gc-1380"
Jan  5 08:11:43.685: INFO: Deleting pod "simpletest.rc-vmf4j" in namespace "gc-1380"
Jan  5 08:11:43.729: INFO: Deleting pod "simpletest.rc-vt8bx" in namespace "gc-1380"
Jan  5 08:11:43.785: INFO: Deleting pod "simpletest.rc-w59k2" in namespace "gc-1380"
Jan  5 08:11:43.830: INFO: Deleting pod "simpletest.rc-wbhcr" in namespace "gc-1380"
Jan  5 08:11:43.880: INFO: Deleting pod "simpletest.rc-wq25f" in namespace "gc-1380"
Jan  5 08:11:43.933: INFO: Deleting pod "simpletest.rc-x55gg" in namespace "gc-1380"
Jan  5 08:11:43.980: INFO: Deleting pod "simpletest.rc-x8nt4" in namespace "gc-1380"
Jan  5 08:11:44.035: INFO: Deleting pod "simpletest.rc-x97qt" in namespace "gc-1380"
Jan  5 08:11:44.080: INFO: Deleting pod "simpletest.rc-xdphj" in namespace "gc-1380"
Jan  5 08:11:44.135: INFO: Deleting pod "simpletest.rc-xgwqj" in namespace "gc-1380"
Jan  5 08:11:44.181: INFO: Deleting pod "simpletest.rc-xhfqt" in namespace "gc-1380"
Jan  5 08:11:44.234: INFO: Deleting pod "simpletest.rc-xqdb4" in namespace "gc-1380"
Jan  5 08:11:44.306: INFO: Deleting pod "simpletest.rc-z26zf" in namespace "gc-1380"
Jan  5 08:11:44.334: INFO: Deleting pod "simpletest.rc-z6rfg" in namespace "gc-1380"
Jan  5 08:11:44.381: INFO: Deleting pod "simpletest.rc-zjzm2" in namespace "gc-1380"
Jan  5 08:11:44.430: INFO: Deleting pod "simpletest.rc-ztbv4" in namespace "gc-1380"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 08:11:44.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1380" for this suite. 01/05/23 08:11:44.52
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":149,"skipped":3082,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.775 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:11:01.801
    Jan  5 08:11:01.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename gc 01/05/23 08:11:01.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:11:01.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:11:01.836
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/05/23 08:11:01.843
    STEP: delete the rc 01/05/23 08:11:06.853
    STEP: wait for the rc to be deleted 01/05/23 08:11:06.862
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/05/23 08:11:11.866
    STEP: Gathering metrics 01/05/23 08:11:41.878
    W0105 08:11:41.881772      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 08:11:41.881: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  5 08:11:41.881: INFO: Deleting pod "simpletest.rc-2gg4v" in namespace "gc-1380"
    Jan  5 08:11:41.891: INFO: Deleting pod "simpletest.rc-2qplv" in namespace "gc-1380"
    Jan  5 08:11:41.910: INFO: Deleting pod "simpletest.rc-2rttn" in namespace "gc-1380"
    Jan  5 08:11:41.945: INFO: Deleting pod "simpletest.rc-452mn" in namespace "gc-1380"
    Jan  5 08:11:41.957: INFO: Deleting pod "simpletest.rc-47mks" in namespace "gc-1380"
    Jan  5 08:11:41.976: INFO: Deleting pod "simpletest.rc-49flg" in namespace "gc-1380"
    Jan  5 08:11:41.991: INFO: Deleting pod "simpletest.rc-4jqt8" in namespace "gc-1380"
    Jan  5 08:11:42.001: INFO: Deleting pod "simpletest.rc-4q67f" in namespace "gc-1380"
    Jan  5 08:11:42.017: INFO: Deleting pod "simpletest.rc-55bkj" in namespace "gc-1380"
    Jan  5 08:11:42.034: INFO: Deleting pod "simpletest.rc-55ln5" in namespace "gc-1380"
    Jan  5 08:11:42.045: INFO: Deleting pod "simpletest.rc-5bpkt" in namespace "gc-1380"
    Jan  5 08:11:42.059: INFO: Deleting pod "simpletest.rc-5jrsx" in namespace "gc-1380"
    Jan  5 08:11:42.088: INFO: Deleting pod "simpletest.rc-5v6gz" in namespace "gc-1380"
    Jan  5 08:11:42.105: INFO: Deleting pod "simpletest.rc-5xz7r" in namespace "gc-1380"
    Jan  5 08:11:42.129: INFO: Deleting pod "simpletest.rc-6b6j4" in namespace "gc-1380"
    Jan  5 08:11:42.146: INFO: Deleting pod "simpletest.rc-6mpzk" in namespace "gc-1380"
    Jan  5 08:11:42.160: INFO: Deleting pod "simpletest.rc-6rjl7" in namespace "gc-1380"
    Jan  5 08:11:42.172: INFO: Deleting pod "simpletest.rc-6w7kj" in namespace "gc-1380"
    Jan  5 08:11:42.212: INFO: Deleting pod "simpletest.rc-75cnz" in namespace "gc-1380"
    Jan  5 08:11:42.224: INFO: Deleting pod "simpletest.rc-7mhlc" in namespace "gc-1380"
    Jan  5 08:11:42.242: INFO: Deleting pod "simpletest.rc-7tkp8" in namespace "gc-1380"
    Jan  5 08:11:42.258: INFO: Deleting pod "simpletest.rc-7vg49" in namespace "gc-1380"
    Jan  5 08:11:42.269: INFO: Deleting pod "simpletest.rc-8mld4" in namespace "gc-1380"
    Jan  5 08:11:42.288: INFO: Deleting pod "simpletest.rc-8msqq" in namespace "gc-1380"
    Jan  5 08:11:42.301: INFO: Deleting pod "simpletest.rc-8r2kg" in namespace "gc-1380"
    Jan  5 08:11:42.320: INFO: Deleting pod "simpletest.rc-8zknk" in namespace "gc-1380"
    Jan  5 08:11:42.333: INFO: Deleting pod "simpletest.rc-8zs5f" in namespace "gc-1380"
    Jan  5 08:11:42.354: INFO: Deleting pod "simpletest.rc-9nhm4" in namespace "gc-1380"
    Jan  5 08:11:42.366: INFO: Deleting pod "simpletest.rc-9nhzk" in namespace "gc-1380"
    Jan  5 08:11:42.376: INFO: Deleting pod "simpletest.rc-9t5v2" in namespace "gc-1380"
    Jan  5 08:11:42.393: INFO: Deleting pod "simpletest.rc-b86sj" in namespace "gc-1380"
    Jan  5 08:11:42.404: INFO: Deleting pod "simpletest.rc-b8n9d" in namespace "gc-1380"
    Jan  5 08:11:42.433: INFO: Deleting pod "simpletest.rc-bc9ts" in namespace "gc-1380"
    Jan  5 08:11:42.452: INFO: Deleting pod "simpletest.rc-bqxgq" in namespace "gc-1380"
    Jan  5 08:11:42.466: INFO: Deleting pod "simpletest.rc-brhpb" in namespace "gc-1380"
    Jan  5 08:11:42.480: INFO: Deleting pod "simpletest.rc-btdxr" in namespace "gc-1380"
    Jan  5 08:11:42.490: INFO: Deleting pod "simpletest.rc-c6hjg" in namespace "gc-1380"
    Jan  5 08:11:42.503: INFO: Deleting pod "simpletest.rc-c8ch2" in namespace "gc-1380"
    Jan  5 08:11:42.515: INFO: Deleting pod "simpletest.rc-csv9r" in namespace "gc-1380"
    Jan  5 08:11:42.524: INFO: Deleting pod "simpletest.rc-cwz2h" in namespace "gc-1380"
    Jan  5 08:11:42.550: INFO: Deleting pod "simpletest.rc-d2wrw" in namespace "gc-1380"
    Jan  5 08:11:42.565: INFO: Deleting pod "simpletest.rc-dbjgc" in namespace "gc-1380"
    Jan  5 08:11:42.576: INFO: Deleting pod "simpletest.rc-dd5r7" in namespace "gc-1380"
    Jan  5 08:11:42.590: INFO: Deleting pod "simpletest.rc-dp75x" in namespace "gc-1380"
    Jan  5 08:11:42.601: INFO: Deleting pod "simpletest.rc-dsd6h" in namespace "gc-1380"
    Jan  5 08:11:42.617: INFO: Deleting pod "simpletest.rc-fpjhc" in namespace "gc-1380"
    Jan  5 08:11:42.627: INFO: Deleting pod "simpletest.rc-g4nwt" in namespace "gc-1380"
    Jan  5 08:11:42.639: INFO: Deleting pod "simpletest.rc-g5499" in namespace "gc-1380"
    Jan  5 08:11:42.656: INFO: Deleting pod "simpletest.rc-g5b8h" in namespace "gc-1380"
    Jan  5 08:11:42.666: INFO: Deleting pod "simpletest.rc-gdtzg" in namespace "gc-1380"
    Jan  5 08:11:42.682: INFO: Deleting pod "simpletest.rc-gmkm7" in namespace "gc-1380"
    Jan  5 08:11:42.698: INFO: Deleting pod "simpletest.rc-gn26x" in namespace "gc-1380"
    Jan  5 08:11:42.709: INFO: Deleting pod "simpletest.rc-hr2ql" in namespace "gc-1380"
    Jan  5 08:11:42.725: INFO: Deleting pod "simpletest.rc-hsgq4" in namespace "gc-1380"
    Jan  5 08:11:42.736: INFO: Deleting pod "simpletest.rc-j7wzr" in namespace "gc-1380"
    Jan  5 08:11:42.751: INFO: Deleting pod "simpletest.rc-l6str" in namespace "gc-1380"
    Jan  5 08:11:42.771: INFO: Deleting pod "simpletest.rc-m8zl9" in namespace "gc-1380"
    Jan  5 08:11:42.781: INFO: Deleting pod "simpletest.rc-mtn9s" in namespace "gc-1380"
    Jan  5 08:11:42.791: INFO: Deleting pod "simpletest.rc-nfr2b" in namespace "gc-1380"
    Jan  5 08:11:42.800: INFO: Deleting pod "simpletest.rc-nkdkm" in namespace "gc-1380"
    Jan  5 08:11:42.809: INFO: Deleting pod "simpletest.rc-nmrt2" in namespace "gc-1380"
    Jan  5 08:11:42.829: INFO: Deleting pod "simpletest.rc-nqctv" in namespace "gc-1380"
    Jan  5 08:11:42.845: INFO: Deleting pod "simpletest.rc-nxdc5" in namespace "gc-1380"
    Jan  5 08:11:42.856: INFO: Deleting pod "simpletest.rc-nxmd6" in namespace "gc-1380"
    Jan  5 08:11:42.873: INFO: Deleting pod "simpletest.rc-pb2z6" in namespace "gc-1380"
    Jan  5 08:11:42.889: INFO: Deleting pod "simpletest.rc-pc5wf" in namespace "gc-1380"
    Jan  5 08:11:42.900: INFO: Deleting pod "simpletest.rc-pcdzr" in namespace "gc-1380"
    Jan  5 08:11:42.916: INFO: Deleting pod "simpletest.rc-pgc2c" in namespace "gc-1380"
    Jan  5 08:11:42.926: INFO: Deleting pod "simpletest.rc-ppl9k" in namespace "gc-1380"
    Jan  5 08:11:42.937: INFO: Deleting pod "simpletest.rc-ptwdg" in namespace "gc-1380"
    Jan  5 08:11:42.988: INFO: Deleting pod "simpletest.rc-pw7sg" in namespace "gc-1380"
    Jan  5 08:11:43.034: INFO: Deleting pod "simpletest.rc-q7s6v" in namespace "gc-1380"
    Jan  5 08:11:43.089: INFO: Deleting pod "simpletest.rc-qcbt4" in namespace "gc-1380"
    Jan  5 08:11:43.130: INFO: Deleting pod "simpletest.rc-qcgsp" in namespace "gc-1380"
    Jan  5 08:11:43.187: INFO: Deleting pod "simpletest.rc-qk676" in namespace "gc-1380"
    Jan  5 08:11:43.230: INFO: Deleting pod "simpletest.rc-qqrtz" in namespace "gc-1380"
    Jan  5 08:11:43.290: INFO: Deleting pod "simpletest.rc-r5rkg" in namespace "gc-1380"
    Jan  5 08:11:43.333: INFO: Deleting pod "simpletest.rc-r9gt6" in namespace "gc-1380"
    Jan  5 08:11:43.379: INFO: Deleting pod "simpletest.rc-s7hsc" in namespace "gc-1380"
    Jan  5 08:11:43.433: INFO: Deleting pod "simpletest.rc-sbzf6" in namespace "gc-1380"
    Jan  5 08:11:43.479: INFO: Deleting pod "simpletest.rc-sdxpb" in namespace "gc-1380"
    Jan  5 08:11:43.534: INFO: Deleting pod "simpletest.rc-sh7gf" in namespace "gc-1380"
    Jan  5 08:11:43.579: INFO: Deleting pod "simpletest.rc-sth75" in namespace "gc-1380"
    Jan  5 08:11:43.635: INFO: Deleting pod "simpletest.rc-tk6kp" in namespace "gc-1380"
    Jan  5 08:11:43.685: INFO: Deleting pod "simpletest.rc-vmf4j" in namespace "gc-1380"
    Jan  5 08:11:43.729: INFO: Deleting pod "simpletest.rc-vt8bx" in namespace "gc-1380"
    Jan  5 08:11:43.785: INFO: Deleting pod "simpletest.rc-w59k2" in namespace "gc-1380"
    Jan  5 08:11:43.830: INFO: Deleting pod "simpletest.rc-wbhcr" in namespace "gc-1380"
    Jan  5 08:11:43.880: INFO: Deleting pod "simpletest.rc-wq25f" in namespace "gc-1380"
    Jan  5 08:11:43.933: INFO: Deleting pod "simpletest.rc-x55gg" in namespace "gc-1380"
    Jan  5 08:11:43.980: INFO: Deleting pod "simpletest.rc-x8nt4" in namespace "gc-1380"
    Jan  5 08:11:44.035: INFO: Deleting pod "simpletest.rc-x97qt" in namespace "gc-1380"
    Jan  5 08:11:44.080: INFO: Deleting pod "simpletest.rc-xdphj" in namespace "gc-1380"
    Jan  5 08:11:44.135: INFO: Deleting pod "simpletest.rc-xgwqj" in namespace "gc-1380"
    Jan  5 08:11:44.181: INFO: Deleting pod "simpletest.rc-xhfqt" in namespace "gc-1380"
    Jan  5 08:11:44.234: INFO: Deleting pod "simpletest.rc-xqdb4" in namespace "gc-1380"
    Jan  5 08:11:44.306: INFO: Deleting pod "simpletest.rc-z26zf" in namespace "gc-1380"
    Jan  5 08:11:44.334: INFO: Deleting pod "simpletest.rc-z6rfg" in namespace "gc-1380"
    Jan  5 08:11:44.381: INFO: Deleting pod "simpletest.rc-zjzm2" in namespace "gc-1380"
    Jan  5 08:11:44.430: INFO: Deleting pod "simpletest.rc-ztbv4" in namespace "gc-1380"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 08:11:44.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1380" for this suite. 01/05/23 08:11:44.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:11:44.577
Jan  5 08:11:44.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 08:11:44.578
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:11:44.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:11:44.594
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 08:11:44.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4201" for this suite. 01/05/23 08:11:44.635
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":150,"skipped":3102,"failed":0}
------------------------------
â€¢ [0.061 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:11:44.577
    Jan  5 08:11:44.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 08:11:44.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:11:44.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:11:44.594
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 08:11:44.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4201" for this suite. 01/05/23 08:11:44.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:11:44.64
Jan  5 08:11:44.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename cronjob 01/05/23 08:11:44.641
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:11:44.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:11:44.669
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/05/23 08:11:44.673
STEP: Ensuring a job is scheduled 01/05/23 08:11:44.676
STEP: Ensuring exactly one is scheduled 01/05/23 08:12:00.68
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 08:12:00.683
STEP: Ensuring the job is replaced with a new one 01/05/23 08:12:00.685
STEP: Removing cronjob 01/05/23 08:13:00.691
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  5 08:13:00.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6443" for this suite. 01/05/23 08:13:00.725
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":151,"skipped":3130,"failed":0}
------------------------------
â€¢ [SLOW TEST] [76.098 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:11:44.64
    Jan  5 08:11:44.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename cronjob 01/05/23 08:11:44.641
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:11:44.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:11:44.669
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/05/23 08:11:44.673
    STEP: Ensuring a job is scheduled 01/05/23 08:11:44.676
    STEP: Ensuring exactly one is scheduled 01/05/23 08:12:00.68
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 08:12:00.683
    STEP: Ensuring the job is replaced with a new one 01/05/23 08:12:00.685
    STEP: Removing cronjob 01/05/23 08:13:00.691
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  5 08:13:00.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6443" for this suite. 01/05/23 08:13:00.725
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:13:00.738
Jan  5 08:13:00.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename watch 01/05/23 08:13:00.739
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:00.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:00.771
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/05/23 08:13:00.774
STEP: creating a watch on configmaps with label B 01/05/23 08:13:00.775
STEP: creating a watch on configmaps with label A or B 01/05/23 08:13:00.777
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/05/23 08:13:00.778
Jan  5 08:13:00.797: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20431 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 08:13:00.797: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20431 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/05/23 08:13:00.797
Jan  5 08:13:00.809: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20433 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 08:13:00.809: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20433 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/05/23 08:13:00.81
Jan  5 08:13:00.817: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20434 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 08:13:00.818: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20434 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/05/23 08:13:00.818
Jan  5 08:13:00.827: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20435 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 08:13:00.827: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20435 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/05/23 08:13:00.827
Jan  5 08:13:00.838: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7889  f099ad70-b6b4-4bb0-982b-c643a7b33eb4 20436 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 08:13:00.838: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7889  f099ad70-b6b4-4bb0-982b-c643a7b33eb4 20436 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/05/23 08:13:10.839
Jan  5 08:13:10.855: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7889  f099ad70-b6b4-4bb0-982b-c643a7b33eb4 20471 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 08:13:10.855: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7889  f099ad70-b6b4-4bb0-982b-c643a7b33eb4 20471 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  5 08:13:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7889" for this suite. 01/05/23 08:13:20.859
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":152,"skipped":3132,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.129 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:13:00.738
    Jan  5 08:13:00.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename watch 01/05/23 08:13:00.739
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:00.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:00.771
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/05/23 08:13:00.774
    STEP: creating a watch on configmaps with label B 01/05/23 08:13:00.775
    STEP: creating a watch on configmaps with label A or B 01/05/23 08:13:00.777
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/05/23 08:13:00.778
    Jan  5 08:13:00.797: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20431 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 08:13:00.797: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20431 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/05/23 08:13:00.797
    Jan  5 08:13:00.809: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20433 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 08:13:00.809: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20433 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/05/23 08:13:00.81
    Jan  5 08:13:00.817: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20434 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 08:13:00.818: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20434 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/05/23 08:13:00.818
    Jan  5 08:13:00.827: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20435 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 08:13:00.827: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7889  065965d7-eb66-4315-8ce9-af0d56f3e31c 20435 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/05/23 08:13:00.827
    Jan  5 08:13:00.838: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7889  f099ad70-b6b4-4bb0-982b-c643a7b33eb4 20436 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 08:13:00.838: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7889  f099ad70-b6b4-4bb0-982b-c643a7b33eb4 20436 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/05/23 08:13:10.839
    Jan  5 08:13:10.855: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7889  f099ad70-b6b4-4bb0-982b-c643a7b33eb4 20471 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 08:13:10.855: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7889  f099ad70-b6b4-4bb0-982b-c643a7b33eb4 20471 0 2023-01-05 08:13:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 08:13:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  5 08:13:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7889" for this suite. 01/05/23 08:13:20.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:13:20.87
Jan  5 08:13:20.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:13:20.871
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:20.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:20.89
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:13:20.905
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:13:21.506
STEP: Deploying the webhook pod 01/05/23 08:13:21.513
STEP: Wait for the deployment to be ready 01/05/23 08:13:21.526
Jan  5 08:13:21.537: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  5 08:13:23.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 13, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 13, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 13, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 13, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/05/23 08:13:25.548
STEP: Verifying the service has paired with the endpoint 01/05/23 08:13:25.561
Jan  5 08:13:26.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/05/23 08:13:26.564
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 08:13:26.564
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/05/23 08:13:26.578
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/05/23 08:13:27.585
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 08:13:27.585
STEP: Having no error when timeout is longer than webhook latency 01/05/23 08:13:28.83
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 08:13:28.831
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/05/23 08:13:33.863
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 08:13:33.863
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:13:38.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5289" for this suite. 01/05/23 08:13:38.903
STEP: Destroying namespace "webhook-5289-markers" for this suite. 01/05/23 08:13:38.911
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":153,"skipped":3153,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.089 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:13:20.87
    Jan  5 08:13:20.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:13:20.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:20.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:20.89
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:13:20.905
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:13:21.506
    STEP: Deploying the webhook pod 01/05/23 08:13:21.513
    STEP: Wait for the deployment to be ready 01/05/23 08:13:21.526
    Jan  5 08:13:21.537: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  5 08:13:23.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 13, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 13, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 13, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 13, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/05/23 08:13:25.548
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:13:25.561
    Jan  5 08:13:26.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/05/23 08:13:26.564
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 08:13:26.564
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/05/23 08:13:26.578
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/05/23 08:13:27.585
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 08:13:27.585
    STEP: Having no error when timeout is longer than webhook latency 01/05/23 08:13:28.83
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 08:13:28.831
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/05/23 08:13:33.863
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 08:13:33.863
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:13:38.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5289" for this suite. 01/05/23 08:13:38.903
    STEP: Destroying namespace "webhook-5289-markers" for this suite. 01/05/23 08:13:38.911
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:13:38.961
Jan  5 08:13:38.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename job 01/05/23 08:13:38.963
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:39.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:39.032
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/05/23 08:13:39.034
STEP: Ensuring active pods == parallelism 01/05/23 08:13:39.041
STEP: Orphaning one of the Job's Pods 01/05/23 08:13:43.044
Jan  5 08:13:43.556: INFO: Successfully updated pod "adopt-release-p9646"
STEP: Checking that the Job readopts the Pod 01/05/23 08:13:43.556
Jan  5 08:13:43.556: INFO: Waiting up to 15m0s for pod "adopt-release-p9646" in namespace "job-7639" to be "adopted"
Jan  5 08:13:43.557: INFO: Pod "adopt-release-p9646": Phase="Running", Reason="", readiness=true. Elapsed: 1.59112ms
Jan  5 08:13:45.561: INFO: Pod "adopt-release-p9646": Phase="Running", Reason="", readiness=true. Elapsed: 2.004904482s
Jan  5 08:13:45.561: INFO: Pod "adopt-release-p9646" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/05/23 08:13:45.561
Jan  5 08:13:46.070: INFO: Successfully updated pod "adopt-release-p9646"
STEP: Checking that the Job releases the Pod 01/05/23 08:13:46.07
Jan  5 08:13:46.070: INFO: Waiting up to 15m0s for pod "adopt-release-p9646" in namespace "job-7639" to be "released"
Jan  5 08:13:46.071: INFO: Pod "adopt-release-p9646": Phase="Running", Reason="", readiness=true. Elapsed: 1.359279ms
Jan  5 08:13:48.076: INFO: Pod "adopt-release-p9646": Phase="Running", Reason="", readiness=true. Elapsed: 2.005732252s
Jan  5 08:13:48.076: INFO: Pod "adopt-release-p9646" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 08:13:48.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7639" for this suite. 01/05/23 08:13:48.079
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":154,"skipped":3156,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.124 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:13:38.961
    Jan  5 08:13:38.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename job 01/05/23 08:13:38.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:39.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:39.032
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/05/23 08:13:39.034
    STEP: Ensuring active pods == parallelism 01/05/23 08:13:39.041
    STEP: Orphaning one of the Job's Pods 01/05/23 08:13:43.044
    Jan  5 08:13:43.556: INFO: Successfully updated pod "adopt-release-p9646"
    STEP: Checking that the Job readopts the Pod 01/05/23 08:13:43.556
    Jan  5 08:13:43.556: INFO: Waiting up to 15m0s for pod "adopt-release-p9646" in namespace "job-7639" to be "adopted"
    Jan  5 08:13:43.557: INFO: Pod "adopt-release-p9646": Phase="Running", Reason="", readiness=true. Elapsed: 1.59112ms
    Jan  5 08:13:45.561: INFO: Pod "adopt-release-p9646": Phase="Running", Reason="", readiness=true. Elapsed: 2.004904482s
    Jan  5 08:13:45.561: INFO: Pod "adopt-release-p9646" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/05/23 08:13:45.561
    Jan  5 08:13:46.070: INFO: Successfully updated pod "adopt-release-p9646"
    STEP: Checking that the Job releases the Pod 01/05/23 08:13:46.07
    Jan  5 08:13:46.070: INFO: Waiting up to 15m0s for pod "adopt-release-p9646" in namespace "job-7639" to be "released"
    Jan  5 08:13:46.071: INFO: Pod "adopt-release-p9646": Phase="Running", Reason="", readiness=true. Elapsed: 1.359279ms
    Jan  5 08:13:48.076: INFO: Pod "adopt-release-p9646": Phase="Running", Reason="", readiness=true. Elapsed: 2.005732252s
    Jan  5 08:13:48.076: INFO: Pod "adopt-release-p9646" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 08:13:48.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7639" for this suite. 01/05/23 08:13:48.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:13:48.085
Jan  5 08:13:48.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename events 01/05/23 08:13:48.086
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:48.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:48.105
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/05/23 08:13:48.108
Jan  5 08:13:48.120: INFO: created test-event-1
Jan  5 08:13:48.124: INFO: created test-event-2
Jan  5 08:13:48.138: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/05/23 08:13:48.138
STEP: delete collection of events 01/05/23 08:13:48.14
Jan  5 08:13:48.141: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/05/23 08:13:48.16
Jan  5 08:13:48.161: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan  5 08:13:48.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3202" for this suite. 01/05/23 08:13:48.165
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":155,"skipped":3177,"failed":0}
------------------------------
â€¢ [0.086 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:13:48.085
    Jan  5 08:13:48.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename events 01/05/23 08:13:48.086
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:48.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:48.105
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/05/23 08:13:48.108
    Jan  5 08:13:48.120: INFO: created test-event-1
    Jan  5 08:13:48.124: INFO: created test-event-2
    Jan  5 08:13:48.138: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/05/23 08:13:48.138
    STEP: delete collection of events 01/05/23 08:13:48.14
    Jan  5 08:13:48.141: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/05/23 08:13:48.16
    Jan  5 08:13:48.161: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan  5 08:13:48.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3202" for this suite. 01/05/23 08:13:48.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:13:48.173
Jan  5 08:13:48.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:13:48.174
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:48.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:48.189
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-61aecb11-a99a-438d-830d-edc8db84fc3b 01/05/23 08:13:48.193
STEP: Creating configMap with name cm-test-opt-upd-e34c0c43-a926-40f7-9242-abad44dce265 01/05/23 08:13:48.202
STEP: Creating the pod 01/05/23 08:13:48.206
Jan  5 08:13:48.217: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915" in namespace "projected-5771" to be "running and ready"
Jan  5 08:13:48.219: INFO: Pod "pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.657984ms
Jan  5 08:13:48.220: INFO: The phase of Pod pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:13:50.222: INFO: Pod "pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005473228s
Jan  5 08:13:50.222: INFO: The phase of Pod pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:13:52.224: INFO: Pod "pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915": Phase="Running", Reason="", readiness=true. Elapsed: 4.006787824s
Jan  5 08:13:52.224: INFO: The phase of Pod pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915 is Running (Ready = true)
Jan  5 08:13:52.224: INFO: Pod "pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-61aecb11-a99a-438d-830d-edc8db84fc3b 01/05/23 08:13:52.245
STEP: Updating configmap cm-test-opt-upd-e34c0c43-a926-40f7-9242-abad44dce265 01/05/23 08:13:52.249
STEP: Creating configMap with name cm-test-opt-create-98008507-dcfb-440c-a799-b13dc45a2f57 01/05/23 08:13:52.271
STEP: waiting to observe update in volume 01/05/23 08:13:52.276
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 08:13:54.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5771" for this suite. 01/05/23 08:13:54.299
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":156,"skipped":3184,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.135 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:13:48.173
    Jan  5 08:13:48.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:13:48.174
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:48.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:48.189
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-61aecb11-a99a-438d-830d-edc8db84fc3b 01/05/23 08:13:48.193
    STEP: Creating configMap with name cm-test-opt-upd-e34c0c43-a926-40f7-9242-abad44dce265 01/05/23 08:13:48.202
    STEP: Creating the pod 01/05/23 08:13:48.206
    Jan  5 08:13:48.217: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915" in namespace "projected-5771" to be "running and ready"
    Jan  5 08:13:48.219: INFO: Pod "pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.657984ms
    Jan  5 08:13:48.220: INFO: The phase of Pod pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:13:50.222: INFO: Pod "pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005473228s
    Jan  5 08:13:50.222: INFO: The phase of Pod pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:13:52.224: INFO: Pod "pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915": Phase="Running", Reason="", readiness=true. Elapsed: 4.006787824s
    Jan  5 08:13:52.224: INFO: The phase of Pod pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915 is Running (Ready = true)
    Jan  5 08:13:52.224: INFO: Pod "pod-projected-configmaps-d58b35ba-a096-49ef-a0c9-d6b90d095915" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-61aecb11-a99a-438d-830d-edc8db84fc3b 01/05/23 08:13:52.245
    STEP: Updating configmap cm-test-opt-upd-e34c0c43-a926-40f7-9242-abad44dce265 01/05/23 08:13:52.249
    STEP: Creating configMap with name cm-test-opt-create-98008507-dcfb-440c-a799-b13dc45a2f57 01/05/23 08:13:52.271
    STEP: waiting to observe update in volume 01/05/23 08:13:52.276
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 08:13:54.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5771" for this suite. 01/05/23 08:13:54.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:13:54.309
Jan  5 08:13:54.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename replication-controller 01/05/23 08:13:54.31
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:54.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:54.325
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/05/23 08:13:54.327
STEP: When the matched label of one of its pods change 01/05/23 08:13:54.338
Jan  5 08:13:54.340: INFO: Pod name pod-release: Found 0 pods out of 1
Jan  5 08:13:59.343: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/05/23 08:13:59.356
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  5 08:14:00.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8318" for this suite. 01/05/23 08:14:00.364
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":157,"skipped":3193,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.059 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:13:54.309
    Jan  5 08:13:54.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename replication-controller 01/05/23 08:13:54.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:13:54.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:13:54.325
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/05/23 08:13:54.327
    STEP: When the matched label of one of its pods change 01/05/23 08:13:54.338
    Jan  5 08:13:54.340: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan  5 08:13:59.343: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/05/23 08:13:59.356
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  5 08:14:00.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8318" for this suite. 01/05/23 08:14:00.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:14:00.369
Jan  5 08:14:00.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename security-context 01/05/23 08:14:00.37
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:00.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:00.386
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 08:14:00.39
Jan  5 08:14:00.396: INFO: Waiting up to 5m0s for pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a" in namespace "security-context-8434" to be "Succeeded or Failed"
Jan  5 08:14:00.399: INFO: Pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.348046ms
Jan  5 08:14:02.401: INFO: Pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00480454s
Jan  5 08:14:04.403: INFO: Pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a": Phase="Running", Reason="", readiness=false. Elapsed: 4.006382515s
Jan  5 08:14:06.403: INFO: Pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006498963s
STEP: Saw pod success 01/05/23 08:14:06.403
Jan  5 08:14:06.403: INFO: Pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a" satisfied condition "Succeeded or Failed"
Jan  5 08:14:06.405: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a container test-container: <nil>
STEP: delete the pod 01/05/23 08:14:06.408
Jan  5 08:14:06.418: INFO: Waiting for pod security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a to disappear
Jan  5 08:14:06.419: INFO: Pod security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 08:14:06.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8434" for this suite. 01/05/23 08:14:06.421
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":158,"skipped":3202,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.056 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:14:00.369
    Jan  5 08:14:00.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename security-context 01/05/23 08:14:00.37
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:00.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:00.386
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 08:14:00.39
    Jan  5 08:14:00.396: INFO: Waiting up to 5m0s for pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a" in namespace "security-context-8434" to be "Succeeded or Failed"
    Jan  5 08:14:00.399: INFO: Pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.348046ms
    Jan  5 08:14:02.401: INFO: Pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00480454s
    Jan  5 08:14:04.403: INFO: Pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a": Phase="Running", Reason="", readiness=false. Elapsed: 4.006382515s
    Jan  5 08:14:06.403: INFO: Pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006498963s
    STEP: Saw pod success 01/05/23 08:14:06.403
    Jan  5 08:14:06.403: INFO: Pod "security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a" satisfied condition "Succeeded or Failed"
    Jan  5 08:14:06.405: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a container test-container: <nil>
    STEP: delete the pod 01/05/23 08:14:06.408
    Jan  5 08:14:06.418: INFO: Waiting for pod security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a to disappear
    Jan  5 08:14:06.419: INFO: Pod security-context-e1b707d3-2b65-46a9-aec2-6f0eef3c0d2a no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 08:14:06.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-8434" for this suite. 01/05/23 08:14:06.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:14:06.425
Jan  5 08:14:06.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:14:06.426
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:06.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:06.49
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-dc3cf086-f28e-451d-a480-ef68da2f8b95 01/05/23 08:14:06.493
STEP: Creating a pod to test consume configMaps 01/05/23 08:14:06.497
Jan  5 08:14:06.509: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924" in namespace "projected-8281" to be "Succeeded or Failed"
Jan  5 08:14:06.511: INFO: Pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924": Phase="Pending", Reason="", readiness=false. Elapsed: 1.916647ms
Jan  5 08:14:08.514: INFO: Pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005506084s
Jan  5 08:14:10.515: INFO: Pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005786041s
Jan  5 08:14:12.515: INFO: Pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006424129s
STEP: Saw pod success 01/05/23 08:14:12.515
Jan  5 08:14:12.516: INFO: Pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924" satisfied condition "Succeeded or Failed"
Jan  5 08:14:12.518: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:14:12.522
Jan  5 08:14:12.535: INFO: Waiting for pod pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924 to disappear
Jan  5 08:14:12.536: INFO: Pod pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 08:14:12.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8281" for this suite. 01/05/23 08:14:12.538
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":159,"skipped":3208,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.122 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:14:06.425
    Jan  5 08:14:06.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:14:06.426
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:06.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:06.49
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-dc3cf086-f28e-451d-a480-ef68da2f8b95 01/05/23 08:14:06.493
    STEP: Creating a pod to test consume configMaps 01/05/23 08:14:06.497
    Jan  5 08:14:06.509: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924" in namespace "projected-8281" to be "Succeeded or Failed"
    Jan  5 08:14:06.511: INFO: Pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924": Phase="Pending", Reason="", readiness=false. Elapsed: 1.916647ms
    Jan  5 08:14:08.514: INFO: Pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005506084s
    Jan  5 08:14:10.515: INFO: Pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005786041s
    Jan  5 08:14:12.515: INFO: Pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006424129s
    STEP: Saw pod success 01/05/23 08:14:12.515
    Jan  5 08:14:12.516: INFO: Pod "pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924" satisfied condition "Succeeded or Failed"
    Jan  5 08:14:12.518: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:14:12.522
    Jan  5 08:14:12.535: INFO: Waiting for pod pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924 to disappear
    Jan  5 08:14:12.536: INFO: Pod pod-projected-configmaps-9ce88034-67ca-498a-a40b-5e4ca884d924 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 08:14:12.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8281" for this suite. 01/05/23 08:14:12.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:14:12.548
Jan  5 08:14:12.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 08:14:12.549
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:12.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:12.566
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/05/23 08:14:12.573
Jan  5 08:14:12.573: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-1776 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/05/23 08:14:12.617
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 08:14:12.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1776" for this suite. 01/05/23 08:14:12.632
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":160,"skipped":3244,"failed":0}
------------------------------
â€¢ [0.089 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:14:12.548
    Jan  5 08:14:12.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 08:14:12.549
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:12.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:12.566
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/05/23 08:14:12.573
    Jan  5 08:14:12.573: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-1776 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/05/23 08:14:12.617
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 08:14:12.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1776" for this suite. 01/05/23 08:14:12.632
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:14:12.637
Jan  5 08:14:12.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename deployment 01/05/23 08:14:12.638
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:12.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:12.655
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan  5 08:14:12.659: INFO: Creating deployment "webserver-deployment"
Jan  5 08:14:12.662: INFO: Waiting for observed generation 1
Jan  5 08:14:14.667: INFO: Waiting for all required pods to come up
Jan  5 08:14:14.670: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/05/23 08:14:14.67
Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-z7kpx" in namespace "deployment-7313" to be "running"
Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5mbnh" in namespace "deployment-7313" to be "running"
Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qn94r" in namespace "deployment-7313" to be "running"
Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rzkx4" in namespace "deployment-7313" to be "running"
Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-tcc6n" in namespace "deployment-7313" to be "running"
Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-9tsl2" in namespace "deployment-7313" to be "running"
Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xcfsd" in namespace "deployment-7313" to be "running"
Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-c5fvb" in namespace "deployment-7313" to be "running"
Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-9q9dz" in namespace "deployment-7313" to be "running"
Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-nmxfm" in namespace "deployment-7313" to be "running"
Jan  5 08:14:14.672: INFO: Pod "webserver-deployment-845c8977d9-z7kpx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.917492ms
Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-9q9dz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790527ms
Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-qn94r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.997656ms
Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-nmxfm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790528ms
Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-tcc6n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.985226ms
Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-5mbnh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16001ms
Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-9tsl2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923703ms
Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-c5fvb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.913531ms
Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-xcfsd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.961046ms
Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-rzkx4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.084508ms
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-9tsl2": Phase="Running", Reason="", readiness=true. Elapsed: 2.01022656s
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-9tsl2" satisfied condition "running"
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-rzkx4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010300421s
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-nmxfm": Phase="Running", Reason="", readiness=true. Elapsed: 2.010141355s
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-nmxfm" satisfied condition "running"
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-z7kpx": Phase="Running", Reason="", readiness=true. Elapsed: 2.010502853s
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-z7kpx" satisfied condition "running"
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-qn94r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010405243s
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-tcc6n": Phase="Running", Reason="", readiness=true. Elapsed: 2.010335016s
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-tcc6n" satisfied condition "running"
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-xcfsd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010310699s
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-xcfsd" satisfied condition "running"
Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-9q9dz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010319903s
Jan  5 08:14:16.681: INFO: Pod "webserver-deployment-845c8977d9-5mbnh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010927453s
Jan  5 08:14:16.681: INFO: Pod "webserver-deployment-845c8977d9-c5fvb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010706209s
Jan  5 08:14:16.681: INFO: Pod "webserver-deployment-845c8977d9-c5fvb" satisfied condition "running"
Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-rzkx4": Phase="Running", Reason="", readiness=true. Elapsed: 4.006612874s
Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-rzkx4" satisfied condition "running"
Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-9q9dz": Phase="Running", Reason="", readiness=true. Elapsed: 4.006458621s
Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-9q9dz" satisfied condition "running"
Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-5mbnh": Phase="Running", Reason="", readiness=true. Elapsed: 4.007372836s
Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-5mbnh" satisfied condition "running"
Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-qn94r": Phase="Running", Reason="", readiness=true. Elapsed: 4.007362066s
Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-qn94r" satisfied condition "running"
Jan  5 08:14:18.677: INFO: Waiting for deployment "webserver-deployment" to complete
Jan  5 08:14:18.681: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan  5 08:14:18.689: INFO: Updating deployment webserver-deployment
Jan  5 08:14:18.689: INFO: Waiting for observed generation 2
Jan  5 08:14:20.694: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan  5 08:14:20.706: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan  5 08:14:20.708: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  5 08:14:20.713: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan  5 08:14:20.713: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan  5 08:14:20.719: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  5 08:14:20.723: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan  5 08:14:20.723: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan  5 08:14:20.734: INFO: Updating deployment webserver-deployment
Jan  5 08:14:20.734: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan  5 08:14:20.737: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan  5 08:14:20.738: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 08:14:22.744: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7313  4e46197d-f25c-487e-a53e-48c91c7baeae 21232 3 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050e7a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 08:14:20 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-05 08:14:20 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan  5 08:14:22.748: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-7313  40a3f70c-2156-4dbe-b014-15662b7cffe5 21223 3 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4e46197d-f25c-487e-a53e-48c91c7baeae 0xc0050e7e47 0xc0050e7e48}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e46197d-f25c-487e-a53e-48c91c7baeae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050e7f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 08:14:22.748: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan  5 08:14:22.748: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-7313  7126e99e-d98f-41c3-b967-d907b38e4baa 21225 3 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4e46197d-f25c-487e-a53e-48c91c7baeae 0xc0050e7f77 0xc0050e7f78}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e46197d-f25c-487e-a53e-48c91c7baeae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038cc008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-2mpdk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-2mpdk webserver-deployment-69b7448995- deployment-7313  ba5bbcc4-84db-48a1-a892-b94cc5840fcb 21212 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cc4c7 0xc0038cc4c8}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t4d9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t4d9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-5498d" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5498d webserver-deployment-69b7448995- deployment-7313  7067879e-a17c-474a-a1ea-5bca35cb1b3d 21178 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cc620 0xc0038cc621}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfzh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfzh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-55n8t" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-55n8t webserver-deployment-69b7448995- deployment-7313  c15b3843-b4f7-4032-acee-42e952e5a75a 21127 0 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:35464f13bf3b2497921462455f2149e13961d530550318265f9a1ecfea59a1fb cni.projectcalico.org/podIP:10.244.1.36/32 cni.projectcalico.org/podIPs:10.244.1.36/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cc7a0 0xc0038cc7a1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-05 08:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2jrd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2jrd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-8pw5x" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-8pw5x webserver-deployment-69b7448995- deployment-7313  4c9548d0-a5a4-4079-b7a5-888203e89d18 21193 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cc990 0xc0038cc991}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xr7nh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xr7nh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-bcjxh" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-bcjxh webserver-deployment-69b7448995- deployment-7313  0c254a62-3422-46d6-bce5-c602e832089d 21270 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e48b2a67a18d140f4b112cf3e1f266a5c22ed2ef9285eb5ea1ad8156e303fdb3 cni.projectcalico.org/podIP:10.244.1.40/32 cni.projectcalico.org/podIPs:10.244.1.40/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038ccb10 0xc0038ccb11}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mg9fq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mg9fq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-cszpx" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-cszpx webserver-deployment-69b7448995- deployment-7313  690095bb-1a7f-4418-9827-174b3fdf3f1f 21140 0 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:56fcaf13a3608ab43b92bd5081fe5e7450837237918c520e423aa7c602fb10de cni.projectcalico.org/podIP:10.244.1.34/32 cni.projectcalico.org/podIPs:10.244.1.34/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cccb0 0xc0038cccb1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wpn7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wpn7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.34,StartTime:2023-01-05 08:14:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-fk4ng" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-fk4ng webserver-deployment-69b7448995- deployment-7313  ebc13410-1f80-485b-baf6-7f4ccbd1ffb6 21200 0 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e1e26e440079d7d3d083888d75762668cc080b11beaaf6fb9dc148d13df7ff80 cni.projectcalico.org/podIP:10.244.0.153/32 cni.projectcalico.org/podIPs:10.244.0.153/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038ccf00 0xc0038ccf01}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f599l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f599l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.153,StartTime:2023-01-05 08:14:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-hcpkb" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hcpkb webserver-deployment-69b7448995- deployment-7313  631edb77-d441-48ff-ae41-3b97eb356d04 21190 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cd120 0xc0038cd121}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvfdb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvfdb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-p7cbt" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-p7cbt webserver-deployment-69b7448995- deployment-7313  d8d32d34-3d27-4a6e-adab-5bd6e8093a50 21228 0 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0e23803da26e6ad91ef37edaa28171fbb1d931146516761244d99727e68df0f7 cni.projectcalico.org/podIP:10.244.0.152/32 cni.projectcalico.org/podIPs:10.244.0.152/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cd2a0 0xc0038cd2a1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hcbl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hcbl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.152,StartTime:2023-01-05 08:14:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-tjvq7" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tjvq7 webserver-deployment-69b7448995- deployment-7313  ae281ad4-5fe6-4b40-b626-2a0e5d9541c8 21271 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:96bbc2c01c585b9db4c70f86bd05661747cc794eb879bf2b5e56953e4dd88533 cni.projectcalico.org/podIP:10.244.0.157/32 cni.projectcalico.org/podIPs:10.244.0.157/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cd4e0 0xc0038cd4e1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tlmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tlmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-wrjwd" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-wrjwd webserver-deployment-69b7448995- deployment-7313  84454cd8-a278-45b9-b337-274017203915 21231 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cd660 0xc0038cd661}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slf9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slf9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-xbqv2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-xbqv2 webserver-deployment-69b7448995- deployment-7313  71a477cd-f12d-4b01-baae-33371db0cad3 21124 0 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4f687579c4d6b64d3c6a66215923bada1463c1f0b17b0b2e2f0feb6153d7f0f9 cni.projectcalico.org/podIP:10.244.1.35/32 cni.projectcalico.org/podIPs:10.244.1.35/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cd850 0xc0038cd851}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-05 08:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7b2k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7b2k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-zhx8n" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zhx8n webserver-deployment-69b7448995- deployment-7313  61b9d5a1-8c53-4e77-84b9-8b368d72b2e3 21272 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:99fc5260ff32fe9e80867582091940355b7a9c744baff77ef930c802b08a4724 cni.projectcalico.org/podIP:10.244.0.156/32 cni.projectcalico.org/podIPs:10.244.0.156/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cda60 0xc0038cda61}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xjbww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xjbww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-845c8977d9-7ht92" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7ht92 webserver-deployment-845c8977d9- deployment-7313  aee91777-4d6d-41f2-b9ee-69eefac4dfa7 21176 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc0038cdc50 0xc0038cdc51}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vb7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vb7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-845c8977d9-8c6nw" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8c6nw webserver-deployment-845c8977d9- deployment-7313  d0864d2f-d496-4611-a222-f11be975242c 21255 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f9b3d42bbd7c96154a23ee3574ca6172e65dfffb62bd410478ddbad0c69fce4f cni.projectcalico.org/podIP:10.244.0.154/32 cni.projectcalico.org/podIPs:10.244.0.154/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc0038cde37 0xc0038cde38}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qpwh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qpwh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-845c8977d9-8nvj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8nvj5 webserver-deployment-845c8977d9- deployment-7313  fc244367-9457-473a-b212-56c2e1c4d049 21224 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc0007780e7 0xc0007780e8}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fw4f9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fw4f9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-9tsl2" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9tsl2 webserver-deployment-845c8977d9- deployment-7313  38f2ca55-54b5-4596-a708-d4f4cf602532 21022 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8fc3ae3836c2d5dd410dedb944a8c474bef276b2f6cf23fa9a8d960a858c161d cni.projectcalico.org/podIP:10.244.0.149/32 cni.projectcalico.org/podIPs:10.244.0.149/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000778657 0xc000778658}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-664kk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-664kk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.149,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://dc47985b79e82d383f8c74121eeda5b2c1097a516bcbf237184e432cae8b2ad1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-bmt6x" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bmt6x webserver-deployment-845c8977d9- deployment-7313  f3af4e8c-3341-442c-91ba-e0e8e565f0e6 21203 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000778a57 0xc000778a58}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sh29p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sh29p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-c5fvb" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-c5fvb webserver-deployment-845c8977d9- deployment-7313  ff7f9826-85b9-4851-9f50-c51e5b09260b 21014 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:766697ab65ac0f9b6024461cb351e1917ede13a76be64549c6e0e6a4e9b3589a cni.projectcalico.org/podIP:10.244.1.31/32 cni.projectcalico.org/podIPs:10.244.1.31/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000778cd0 0xc000778cd1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rk89x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rk89x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.31,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://eb0adcd7265ba12134b5e530819ccd1d48ecff3d2ca81c8d5725511df1d36b8d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-gmb4v" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gmb4v webserver-deployment-845c8977d9- deployment-7313  fd764654-d8ae-4ad6-a83f-a12c3f5bcc74 21265 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8fc353da641585e25eb10b6921b97d2743e6779819ce260e638e046e42e1153f cni.projectcalico.org/podIP:10.244.1.38/32 cni.projectcalico.org/podIPs:10.244.1.38/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000778fe0 0xc000778fe1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5n8d5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5n8d5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-j7r69" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-j7r69 webserver-deployment-845c8977d9- deployment-7313  dda4e439-3a71-40d4-bd1d-bf6ad2c01b4d 21286 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0fa77c8d3c855397a8873c191d1cbf6b8af66e23e0d0d226eada1dd98ee04f8a cni.projectcalico.org/podIP:10.244.1.41/32 cni.projectcalico.org/podIPs:10.244.1.41/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc0007791d0 0xc0007791d1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2f9qn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2f9qn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-n6w9v" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n6w9v webserver-deployment-845c8977d9- deployment-7313  44a03f3b-a3ff-4a84-aeaa-3f6ca88d7b0a 21245 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc0007793d0 0xc0007793d1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wwgvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wwgvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-nmxfm" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nmxfm webserver-deployment-845c8977d9- deployment-7313  f7c6430d-0b51-4e47-8a6a-b7eac69ea0ff 20995 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a72448376b0bd960894f88befeb9e18de4b37ea9c536f7ea2ef44acacb1275d8 cni.projectcalico.org/podIP:10.244.1.29/32 cni.projectcalico.org/podIPs:10.244.1.29/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000779c47 0xc000779c48}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cpcsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cpcsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.29,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://efe575f5a0cdd234eecc1c91c384721637ea65679db849f5b477fadcbdc15acb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-qbfng" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qbfng webserver-deployment-845c8977d9- deployment-7313  aa313174-7cf1-4011-84ad-10c0acf62efe 21269 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c7754b2356145518afa0965b3185512e1053cd22b81aabaaec25b1887d6daf7f cni.projectcalico.org/podIP:10.244.1.39/32 cni.projectcalico.org/podIPs:10.244.1.39/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000d06010 0xc000d06011}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkgrm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkgrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-qn94r" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qn94r webserver-deployment-845c8977d9- deployment-7313  6c5834f5-9be9-4260-8aa5-f629a65f7f02 21047 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:12766bfb02e03cfadfb64b8da8d99b7e1a2a7ad0223155a3172e39321522bd4c cni.projectcalico.org/podIP:10.244.0.151/32 cni.projectcalico.org/podIPs:10.244.0.151/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000d06917 0xc000d06918}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h68r5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h68r5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.151,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3d2a6f62462e0beb350d193d024c27a6aa30057100ff09a7a60168b2f7537074,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-rzkx4" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rzkx4 webserver-deployment-845c8977d9- deployment-7313  0ccd353d-16df-47d9-8ff2-9660e1971bfb 21045 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1a5f27fcca44b72394cfc05a961036e8691ac4763a692a7e8a00d3ae1cdce9be cni.projectcalico.org/podIP:10.244.0.150/32 cni.projectcalico.org/podIPs:10.244.0.150/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000d073b7 0xc000d073b8}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8hzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8hzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.150,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0be06ba0eacecb68585151c1424f5b1afde1c92606e855eeec41b5930ee065f4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-sq5xv" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-sq5xv webserver-deployment-845c8977d9- deployment-7313  bd5040ad-6f8e-4b6f-8c89-1bf8f3da57c5 21285 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d93eaa6e148bcd13da2809c4e092df7ab7766b63af474dc923fd9fc1b7855335 cni.projectcalico.org/podIP:10.244.0.158/32 cni.projectcalico.org/podIPs:10.244.0.158/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000d07a17 0xc000d07a18}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c7lvs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c7lvs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-tcc6n" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-tcc6n webserver-deployment-845c8977d9- deployment-7313  b32ffa6b-3931-4561-b773-c0c4c5ded188 21020 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cd8b1b5d41181c1736469123dd113930181ba8a9ce02edb32a48f0732336c7e4 cni.projectcalico.org/podIP:10.244.0.147/32 cni.projectcalico.org/podIPs:10.244.0.147/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351c157 0xc00351c158}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8jsk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8jsk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.147,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://15e34979661f2307e9ab49001c17faf2c066de72511bdae563746463a81e5de9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-tsdrq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-tsdrq webserver-deployment-845c8977d9- deployment-7313  5a71a5ff-ab0b-436c-8920-0da60617f78c 21264 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:20fc4394462f3f0eb897304e6ae763e3260d48b4c22fc5961caa382da95c3035 cni.projectcalico.org/podIP:10.244.0.155/32 cni.projectcalico.org/podIPs:10.244.0.155/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351c447 0xc00351c448}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqxq2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqxq2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-vmh62" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vmh62 webserver-deployment-845c8977d9- deployment-7313  3bf9812e-6744-4088-8a2b-28b22f7b50b3 21204 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351c5c0 0xc00351c5c1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nts9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nts9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-wbsvx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wbsvx webserver-deployment-845c8977d9- deployment-7313  6ef9ffd5-6550-44ea-9739-0358318902d5 21254 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:66d59c36f69f645f4ff91d903847ca7102466c52af8b3a052bf75049674ffbbe cni.projectcalico.org/podIP:10.244.1.37/32 cni.projectcalico.org/podIPs:10.244.1.37/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351c730 0xc00351c731}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rs62n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rs62n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-xcfsd" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xcfsd webserver-deployment-845c8977d9- deployment-7313  70192a7b-5d4c-4a93-a24a-550eb1cd338a 21016 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c119b0b6e790d713e3f0e46c79f6373add1fd654c492de3526f66b8a05c1cdff cni.projectcalico.org/podIP:10.244.1.30/32 cni.projectcalico.org/podIPs:10.244.1.30/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351c8c0 0xc00351c8c1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p29ng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p29ng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.30,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://850552b3f01aeac2f8c79442f5f84c7c4828ddf1e94a977b252af501d125c2ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-z7kpx" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-z7kpx webserver-deployment-845c8977d9- deployment-7313  77d9f33a-ebc7-4ab7-9907-89c0a4c7cd44 20998 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3efd4f5dfcf485528dd1a9924f97179b56da36f1a31b2569804755590b66b88e cni.projectcalico.org/podIP:10.244.0.148/32 cni.projectcalico.org/podIPs:10.244.0.148/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351cad0 0xc00351cad1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z56nn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z56nn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.148,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9cc112380b52ace23c0a978ae1cbfddfcc9bc75647f8059e0de9cbf70b0f4d61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 08:14:22.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7313" for this suite. 01/05/23 08:14:22.762
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":161,"skipped":3247,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.144 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:14:12.637
    Jan  5 08:14:12.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename deployment 01/05/23 08:14:12.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:12.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:12.655
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan  5 08:14:12.659: INFO: Creating deployment "webserver-deployment"
    Jan  5 08:14:12.662: INFO: Waiting for observed generation 1
    Jan  5 08:14:14.667: INFO: Waiting for all required pods to come up
    Jan  5 08:14:14.670: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/05/23 08:14:14.67
    Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-z7kpx" in namespace "deployment-7313" to be "running"
    Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5mbnh" in namespace "deployment-7313" to be "running"
    Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qn94r" in namespace "deployment-7313" to be "running"
    Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rzkx4" in namespace "deployment-7313" to be "running"
    Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-tcc6n" in namespace "deployment-7313" to be "running"
    Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-9tsl2" in namespace "deployment-7313" to be "running"
    Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xcfsd" in namespace "deployment-7313" to be "running"
    Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-c5fvb" in namespace "deployment-7313" to be "running"
    Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-9q9dz" in namespace "deployment-7313" to be "running"
    Jan  5 08:14:14.670: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-nmxfm" in namespace "deployment-7313" to be "running"
    Jan  5 08:14:14.672: INFO: Pod "webserver-deployment-845c8977d9-z7kpx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.917492ms
    Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-9q9dz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790527ms
    Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-qn94r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.997656ms
    Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-nmxfm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790528ms
    Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-tcc6n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.985226ms
    Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-5mbnh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16001ms
    Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-9tsl2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923703ms
    Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-c5fvb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.913531ms
    Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-xcfsd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.961046ms
    Jan  5 08:14:14.673: INFO: Pod "webserver-deployment-845c8977d9-rzkx4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.084508ms
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-9tsl2": Phase="Running", Reason="", readiness=true. Elapsed: 2.01022656s
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-9tsl2" satisfied condition "running"
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-rzkx4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010300421s
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-nmxfm": Phase="Running", Reason="", readiness=true. Elapsed: 2.010141355s
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-nmxfm" satisfied condition "running"
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-z7kpx": Phase="Running", Reason="", readiness=true. Elapsed: 2.010502853s
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-z7kpx" satisfied condition "running"
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-qn94r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010405243s
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-tcc6n": Phase="Running", Reason="", readiness=true. Elapsed: 2.010335016s
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-tcc6n" satisfied condition "running"
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-xcfsd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010310699s
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-xcfsd" satisfied condition "running"
    Jan  5 08:14:16.680: INFO: Pod "webserver-deployment-845c8977d9-9q9dz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010319903s
    Jan  5 08:14:16.681: INFO: Pod "webserver-deployment-845c8977d9-5mbnh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010927453s
    Jan  5 08:14:16.681: INFO: Pod "webserver-deployment-845c8977d9-c5fvb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010706209s
    Jan  5 08:14:16.681: INFO: Pod "webserver-deployment-845c8977d9-c5fvb" satisfied condition "running"
    Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-rzkx4": Phase="Running", Reason="", readiness=true. Elapsed: 4.006612874s
    Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-rzkx4" satisfied condition "running"
    Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-9q9dz": Phase="Running", Reason="", readiness=true. Elapsed: 4.006458621s
    Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-9q9dz" satisfied condition "running"
    Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-5mbnh": Phase="Running", Reason="", readiness=true. Elapsed: 4.007372836s
    Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-5mbnh" satisfied condition "running"
    Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-qn94r": Phase="Running", Reason="", readiness=true. Elapsed: 4.007362066s
    Jan  5 08:14:18.677: INFO: Pod "webserver-deployment-845c8977d9-qn94r" satisfied condition "running"
    Jan  5 08:14:18.677: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan  5 08:14:18.681: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan  5 08:14:18.689: INFO: Updating deployment webserver-deployment
    Jan  5 08:14:18.689: INFO: Waiting for observed generation 2
    Jan  5 08:14:20.694: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan  5 08:14:20.706: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan  5 08:14:20.708: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  5 08:14:20.713: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan  5 08:14:20.713: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan  5 08:14:20.719: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  5 08:14:20.723: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan  5 08:14:20.723: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan  5 08:14:20.734: INFO: Updating deployment webserver-deployment
    Jan  5 08:14:20.734: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan  5 08:14:20.737: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan  5 08:14:20.738: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 08:14:22.744: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-7313  4e46197d-f25c-487e-a53e-48c91c7baeae 21232 3 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050e7a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 08:14:20 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-05 08:14:20 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan  5 08:14:22.748: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-7313  40a3f70c-2156-4dbe-b014-15662b7cffe5 21223 3 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4e46197d-f25c-487e-a53e-48c91c7baeae 0xc0050e7e47 0xc0050e7e48}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e46197d-f25c-487e-a53e-48c91c7baeae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050e7f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 08:14:22.748: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan  5 08:14:22.748: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-7313  7126e99e-d98f-41c3-b967-d907b38e4baa 21225 3 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4e46197d-f25c-487e-a53e-48c91c7baeae 0xc0050e7f77 0xc0050e7f78}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e46197d-f25c-487e-a53e-48c91c7baeae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038cc008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-2mpdk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-2mpdk webserver-deployment-69b7448995- deployment-7313  ba5bbcc4-84db-48a1-a892-b94cc5840fcb 21212 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cc4c7 0xc0038cc4c8}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t4d9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t4d9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-5498d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5498d webserver-deployment-69b7448995- deployment-7313  7067879e-a17c-474a-a1ea-5bca35cb1b3d 21178 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cc620 0xc0038cc621}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfzh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfzh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-55n8t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-55n8t webserver-deployment-69b7448995- deployment-7313  c15b3843-b4f7-4032-acee-42e952e5a75a 21127 0 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:35464f13bf3b2497921462455f2149e13961d530550318265f9a1ecfea59a1fb cni.projectcalico.org/podIP:10.244.1.36/32 cni.projectcalico.org/podIPs:10.244.1.36/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cc7a0 0xc0038cc7a1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-05 08:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2jrd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2jrd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-8pw5x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-8pw5x webserver-deployment-69b7448995- deployment-7313  4c9548d0-a5a4-4079-b7a5-888203e89d18 21193 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cc990 0xc0038cc991}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xr7nh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xr7nh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-bcjxh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-bcjxh webserver-deployment-69b7448995- deployment-7313  0c254a62-3422-46d6-bce5-c602e832089d 21270 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e48b2a67a18d140f4b112cf3e1f266a5c22ed2ef9285eb5ea1ad8156e303fdb3 cni.projectcalico.org/podIP:10.244.1.40/32 cni.projectcalico.org/podIPs:10.244.1.40/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038ccb10 0xc0038ccb11}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mg9fq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mg9fq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-cszpx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-cszpx webserver-deployment-69b7448995- deployment-7313  690095bb-1a7f-4418-9827-174b3fdf3f1f 21140 0 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:56fcaf13a3608ab43b92bd5081fe5e7450837237918c520e423aa7c602fb10de cni.projectcalico.org/podIP:10.244.1.34/32 cni.projectcalico.org/podIPs:10.244.1.34/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cccb0 0xc0038cccb1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wpn7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wpn7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.34,StartTime:2023-01-05 08:14:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.752: INFO: Pod "webserver-deployment-69b7448995-fk4ng" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-fk4ng webserver-deployment-69b7448995- deployment-7313  ebc13410-1f80-485b-baf6-7f4ccbd1ffb6 21200 0 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e1e26e440079d7d3d083888d75762668cc080b11beaaf6fb9dc148d13df7ff80 cni.projectcalico.org/podIP:10.244.0.153/32 cni.projectcalico.org/podIPs:10.244.0.153/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038ccf00 0xc0038ccf01}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f599l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f599l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.153,StartTime:2023-01-05 08:14:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-hcpkb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hcpkb webserver-deployment-69b7448995- deployment-7313  631edb77-d441-48ff-ae41-3b97eb356d04 21190 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cd120 0xc0038cd121}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvfdb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvfdb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-p7cbt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-p7cbt webserver-deployment-69b7448995- deployment-7313  d8d32d34-3d27-4a6e-adab-5bd6e8093a50 21228 0 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0e23803da26e6ad91ef37edaa28171fbb1d931146516761244d99727e68df0f7 cni.projectcalico.org/podIP:10.244.0.152/32 cni.projectcalico.org/podIPs:10.244.0.152/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cd2a0 0xc0038cd2a1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hcbl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hcbl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.152,StartTime:2023-01-05 08:14:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-tjvq7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tjvq7 webserver-deployment-69b7448995- deployment-7313  ae281ad4-5fe6-4b40-b626-2a0e5d9541c8 21271 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:96bbc2c01c585b9db4c70f86bd05661747cc794eb879bf2b5e56953e4dd88533 cni.projectcalico.org/podIP:10.244.0.157/32 cni.projectcalico.org/podIPs:10.244.0.157/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cd4e0 0xc0038cd4e1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tlmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tlmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-wrjwd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-wrjwd webserver-deployment-69b7448995- deployment-7313  84454cd8-a278-45b9-b337-274017203915 21231 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cd660 0xc0038cd661}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slf9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slf9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-xbqv2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-xbqv2 webserver-deployment-69b7448995- deployment-7313  71a477cd-f12d-4b01-baae-33371db0cad3 21124 0 2023-01-05 08:14:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4f687579c4d6b64d3c6a66215923bada1463c1f0b17b0b2e2f0feb6153d7f0f9 cni.projectcalico.org/podIP:10.244.1.35/32 cni.projectcalico.org/podIPs:10.244.1.35/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cd850 0xc0038cd851}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-05 08:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7b2k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7b2k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-69b7448995-zhx8n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zhx8n webserver-deployment-69b7448995- deployment-7313  61b9d5a1-8c53-4e77-84b9-8b368d72b2e3 21272 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:99fc5260ff32fe9e80867582091940355b7a9c744baff77ef930c802b08a4724 cni.projectcalico.org/podIP:10.244.0.156/32 cni.projectcalico.org/podIPs:10.244.0.156/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 40a3f70c-2156-4dbe-b014-15662b7cffe5 0xc0038cda60 0xc0038cda61}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40a3f70c-2156-4dbe-b014-15662b7cffe5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xjbww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xjbww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-845c8977d9-7ht92" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7ht92 webserver-deployment-845c8977d9- deployment-7313  aee91777-4d6d-41f2-b9ee-69eefac4dfa7 21176 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc0038cdc50 0xc0038cdc51}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vb7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vb7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-845c8977d9-8c6nw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8c6nw webserver-deployment-845c8977d9- deployment-7313  d0864d2f-d496-4611-a222-f11be975242c 21255 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f9b3d42bbd7c96154a23ee3574ca6172e65dfffb62bd410478ddbad0c69fce4f cni.projectcalico.org/podIP:10.244.0.154/32 cni.projectcalico.org/podIPs:10.244.0.154/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc0038cde37 0xc0038cde38}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qpwh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qpwh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.753: INFO: Pod "webserver-deployment-845c8977d9-8nvj5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8nvj5 webserver-deployment-845c8977d9- deployment-7313  fc244367-9457-473a-b212-56c2e1c4d049 21224 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc0007780e7 0xc0007780e8}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fw4f9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fw4f9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-9tsl2" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9tsl2 webserver-deployment-845c8977d9- deployment-7313  38f2ca55-54b5-4596-a708-d4f4cf602532 21022 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8fc3ae3836c2d5dd410dedb944a8c474bef276b2f6cf23fa9a8d960a858c161d cni.projectcalico.org/podIP:10.244.0.149/32 cni.projectcalico.org/podIPs:10.244.0.149/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000778657 0xc000778658}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-664kk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-664kk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.149,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://dc47985b79e82d383f8c74121eeda5b2c1097a516bcbf237184e432cae8b2ad1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-bmt6x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bmt6x webserver-deployment-845c8977d9- deployment-7313  f3af4e8c-3341-442c-91ba-e0e8e565f0e6 21203 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000778a57 0xc000778a58}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sh29p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sh29p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-c5fvb" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-c5fvb webserver-deployment-845c8977d9- deployment-7313  ff7f9826-85b9-4851-9f50-c51e5b09260b 21014 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:766697ab65ac0f9b6024461cb351e1917ede13a76be64549c6e0e6a4e9b3589a cni.projectcalico.org/podIP:10.244.1.31/32 cni.projectcalico.org/podIPs:10.244.1.31/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000778cd0 0xc000778cd1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rk89x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rk89x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.31,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://eb0adcd7265ba12134b5e530819ccd1d48ecff3d2ca81c8d5725511df1d36b8d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-gmb4v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gmb4v webserver-deployment-845c8977d9- deployment-7313  fd764654-d8ae-4ad6-a83f-a12c3f5bcc74 21265 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8fc353da641585e25eb10b6921b97d2743e6779819ce260e638e046e42e1153f cni.projectcalico.org/podIP:10.244.1.38/32 cni.projectcalico.org/podIPs:10.244.1.38/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000778fe0 0xc000778fe1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5n8d5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5n8d5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-j7r69" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-j7r69 webserver-deployment-845c8977d9- deployment-7313  dda4e439-3a71-40d4-bd1d-bf6ad2c01b4d 21286 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0fa77c8d3c855397a8873c191d1cbf6b8af66e23e0d0d226eada1dd98ee04f8a cni.projectcalico.org/podIP:10.244.1.41/32 cni.projectcalico.org/podIPs:10.244.1.41/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc0007791d0 0xc0007791d1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2f9qn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2f9qn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-n6w9v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n6w9v webserver-deployment-845c8977d9- deployment-7313  44a03f3b-a3ff-4a84-aeaa-3f6ca88d7b0a 21245 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc0007793d0 0xc0007793d1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wwgvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wwgvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-nmxfm" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nmxfm webserver-deployment-845c8977d9- deployment-7313  f7c6430d-0b51-4e47-8a6a-b7eac69ea0ff 20995 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a72448376b0bd960894f88befeb9e18de4b37ea9c536f7ea2ef44acacb1275d8 cni.projectcalico.org/podIP:10.244.1.29/32 cni.projectcalico.org/podIPs:10.244.1.29/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000779c47 0xc000779c48}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cpcsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cpcsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.29,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://efe575f5a0cdd234eecc1c91c384721637ea65679db849f5b477fadcbdc15acb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-qbfng" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qbfng webserver-deployment-845c8977d9- deployment-7313  aa313174-7cf1-4011-84ad-10c0acf62efe 21269 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c7754b2356145518afa0965b3185512e1053cd22b81aabaaec25b1887d6daf7f cni.projectcalico.org/podIP:10.244.1.39/32 cni.projectcalico.org/podIPs:10.244.1.39/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000d06010 0xc000d06011}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkgrm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkgrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.754: INFO: Pod "webserver-deployment-845c8977d9-qn94r" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qn94r webserver-deployment-845c8977d9- deployment-7313  6c5834f5-9be9-4260-8aa5-f629a65f7f02 21047 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:12766bfb02e03cfadfb64b8da8d99b7e1a2a7ad0223155a3172e39321522bd4c cni.projectcalico.org/podIP:10.244.0.151/32 cni.projectcalico.org/podIPs:10.244.0.151/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000d06917 0xc000d06918}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h68r5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h68r5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.151,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3d2a6f62462e0beb350d193d024c27a6aa30057100ff09a7a60168b2f7537074,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-rzkx4" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rzkx4 webserver-deployment-845c8977d9- deployment-7313  0ccd353d-16df-47d9-8ff2-9660e1971bfb 21045 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1a5f27fcca44b72394cfc05a961036e8691ac4763a692a7e8a00d3ae1cdce9be cni.projectcalico.org/podIP:10.244.0.150/32 cni.projectcalico.org/podIPs:10.244.0.150/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000d073b7 0xc000d073b8}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8hzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8hzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.150,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0be06ba0eacecb68585151c1424f5b1afde1c92606e855eeec41b5930ee065f4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-sq5xv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-sq5xv webserver-deployment-845c8977d9- deployment-7313  bd5040ad-6f8e-4b6f-8c89-1bf8f3da57c5 21285 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d93eaa6e148bcd13da2809c4e092df7ab7766b63af474dc923fd9fc1b7855335 cni.projectcalico.org/podIP:10.244.0.158/32 cni.projectcalico.org/podIPs:10.244.0.158/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc000d07a17 0xc000d07a18}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-05 08:14:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c7lvs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c7lvs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:,StartTime:2023-01-05 08:14:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-tcc6n" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-tcc6n webserver-deployment-845c8977d9- deployment-7313  b32ffa6b-3931-4561-b773-c0c4c5ded188 21020 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cd8b1b5d41181c1736469123dd113930181ba8a9ce02edb32a48f0732336c7e4 cni.projectcalico.org/podIP:10.244.0.147/32 cni.projectcalico.org/podIPs:10.244.0.147/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351c157 0xc00351c158}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8jsk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8jsk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.147,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://15e34979661f2307e9ab49001c17faf2c066de72511bdae563746463a81e5de9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-tsdrq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-tsdrq webserver-deployment-845c8977d9- deployment-7313  5a71a5ff-ab0b-436c-8920-0da60617f78c 21264 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:20fc4394462f3f0eb897304e6ae763e3260d48b4c22fc5961caa382da95c3035 cni.projectcalico.org/podIP:10.244.0.155/32 cni.projectcalico.org/podIPs:10.244.0.155/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351c447 0xc00351c448}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqxq2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqxq2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-vmh62" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vmh62 webserver-deployment-845c8977d9- deployment-7313  3bf9812e-6744-4088-8a2b-28b22f7b50b3 21204 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351c5c0 0xc00351c5c1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nts9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nts9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-wbsvx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wbsvx webserver-deployment-845c8977d9- deployment-7313  6ef9ffd5-6550-44ea-9739-0358318902d5 21254 0 2023-01-05 08:14:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:66d59c36f69f645f4ff91d903847ca7102466c52af8b3a052bf75049674ffbbe cni.projectcalico.org/podIP:10.244.1.37/32 cni.projectcalico.org/podIPs:10.244.1.37/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351c730 0xc00351c731}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rs62n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rs62n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-xcfsd" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xcfsd webserver-deployment-845c8977d9- deployment-7313  70192a7b-5d4c-4a93-a24a-550eb1cd338a 21016 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c119b0b6e790d713e3f0e46c79f6373add1fd654c492de3526f66b8a05c1cdff cni.projectcalico.org/podIP:10.244.1.30/32 cni.projectcalico.org/podIPs:10.244.1.30/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351c8c0 0xc00351c8c1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p29ng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p29ng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.30,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://850552b3f01aeac2f8c79442f5f84c7c4828ddf1e94a977b252af501d125c2ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:14:22.755: INFO: Pod "webserver-deployment-845c8977d9-z7kpx" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-z7kpx webserver-deployment-845c8977d9- deployment-7313  77d9f33a-ebc7-4ab7-9907-89c0a4c7cd44 20998 0 2023-01-05 08:14:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3efd4f5dfcf485528dd1a9924f97179b56da36f1a31b2569804755590b66b88e cni.projectcalico.org/podIP:10.244.0.148/32 cni.projectcalico.org/podIPs:10.244.0.148/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 7126e99e-d98f-41c3-b967-d907b38e4baa 0xc00351cad0 0xc00351cad1}] [] [{kube-controller-manager Update v1 2023-01-05 08:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7126e99e-d98f-41c3-b967-d907b38e4baa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:14:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z56nn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z56nn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.212,PodIP:10.244.0.148,StartTime:2023-01-05 08:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:14:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9cc112380b52ace23c0a978ae1cbfddfcc9bc75647f8059e0de9cbf70b0f4d61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 08:14:22.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7313" for this suite. 01/05/23 08:14:22.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:14:22.782
Jan  5 08:14:22.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:14:22.782
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:22.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:22.796
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/05/23 08:14:22.797
Jan  5 08:14:22.812: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5" in namespace "projected-2206" to be "Succeeded or Failed"
Jan  5 08:14:22.822: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.203411ms
Jan  5 08:14:24.825: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013255178s
Jan  5 08:14:26.824: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012710315s
Jan  5 08:14:28.834: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021790109s
Jan  5 08:14:30.826: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014225968s
Jan  5 08:14:32.825: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.013264593s
STEP: Saw pod success 01/05/23 08:14:32.825
Jan  5 08:14:32.825: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5" satisfied condition "Succeeded or Failed"
Jan  5 08:14:32.827: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5 container client-container: <nil>
STEP: delete the pod 01/05/23 08:14:32.83
Jan  5 08:14:32.844: INFO: Waiting for pod downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5 to disappear
Jan  5 08:14:32.845: INFO: Pod downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 08:14:32.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2206" for this suite. 01/05/23 08:14:32.847
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":162,"skipped":3253,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.074 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:14:22.782
    Jan  5 08:14:22.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:14:22.782
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:22.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:22.796
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/05/23 08:14:22.797
    Jan  5 08:14:22.812: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5" in namespace "projected-2206" to be "Succeeded or Failed"
    Jan  5 08:14:22.822: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.203411ms
    Jan  5 08:14:24.825: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013255178s
    Jan  5 08:14:26.824: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012710315s
    Jan  5 08:14:28.834: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021790109s
    Jan  5 08:14:30.826: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014225968s
    Jan  5 08:14:32.825: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.013264593s
    STEP: Saw pod success 01/05/23 08:14:32.825
    Jan  5 08:14:32.825: INFO: Pod "downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5" satisfied condition "Succeeded or Failed"
    Jan  5 08:14:32.827: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5 container client-container: <nil>
    STEP: delete the pod 01/05/23 08:14:32.83
    Jan  5 08:14:32.844: INFO: Waiting for pod downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5 to disappear
    Jan  5 08:14:32.845: INFO: Pod downwardapi-volume-b51f028d-eebb-4128-bd13-a8f41e4084b5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 08:14:32.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2206" for this suite. 01/05/23 08:14:32.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:14:32.856
Jan  5 08:14:32.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:14:32.856
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:32.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:32.868
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/05/23 08:14:32.87
STEP: waiting for available Endpoint 01/05/23 08:14:32.88
STEP: listing all Endpoints 01/05/23 08:14:32.881
STEP: updating the Endpoint 01/05/23 08:14:32.883
STEP: fetching the Endpoint 01/05/23 08:14:32.887
STEP: patching the Endpoint 01/05/23 08:14:32.888
STEP: fetching the Endpoint 01/05/23 08:14:32.896
STEP: deleting the Endpoint by Collection 01/05/23 08:14:32.897
STEP: waiting for Endpoint deletion 01/05/23 08:14:32.901
STEP: fetching the Endpoint 01/05/23 08:14:32.901
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:14:32.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3282" for this suite. 01/05/23 08:14:32.904
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":163,"skipped":3267,"failed":0}
------------------------------
â€¢ [0.062 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:14:32.856
    Jan  5 08:14:32.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:14:32.856
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:32.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:32.868
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/05/23 08:14:32.87
    STEP: waiting for available Endpoint 01/05/23 08:14:32.88
    STEP: listing all Endpoints 01/05/23 08:14:32.881
    STEP: updating the Endpoint 01/05/23 08:14:32.883
    STEP: fetching the Endpoint 01/05/23 08:14:32.887
    STEP: patching the Endpoint 01/05/23 08:14:32.888
    STEP: fetching the Endpoint 01/05/23 08:14:32.896
    STEP: deleting the Endpoint by Collection 01/05/23 08:14:32.897
    STEP: waiting for Endpoint deletion 01/05/23 08:14:32.901
    STEP: fetching the Endpoint 01/05/23 08:14:32.901
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:14:32.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3282" for this suite. 01/05/23 08:14:32.904
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:14:32.918
Jan  5 08:14:32.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:14:32.919
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:32.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:32.932
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-5c0aa316-4098-4675-bd8a-305de86b9244 01/05/23 08:14:32.935
STEP: Creating the pod 01/05/23 08:14:32.942
Jan  5 08:14:32.947: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009" in namespace "projected-7232" to be "running and ready"
Jan  5 08:14:32.948: INFO: Pod "pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009": Phase="Pending", Reason="", readiness=false. Elapsed: 1.128852ms
Jan  5 08:14:32.948: INFO: The phase of Pod pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:14:34.951: INFO: Pod "pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004441712s
Jan  5 08:14:34.951: INFO: The phase of Pod pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:14:36.952: INFO: Pod "pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009": Phase="Running", Reason="", readiness=true. Elapsed: 4.004570253s
Jan  5 08:14:36.952: INFO: The phase of Pod pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009 is Running (Ready = true)
Jan  5 08:14:36.952: INFO: Pod "pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-5c0aa316-4098-4675-bd8a-305de86b9244 01/05/23 08:14:36.957
STEP: waiting to observe update in volume 01/05/23 08:14:36.961
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 08:15:47.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7232" for this suite. 01/05/23 08:15:47.171
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":164,"skipped":3274,"failed":0}
------------------------------
â€¢ [SLOW TEST] [74.257 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:14:32.918
    Jan  5 08:14:32.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:14:32.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:14:32.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:14:32.932
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-5c0aa316-4098-4675-bd8a-305de86b9244 01/05/23 08:14:32.935
    STEP: Creating the pod 01/05/23 08:14:32.942
    Jan  5 08:14:32.947: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009" in namespace "projected-7232" to be "running and ready"
    Jan  5 08:14:32.948: INFO: Pod "pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009": Phase="Pending", Reason="", readiness=false. Elapsed: 1.128852ms
    Jan  5 08:14:32.948: INFO: The phase of Pod pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:14:34.951: INFO: Pod "pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004441712s
    Jan  5 08:14:34.951: INFO: The phase of Pod pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:14:36.952: INFO: Pod "pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009": Phase="Running", Reason="", readiness=true. Elapsed: 4.004570253s
    Jan  5 08:14:36.952: INFO: The phase of Pod pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009 is Running (Ready = true)
    Jan  5 08:14:36.952: INFO: Pod "pod-projected-configmaps-5ffe84ae-f267-40d7-b8d4-c33252c8d009" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-5c0aa316-4098-4675-bd8a-305de86b9244 01/05/23 08:14:36.957
    STEP: waiting to observe update in volume 01/05/23 08:14:36.961
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 08:15:47.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7232" for this suite. 01/05/23 08:15:47.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:15:47.177
Jan  5 08:15:47.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 08:15:47.178
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:15:47.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:15:47.199
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/05/23 08:15:47.201
Jan  5 08:15:47.201: INFO: namespace kubectl-9501
Jan  5 08:15:47.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9501 create -f -'
Jan  5 08:15:48.036: INFO: stderr: ""
Jan  5 08:15:48.036: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/05/23 08:15:48.036
Jan  5 08:15:49.039: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 08:15:49.040: INFO: Found 0 / 1
Jan  5 08:15:50.040: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 08:15:50.040: INFO: Found 1 / 1
Jan  5 08:15:50.040: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  5 08:15:50.043: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 08:15:50.043: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  5 08:15:50.043: INFO: wait on agnhost-primary startup in kubectl-9501 
Jan  5 08:15:50.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9501 logs agnhost-primary-q547x agnhost-primary'
Jan  5 08:15:50.106: INFO: stderr: ""
Jan  5 08:15:50.106: INFO: stdout: "Paused\n"
STEP: exposing RC 01/05/23 08:15:50.106
Jan  5 08:15:50.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9501 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan  5 08:15:50.191: INFO: stderr: ""
Jan  5 08:15:50.191: INFO: stdout: "service/rm2 exposed\n"
Jan  5 08:15:50.194: INFO: Service rm2 in namespace kubectl-9501 found.
STEP: exposing service 01/05/23 08:15:52.204
Jan  5 08:15:52.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9501 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan  5 08:15:52.281: INFO: stderr: ""
Jan  5 08:15:52.282: INFO: stdout: "service/rm3 exposed\n"
Jan  5 08:15:52.292: INFO: Service rm3 in namespace kubectl-9501 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 08:15:54.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9501" for this suite. 01/05/23 08:15:54.3
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":165,"skipped":3319,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.127 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:15:47.177
    Jan  5 08:15:47.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 08:15:47.178
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:15:47.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:15:47.199
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/05/23 08:15:47.201
    Jan  5 08:15:47.201: INFO: namespace kubectl-9501
    Jan  5 08:15:47.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9501 create -f -'
    Jan  5 08:15:48.036: INFO: stderr: ""
    Jan  5 08:15:48.036: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/05/23 08:15:48.036
    Jan  5 08:15:49.039: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 08:15:49.040: INFO: Found 0 / 1
    Jan  5 08:15:50.040: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 08:15:50.040: INFO: Found 1 / 1
    Jan  5 08:15:50.040: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  5 08:15:50.043: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 08:15:50.043: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  5 08:15:50.043: INFO: wait on agnhost-primary startup in kubectl-9501 
    Jan  5 08:15:50.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9501 logs agnhost-primary-q547x agnhost-primary'
    Jan  5 08:15:50.106: INFO: stderr: ""
    Jan  5 08:15:50.106: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/05/23 08:15:50.106
    Jan  5 08:15:50.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9501 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan  5 08:15:50.191: INFO: stderr: ""
    Jan  5 08:15:50.191: INFO: stdout: "service/rm2 exposed\n"
    Jan  5 08:15:50.194: INFO: Service rm2 in namespace kubectl-9501 found.
    STEP: exposing service 01/05/23 08:15:52.204
    Jan  5 08:15:52.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9501 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan  5 08:15:52.281: INFO: stderr: ""
    Jan  5 08:15:52.282: INFO: stdout: "service/rm3 exposed\n"
    Jan  5 08:15:52.292: INFO: Service rm3 in namespace kubectl-9501 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 08:15:54.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9501" for this suite. 01/05/23 08:15:54.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:15:54.305
Jan  5 08:15:54.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename init-container 01/05/23 08:15:54.306
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:15:54.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:15:54.321
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/05/23 08:15:54.326
Jan  5 08:15:54.326: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 08:15:59.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6830" for this suite. 01/05/23 08:15:59.985
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":166,"skipped":3338,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.684 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:15:54.305
    Jan  5 08:15:54.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename init-container 01/05/23 08:15:54.306
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:15:54.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:15:54.321
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/05/23 08:15:54.326
    Jan  5 08:15:54.326: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 08:15:59.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6830" for this suite. 01/05/23 08:15:59.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:15:59.99
Jan  5 08:15:59.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename subpath 01/05/23 08:15:59.99
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:16:00.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:16:00.017
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 08:16:00.019
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-7547 01/05/23 08:16:00.028
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 08:16:00.029
Jan  5 08:16:00.034: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7547" in namespace "subpath-461" to be "Succeeded or Failed"
Jan  5 08:16:00.037: INFO: Pod "pod-subpath-test-projected-7547": Phase="Pending", Reason="", readiness=false. Elapsed: 2.424577ms
Jan  5 08:16:02.040: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 2.005994914s
Jan  5 08:16:04.041: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 4.006753137s
Jan  5 08:16:06.041: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 6.00620607s
Jan  5 08:16:08.041: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 8.006558822s
Jan  5 08:16:10.039: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 10.004972965s
Jan  5 08:16:12.042: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 12.007959109s
Jan  5 08:16:14.041: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 14.00688659s
Jan  5 08:16:16.042: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 16.007157923s
Jan  5 08:16:18.041: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 18.006993238s
Jan  5 08:16:20.040: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 20.005442897s
Jan  5 08:16:22.042: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=false. Elapsed: 22.007095032s
Jan  5 08:16:24.039: INFO: Pod "pod-subpath-test-projected-7547": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004993131s
STEP: Saw pod success 01/05/23 08:16:24.039
Jan  5 08:16:24.040: INFO: Pod "pod-subpath-test-projected-7547" satisfied condition "Succeeded or Failed"
Jan  5 08:16:24.041: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-subpath-test-projected-7547 container test-container-subpath-projected-7547: <nil>
STEP: delete the pod 01/05/23 08:16:24.045
Jan  5 08:16:24.057: INFO: Waiting for pod pod-subpath-test-projected-7547 to disappear
Jan  5 08:16:24.059: INFO: Pod pod-subpath-test-projected-7547 no longer exists
STEP: Deleting pod pod-subpath-test-projected-7547 01/05/23 08:16:24.059
Jan  5 08:16:24.059: INFO: Deleting pod "pod-subpath-test-projected-7547" in namespace "subpath-461"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  5 08:16:24.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-461" for this suite. 01/05/23 08:16:24.063
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":167,"skipped":3363,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.081 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:15:59.99
    Jan  5 08:15:59.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename subpath 01/05/23 08:15:59.99
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:16:00.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:16:00.017
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 08:16:00.019
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-7547 01/05/23 08:16:00.028
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 08:16:00.029
    Jan  5 08:16:00.034: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7547" in namespace "subpath-461" to be "Succeeded or Failed"
    Jan  5 08:16:00.037: INFO: Pod "pod-subpath-test-projected-7547": Phase="Pending", Reason="", readiness=false. Elapsed: 2.424577ms
    Jan  5 08:16:02.040: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 2.005994914s
    Jan  5 08:16:04.041: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 4.006753137s
    Jan  5 08:16:06.041: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 6.00620607s
    Jan  5 08:16:08.041: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 8.006558822s
    Jan  5 08:16:10.039: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 10.004972965s
    Jan  5 08:16:12.042: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 12.007959109s
    Jan  5 08:16:14.041: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 14.00688659s
    Jan  5 08:16:16.042: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 16.007157923s
    Jan  5 08:16:18.041: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 18.006993238s
    Jan  5 08:16:20.040: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=true. Elapsed: 20.005442897s
    Jan  5 08:16:22.042: INFO: Pod "pod-subpath-test-projected-7547": Phase="Running", Reason="", readiness=false. Elapsed: 22.007095032s
    Jan  5 08:16:24.039: INFO: Pod "pod-subpath-test-projected-7547": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004993131s
    STEP: Saw pod success 01/05/23 08:16:24.039
    Jan  5 08:16:24.040: INFO: Pod "pod-subpath-test-projected-7547" satisfied condition "Succeeded or Failed"
    Jan  5 08:16:24.041: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-subpath-test-projected-7547 container test-container-subpath-projected-7547: <nil>
    STEP: delete the pod 01/05/23 08:16:24.045
    Jan  5 08:16:24.057: INFO: Waiting for pod pod-subpath-test-projected-7547 to disappear
    Jan  5 08:16:24.059: INFO: Pod pod-subpath-test-projected-7547 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-7547 01/05/23 08:16:24.059
    Jan  5 08:16:24.059: INFO: Deleting pod "pod-subpath-test-projected-7547" in namespace "subpath-461"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  5 08:16:24.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-461" for this suite. 01/05/23 08:16:24.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:16:24.071
Jan  5 08:16:24.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pods 01/05/23 08:16:24.072
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:16:24.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:16:24.085
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/05/23 08:16:24.087
STEP: submitting the pod to kubernetes 01/05/23 08:16:24.087
Jan  5 08:16:24.096: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90" in namespace "pods-2133" to be "running and ready"
Jan  5 08:16:24.097: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Pending", Reason="", readiness=false. Elapsed: 1.554314ms
Jan  5 08:16:24.097: INFO: The phase of Pod pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:16:26.101: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Running", Reason="", readiness=true. Elapsed: 2.005458136s
Jan  5 08:16:26.101: INFO: The phase of Pod pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90 is Running (Ready = true)
Jan  5 08:16:26.101: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/05/23 08:16:26.103
STEP: updating the pod 01/05/23 08:16:26.104
Jan  5 08:16:26.619: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90"
Jan  5 08:16:26.619: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90" in namespace "pods-2133" to be "terminated with reason DeadlineExceeded"
Jan  5 08:16:26.621: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Running", Reason="", readiness=true. Elapsed: 1.909264ms
Jan  5 08:16:28.623: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Running", Reason="", readiness=true. Elapsed: 2.004017453s
Jan  5 08:16:30.624: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Running", Reason="", readiness=false. Elapsed: 4.005594043s
Jan  5 08:16:32.626: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.00687953s
Jan  5 08:16:32.626: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 08:16:32.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2133" for this suite. 01/05/23 08:16:32.628
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":168,"skipped":3369,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.561 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:16:24.071
    Jan  5 08:16:24.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pods 01/05/23 08:16:24.072
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:16:24.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:16:24.085
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/05/23 08:16:24.087
    STEP: submitting the pod to kubernetes 01/05/23 08:16:24.087
    Jan  5 08:16:24.096: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90" in namespace "pods-2133" to be "running and ready"
    Jan  5 08:16:24.097: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Pending", Reason="", readiness=false. Elapsed: 1.554314ms
    Jan  5 08:16:24.097: INFO: The phase of Pod pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:16:26.101: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Running", Reason="", readiness=true. Elapsed: 2.005458136s
    Jan  5 08:16:26.101: INFO: The phase of Pod pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90 is Running (Ready = true)
    Jan  5 08:16:26.101: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/05/23 08:16:26.103
    STEP: updating the pod 01/05/23 08:16:26.104
    Jan  5 08:16:26.619: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90"
    Jan  5 08:16:26.619: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90" in namespace "pods-2133" to be "terminated with reason DeadlineExceeded"
    Jan  5 08:16:26.621: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Running", Reason="", readiness=true. Elapsed: 1.909264ms
    Jan  5 08:16:28.623: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Running", Reason="", readiness=true. Elapsed: 2.004017453s
    Jan  5 08:16:30.624: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Running", Reason="", readiness=false. Elapsed: 4.005594043s
    Jan  5 08:16:32.626: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.00687953s
    Jan  5 08:16:32.626: INFO: Pod "pod-update-activedeadlineseconds-a61f1716-394a-47b5-84fd-e897cbcdcd90" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 08:16:32.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2133" for this suite. 01/05/23 08:16:32.628
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:16:32.633
Jan  5 08:16:32.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:16:32.634
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:16:32.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:16:32.657
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:16:32.685
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:16:33.299
STEP: Deploying the webhook pod 01/05/23 08:16:33.305
STEP: Wait for the deployment to be ready 01/05/23 08:16:33.346
Jan  5 08:16:33.349: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 08:16:35.356
STEP: Verifying the service has paired with the endpoint 01/05/23 08:16:35.371
Jan  5 08:16:36.372: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/05/23 08:16:36.464
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 08:16:36.699
STEP: Deleting the collection of validation webhooks 01/05/23 08:16:36.721
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 08:16:36.787
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:16:36.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-942" for this suite. 01/05/23 08:16:36.803
STEP: Destroying namespace "webhook-942-markers" for this suite. 01/05/23 08:16:36.806
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":169,"skipped":3369,"failed":0}
------------------------------
â€¢ [4.241 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:16:32.633
    Jan  5 08:16:32.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:16:32.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:16:32.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:16:32.657
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:16:32.685
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:16:33.299
    STEP: Deploying the webhook pod 01/05/23 08:16:33.305
    STEP: Wait for the deployment to be ready 01/05/23 08:16:33.346
    Jan  5 08:16:33.349: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 08:16:35.356
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:16:35.371
    Jan  5 08:16:36.372: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/05/23 08:16:36.464
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 08:16:36.699
    STEP: Deleting the collection of validation webhooks 01/05/23 08:16:36.721
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 08:16:36.787
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:16:36.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-942" for this suite. 01/05/23 08:16:36.803
    STEP: Destroying namespace "webhook-942-markers" for this suite. 01/05/23 08:16:36.806
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:16:36.874
Jan  5 08:16:36.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-probe 01/05/23 08:16:36.874
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:16:36.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:16:36.923
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 08:17:36.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9228" for this suite. 01/05/23 08:17:36.943
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":170,"skipped":3381,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.075 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:16:36.874
    Jan  5 08:16:36.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-probe 01/05/23 08:16:36.874
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:16:36.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:16:36.923
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 08:17:36.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9228" for this suite. 01/05/23 08:17:36.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:17:36.95
Jan  5 08:17:36.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pod-network-test 01/05/23 08:17:36.951
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:17:36.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:17:36.97
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-6881 01/05/23 08:17:36.973
STEP: creating a selector 01/05/23 08:17:36.973
STEP: Creating the service pods in kubernetes 01/05/23 08:17:36.974
Jan  5 08:17:36.974: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 08:17:36.989: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6881" to be "running and ready"
Jan  5 08:17:36.992: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315229ms
Jan  5 08:17:36.992: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:17:38.995: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005338601s
Jan  5 08:17:38.995: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:17:40.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005212105s
Jan  5 08:17:40.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:17:42.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005591726s
Jan  5 08:17:42.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:17:44.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005645825s
Jan  5 08:17:44.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:17:46.998: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008231352s
Jan  5 08:17:46.998: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:17:48.998: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008820979s
Jan  5 08:17:48.998: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:17:50.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005074986s
Jan  5 08:17:50.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:17:52.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005667868s
Jan  5 08:17:52.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:17:54.996: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.006363629s
Jan  5 08:17:54.996: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:17:56.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.005272863s
Jan  5 08:17:56.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:17:58.996: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.006518974s
Jan  5 08:17:58.996: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 08:17:58.996: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 08:17:58.998: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6881" to be "running and ready"
Jan  5 08:17:59.000: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.525416ms
Jan  5 08:17:59.000: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 08:17:59.000: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 08:17:59.003
Jan  5 08:17:59.012: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6881" to be "running"
Jan  5 08:17:59.014: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.562028ms
Jan  5 08:18:01.025: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013151694s
Jan  5 08:18:03.018: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005954605s
Jan  5 08:18:03.018: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 08:18:03.020: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan  5 08:18:03.020: INFO: Breadth first check of 10.244.0.161 on host 16.0.14.212...
Jan  5 08:18:03.022: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.54:9080/dial?request=hostname&protocol=http&host=10.244.0.161&port=8083&tries=1'] Namespace:pod-network-test-6881 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:18:03.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:18:03.022: INFO: ExecWithOptions: Clientset creation
Jan  5 08:18:03.022: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6881/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.54%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.161%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 08:18:03.091: INFO: Waiting for responses: map[]
Jan  5 08:18:03.091: INFO: reached 10.244.0.161 after 0/1 tries
Jan  5 08:18:03.091: INFO: Breadth first check of 10.244.1.53 on host 16.0.14.214...
Jan  5 08:18:03.150: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.54:9080/dial?request=hostname&protocol=http&host=10.244.1.53&port=8083&tries=1'] Namespace:pod-network-test-6881 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:18:03.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:18:03.151: INFO: ExecWithOptions: Clientset creation
Jan  5 08:18:03.151: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6881/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.54%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.53%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 08:18:03.212: INFO: Waiting for responses: map[]
Jan  5 08:18:03.213: INFO: reached 10.244.1.53 after 0/1 tries
Jan  5 08:18:03.213: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  5 08:18:03.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6881" for this suite. 01/05/23 08:18:03.215
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":171,"skipped":3398,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.271 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:17:36.95
    Jan  5 08:17:36.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 08:17:36.951
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:17:36.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:17:36.97
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-6881 01/05/23 08:17:36.973
    STEP: creating a selector 01/05/23 08:17:36.973
    STEP: Creating the service pods in kubernetes 01/05/23 08:17:36.974
    Jan  5 08:17:36.974: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 08:17:36.989: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6881" to be "running and ready"
    Jan  5 08:17:36.992: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315229ms
    Jan  5 08:17:36.992: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:17:38.995: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005338601s
    Jan  5 08:17:38.995: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:17:40.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005212105s
    Jan  5 08:17:40.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:17:42.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005591726s
    Jan  5 08:17:42.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:17:44.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005645825s
    Jan  5 08:17:44.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:17:46.998: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008231352s
    Jan  5 08:17:46.998: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:17:48.998: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008820979s
    Jan  5 08:17:48.998: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:17:50.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005074986s
    Jan  5 08:17:50.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:17:52.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005667868s
    Jan  5 08:17:52.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:17:54.996: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.006363629s
    Jan  5 08:17:54.996: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:17:56.995: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.005272863s
    Jan  5 08:17:56.995: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:17:58.996: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.006518974s
    Jan  5 08:17:58.996: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 08:17:58.996: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 08:17:58.998: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6881" to be "running and ready"
    Jan  5 08:17:59.000: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.525416ms
    Jan  5 08:17:59.000: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 08:17:59.000: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 08:17:59.003
    Jan  5 08:17:59.012: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6881" to be "running"
    Jan  5 08:17:59.014: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.562028ms
    Jan  5 08:18:01.025: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013151694s
    Jan  5 08:18:03.018: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005954605s
    Jan  5 08:18:03.018: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 08:18:03.020: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan  5 08:18:03.020: INFO: Breadth first check of 10.244.0.161 on host 16.0.14.212...
    Jan  5 08:18:03.022: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.54:9080/dial?request=hostname&protocol=http&host=10.244.0.161&port=8083&tries=1'] Namespace:pod-network-test-6881 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:18:03.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:18:03.022: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:18:03.022: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6881/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.54%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.161%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 08:18:03.091: INFO: Waiting for responses: map[]
    Jan  5 08:18:03.091: INFO: reached 10.244.0.161 after 0/1 tries
    Jan  5 08:18:03.091: INFO: Breadth first check of 10.244.1.53 on host 16.0.14.214...
    Jan  5 08:18:03.150: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.54:9080/dial?request=hostname&protocol=http&host=10.244.1.53&port=8083&tries=1'] Namespace:pod-network-test-6881 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:18:03.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:18:03.151: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:18:03.151: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6881/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.54%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.53%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 08:18:03.212: INFO: Waiting for responses: map[]
    Jan  5 08:18:03.213: INFO: reached 10.244.1.53 after 0/1 tries
    Jan  5 08:18:03.213: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  5 08:18:03.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6881" for this suite. 01/05/23 08:18:03.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:18:03.221
Jan  5 08:18:03.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:18:03.222
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:03.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:03.241
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-ee0b4970-d763-46c7-9a5f-a4b685224acb 01/05/23 08:18:03.243
STEP: Creating a pod to test consume configMaps 01/05/23 08:18:03.246
Jan  5 08:18:03.265: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7" in namespace "projected-5155" to be "Succeeded or Failed"
Jan  5 08:18:03.267: INFO: Pod "pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592025ms
Jan  5 08:18:05.270: INFO: Pod "pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005464847s
Jan  5 08:18:07.272: INFO: Pod "pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006921574s
STEP: Saw pod success 01/05/23 08:18:07.272
Jan  5 08:18:07.272: INFO: Pod "pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7" satisfied condition "Succeeded or Failed"
Jan  5 08:18:07.274: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:18:07.287
Jan  5 08:18:07.323: INFO: Waiting for pod pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7 to disappear
Jan  5 08:18:07.325: INFO: Pod pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 08:18:07.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5155" for this suite. 01/05/23 08:18:07.328
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":172,"skipped":3417,"failed":0}
------------------------------
â€¢ [4.116 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:18:03.221
    Jan  5 08:18:03.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:18:03.222
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:03.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:03.241
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-ee0b4970-d763-46c7-9a5f-a4b685224acb 01/05/23 08:18:03.243
    STEP: Creating a pod to test consume configMaps 01/05/23 08:18:03.246
    Jan  5 08:18:03.265: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7" in namespace "projected-5155" to be "Succeeded or Failed"
    Jan  5 08:18:03.267: INFO: Pod "pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592025ms
    Jan  5 08:18:05.270: INFO: Pod "pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005464847s
    Jan  5 08:18:07.272: INFO: Pod "pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006921574s
    STEP: Saw pod success 01/05/23 08:18:07.272
    Jan  5 08:18:07.272: INFO: Pod "pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7" satisfied condition "Succeeded or Failed"
    Jan  5 08:18:07.274: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:18:07.287
    Jan  5 08:18:07.323: INFO: Waiting for pod pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7 to disappear
    Jan  5 08:18:07.325: INFO: Pod pod-projected-configmaps-2b068067-928c-4e68-a038-e77b1f84fcd7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 08:18:07.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5155" for this suite. 01/05/23 08:18:07.328
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:18:07.337
Jan  5 08:18:07.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sched-pred 01/05/23 08:18:07.338
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:07.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:07.353
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  5 08:18:07.360: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 08:18:07.364: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 08:18:07.365: INFO: 
Logging pods the apiserver thinks is on node mip-bd-vm722.mip.storage.hpecorp.net before test
Jan  5 08:18:07.369: INFO: csi-hostpathplugin-0 from default started at 2023-01-05 07:12:21 +0000 UTC (8 container statuses recorded)
Jan  5 08:18:07.369: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  5 08:18:07.369: INFO: 	Container csi-external-health-monitor-controller ready: true, restart count 1
Jan  5 08:18:07.369: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  5 08:18:07.369: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  5 08:18:07.369: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  5 08:18:07.369: INFO: 	Container hostpath ready: true, restart count 0
Jan  5 08:18:07.369: INFO: 	Container liveness-probe ready: true, restart count 0
Jan  5 08:18:07.369: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan  5 08:18:07.369: INFO: canal-6z7xb from kube-system started at 2023-01-05 07:12:02 +0000 UTC (2 container statuses recorded)
Jan  5 08:18:07.369: INFO: 	Container calico-node ready: true, restart count 0
Jan  5 08:18:07.369: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  5 08:18:07.369: INFO: coredns-564fd8c776-nwmff from kube-system started at 2023-01-05 07:12:21 +0000 UTC (1 container statuses recorded)
Jan  5 08:18:07.369: INFO: 	Container coredns ready: true, restart count 0
Jan  5 08:18:07.369: INFO: netserver-0 from pod-network-test-6881 started at 2023-01-05 08:17:36 +0000 UTC (1 container statuses recorded)
Jan  5 08:18:07.369: INFO: 	Container webserver ready: true, restart count 0
Jan  5 08:18:07.369: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2p88t from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
Jan  5 08:18:07.370: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 08:18:07.370: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 08:18:07.370: INFO: 
Logging pods the apiserver thinks is on node mip-bd-vm724.mip.storage.hpecorp.net before test
Jan  5 08:18:07.373: INFO: canal-6sfgp from kube-system started at 2023-01-05 07:21:40 +0000 UTC (2 container statuses recorded)
Jan  5 08:18:07.373: INFO: 	Container calico-node ready: true, restart count 0
Jan  5 08:18:07.373: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  5 08:18:07.373: INFO: netserver-1 from pod-network-test-6881 started at 2023-01-05 08:17:37 +0000 UTC (1 container statuses recorded)
Jan  5 08:18:07.373: INFO: 	Container webserver ready: true, restart count 0
Jan  5 08:18:07.373: INFO: test-container-pod from pod-network-test-6881 started at 2023-01-05 08:17:59 +0000 UTC (1 container statuses recorded)
Jan  5 08:18:07.373: INFO: 	Container webserver ready: true, restart count 0
Jan  5 08:18:07.373: INFO: sonobuoy from sonobuoy started at 2023-01-05 07:22:09 +0000 UTC (1 container statuses recorded)
Jan  5 08:18:07.373: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 08:18:07.373: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2qcx8 from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
Jan  5 08:18:07.373: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 08:18:07.373: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 08:18:07.373
Jan  5 08:18:07.379: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6578" to be "running"
Jan  5 08:18:07.389: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.13047ms
Jan  5 08:18:09.394: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014846235s
Jan  5 08:18:09.394: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 08:18:09.396
STEP: Trying to apply a random label on the found node. 01/05/23 08:18:09.426
STEP: verifying the node has the label kubernetes.io/e2e-74034043-97fd-4a2e-9907-38a0a68f7127 42 01/05/23 08:18:09.44
STEP: Trying to relaunch the pod, now with labels. 01/05/23 08:18:09.444
Jan  5 08:18:09.448: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6578" to be "not pending"
Jan  5 08:18:09.450: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578784ms
Jan  5 08:18:11.454: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.005432077s
Jan  5 08:18:11.454: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-74034043-97fd-4a2e-9907-38a0a68f7127 off the node mip-bd-vm724.mip.storage.hpecorp.net 01/05/23 08:18:11.455
STEP: verifying the node doesn't have the label kubernetes.io/e2e-74034043-97fd-4a2e-9907-38a0a68f7127 01/05/23 08:18:11.467
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:18:11.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6578" for this suite. 01/05/23 08:18:11.471
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":173,"skipped":3417,"failed":0}
------------------------------
â€¢ [4.137 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:18:07.337
    Jan  5 08:18:07.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sched-pred 01/05/23 08:18:07.338
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:07.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:07.353
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  5 08:18:07.360: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 08:18:07.364: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 08:18:07.365: INFO: 
    Logging pods the apiserver thinks is on node mip-bd-vm722.mip.storage.hpecorp.net before test
    Jan  5 08:18:07.369: INFO: csi-hostpathplugin-0 from default started at 2023-01-05 07:12:21 +0000 UTC (8 container statuses recorded)
    Jan  5 08:18:07.369: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  5 08:18:07.369: INFO: 	Container csi-external-health-monitor-controller ready: true, restart count 1
    Jan  5 08:18:07.369: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  5 08:18:07.369: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  5 08:18:07.369: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  5 08:18:07.369: INFO: 	Container hostpath ready: true, restart count 0
    Jan  5 08:18:07.369: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan  5 08:18:07.369: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan  5 08:18:07.369: INFO: canal-6z7xb from kube-system started at 2023-01-05 07:12:02 +0000 UTC (2 container statuses recorded)
    Jan  5 08:18:07.369: INFO: 	Container calico-node ready: true, restart count 0
    Jan  5 08:18:07.369: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  5 08:18:07.369: INFO: coredns-564fd8c776-nwmff from kube-system started at 2023-01-05 07:12:21 +0000 UTC (1 container statuses recorded)
    Jan  5 08:18:07.369: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 08:18:07.369: INFO: netserver-0 from pod-network-test-6881 started at 2023-01-05 08:17:36 +0000 UTC (1 container statuses recorded)
    Jan  5 08:18:07.369: INFO: 	Container webserver ready: true, restart count 0
    Jan  5 08:18:07.369: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2p88t from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
    Jan  5 08:18:07.370: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 08:18:07.370: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 08:18:07.370: INFO: 
    Logging pods the apiserver thinks is on node mip-bd-vm724.mip.storage.hpecorp.net before test
    Jan  5 08:18:07.373: INFO: canal-6sfgp from kube-system started at 2023-01-05 07:21:40 +0000 UTC (2 container statuses recorded)
    Jan  5 08:18:07.373: INFO: 	Container calico-node ready: true, restart count 0
    Jan  5 08:18:07.373: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  5 08:18:07.373: INFO: netserver-1 from pod-network-test-6881 started at 2023-01-05 08:17:37 +0000 UTC (1 container statuses recorded)
    Jan  5 08:18:07.373: INFO: 	Container webserver ready: true, restart count 0
    Jan  5 08:18:07.373: INFO: test-container-pod from pod-network-test-6881 started at 2023-01-05 08:17:59 +0000 UTC (1 container statuses recorded)
    Jan  5 08:18:07.373: INFO: 	Container webserver ready: true, restart count 0
    Jan  5 08:18:07.373: INFO: sonobuoy from sonobuoy started at 2023-01-05 07:22:09 +0000 UTC (1 container statuses recorded)
    Jan  5 08:18:07.373: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 08:18:07.373: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2qcx8 from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
    Jan  5 08:18:07.373: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 08:18:07.373: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 08:18:07.373
    Jan  5 08:18:07.379: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6578" to be "running"
    Jan  5 08:18:07.389: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.13047ms
    Jan  5 08:18:09.394: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014846235s
    Jan  5 08:18:09.394: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 08:18:09.396
    STEP: Trying to apply a random label on the found node. 01/05/23 08:18:09.426
    STEP: verifying the node has the label kubernetes.io/e2e-74034043-97fd-4a2e-9907-38a0a68f7127 42 01/05/23 08:18:09.44
    STEP: Trying to relaunch the pod, now with labels. 01/05/23 08:18:09.444
    Jan  5 08:18:09.448: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6578" to be "not pending"
    Jan  5 08:18:09.450: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578784ms
    Jan  5 08:18:11.454: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.005432077s
    Jan  5 08:18:11.454: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-74034043-97fd-4a2e-9907-38a0a68f7127 off the node mip-bd-vm724.mip.storage.hpecorp.net 01/05/23 08:18:11.455
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-74034043-97fd-4a2e-9907-38a0a68f7127 01/05/23 08:18:11.467
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:18:11.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6578" for this suite. 01/05/23 08:18:11.471
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:18:11.474
Jan  5 08:18:11.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename runtimeclass 01/05/23 08:18:11.475
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:11.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:11.492
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan  5 08:18:11.523: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9268 to be scheduled
Jan  5 08:18:11.525: INFO: 1 pods are not scheduled: [runtimeclass-9268/test-runtimeclass-runtimeclass-9268-preconfigured-handler-h7gn6(41a16b55-7053-4f82-a135-81d9ebdef086)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  5 08:18:13.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9268" for this suite. 01/05/23 08:18:13.535
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":174,"skipped":3419,"failed":0}
------------------------------
â€¢ [2.069 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:18:11.474
    Jan  5 08:18:11.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 08:18:11.475
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:11.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:11.492
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan  5 08:18:11.523: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9268 to be scheduled
    Jan  5 08:18:11.525: INFO: 1 pods are not scheduled: [runtimeclass-9268/test-runtimeclass-runtimeclass-9268-preconfigured-handler-h7gn6(41a16b55-7053-4f82-a135-81d9ebdef086)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  5 08:18:13.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9268" for this suite. 01/05/23 08:18:13.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:18:13.544
Jan  5 08:18:13.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename resourcequota 01/05/23 08:18:13.544
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:13.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:13.558
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/05/23 08:18:13.56
STEP: Getting a ResourceQuota 01/05/23 08:18:13.57
STEP: Updating a ResourceQuota 01/05/23 08:18:13.572
STEP: Verifying a ResourceQuota was modified 01/05/23 08:18:13.587
STEP: Deleting a ResourceQuota 01/05/23 08:18:13.59
STEP: Verifying the deleted ResourceQuota 01/05/23 08:18:13.605
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 08:18:13.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8600" for this suite. 01/05/23 08:18:13.609
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":175,"skipped":3426,"failed":0}
------------------------------
â€¢ [0.076 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:18:13.544
    Jan  5 08:18:13.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename resourcequota 01/05/23 08:18:13.544
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:13.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:13.558
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/05/23 08:18:13.56
    STEP: Getting a ResourceQuota 01/05/23 08:18:13.57
    STEP: Updating a ResourceQuota 01/05/23 08:18:13.572
    STEP: Verifying a ResourceQuota was modified 01/05/23 08:18:13.587
    STEP: Deleting a ResourceQuota 01/05/23 08:18:13.59
    STEP: Verifying the deleted ResourceQuota 01/05/23 08:18:13.605
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 08:18:13.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8600" for this suite. 01/05/23 08:18:13.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:18:13.62
Jan  5 08:18:13.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:18:13.621
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:13.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:13.635
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-b5309d0c-0b5c-4fcc-a3fa-2a71f102996f 01/05/23 08:18:13.637
STEP: Creating a pod to test consume configMaps 01/05/23 08:18:13.646
Jan  5 08:18:13.651: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81" in namespace "projected-8751" to be "Succeeded or Failed"
Jan  5 08:18:13.653: INFO: Pod "pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058063ms
Jan  5 08:18:15.656: INFO: Pod "pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005169221s
Jan  5 08:18:17.656: INFO: Pod "pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004464084s
STEP: Saw pod success 01/05/23 08:18:17.656
Jan  5 08:18:17.656: INFO: Pod "pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81" satisfied condition "Succeeded or Failed"
Jan  5 08:18:17.657: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:18:17.661
Jan  5 08:18:17.672: INFO: Waiting for pod pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81 to disappear
Jan  5 08:18:17.673: INFO: Pod pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 08:18:17.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8751" for this suite. 01/05/23 08:18:17.675
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":176,"skipped":3433,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:18:13.62
    Jan  5 08:18:13.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:18:13.621
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:13.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:13.635
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-b5309d0c-0b5c-4fcc-a3fa-2a71f102996f 01/05/23 08:18:13.637
    STEP: Creating a pod to test consume configMaps 01/05/23 08:18:13.646
    Jan  5 08:18:13.651: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81" in namespace "projected-8751" to be "Succeeded or Failed"
    Jan  5 08:18:13.653: INFO: Pod "pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058063ms
    Jan  5 08:18:15.656: INFO: Pod "pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005169221s
    Jan  5 08:18:17.656: INFO: Pod "pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004464084s
    STEP: Saw pod success 01/05/23 08:18:17.656
    Jan  5 08:18:17.656: INFO: Pod "pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81" satisfied condition "Succeeded or Failed"
    Jan  5 08:18:17.657: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:18:17.661
    Jan  5 08:18:17.672: INFO: Waiting for pod pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81 to disappear
    Jan  5 08:18:17.673: INFO: Pod pod-projected-configmaps-a3f11d86-1770-411f-92ed-87879cd58f81 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 08:18:17.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8751" for this suite. 01/05/23 08:18:17.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:18:17.684
Jan  5 08:18:17.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 08:18:17.685
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:17.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:17.698
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/05/23 08:18:17.7
Jan  5 08:18:17.710: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746" in namespace "downward-api-5053" to be "Succeeded or Failed"
Jan  5 08:18:17.711: INFO: Pod "downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746": Phase="Pending", Reason="", readiness=false. Elapsed: 1.290057ms
Jan  5 08:18:19.714: INFO: Pod "downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004310197s
Jan  5 08:18:21.714: INFO: Pod "downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004423681s
STEP: Saw pod success 01/05/23 08:18:21.714
Jan  5 08:18:21.714: INFO: Pod "downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746" satisfied condition "Succeeded or Failed"
Jan  5 08:18:21.716: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746 container client-container: <nil>
STEP: delete the pod 01/05/23 08:18:21.72
Jan  5 08:18:21.735: INFO: Waiting for pod downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746 to disappear
Jan  5 08:18:21.737: INFO: Pod downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 08:18:21.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5053" for this suite. 01/05/23 08:18:21.739
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":177,"skipped":3453,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:18:17.684
    Jan  5 08:18:17.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 08:18:17.685
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:17.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:17.698
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/05/23 08:18:17.7
    Jan  5 08:18:17.710: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746" in namespace "downward-api-5053" to be "Succeeded or Failed"
    Jan  5 08:18:17.711: INFO: Pod "downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746": Phase="Pending", Reason="", readiness=false. Elapsed: 1.290057ms
    Jan  5 08:18:19.714: INFO: Pod "downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004310197s
    Jan  5 08:18:21.714: INFO: Pod "downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004423681s
    STEP: Saw pod success 01/05/23 08:18:21.714
    Jan  5 08:18:21.714: INFO: Pod "downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746" satisfied condition "Succeeded or Failed"
    Jan  5 08:18:21.716: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746 container client-container: <nil>
    STEP: delete the pod 01/05/23 08:18:21.72
    Jan  5 08:18:21.735: INFO: Waiting for pod downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746 to disappear
    Jan  5 08:18:21.737: INFO: Pod downwardapi-volume-7945a358-1109-4cde-8d61-dad9098be746 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 08:18:21.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5053" for this suite. 01/05/23 08:18:21.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:18:21.749
Jan  5 08:18:21.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sched-preemption 01/05/23 08:18:21.75
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:21.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:21.768
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  5 08:18:21.828: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 08:19:21.849: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/05/23 08:19:21.851
Jan  5 08:19:21.875: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  5 08:19:21.885: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  5 08:19:21.909: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  5 08:19:21.914: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/05/23 08:19:21.914
Jan  5 08:19:21.914: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8446" to be "running"
Jan  5 08:19:21.918: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.113962ms
Jan  5 08:19:23.922: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007485724s
Jan  5 08:19:25.922: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007722585s
Jan  5 08:19:27.923: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008745688s
Jan  5 08:19:29.922: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.007451233s
Jan  5 08:19:29.922: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  5 08:19:29.922: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8446" to be "running"
Jan  5 08:19:29.923: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.846209ms
Jan  5 08:19:29.924: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 08:19:29.924: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8446" to be "running"
Jan  5 08:19:29.925: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.697411ms
Jan  5 08:19:31.929: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005620798s
Jan  5 08:19:31.929: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 08:19:31.929: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8446" to be "running"
Jan  5 08:19:31.931: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.586562ms
Jan  5 08:19:31.931: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/05/23 08:19:31.931
Jan  5 08:19:31.935: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8446" to be "running"
Jan  5 08:19:31.936: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.587373ms
Jan  5 08:19:33.940: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005383015s
Jan  5 08:19:35.939: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004606028s
Jan  5 08:19:35.939: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:19:35.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8446" for this suite. 01/05/23 08:19:35.949
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":178,"skipped":3476,"failed":0}
------------------------------
â€¢ [SLOW TEST] [74.235 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:18:21.749
    Jan  5 08:18:21.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 08:18:21.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:18:21.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:18:21.768
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  5 08:18:21.828: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 08:19:21.849: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/05/23 08:19:21.851
    Jan  5 08:19:21.875: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  5 08:19:21.885: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  5 08:19:21.909: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  5 08:19:21.914: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/05/23 08:19:21.914
    Jan  5 08:19:21.914: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8446" to be "running"
    Jan  5 08:19:21.918: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.113962ms
    Jan  5 08:19:23.922: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007485724s
    Jan  5 08:19:25.922: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007722585s
    Jan  5 08:19:27.923: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008745688s
    Jan  5 08:19:29.922: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.007451233s
    Jan  5 08:19:29.922: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  5 08:19:29.922: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8446" to be "running"
    Jan  5 08:19:29.923: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.846209ms
    Jan  5 08:19:29.924: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 08:19:29.924: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8446" to be "running"
    Jan  5 08:19:29.925: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.697411ms
    Jan  5 08:19:31.929: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005620798s
    Jan  5 08:19:31.929: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 08:19:31.929: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8446" to be "running"
    Jan  5 08:19:31.931: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.586562ms
    Jan  5 08:19:31.931: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/05/23 08:19:31.931
    Jan  5 08:19:31.935: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8446" to be "running"
    Jan  5 08:19:31.936: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.587373ms
    Jan  5 08:19:33.940: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005383015s
    Jan  5 08:19:35.939: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004606028s
    Jan  5 08:19:35.939: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:19:35.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8446" for this suite. 01/05/23 08:19:35.949
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:19:35.985
Jan  5 08:19:35.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename svcaccounts 01/05/23 08:19:35.986
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:19:35.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:19:36
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/05/23 08:19:36.002
Jan  5 08:19:36.014: INFO: Waiting up to 5m0s for pod "test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8" in namespace "svcaccounts-4640" to be "Succeeded or Failed"
Jan  5 08:19:36.017: INFO: Pod "test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718978ms
Jan  5 08:19:38.021: INFO: Pod "test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007112371s
Jan  5 08:19:40.020: INFO: Pod "test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005776558s
STEP: Saw pod success 01/05/23 08:19:40.02
Jan  5 08:19:40.020: INFO: Pod "test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8" satisfied condition "Succeeded or Failed"
Jan  5 08:19:40.022: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:19:40.027
Jan  5 08:19:40.038: INFO: Waiting for pod test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8 to disappear
Jan  5 08:19:40.039: INFO: Pod test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 08:19:40.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4640" for this suite. 01/05/23 08:19:40.041
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":179,"skipped":3490,"failed":0}
------------------------------
â€¢ [4.060 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:19:35.985
    Jan  5 08:19:35.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 08:19:35.986
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:19:35.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:19:36
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/05/23 08:19:36.002
    Jan  5 08:19:36.014: INFO: Waiting up to 5m0s for pod "test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8" in namespace "svcaccounts-4640" to be "Succeeded or Failed"
    Jan  5 08:19:36.017: INFO: Pod "test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718978ms
    Jan  5 08:19:38.021: INFO: Pod "test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007112371s
    Jan  5 08:19:40.020: INFO: Pod "test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005776558s
    STEP: Saw pod success 01/05/23 08:19:40.02
    Jan  5 08:19:40.020: INFO: Pod "test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8" satisfied condition "Succeeded or Failed"
    Jan  5 08:19:40.022: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:19:40.027
    Jan  5 08:19:40.038: INFO: Waiting for pod test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8 to disappear
    Jan  5 08:19:40.039: INFO: Pod test-pod-8940f57f-e8ab-43a1-84ce-fe11c4cbc5f8 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 08:19:40.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4640" for this suite. 01/05/23 08:19:40.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:19:40.045
Jan  5 08:19:40.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 08:19:40.046
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:19:40.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:19:40.068
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/05/23 08:19:40.07
Jan  5 08:19:40.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: mark a version not serverd 01/05/23 08:19:46.088
STEP: check the unserved version gets removed 01/05/23 08:19:46.101
STEP: check the other version is not changed 01/05/23 08:19:48.679
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:19:54.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1332" for this suite. 01/05/23 08:19:54.378
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":180,"skipped":3511,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.336 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:19:40.045
    Jan  5 08:19:40.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 08:19:40.046
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:19:40.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:19:40.068
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/05/23 08:19:40.07
    Jan  5 08:19:40.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: mark a version not serverd 01/05/23 08:19:46.088
    STEP: check the unserved version gets removed 01/05/23 08:19:46.101
    STEP: check the other version is not changed 01/05/23 08:19:48.679
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:19:54.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1332" for this suite. 01/05/23 08:19:54.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:19:54.382
Jan  5 08:19:54.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sched-pred 01/05/23 08:19:54.383
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:19:54.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:19:54.401
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  5 08:19:54.405: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 08:19:54.409: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 08:19:54.411: INFO: 
Logging pods the apiserver thinks is on node mip-bd-vm722.mip.storage.hpecorp.net before test
Jan  5 08:19:54.424: INFO: csi-hostpathplugin-0 from default started at 2023-01-05 07:12:21 +0000 UTC (8 container statuses recorded)
Jan  5 08:19:54.424: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  5 08:19:54.424: INFO: 	Container csi-external-health-monitor-controller ready: true, restart count 1
Jan  5 08:19:54.424: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  5 08:19:54.424: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  5 08:19:54.424: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  5 08:19:54.424: INFO: 	Container hostpath ready: true, restart count 0
Jan  5 08:19:54.424: INFO: 	Container liveness-probe ready: true, restart count 0
Jan  5 08:19:54.424: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan  5 08:19:54.424: INFO: canal-6z7xb from kube-system started at 2023-01-05 07:12:02 +0000 UTC (2 container statuses recorded)
Jan  5 08:19:54.424: INFO: 	Container calico-node ready: true, restart count 0
Jan  5 08:19:54.424: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  5 08:19:54.424: INFO: coredns-564fd8c776-nwmff from kube-system started at 2023-01-05 07:12:21 +0000 UTC (1 container statuses recorded)
Jan  5 08:19:54.424: INFO: 	Container coredns ready: true, restart count 0
Jan  5 08:19:54.424: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2p88t from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
Jan  5 08:19:54.424: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 08:19:54.424: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 08:19:54.424: INFO: 
Logging pods the apiserver thinks is on node mip-bd-vm724.mip.storage.hpecorp.net before test
Jan  5 08:19:54.429: INFO: canal-6sfgp from kube-system started at 2023-01-05 07:21:40 +0000 UTC (2 container statuses recorded)
Jan  5 08:19:54.429: INFO: 	Container calico-node ready: true, restart count 0
Jan  5 08:19:54.429: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  5 08:19:54.429: INFO: sonobuoy from sonobuoy started at 2023-01-05 07:22:09 +0000 UTC (1 container statuses recorded)
Jan  5 08:19:54.429: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 08:19:54.429: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2qcx8 from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
Jan  5 08:19:54.429: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 08:19:54.429: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node mip-bd-vm722.mip.storage.hpecorp.net 01/05/23 08:19:54.441
STEP: verifying the node has the label node mip-bd-vm724.mip.storage.hpecorp.net 01/05/23 08:19:54.456
Jan  5 08:19:54.466: INFO: Pod csi-hostpathplugin-0 requesting resource cpu=0m on Node mip-bd-vm722.mip.storage.hpecorp.net
Jan  5 08:19:54.466: INFO: Pod canal-6sfgp requesting resource cpu=250m on Node mip-bd-vm724.mip.storage.hpecorp.net
Jan  5 08:19:54.466: INFO: Pod canal-6z7xb requesting resource cpu=250m on Node mip-bd-vm722.mip.storage.hpecorp.net
Jan  5 08:19:54.466: INFO: Pod coredns-564fd8c776-nwmff requesting resource cpu=100m on Node mip-bd-vm722.mip.storage.hpecorp.net
Jan  5 08:19:54.466: INFO: Pod sonobuoy requesting resource cpu=0m on Node mip-bd-vm724.mip.storage.hpecorp.net
Jan  5 08:19:54.466: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2p88t requesting resource cpu=0m on Node mip-bd-vm722.mip.storage.hpecorp.net
Jan  5 08:19:54.466: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2qcx8 requesting resource cpu=0m on Node mip-bd-vm724.mip.storage.hpecorp.net
STEP: Starting Pods to consume most of the cluster CPU. 01/05/23 08:19:54.466
Jan  5 08:19:54.466: INFO: Creating a pod which consumes cpu=5355m on Node mip-bd-vm722.mip.storage.hpecorp.net
Jan  5 08:19:54.473: INFO: Creating a pod which consumes cpu=5425m on Node mip-bd-vm724.mip.storage.hpecorp.net
Jan  5 08:19:54.485: INFO: Waiting up to 5m0s for pod "filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd" in namespace "sched-pred-1675" to be "running"
Jan  5 08:19:54.490: INFO: Pod "filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.507002ms
Jan  5 08:19:56.494: INFO: Pod "filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008799747s
Jan  5 08:19:58.494: INFO: Pod "filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd": Phase="Running", Reason="", readiness=true. Elapsed: 4.008647032s
Jan  5 08:19:58.494: INFO: Pod "filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd" satisfied condition "running"
Jan  5 08:19:58.494: INFO: Waiting up to 5m0s for pod "filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11" in namespace "sched-pred-1675" to be "running"
Jan  5 08:19:58.496: INFO: Pod "filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11": Phase="Running", Reason="", readiness=true. Elapsed: 1.866922ms
Jan  5 08:19:58.496: INFO: Pod "filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/05/23 08:19:58.496
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11.17375bddbc707d62], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1675/filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11 to mip-bd-vm724.mip.storage.hpecorp.net] 01/05/23 08:19:58.498
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11.17375bdddc8ef0f7], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 01/05/23 08:19:58.498
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11.17375bde020e0487], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 629.069928ms] 01/05/23 08:19:58.499
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11.17375bde04bcc678], Reason = [Created], Message = [Created container filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11] 01/05/23 08:19:58.499
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11.17375bde0932e605], Reason = [Started], Message = [Started container filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11] 01/05/23 08:19:58.499
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd.17375bddbbc5cf39], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1675/filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd to mip-bd-vm722.mip.storage.hpecorp.net] 01/05/23 08:19:58.499
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd.17375bdddd114e24], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 01/05/23 08:19:58.499
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd.17375bde0116a070], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 604.309391ms] 01/05/23 08:19:58.499
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd.17375bde05497457], Reason = [Created], Message = [Created container filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd] 01/05/23 08:19:58.499
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd.17375bde0b051bb3], Reason = [Started], Message = [Started container filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd] 01/05/23 08:19:58.499
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17375bdeab3249dc], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.] 01/05/23 08:19:58.528
STEP: removing the label node off the node mip-bd-vm722.mip.storage.hpecorp.net 01/05/23 08:19:59.506
STEP: verifying the node doesn't have the label node 01/05/23 08:19:59.52
STEP: removing the label node off the node mip-bd-vm724.mip.storage.hpecorp.net 01/05/23 08:19:59.522
STEP: verifying the node doesn't have the label node 01/05/23 08:19:59.533
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:19:59.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1675" for this suite. 01/05/23 08:19:59.537
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":181,"skipped":3521,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.159 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:19:54.382
    Jan  5 08:19:54.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sched-pred 01/05/23 08:19:54.383
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:19:54.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:19:54.401
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  5 08:19:54.405: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 08:19:54.409: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 08:19:54.411: INFO: 
    Logging pods the apiserver thinks is on node mip-bd-vm722.mip.storage.hpecorp.net before test
    Jan  5 08:19:54.424: INFO: csi-hostpathplugin-0 from default started at 2023-01-05 07:12:21 +0000 UTC (8 container statuses recorded)
    Jan  5 08:19:54.424: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: 	Container csi-external-health-monitor-controller ready: true, restart count 1
    Jan  5 08:19:54.424: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: 	Container hostpath ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: canal-6z7xb from kube-system started at 2023-01-05 07:12:02 +0000 UTC (2 container statuses recorded)
    Jan  5 08:19:54.424: INFO: 	Container calico-node ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: coredns-564fd8c776-nwmff from kube-system started at 2023-01-05 07:12:21 +0000 UTC (1 container statuses recorded)
    Jan  5 08:19:54.424: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2p88t from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
    Jan  5 08:19:54.424: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 08:19:54.424: INFO: 
    Logging pods the apiserver thinks is on node mip-bd-vm724.mip.storage.hpecorp.net before test
    Jan  5 08:19:54.429: INFO: canal-6sfgp from kube-system started at 2023-01-05 07:21:40 +0000 UTC (2 container statuses recorded)
    Jan  5 08:19:54.429: INFO: 	Container calico-node ready: true, restart count 0
    Jan  5 08:19:54.429: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  5 08:19:54.429: INFO: sonobuoy from sonobuoy started at 2023-01-05 07:22:09 +0000 UTC (1 container statuses recorded)
    Jan  5 08:19:54.429: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 08:19:54.429: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2qcx8 from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
    Jan  5 08:19:54.429: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 08:19:54.429: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node mip-bd-vm722.mip.storage.hpecorp.net 01/05/23 08:19:54.441
    STEP: verifying the node has the label node mip-bd-vm724.mip.storage.hpecorp.net 01/05/23 08:19:54.456
    Jan  5 08:19:54.466: INFO: Pod csi-hostpathplugin-0 requesting resource cpu=0m on Node mip-bd-vm722.mip.storage.hpecorp.net
    Jan  5 08:19:54.466: INFO: Pod canal-6sfgp requesting resource cpu=250m on Node mip-bd-vm724.mip.storage.hpecorp.net
    Jan  5 08:19:54.466: INFO: Pod canal-6z7xb requesting resource cpu=250m on Node mip-bd-vm722.mip.storage.hpecorp.net
    Jan  5 08:19:54.466: INFO: Pod coredns-564fd8c776-nwmff requesting resource cpu=100m on Node mip-bd-vm722.mip.storage.hpecorp.net
    Jan  5 08:19:54.466: INFO: Pod sonobuoy requesting resource cpu=0m on Node mip-bd-vm724.mip.storage.hpecorp.net
    Jan  5 08:19:54.466: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2p88t requesting resource cpu=0m on Node mip-bd-vm722.mip.storage.hpecorp.net
    Jan  5 08:19:54.466: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2qcx8 requesting resource cpu=0m on Node mip-bd-vm724.mip.storage.hpecorp.net
    STEP: Starting Pods to consume most of the cluster CPU. 01/05/23 08:19:54.466
    Jan  5 08:19:54.466: INFO: Creating a pod which consumes cpu=5355m on Node mip-bd-vm722.mip.storage.hpecorp.net
    Jan  5 08:19:54.473: INFO: Creating a pod which consumes cpu=5425m on Node mip-bd-vm724.mip.storage.hpecorp.net
    Jan  5 08:19:54.485: INFO: Waiting up to 5m0s for pod "filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd" in namespace "sched-pred-1675" to be "running"
    Jan  5 08:19:54.490: INFO: Pod "filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.507002ms
    Jan  5 08:19:56.494: INFO: Pod "filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008799747s
    Jan  5 08:19:58.494: INFO: Pod "filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd": Phase="Running", Reason="", readiness=true. Elapsed: 4.008647032s
    Jan  5 08:19:58.494: INFO: Pod "filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd" satisfied condition "running"
    Jan  5 08:19:58.494: INFO: Waiting up to 5m0s for pod "filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11" in namespace "sched-pred-1675" to be "running"
    Jan  5 08:19:58.496: INFO: Pod "filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11": Phase="Running", Reason="", readiness=true. Elapsed: 1.866922ms
    Jan  5 08:19:58.496: INFO: Pod "filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/05/23 08:19:58.496
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11.17375bddbc707d62], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1675/filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11 to mip-bd-vm724.mip.storage.hpecorp.net] 01/05/23 08:19:58.498
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11.17375bdddc8ef0f7], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 01/05/23 08:19:58.498
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11.17375bde020e0487], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 629.069928ms] 01/05/23 08:19:58.499
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11.17375bde04bcc678], Reason = [Created], Message = [Created container filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11] 01/05/23 08:19:58.499
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11.17375bde0932e605], Reason = [Started], Message = [Started container filler-pod-7c6d8346-655d-4a8c-839b-b37fecbc8a11] 01/05/23 08:19:58.499
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd.17375bddbbc5cf39], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1675/filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd to mip-bd-vm722.mip.storage.hpecorp.net] 01/05/23 08:19:58.499
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd.17375bdddd114e24], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 01/05/23 08:19:58.499
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd.17375bde0116a070], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 604.309391ms] 01/05/23 08:19:58.499
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd.17375bde05497457], Reason = [Created], Message = [Created container filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd] 01/05/23 08:19:58.499
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd.17375bde0b051bb3], Reason = [Started], Message = [Started container filler-pod-df9e60ca-bdcc-4ee3-be20-893c139aefcd] 01/05/23 08:19:58.499
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17375bdeab3249dc], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.] 01/05/23 08:19:58.528
    STEP: removing the label node off the node mip-bd-vm722.mip.storage.hpecorp.net 01/05/23 08:19:59.506
    STEP: verifying the node doesn't have the label node 01/05/23 08:19:59.52
    STEP: removing the label node off the node mip-bd-vm724.mip.storage.hpecorp.net 01/05/23 08:19:59.522
    STEP: verifying the node doesn't have the label node 01/05/23 08:19:59.533
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:19:59.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1675" for this suite. 01/05/23 08:19:59.537
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:19:59.542
Jan  5 08:19:59.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename var-expansion 01/05/23 08:19:59.543
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:19:59.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:19:59.564
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan  5 08:19:59.592: INFO: Waiting up to 2m0s for pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1" in namespace "var-expansion-8450" to be "container 0 failed with reason CreateContainerConfigError"
Jan  5 08:19:59.594: INFO: Pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.207702ms
Jan  5 08:20:01.597: INFO: Pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004867154s
Jan  5 08:20:01.597: INFO: Pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  5 08:20:01.597: INFO: Deleting pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1" in namespace "var-expansion-8450"
Jan  5 08:20:01.609: INFO: Wait up to 5m0s for pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 08:20:03.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8450" for this suite. 01/05/23 08:20:03.617
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":182,"skipped":3540,"failed":0}
------------------------------
â€¢ [4.080 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:19:59.542
    Jan  5 08:19:59.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename var-expansion 01/05/23 08:19:59.543
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:19:59.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:19:59.564
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan  5 08:19:59.592: INFO: Waiting up to 2m0s for pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1" in namespace "var-expansion-8450" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  5 08:19:59.594: INFO: Pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.207702ms
    Jan  5 08:20:01.597: INFO: Pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004867154s
    Jan  5 08:20:01.597: INFO: Pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  5 08:20:01.597: INFO: Deleting pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1" in namespace "var-expansion-8450"
    Jan  5 08:20:01.609: INFO: Wait up to 5m0s for pod "var-expansion-0d30be30-a307-4890-8bab-1669d43fd9e1" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 08:20:03.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8450" for this suite. 01/05/23 08:20:03.617
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:20:03.622
Jan  5 08:20:03.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename svcaccounts 01/05/23 08:20:03.623
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:03.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:03.648
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/05/23 08:20:03.652
STEP: watching for the ServiceAccount to be added 01/05/23 08:20:03.658
STEP: patching the ServiceAccount 01/05/23 08:20:03.658
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/05/23 08:20:03.67
STEP: deleting the ServiceAccount 01/05/23 08:20:03.674
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 08:20:03.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1666" for this suite. 01/05/23 08:20:03.687
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":183,"skipped":3542,"failed":0}
------------------------------
â€¢ [0.076 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:20:03.622
    Jan  5 08:20:03.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 08:20:03.623
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:03.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:03.648
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/05/23 08:20:03.652
    STEP: watching for the ServiceAccount to be added 01/05/23 08:20:03.658
    STEP: patching the ServiceAccount 01/05/23 08:20:03.658
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/05/23 08:20:03.67
    STEP: deleting the ServiceAccount 01/05/23 08:20:03.674
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 08:20:03.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1666" for this suite. 01/05/23 08:20:03.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:20:03.7
Jan  5 08:20:03.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-runtime 01/05/23 08:20:03.701
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:03.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:03.72
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/05/23 08:20:03.722
STEP: wait for the container to reach Succeeded 01/05/23 08:20:03.733
STEP: get the container status 01/05/23 08:20:07.747
STEP: the container should be terminated 01/05/23 08:20:07.749
STEP: the termination message should be set 01/05/23 08:20:07.749
Jan  5 08:20:07.749: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/05/23 08:20:07.749
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  5 08:20:07.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2049" for this suite. 01/05/23 08:20:07.768
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":184,"skipped":3588,"failed":0}
------------------------------
â€¢ [4.075 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:20:03.7
    Jan  5 08:20:03.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-runtime 01/05/23 08:20:03.701
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:03.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:03.72
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/05/23 08:20:03.722
    STEP: wait for the container to reach Succeeded 01/05/23 08:20:03.733
    STEP: get the container status 01/05/23 08:20:07.747
    STEP: the container should be terminated 01/05/23 08:20:07.749
    STEP: the termination message should be set 01/05/23 08:20:07.749
    Jan  5 08:20:07.749: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/05/23 08:20:07.749
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  5 08:20:07.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2049" for this suite. 01/05/23 08:20:07.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:20:07.776
Jan  5 08:20:07.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 08:20:07.776
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:07.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:07.793
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/05/23 08:20:07.795
Jan  5 08:20:07.804: INFO: Waiting up to 5m0s for pod "downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a" in namespace "downward-api-7566" to be "Succeeded or Failed"
Jan  5 08:20:07.805: INFO: Pod "downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.616442ms
Jan  5 08:20:09.808: INFO: Pod "downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004160474s
Jan  5 08:20:11.808: INFO: Pod "downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004408237s
STEP: Saw pod success 01/05/23 08:20:11.808
Jan  5 08:20:11.808: INFO: Pod "downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a" satisfied condition "Succeeded or Failed"
Jan  5 08:20:11.810: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a container dapi-container: <nil>
STEP: delete the pod 01/05/23 08:20:11.815
Jan  5 08:20:11.825: INFO: Waiting for pod downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a to disappear
Jan  5 08:20:11.827: INFO: Pod downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  5 08:20:11.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7566" for this suite. 01/05/23 08:20:11.829
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":185,"skipped":3608,"failed":0}
------------------------------
â€¢ [4.057 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:20:07.776
    Jan  5 08:20:07.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 08:20:07.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:07.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:07.793
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/05/23 08:20:07.795
    Jan  5 08:20:07.804: INFO: Waiting up to 5m0s for pod "downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a" in namespace "downward-api-7566" to be "Succeeded or Failed"
    Jan  5 08:20:07.805: INFO: Pod "downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.616442ms
    Jan  5 08:20:09.808: INFO: Pod "downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004160474s
    Jan  5 08:20:11.808: INFO: Pod "downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004408237s
    STEP: Saw pod success 01/05/23 08:20:11.808
    Jan  5 08:20:11.808: INFO: Pod "downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a" satisfied condition "Succeeded or Failed"
    Jan  5 08:20:11.810: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a container dapi-container: <nil>
    STEP: delete the pod 01/05/23 08:20:11.815
    Jan  5 08:20:11.825: INFO: Waiting for pod downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a to disappear
    Jan  5 08:20:11.827: INFO: Pod downward-api-cdc7952c-975e-48ce-97dc-c421abf2202a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  5 08:20:11.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7566" for this suite. 01/05/23 08:20:11.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:20:11.834
Jan  5 08:20:11.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename runtimeclass 01/05/23 08:20:11.835
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:11.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:11.848
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan  5 08:20:11.864: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3254 to be scheduled
Jan  5 08:20:11.865: INFO: 1 pods are not scheduled: [runtimeclass-3254/test-runtimeclass-runtimeclass-3254-preconfigured-handler-r8wpn(3f4823e9-0606-467a-bdfe-e952476ab6c3)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  5 08:20:13.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3254" for this suite. 01/05/23 08:20:13.873
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":186,"skipped":3678,"failed":0}
------------------------------
â€¢ [2.050 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:20:11.834
    Jan  5 08:20:11.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 08:20:11.835
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:11.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:11.848
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan  5 08:20:11.864: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3254 to be scheduled
    Jan  5 08:20:11.865: INFO: 1 pods are not scheduled: [runtimeclass-3254/test-runtimeclass-runtimeclass-3254-preconfigured-handler-r8wpn(3f4823e9-0606-467a-bdfe-e952476ab6c3)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  5 08:20:13.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3254" for this suite. 01/05/23 08:20:13.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:20:13.888
Jan  5 08:20:13.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename daemonsets 01/05/23 08:20:13.889
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:13.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:13.901
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan  5 08:20:13.920: INFO: Create a RollingUpdate DaemonSet
Jan  5 08:20:13.935: INFO: Check that daemon pods launch on every node of the cluster
Jan  5 08:20:13.937: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:20:13.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:20:13.940: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:20:14.943: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:20:14.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:20:14.945: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:20:15.943: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:20:15.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 08:20:15.951: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan  5 08:20:15.951: INFO: Update the DaemonSet to trigger a rollout
Jan  5 08:20:15.957: INFO: Updating DaemonSet daemon-set
Jan  5 08:20:18.966: INFO: Roll back the DaemonSet before rollout is complete
Jan  5 08:20:18.971: INFO: Updating DaemonSet daemon-set
Jan  5 08:20:18.971: INFO: Make sure DaemonSet rollback is complete
Jan  5 08:20:18.973: INFO: Wrong image for pod: daemon-set-l7fct. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan  5 08:20:18.973: INFO: Pod daemon-set-l7fct is not available
Jan  5 08:20:18.980: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:20:19.989: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:20:20.985: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:20:21.983: INFO: Pod daemon-set-txjnq is not available
Jan  5 08:20:21.985: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 08:20:21.989
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9869, will wait for the garbage collector to delete the pods 01/05/23 08:20:21.989
Jan  5 08:20:22.046: INFO: Deleting DaemonSet.extensions daemon-set took: 4.966077ms
Jan  5 08:20:22.147: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.972075ms
Jan  5 08:20:24.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:20:24.656: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 08:20:24.657: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23216"},"items":null}

Jan  5 08:20:24.659: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23216"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:20:24.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9869" for this suite. 01/05/23 08:20:24.668
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":187,"skipped":3732,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.784 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:20:13.888
    Jan  5 08:20:13.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename daemonsets 01/05/23 08:20:13.889
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:13.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:13.901
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan  5 08:20:13.920: INFO: Create a RollingUpdate DaemonSet
    Jan  5 08:20:13.935: INFO: Check that daemon pods launch on every node of the cluster
    Jan  5 08:20:13.937: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:20:13.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:20:13.940: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:20:14.943: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:20:14.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:20:14.945: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:20:15.943: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:20:15.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 08:20:15.951: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan  5 08:20:15.951: INFO: Update the DaemonSet to trigger a rollout
    Jan  5 08:20:15.957: INFO: Updating DaemonSet daemon-set
    Jan  5 08:20:18.966: INFO: Roll back the DaemonSet before rollout is complete
    Jan  5 08:20:18.971: INFO: Updating DaemonSet daemon-set
    Jan  5 08:20:18.971: INFO: Make sure DaemonSet rollback is complete
    Jan  5 08:20:18.973: INFO: Wrong image for pod: daemon-set-l7fct. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan  5 08:20:18.973: INFO: Pod daemon-set-l7fct is not available
    Jan  5 08:20:18.980: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:20:19.989: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:20:20.985: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:20:21.983: INFO: Pod daemon-set-txjnq is not available
    Jan  5 08:20:21.985: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 08:20:21.989
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9869, will wait for the garbage collector to delete the pods 01/05/23 08:20:21.989
    Jan  5 08:20:22.046: INFO: Deleting DaemonSet.extensions daemon-set took: 4.966077ms
    Jan  5 08:20:22.147: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.972075ms
    Jan  5 08:20:24.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:20:24.656: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 08:20:24.657: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23216"},"items":null}

    Jan  5 08:20:24.659: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23216"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:20:24.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9869" for this suite. 01/05/23 08:20:24.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:20:24.675
Jan  5 08:20:24.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sysctl 01/05/23 08:20:24.677
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:24.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:24.699
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/05/23 08:20:24.7
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 08:20:24.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-189" for this suite. 01/05/23 08:20:24.704
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":188,"skipped":3747,"failed":0}
------------------------------
â€¢ [0.032 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:20:24.675
    Jan  5 08:20:24.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sysctl 01/05/23 08:20:24.677
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:24.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:24.699
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/05/23 08:20:24.7
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 08:20:24.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-189" for this suite. 01/05/23 08:20:24.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:20:24.71
Jan  5 08:20:24.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pod-network-test 01/05/23 08:20:24.71
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:24.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:24.732
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-7810 01/05/23 08:20:24.734
STEP: creating a selector 01/05/23 08:20:24.734
STEP: Creating the service pods in kubernetes 01/05/23 08:20:24.734
Jan  5 08:20:24.734: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 08:20:24.766: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7810" to be "running and ready"
Jan  5 08:20:24.779: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.30365ms
Jan  5 08:20:24.779: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:20:26.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01592183s
Jan  5 08:20:26.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:20:28.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016009725s
Jan  5 08:20:28.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:20:30.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015587293s
Jan  5 08:20:30.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:20:32.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016382128s
Jan  5 08:20:32.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:20:34.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016279151s
Jan  5 08:20:34.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:20:36.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015426156s
Jan  5 08:20:36.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:20:38.784: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017686744s
Jan  5 08:20:38.784: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:20:40.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01573276s
Jan  5 08:20:40.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:20:42.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.017048145s
Jan  5 08:20:42.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:20:44.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01656265s
Jan  5 08:20:44.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 08:20:46.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015887658s
Jan  5 08:20:46.782: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 08:20:46.782: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 08:20:46.785: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7810" to be "running and ready"
Jan  5 08:20:46.786: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.715975ms
Jan  5 08:20:46.786: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 08:20:46.786: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 08:20:46.788
Jan  5 08:20:46.793: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7810" to be "running"
Jan  5 08:20:46.794: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.243847ms
Jan  5 08:20:48.799: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00631047s
Jan  5 08:20:48.799: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 08:20:48.801: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan  5 08:20:48.801: INFO: Breadth first check of 10.244.0.169 on host 16.0.14.212...
Jan  5 08:20:48.803: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.69:9080/dial?request=hostname&protocol=udp&host=10.244.0.169&port=8081&tries=1'] Namespace:pod-network-test-7810 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:20:48.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:20:48.803: INFO: ExecWithOptions: Clientset creation
Jan  5 08:20:48.803: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7810/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.169%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 08:20:48.870: INFO: Waiting for responses: map[]
Jan  5 08:20:48.870: INFO: reached 10.244.0.169 after 0/1 tries
Jan  5 08:20:48.870: INFO: Breadth first check of 10.244.1.68 on host 16.0.14.214...
Jan  5 08:20:48.873: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.69:9080/dial?request=hostname&protocol=udp&host=10.244.1.68&port=8081&tries=1'] Namespace:pod-network-test-7810 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:20:48.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:20:48.873: INFO: ExecWithOptions: Clientset creation
Jan  5 08:20:48.873: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7810/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.68%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 08:20:48.936: INFO: Waiting for responses: map[]
Jan  5 08:20:48.936: INFO: reached 10.244.1.68 after 0/1 tries
Jan  5 08:20:48.936: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  5 08:20:48.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7810" for this suite. 01/05/23 08:20:48.94
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":189,"skipped":3823,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.234 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:20:24.71
    Jan  5 08:20:24.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 08:20:24.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:24.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:24.732
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-7810 01/05/23 08:20:24.734
    STEP: creating a selector 01/05/23 08:20:24.734
    STEP: Creating the service pods in kubernetes 01/05/23 08:20:24.734
    Jan  5 08:20:24.734: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 08:20:24.766: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7810" to be "running and ready"
    Jan  5 08:20:24.779: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.30365ms
    Jan  5 08:20:24.779: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:20:26.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01592183s
    Jan  5 08:20:26.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:20:28.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016009725s
    Jan  5 08:20:28.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:20:30.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015587293s
    Jan  5 08:20:30.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:20:32.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016382128s
    Jan  5 08:20:32.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:20:34.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016279151s
    Jan  5 08:20:34.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:20:36.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015426156s
    Jan  5 08:20:36.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:20:38.784: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017686744s
    Jan  5 08:20:38.784: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:20:40.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01573276s
    Jan  5 08:20:40.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:20:42.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.017048145s
    Jan  5 08:20:42.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:20:44.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01656265s
    Jan  5 08:20:44.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 08:20:46.782: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015887658s
    Jan  5 08:20:46.782: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 08:20:46.782: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 08:20:46.785: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7810" to be "running and ready"
    Jan  5 08:20:46.786: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.715975ms
    Jan  5 08:20:46.786: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 08:20:46.786: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 08:20:46.788
    Jan  5 08:20:46.793: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7810" to be "running"
    Jan  5 08:20:46.794: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.243847ms
    Jan  5 08:20:48.799: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00631047s
    Jan  5 08:20:48.799: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 08:20:48.801: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan  5 08:20:48.801: INFO: Breadth first check of 10.244.0.169 on host 16.0.14.212...
    Jan  5 08:20:48.803: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.69:9080/dial?request=hostname&protocol=udp&host=10.244.0.169&port=8081&tries=1'] Namespace:pod-network-test-7810 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:20:48.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:20:48.803: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:20:48.803: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7810/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.169%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 08:20:48.870: INFO: Waiting for responses: map[]
    Jan  5 08:20:48.870: INFO: reached 10.244.0.169 after 0/1 tries
    Jan  5 08:20:48.870: INFO: Breadth first check of 10.244.1.68 on host 16.0.14.214...
    Jan  5 08:20:48.873: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.69:9080/dial?request=hostname&protocol=udp&host=10.244.1.68&port=8081&tries=1'] Namespace:pod-network-test-7810 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:20:48.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:20:48.873: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:20:48.873: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7810/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.68%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 08:20:48.936: INFO: Waiting for responses: map[]
    Jan  5 08:20:48.936: INFO: reached 10.244.1.68 after 0/1 tries
    Jan  5 08:20:48.936: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  5 08:20:48.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7810" for this suite. 01/05/23 08:20:48.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:20:48.946
Jan  5 08:20:48.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename var-expansion 01/05/23 08:20:48.947
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:48.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:48.967
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/05/23 08:20:48.97
Jan  5 08:20:48.975: INFO: Waiting up to 5m0s for pod "var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf" in namespace "var-expansion-8940" to be "Succeeded or Failed"
Jan  5 08:20:48.977: INFO: Pod "var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.420105ms
Jan  5 08:20:50.981: INFO: Pod "var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0056248s
Jan  5 08:20:52.980: INFO: Pod "var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004753872s
STEP: Saw pod success 01/05/23 08:20:52.98
Jan  5 08:20:52.980: INFO: Pod "var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf" satisfied condition "Succeeded or Failed"
Jan  5 08:20:52.983: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf container dapi-container: <nil>
STEP: delete the pod 01/05/23 08:20:52.987
Jan  5 08:20:52.997: INFO: Waiting for pod var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf to disappear
Jan  5 08:20:52.999: INFO: Pod var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 08:20:52.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8940" for this suite. 01/05/23 08:20:53.001
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":190,"skipped":3829,"failed":0}
------------------------------
â€¢ [4.060 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:20:48.946
    Jan  5 08:20:48.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename var-expansion 01/05/23 08:20:48.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:48.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:48.967
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/05/23 08:20:48.97
    Jan  5 08:20:48.975: INFO: Waiting up to 5m0s for pod "var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf" in namespace "var-expansion-8940" to be "Succeeded or Failed"
    Jan  5 08:20:48.977: INFO: Pod "var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.420105ms
    Jan  5 08:20:50.981: INFO: Pod "var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0056248s
    Jan  5 08:20:52.980: INFO: Pod "var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004753872s
    STEP: Saw pod success 01/05/23 08:20:52.98
    Jan  5 08:20:52.980: INFO: Pod "var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf" satisfied condition "Succeeded or Failed"
    Jan  5 08:20:52.983: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf container dapi-container: <nil>
    STEP: delete the pod 01/05/23 08:20:52.987
    Jan  5 08:20:52.997: INFO: Waiting for pod var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf to disappear
    Jan  5 08:20:52.999: INFO: Pod var-expansion-86e0683d-4955-4f57-899b-4ef918a4dbbf no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 08:20:52.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8940" for this suite. 01/05/23 08:20:53.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:20:53.007
Jan  5 08:20:53.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename namespaces 01/05/23 08:20:53.008
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:53.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:53.029
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/05/23 08:20:53.031
STEP: patching the Namespace 01/05/23 08:20:53.043
STEP: get the Namespace and ensuring it has the label 01/05/23 08:20:53.052
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:20:53.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4056" for this suite. 01/05/23 08:20:53.058
STEP: Destroying namespace "nspatchtest-c56ef786-5cad-4aca-9154-f675162c288b-8794" for this suite. 01/05/23 08:20:53.062
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":191,"skipped":3862,"failed":0}
------------------------------
â€¢ [0.063 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:20:53.007
    Jan  5 08:20:53.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename namespaces 01/05/23 08:20:53.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:53.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:53.029
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/05/23 08:20:53.031
    STEP: patching the Namespace 01/05/23 08:20:53.043
    STEP: get the Namespace and ensuring it has the label 01/05/23 08:20:53.052
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:20:53.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4056" for this suite. 01/05/23 08:20:53.058
    STEP: Destroying namespace "nspatchtest-c56ef786-5cad-4aca-9154-f675162c288b-8794" for this suite. 01/05/23 08:20:53.062
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:20:53.07
Jan  5 08:20:53.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename daemonsets 01/05/23 08:20:53.072
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:53.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:53.1
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan  5 08:20:53.117: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/05/23 08:20:53.121
Jan  5 08:20:53.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:20:53.123: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/05/23 08:20:53.123
Jan  5 08:20:53.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:20:53.146: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:20:54.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:20:54.155: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:20:55.149: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 08:20:55.149: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/05/23 08:20:55.151
Jan  5 08:20:55.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 08:20:55.160: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan  5 08:20:56.162: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:20:56.162: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/05/23 08:20:56.162
Jan  5 08:20:56.173: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:20:56.173: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:20:57.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:20:57.176: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:20:58.180: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:20:58.180: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:20:59.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:20:59.176: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:21:00.175: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 08:21:00.175: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/05/23 08:21:00.179
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7874, will wait for the garbage collector to delete the pods 01/05/23 08:21:00.179
Jan  5 08:21:00.237: INFO: Deleting DaemonSet.extensions daemon-set took: 4.737737ms
Jan  5 08:21:00.337: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.826373ms
Jan  5 08:21:02.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:21:02.640: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 08:21:02.642: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23493"},"items":null}

Jan  5 08:21:02.643: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23493"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:21:02.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7874" for this suite. 01/05/23 08:21:02.656
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":192,"skipped":3862,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.605 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:20:53.07
    Jan  5 08:20:53.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename daemonsets 01/05/23 08:20:53.072
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:20:53.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:20:53.1
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan  5 08:20:53.117: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/05/23 08:20:53.121
    Jan  5 08:20:53.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:20:53.123: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/05/23 08:20:53.123
    Jan  5 08:20:53.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:20:53.146: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:20:54.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:20:54.155: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:20:55.149: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 08:20:55.149: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/05/23 08:20:55.151
    Jan  5 08:20:55.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 08:20:55.160: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan  5 08:20:56.162: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:20:56.162: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/05/23 08:20:56.162
    Jan  5 08:20:56.173: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:20:56.173: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:20:57.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:20:57.176: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:20:58.180: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:20:58.180: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:20:59.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:20:59.176: INFO: Node mip-bd-vm724.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:21:00.175: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 08:21:00.175: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 08:21:00.179
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7874, will wait for the garbage collector to delete the pods 01/05/23 08:21:00.179
    Jan  5 08:21:00.237: INFO: Deleting DaemonSet.extensions daemon-set took: 4.737737ms
    Jan  5 08:21:00.337: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.826373ms
    Jan  5 08:21:02.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:21:02.640: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 08:21:02.642: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23493"},"items":null}

    Jan  5 08:21:02.643: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23493"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:21:02.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7874" for this suite. 01/05/23 08:21:02.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:21:02.675
Jan  5 08:21:02.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:21:02.676
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:02.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:02.694
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:21:02.708
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:21:03.372
STEP: Deploying the webhook pod 01/05/23 08:21:03.379
STEP: Wait for the deployment to be ready 01/05/23 08:21:03.398
Jan  5 08:21:03.401: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan  5 08:21:05.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 21, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 21, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 21, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 21, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/05/23 08:21:07.413
STEP: Verifying the service has paired with the endpoint 01/05/23 08:21:07.429
Jan  5 08:21:08.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/05/23 08:21:08.432
STEP: create a pod that should be updated by the webhook 01/05/23 08:21:08.444
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:21:08.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5775" for this suite. 01/05/23 08:21:08.472
STEP: Destroying namespace "webhook-5775-markers" for this suite. 01/05/23 08:21:08.48
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":193,"skipped":3867,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.864 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:21:02.675
    Jan  5 08:21:02.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:21:02.676
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:02.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:02.694
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:21:02.708
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:21:03.372
    STEP: Deploying the webhook pod 01/05/23 08:21:03.379
    STEP: Wait for the deployment to be ready 01/05/23 08:21:03.398
    Jan  5 08:21:03.401: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan  5 08:21:05.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 21, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 21, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 21, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 21, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/05/23 08:21:07.413
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:21:07.429
    Jan  5 08:21:08.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/05/23 08:21:08.432
    STEP: create a pod that should be updated by the webhook 01/05/23 08:21:08.444
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:21:08.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5775" for this suite. 01/05/23 08:21:08.472
    STEP: Destroying namespace "webhook-5775-markers" for this suite. 01/05/23 08:21:08.48
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:21:08.541
Jan  5 08:21:08.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 08:21:08.541
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:08.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:08.569
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/05/23 08:21:08.57
Jan  5 08:21:08.575: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c" in namespace "downward-api-1188" to be "Succeeded or Failed"
Jan  5 08:21:08.576: INFO: Pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.322969ms
Jan  5 08:21:10.579: INFO: Pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004348905s
Jan  5 08:21:12.581: INFO: Pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006010743s
Jan  5 08:21:14.581: INFO: Pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005789806s
STEP: Saw pod success 01/05/23 08:21:14.581
Jan  5 08:21:14.581: INFO: Pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c" satisfied condition "Succeeded or Failed"
Jan  5 08:21:14.583: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c container client-container: <nil>
STEP: delete the pod 01/05/23 08:21:14.587
Jan  5 08:21:14.603: INFO: Waiting for pod downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c to disappear
Jan  5 08:21:14.604: INFO: Pod downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 08:21:14.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1188" for this suite. 01/05/23 08:21:14.607
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":194,"skipped":3914,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.073 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:21:08.541
    Jan  5 08:21:08.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 08:21:08.541
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:08.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:08.569
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/05/23 08:21:08.57
    Jan  5 08:21:08.575: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c" in namespace "downward-api-1188" to be "Succeeded or Failed"
    Jan  5 08:21:08.576: INFO: Pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.322969ms
    Jan  5 08:21:10.579: INFO: Pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004348905s
    Jan  5 08:21:12.581: INFO: Pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006010743s
    Jan  5 08:21:14.581: INFO: Pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005789806s
    STEP: Saw pod success 01/05/23 08:21:14.581
    Jan  5 08:21:14.581: INFO: Pod "downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c" satisfied condition "Succeeded or Failed"
    Jan  5 08:21:14.583: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c container client-container: <nil>
    STEP: delete the pod 01/05/23 08:21:14.587
    Jan  5 08:21:14.603: INFO: Waiting for pod downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c to disappear
    Jan  5 08:21:14.604: INFO: Pod downwardapi-volume-5891669d-166a-4022-a1a0-3860685faf8c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 08:21:14.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1188" for this suite. 01/05/23 08:21:14.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:21:14.614
Jan  5 08:21:14.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename var-expansion 01/05/23 08:21:14.615
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:14.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:14.631
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/05/23 08:21:14.633
Jan  5 08:21:14.649: INFO: Waiting up to 5m0s for pod "var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732" in namespace "var-expansion-2308" to be "Succeeded or Failed"
Jan  5 08:21:14.651: INFO: Pod "var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732": Phase="Pending", Reason="", readiness=false. Elapsed: 1.545441ms
Jan  5 08:21:16.654: INFO: Pod "var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005278707s
Jan  5 08:21:18.662: INFO: Pod "var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013388234s
STEP: Saw pod success 01/05/23 08:21:18.662
Jan  5 08:21:18.663: INFO: Pod "var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732" satisfied condition "Succeeded or Failed"
Jan  5 08:21:18.664: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732 container dapi-container: <nil>
STEP: delete the pod 01/05/23 08:21:18.669
Jan  5 08:21:18.702: INFO: Waiting for pod var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732 to disappear
Jan  5 08:21:18.704: INFO: Pod var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 08:21:18.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2308" for this suite. 01/05/23 08:21:18.706
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":195,"skipped":3921,"failed":0}
------------------------------
â€¢ [4.100 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:21:14.614
    Jan  5 08:21:14.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename var-expansion 01/05/23 08:21:14.615
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:14.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:14.631
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/05/23 08:21:14.633
    Jan  5 08:21:14.649: INFO: Waiting up to 5m0s for pod "var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732" in namespace "var-expansion-2308" to be "Succeeded or Failed"
    Jan  5 08:21:14.651: INFO: Pod "var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732": Phase="Pending", Reason="", readiness=false. Elapsed: 1.545441ms
    Jan  5 08:21:16.654: INFO: Pod "var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005278707s
    Jan  5 08:21:18.662: INFO: Pod "var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013388234s
    STEP: Saw pod success 01/05/23 08:21:18.662
    Jan  5 08:21:18.663: INFO: Pod "var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732" satisfied condition "Succeeded or Failed"
    Jan  5 08:21:18.664: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 08:21:18.669
    Jan  5 08:21:18.702: INFO: Waiting for pod var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732 to disappear
    Jan  5 08:21:18.704: INFO: Pod var-expansion-c60ce0c5-e4d4-440e-a490-f3e1b009a732 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 08:21:18.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2308" for this suite. 01/05/23 08:21:18.706
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:21:18.716
Jan  5 08:21:18.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename disruption 01/05/23 08:21:18.718
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:18.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:18.784
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/05/23 08:21:18.792
STEP: Waiting for all pods to be running 01/05/23 08:21:20.823
Jan  5 08:21:20.824: INFO: running pods: 0 < 3
Jan  5 08:21:22.829: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  5 08:21:24.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2387" for this suite. 01/05/23 08:21:24.833
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":196,"skipped":3923,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.120 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:21:18.716
    Jan  5 08:21:18.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename disruption 01/05/23 08:21:18.718
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:18.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:18.784
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/05/23 08:21:18.792
    STEP: Waiting for all pods to be running 01/05/23 08:21:20.823
    Jan  5 08:21:20.824: INFO: running pods: 0 < 3
    Jan  5 08:21:22.829: INFO: running pods: 1 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  5 08:21:24.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2387" for this suite. 01/05/23 08:21:24.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:21:24.839
Jan  5 08:21:24.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename statefulset 01/05/23 08:21:24.84
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:24.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:24.868
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2769 01/05/23 08:21:24.87
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-2769 01/05/23 08:21:24.884
Jan  5 08:21:24.898: INFO: Found 0 stateful pods, waiting for 1
Jan  5 08:21:34.901: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/05/23 08:21:34.909
STEP: Getting /status 01/05/23 08:21:34.917
Jan  5 08:21:34.919: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/05/23 08:21:34.919
Jan  5 08:21:34.929: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/05/23 08:21:34.929
Jan  5 08:21:34.930: INFO: Observed &StatefulSet event: ADDED
Jan  5 08:21:34.930: INFO: Found Statefulset ss in namespace statefulset-2769 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 08:21:34.930: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/05/23 08:21:34.93
Jan  5 08:21:34.930: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  5 08:21:34.936: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/05/23 08:21:34.936
Jan  5 08:21:34.937: INFO: Observed &StatefulSet event: ADDED
Jan  5 08:21:34.937: INFO: Observed Statefulset ss in namespace statefulset-2769 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 08:21:34.937: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 08:21:34.937: INFO: Deleting all statefulset in ns statefulset-2769
Jan  5 08:21:34.939: INFO: Scaling statefulset ss to 0
Jan  5 08:21:44.961: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 08:21:44.963: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 08:21:44.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2769" for this suite. 01/05/23 08:21:44.974
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":197,"skipped":3948,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.147 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:21:24.839
    Jan  5 08:21:24.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename statefulset 01/05/23 08:21:24.84
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:24.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:24.868
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2769 01/05/23 08:21:24.87
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-2769 01/05/23 08:21:24.884
    Jan  5 08:21:24.898: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 08:21:34.901: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/05/23 08:21:34.909
    STEP: Getting /status 01/05/23 08:21:34.917
    Jan  5 08:21:34.919: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/05/23 08:21:34.919
    Jan  5 08:21:34.929: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/05/23 08:21:34.929
    Jan  5 08:21:34.930: INFO: Observed &StatefulSet event: ADDED
    Jan  5 08:21:34.930: INFO: Found Statefulset ss in namespace statefulset-2769 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 08:21:34.930: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/05/23 08:21:34.93
    Jan  5 08:21:34.930: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  5 08:21:34.936: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/05/23 08:21:34.936
    Jan  5 08:21:34.937: INFO: Observed &StatefulSet event: ADDED
    Jan  5 08:21:34.937: INFO: Observed Statefulset ss in namespace statefulset-2769 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 08:21:34.937: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 08:21:34.937: INFO: Deleting all statefulset in ns statefulset-2769
    Jan  5 08:21:34.939: INFO: Scaling statefulset ss to 0
    Jan  5 08:21:44.961: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 08:21:44.963: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 08:21:44.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2769" for this suite. 01/05/23 08:21:44.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:21:44.988
Jan  5 08:21:44.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 08:21:44.989
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:45.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:45.014
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/05/23 08:21:45.016
Jan  5 08:21:45.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 create -f -'
Jan  5 08:21:45.531: INFO: stderr: ""
Jan  5 08:21:45.531: INFO: stdout: "pod/pause created\n"
Jan  5 08:21:45.531: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan  5 08:21:45.531: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3707" to be "running and ready"
Jan  5 08:21:45.533: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.871873ms
Jan  5 08:21:45.533: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
Jan  5 08:21:47.537: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006095788s
Jan  5 08:21:47.537: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'mip-bd-vm724.mip.storage.hpecorp.net' to be 'Running' but was 'Pending'
Jan  5 08:21:49.538: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.006901003s
Jan  5 08:21:49.538: INFO: Pod "pause" satisfied condition "running and ready"
Jan  5 08:21:49.538: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/05/23 08:21:49.538
Jan  5 08:21:49.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 label pods pause testing-label=testing-label-value'
Jan  5 08:21:49.605: INFO: stderr: ""
Jan  5 08:21:49.605: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/05/23 08:21:49.605
Jan  5 08:21:49.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 get pod pause -L testing-label'
Jan  5 08:21:49.673: INFO: stderr: ""
Jan  5 08:21:49.673: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/05/23 08:21:49.673
Jan  5 08:21:49.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 label pods pause testing-label-'
Jan  5 08:21:49.748: INFO: stderr: ""
Jan  5 08:21:49.748: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/05/23 08:21:49.748
Jan  5 08:21:49.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 get pod pause -L testing-label'
Jan  5 08:21:49.811: INFO: stderr: ""
Jan  5 08:21:49.811: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/05/23 08:21:49.811
Jan  5 08:21:49.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 delete --grace-period=0 --force -f -'
Jan  5 08:21:49.892: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 08:21:49.892: INFO: stdout: "pod \"pause\" force deleted\n"
Jan  5 08:21:49.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 get rc,svc -l name=pause --no-headers'
Jan  5 08:21:49.970: INFO: stderr: "No resources found in kubectl-3707 namespace.\n"
Jan  5 08:21:49.970: INFO: stdout: ""
Jan  5 08:21:49.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  5 08:21:50.041: INFO: stderr: ""
Jan  5 08:21:50.041: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 08:21:50.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3707" for this suite. 01/05/23 08:21:50.046
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":198,"skipped":4015,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.073 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:21:44.988
    Jan  5 08:21:44.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 08:21:44.989
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:45.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:45.014
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/05/23 08:21:45.016
    Jan  5 08:21:45.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 create -f -'
    Jan  5 08:21:45.531: INFO: stderr: ""
    Jan  5 08:21:45.531: INFO: stdout: "pod/pause created\n"
    Jan  5 08:21:45.531: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan  5 08:21:45.531: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3707" to be "running and ready"
    Jan  5 08:21:45.533: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.871873ms
    Jan  5 08:21:45.533: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
    Jan  5 08:21:47.537: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006095788s
    Jan  5 08:21:47.537: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'mip-bd-vm724.mip.storage.hpecorp.net' to be 'Running' but was 'Pending'
    Jan  5 08:21:49.538: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.006901003s
    Jan  5 08:21:49.538: INFO: Pod "pause" satisfied condition "running and ready"
    Jan  5 08:21:49.538: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/05/23 08:21:49.538
    Jan  5 08:21:49.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 label pods pause testing-label=testing-label-value'
    Jan  5 08:21:49.605: INFO: stderr: ""
    Jan  5 08:21:49.605: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/05/23 08:21:49.605
    Jan  5 08:21:49.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 get pod pause -L testing-label'
    Jan  5 08:21:49.673: INFO: stderr: ""
    Jan  5 08:21:49.673: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/05/23 08:21:49.673
    Jan  5 08:21:49.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 label pods pause testing-label-'
    Jan  5 08:21:49.748: INFO: stderr: ""
    Jan  5 08:21:49.748: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/05/23 08:21:49.748
    Jan  5 08:21:49.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 get pod pause -L testing-label'
    Jan  5 08:21:49.811: INFO: stderr: ""
    Jan  5 08:21:49.811: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/05/23 08:21:49.811
    Jan  5 08:21:49.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 delete --grace-period=0 --force -f -'
    Jan  5 08:21:49.892: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 08:21:49.892: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan  5 08:21:49.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 get rc,svc -l name=pause --no-headers'
    Jan  5 08:21:49.970: INFO: stderr: "No resources found in kubectl-3707 namespace.\n"
    Jan  5 08:21:49.970: INFO: stdout: ""
    Jan  5 08:21:49.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3707 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  5 08:21:50.041: INFO: stderr: ""
    Jan  5 08:21:50.041: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 08:21:50.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3707" for this suite. 01/05/23 08:21:50.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:21:50.062
Jan  5 08:21:50.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename dns 01/05/23 08:21:50.063
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:50.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:50.078
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/05/23 08:21:50.08
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5333 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5333;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5333 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5333;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5333.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5333.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5333.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5333.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5333.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5333.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5333.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5333.svc;check="$$(dig +notcp +noall +answer +search 212.88.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.88.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.88.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.88.212_tcp@PTR;sleep 1; done
 01/05/23 08:21:50.116
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5333 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5333;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5333 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5333;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5333.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5333.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5333.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5333.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5333.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5333.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5333.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5333.svc;check="$$(dig +notcp +noall +answer +search 212.88.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.88.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.88.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.88.212_tcp@PTR;sleep 1; done
 01/05/23 08:21:50.116
STEP: creating a pod to probe DNS 01/05/23 08:21:50.116
STEP: submitting the pod to kubernetes 01/05/23 08:21:50.116
Jan  5 08:21:50.135: INFO: Waiting up to 15m0s for pod "dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c" in namespace "dns-5333" to be "running"
Jan  5 08:21:50.138: INFO: Pod "dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033075ms
Jan  5 08:21:52.148: INFO: Pod "dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013106239s
Jan  5 08:21:54.142: INFO: Pod "dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c": Phase="Running", Reason="", readiness=true. Elapsed: 4.006638864s
Jan  5 08:21:54.142: INFO: Pod "dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c" satisfied condition "running"
STEP: retrieving the pod 01/05/23 08:21:54.142
STEP: looking for the results for each expected name from probers 01/05/23 08:21:54.144
Jan  5 08:21:54.147: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.149: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.150: INFO: Unable to read wheezy_udp@dns-test-service.dns-5333 from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.152: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5333 from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.153: INFO: Unable to read wheezy_udp@dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.155: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.156: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.158: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.166: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.167: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.169: INFO: Unable to read jessie_udp@dns-test-service.dns-5333 from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.171: INFO: Unable to read jessie_tcp@dns-test-service.dns-5333 from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.172: INFO: Unable to read jessie_udp@dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.174: INFO: Unable to read jessie_tcp@dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.175: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.178: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
Jan  5 08:21:54.184: INFO: Lookups using dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5333 wheezy_tcp@dns-test-service.dns-5333 wheezy_udp@dns-test-service.dns-5333.svc wheezy_tcp@dns-test-service.dns-5333.svc wheezy_udp@_http._tcp.dns-test-service.dns-5333.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5333.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5333 jessie_tcp@dns-test-service.dns-5333 jessie_udp@dns-test-service.dns-5333.svc jessie_tcp@dns-test-service.dns-5333.svc jessie_udp@_http._tcp.dns-test-service.dns-5333.svc jessie_tcp@_http._tcp.dns-test-service.dns-5333.svc]

Jan  5 08:21:59.225: INFO: DNS probes using dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c succeeded

STEP: deleting the pod 01/05/23 08:21:59.225
STEP: deleting the test service 01/05/23 08:21:59.255
STEP: deleting the test headless service 01/05/23 08:21:59.279
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 08:21:59.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5333" for this suite. 01/05/23 08:21:59.301
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":199,"skipped":4056,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.249 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:21:50.062
    Jan  5 08:21:50.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename dns 01/05/23 08:21:50.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:50.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:50.078
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/05/23 08:21:50.08
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5333 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5333;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5333 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5333;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5333.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5333.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5333.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5333.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5333.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5333.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5333.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5333.svc;check="$$(dig +notcp +noall +answer +search 212.88.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.88.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.88.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.88.212_tcp@PTR;sleep 1; done
     01/05/23 08:21:50.116
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5333 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5333;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5333 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5333;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5333.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5333.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5333.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5333.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5333.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5333.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5333.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5333.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5333.svc;check="$$(dig +notcp +noall +answer +search 212.88.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.88.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.88.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.88.212_tcp@PTR;sleep 1; done
     01/05/23 08:21:50.116
    STEP: creating a pod to probe DNS 01/05/23 08:21:50.116
    STEP: submitting the pod to kubernetes 01/05/23 08:21:50.116
    Jan  5 08:21:50.135: INFO: Waiting up to 15m0s for pod "dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c" in namespace "dns-5333" to be "running"
    Jan  5 08:21:50.138: INFO: Pod "dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033075ms
    Jan  5 08:21:52.148: INFO: Pod "dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013106239s
    Jan  5 08:21:54.142: INFO: Pod "dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c": Phase="Running", Reason="", readiness=true. Elapsed: 4.006638864s
    Jan  5 08:21:54.142: INFO: Pod "dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 08:21:54.142
    STEP: looking for the results for each expected name from probers 01/05/23 08:21:54.144
    Jan  5 08:21:54.147: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.149: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.150: INFO: Unable to read wheezy_udp@dns-test-service.dns-5333 from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.152: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5333 from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.153: INFO: Unable to read wheezy_udp@dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.155: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.156: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.158: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.166: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.167: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.169: INFO: Unable to read jessie_udp@dns-test-service.dns-5333 from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.171: INFO: Unable to read jessie_tcp@dns-test-service.dns-5333 from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.172: INFO: Unable to read jessie_udp@dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.174: INFO: Unable to read jessie_tcp@dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.175: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.178: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5333.svc from pod dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c: the server could not find the requested resource (get pods dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c)
    Jan  5 08:21:54.184: INFO: Lookups using dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5333 wheezy_tcp@dns-test-service.dns-5333 wheezy_udp@dns-test-service.dns-5333.svc wheezy_tcp@dns-test-service.dns-5333.svc wheezy_udp@_http._tcp.dns-test-service.dns-5333.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5333.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5333 jessie_tcp@dns-test-service.dns-5333 jessie_udp@dns-test-service.dns-5333.svc jessie_tcp@dns-test-service.dns-5333.svc jessie_udp@_http._tcp.dns-test-service.dns-5333.svc jessie_tcp@_http._tcp.dns-test-service.dns-5333.svc]

    Jan  5 08:21:59.225: INFO: DNS probes using dns-5333/dns-test-c2239e82-dbe0-436c-8d75-70f5f486668c succeeded

    STEP: deleting the pod 01/05/23 08:21:59.225
    STEP: deleting the test service 01/05/23 08:21:59.255
    STEP: deleting the test headless service 01/05/23 08:21:59.279
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 08:21:59.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5333" for this suite. 01/05/23 08:21:59.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:21:59.312
Jan  5 08:21:59.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:21:59.313
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:59.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:59.325
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-a255d389-4b02-4a18-ae38-af9fd5bc6309 01/05/23 08:21:59.327
STEP: Creating a pod to test consume secrets 01/05/23 08:21:59.33
Jan  5 08:21:59.341: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365" in namespace "projected-7093" to be "Succeeded or Failed"
Jan  5 08:21:59.343: INFO: Pod "pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005237ms
Jan  5 08:22:01.346: INFO: Pod "pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005191011s
Jan  5 08:22:03.347: INFO: Pod "pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006586026s
STEP: Saw pod success 01/05/23 08:22:03.347
Jan  5 08:22:03.347: INFO: Pod "pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365" satisfied condition "Succeeded or Failed"
Jan  5 08:22:03.350: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 08:22:03.355
Jan  5 08:22:03.369: INFO: Waiting for pod pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365 to disappear
Jan  5 08:22:03.372: INFO: Pod pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 08:22:03.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7093" for this suite. 01/05/23 08:22:03.376
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":200,"skipped":4108,"failed":0}
------------------------------
â€¢ [4.075 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:21:59.312
    Jan  5 08:21:59.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:21:59.313
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:21:59.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:21:59.325
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-a255d389-4b02-4a18-ae38-af9fd5bc6309 01/05/23 08:21:59.327
    STEP: Creating a pod to test consume secrets 01/05/23 08:21:59.33
    Jan  5 08:21:59.341: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365" in namespace "projected-7093" to be "Succeeded or Failed"
    Jan  5 08:21:59.343: INFO: Pod "pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005237ms
    Jan  5 08:22:01.346: INFO: Pod "pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005191011s
    Jan  5 08:22:03.347: INFO: Pod "pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006586026s
    STEP: Saw pod success 01/05/23 08:22:03.347
    Jan  5 08:22:03.347: INFO: Pod "pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365" satisfied condition "Succeeded or Failed"
    Jan  5 08:22:03.350: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 08:22:03.355
    Jan  5 08:22:03.369: INFO: Waiting for pod pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365 to disappear
    Jan  5 08:22:03.372: INFO: Pod pod-projected-secrets-58af83d3-00d2-4446-88f2-8ed7f6a30365 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 08:22:03.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7093" for this suite. 01/05/23 08:22:03.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:22:03.393
Jan  5 08:22:03.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 08:22:03.394
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:03.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:03.415
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/05/23 08:22:03.42
Jan  5 08:22:03.427: INFO: Waiting up to 5m0s for pod "downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a" in namespace "downward-api-8675" to be "Succeeded or Failed"
Jan  5 08:22:03.428: INFO: Pod "downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.406781ms
Jan  5 08:22:05.431: INFO: Pod "downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004180126s
Jan  5 08:22:07.431: INFO: Pod "downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004411241s
STEP: Saw pod success 01/05/23 08:22:07.431
Jan  5 08:22:07.432: INFO: Pod "downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a" satisfied condition "Succeeded or Failed"
Jan  5 08:22:07.433: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a container dapi-container: <nil>
STEP: delete the pod 01/05/23 08:22:07.437
Jan  5 08:22:07.450: INFO: Waiting for pod downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a to disappear
Jan  5 08:22:07.451: INFO: Pod downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  5 08:22:07.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8675" for this suite. 01/05/23 08:22:07.453
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":201,"skipped":4123,"failed":0}
------------------------------
â€¢ [4.067 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:22:03.393
    Jan  5 08:22:03.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 08:22:03.394
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:03.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:03.415
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/05/23 08:22:03.42
    Jan  5 08:22:03.427: INFO: Waiting up to 5m0s for pod "downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a" in namespace "downward-api-8675" to be "Succeeded or Failed"
    Jan  5 08:22:03.428: INFO: Pod "downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.406781ms
    Jan  5 08:22:05.431: INFO: Pod "downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004180126s
    Jan  5 08:22:07.431: INFO: Pod "downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004411241s
    STEP: Saw pod success 01/05/23 08:22:07.431
    Jan  5 08:22:07.432: INFO: Pod "downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a" satisfied condition "Succeeded or Failed"
    Jan  5 08:22:07.433: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a container dapi-container: <nil>
    STEP: delete the pod 01/05/23 08:22:07.437
    Jan  5 08:22:07.450: INFO: Waiting for pod downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a to disappear
    Jan  5 08:22:07.451: INFO: Pod downward-api-6bc4e64d-f306-4497-bb83-7c14dedd529a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  5 08:22:07.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8675" for this suite. 01/05/23 08:22:07.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:22:07.46
Jan  5 08:22:07.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-webhook 01/05/23 08:22:07.461
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:07.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:07.476
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/05/23 08:22:07.477
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 08:22:08.004
STEP: Deploying the custom resource conversion webhook pod 01/05/23 08:22:08.01
STEP: Wait for the deployment to be ready 01/05/23 08:22:08.023
Jan  5 08:22:08.034: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 08:22:10.039
STEP: Verifying the service has paired with the endpoint 01/05/23 08:22:10.053
Jan  5 08:22:11.053: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan  5 08:22:11.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Creating a v1 custom resource 01/05/23 08:22:13.648
STEP: v2 custom resource should be converted 01/05/23 08:22:13.652
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:22:14.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1628" for this suite. 01/05/23 08:22:14.173
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":202,"skipped":4132,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.769 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:22:07.46
    Jan  5 08:22:07.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-webhook 01/05/23 08:22:07.461
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:07.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:07.476
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/05/23 08:22:07.477
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 08:22:08.004
    STEP: Deploying the custom resource conversion webhook pod 01/05/23 08:22:08.01
    STEP: Wait for the deployment to be ready 01/05/23 08:22:08.023
    Jan  5 08:22:08.034: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 08:22:10.039
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:22:10.053
    Jan  5 08:22:11.053: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan  5 08:22:11.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Creating a v1 custom resource 01/05/23 08:22:13.648
    STEP: v2 custom resource should be converted 01/05/23 08:22:13.652
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:22:14.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-1628" for this suite. 01/05/23 08:22:14.173
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:22:14.229
Jan  5 08:22:14.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 08:22:14.23
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:14.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:14.26
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-4fa3f6ea-fffa-4906-bd0b-0df072eb2228 01/05/23 08:22:14.262
STEP: Creating a pod to test consume configMaps 01/05/23 08:22:14.273
Jan  5 08:22:14.281: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe" in namespace "configmap-288" to be "Succeeded or Failed"
Jan  5 08:22:14.288: INFO: Pod "pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.423569ms
Jan  5 08:22:16.291: INFO: Pod "pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010093783s
Jan  5 08:22:18.291: INFO: Pod "pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010626312s
STEP: Saw pod success 01/05/23 08:22:18.291
Jan  5 08:22:18.291: INFO: Pod "pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe" satisfied condition "Succeeded or Failed"
Jan  5 08:22:18.293: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:22:18.297
Jan  5 08:22:18.312: INFO: Waiting for pod pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe to disappear
Jan  5 08:22:18.314: INFO: Pod pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 08:22:18.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-288" for this suite. 01/05/23 08:22:18.316
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":203,"skipped":4137,"failed":0}
------------------------------
â€¢ [4.095 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:22:14.229
    Jan  5 08:22:14.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 08:22:14.23
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:14.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:14.26
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-4fa3f6ea-fffa-4906-bd0b-0df072eb2228 01/05/23 08:22:14.262
    STEP: Creating a pod to test consume configMaps 01/05/23 08:22:14.273
    Jan  5 08:22:14.281: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe" in namespace "configmap-288" to be "Succeeded or Failed"
    Jan  5 08:22:14.288: INFO: Pod "pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.423569ms
    Jan  5 08:22:16.291: INFO: Pod "pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010093783s
    Jan  5 08:22:18.291: INFO: Pod "pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010626312s
    STEP: Saw pod success 01/05/23 08:22:18.291
    Jan  5 08:22:18.291: INFO: Pod "pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe" satisfied condition "Succeeded or Failed"
    Jan  5 08:22:18.293: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:22:18.297
    Jan  5 08:22:18.312: INFO: Waiting for pod pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe to disappear
    Jan  5 08:22:18.314: INFO: Pod pod-configmaps-c2ae41ee-2e50-4c6a-b0c5-9e9bddf232fe no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 08:22:18.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-288" for this suite. 01/05/23 08:22:18.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:22:18.324
Jan  5 08:22:18.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename resourcequota 01/05/23 08:22:18.325
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:18.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:18.341
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/05/23 08:22:18.344
STEP: Creating a ResourceQuota 01/05/23 08:22:23.348
STEP: Ensuring resource quota status is calculated 01/05/23 08:22:23.36
STEP: Creating a ReplicationController 01/05/23 08:22:25.365
STEP: Ensuring resource quota status captures replication controller creation 01/05/23 08:22:25.381
STEP: Deleting a ReplicationController 01/05/23 08:22:27.386
STEP: Ensuring resource quota status released usage 01/05/23 08:22:27.39
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 08:22:29.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1064" for this suite. 01/05/23 08:22:29.396
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":204,"skipped":4142,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.076 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:22:18.324
    Jan  5 08:22:18.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename resourcequota 01/05/23 08:22:18.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:18.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:18.341
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/05/23 08:22:18.344
    STEP: Creating a ResourceQuota 01/05/23 08:22:23.348
    STEP: Ensuring resource quota status is calculated 01/05/23 08:22:23.36
    STEP: Creating a ReplicationController 01/05/23 08:22:25.365
    STEP: Ensuring resource quota status captures replication controller creation 01/05/23 08:22:25.381
    STEP: Deleting a ReplicationController 01/05/23 08:22:27.386
    STEP: Ensuring resource quota status released usage 01/05/23 08:22:27.39
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 08:22:29.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1064" for this suite. 01/05/23 08:22:29.396
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:22:29.4
Jan  5 08:22:29.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 08:22:29.401
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:29.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:29.422
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/05/23 08:22:29.425
Jan  5 08:22:29.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan  5 08:22:29.481: INFO: stderr: ""
Jan  5 08:22:29.481: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/05/23 08:22:29.482
Jan  5 08:22:29.482: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan  5 08:22:29.482: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5782" to be "running and ready, or succeeded"
Jan  5 08:22:29.491: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.536038ms
Jan  5 08:22:29.491: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'mip-bd-vm724.mip.storage.hpecorp.net' to be 'Running' but was 'Pending'
Jan  5 08:22:31.495: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.012996471s
Jan  5 08:22:31.495: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan  5 08:22:31.495: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/05/23 08:22:31.495
Jan  5 08:22:31.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator'
Jan  5 08:22:31.569: INFO: stderr: ""
Jan  5 08:22:31.569: INFO: stdout: "I0105 08:22:30.748077       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/6nh 359\nI0105 08:22:30.948245       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/f5x6 234\nI0105 08:22:31.148766       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/vvm2 217\nI0105 08:22:31.348124       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/x57 298\nI0105 08:22:31.548535       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/vrc2 459\n"
STEP: limiting log lines 01/05/23 08:22:31.569
Jan  5 08:22:31.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator --tail=1'
Jan  5 08:22:31.638: INFO: stderr: ""
Jan  5 08:22:31.638: INFO: stdout: "I0105 08:22:31.548535       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/vrc2 459\n"
Jan  5 08:22:31.638: INFO: got output "I0105 08:22:31.548535       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/vrc2 459\n"
STEP: limiting log bytes 01/05/23 08:22:31.638
Jan  5 08:22:31.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator --limit-bytes=1'
Jan  5 08:22:31.707: INFO: stderr: ""
Jan  5 08:22:31.707: INFO: stdout: "I"
Jan  5 08:22:31.707: INFO: got output "I"
STEP: exposing timestamps 01/05/23 08:22:31.707
Jan  5 08:22:31.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator --tail=1 --timestamps'
Jan  5 08:22:31.777: INFO: stderr: ""
Jan  5 08:22:31.777: INFO: stdout: "2023-01-05T00:22:31.749248654-08:00 I0105 08:22:31.749001       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/9wpw 571\n"
Jan  5 08:22:31.777: INFO: got output "2023-01-05T00:22:31.749248654-08:00 I0105 08:22:31.749001       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/9wpw 571\n"
STEP: restricting to a time range 01/05/23 08:22:31.777
Jan  5 08:22:34.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator --since=1s'
Jan  5 08:22:34.360: INFO: stderr: ""
Jan  5 08:22:34.360: INFO: stdout: "I0105 08:22:33.548123       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/lcj 409\nI0105 08:22:33.748457       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/nj8 408\nI0105 08:22:33.948779       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/vcvw 575\nI0105 08:22:34.149114       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/4nf 381\nI0105 08:22:34.348483       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/glc 364\n"
Jan  5 08:22:34.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator --since=24h'
Jan  5 08:22:34.431: INFO: stderr: ""
Jan  5 08:22:34.431: INFO: stdout: "I0105 08:22:30.748077       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/6nh 359\nI0105 08:22:30.948245       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/f5x6 234\nI0105 08:22:31.148766       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/vvm2 217\nI0105 08:22:31.348124       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/x57 298\nI0105 08:22:31.548535       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/vrc2 459\nI0105 08:22:31.749001       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/9wpw 571\nI0105 08:22:31.948262       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/gx2x 419\nI0105 08:22:32.148736       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/9ss4 269\nI0105 08:22:32.349140       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/ngh 204\nI0105 08:22:32.548544       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/qzz 558\nI0105 08:22:32.748897       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/ktfp 466\nI0105 08:22:32.948171       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/zj4 557\nI0105 08:22:33.148513       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/fsz 270\nI0105 08:22:33.348851       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/c4s 468\nI0105 08:22:33.548123       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/lcj 409\nI0105 08:22:33.748457       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/nj8 408\nI0105 08:22:33.948779       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/vcvw 575\nI0105 08:22:34.149114       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/4nf 381\nI0105 08:22:34.348483       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/glc 364\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan  5 08:22:34.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 delete pod logs-generator'
Jan  5 08:22:34.856: INFO: stderr: ""
Jan  5 08:22:34.856: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 08:22:34.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5782" for this suite. 01/05/23 08:22:34.858
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":205,"skipped":4142,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.466 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:22:29.4
    Jan  5 08:22:29.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 08:22:29.401
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:29.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:29.422
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/05/23 08:22:29.425
    Jan  5 08:22:29.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan  5 08:22:29.481: INFO: stderr: ""
    Jan  5 08:22:29.481: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/05/23 08:22:29.482
    Jan  5 08:22:29.482: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan  5 08:22:29.482: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5782" to be "running and ready, or succeeded"
    Jan  5 08:22:29.491: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.536038ms
    Jan  5 08:22:29.491: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'mip-bd-vm724.mip.storage.hpecorp.net' to be 'Running' but was 'Pending'
    Jan  5 08:22:31.495: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.012996471s
    Jan  5 08:22:31.495: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan  5 08:22:31.495: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/05/23 08:22:31.495
    Jan  5 08:22:31.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator'
    Jan  5 08:22:31.569: INFO: stderr: ""
    Jan  5 08:22:31.569: INFO: stdout: "I0105 08:22:30.748077       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/6nh 359\nI0105 08:22:30.948245       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/f5x6 234\nI0105 08:22:31.148766       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/vvm2 217\nI0105 08:22:31.348124       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/x57 298\nI0105 08:22:31.548535       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/vrc2 459\n"
    STEP: limiting log lines 01/05/23 08:22:31.569
    Jan  5 08:22:31.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator --tail=1'
    Jan  5 08:22:31.638: INFO: stderr: ""
    Jan  5 08:22:31.638: INFO: stdout: "I0105 08:22:31.548535       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/vrc2 459\n"
    Jan  5 08:22:31.638: INFO: got output "I0105 08:22:31.548535       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/vrc2 459\n"
    STEP: limiting log bytes 01/05/23 08:22:31.638
    Jan  5 08:22:31.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator --limit-bytes=1'
    Jan  5 08:22:31.707: INFO: stderr: ""
    Jan  5 08:22:31.707: INFO: stdout: "I"
    Jan  5 08:22:31.707: INFO: got output "I"
    STEP: exposing timestamps 01/05/23 08:22:31.707
    Jan  5 08:22:31.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan  5 08:22:31.777: INFO: stderr: ""
    Jan  5 08:22:31.777: INFO: stdout: "2023-01-05T00:22:31.749248654-08:00 I0105 08:22:31.749001       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/9wpw 571\n"
    Jan  5 08:22:31.777: INFO: got output "2023-01-05T00:22:31.749248654-08:00 I0105 08:22:31.749001       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/9wpw 571\n"
    STEP: restricting to a time range 01/05/23 08:22:31.777
    Jan  5 08:22:34.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator --since=1s'
    Jan  5 08:22:34.360: INFO: stderr: ""
    Jan  5 08:22:34.360: INFO: stdout: "I0105 08:22:33.548123       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/lcj 409\nI0105 08:22:33.748457       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/nj8 408\nI0105 08:22:33.948779       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/vcvw 575\nI0105 08:22:34.149114       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/4nf 381\nI0105 08:22:34.348483       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/glc 364\n"
    Jan  5 08:22:34.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 logs logs-generator logs-generator --since=24h'
    Jan  5 08:22:34.431: INFO: stderr: ""
    Jan  5 08:22:34.431: INFO: stdout: "I0105 08:22:30.748077       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/6nh 359\nI0105 08:22:30.948245       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/f5x6 234\nI0105 08:22:31.148766       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/vvm2 217\nI0105 08:22:31.348124       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/x57 298\nI0105 08:22:31.548535       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/vrc2 459\nI0105 08:22:31.749001       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/9wpw 571\nI0105 08:22:31.948262       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/gx2x 419\nI0105 08:22:32.148736       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/9ss4 269\nI0105 08:22:32.349140       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/ngh 204\nI0105 08:22:32.548544       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/qzz 558\nI0105 08:22:32.748897       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/ktfp 466\nI0105 08:22:32.948171       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/zj4 557\nI0105 08:22:33.148513       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/fsz 270\nI0105 08:22:33.348851       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/c4s 468\nI0105 08:22:33.548123       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/lcj 409\nI0105 08:22:33.748457       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/nj8 408\nI0105 08:22:33.948779       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/vcvw 575\nI0105 08:22:34.149114       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/4nf 381\nI0105 08:22:34.348483       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/glc 364\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan  5 08:22:34.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5782 delete pod logs-generator'
    Jan  5 08:22:34.856: INFO: stderr: ""
    Jan  5 08:22:34.856: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 08:22:34.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5782" for this suite. 01/05/23 08:22:34.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:22:34.867
Jan  5 08:22:34.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename var-expansion 01/05/23 08:22:34.867
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:34.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:34.881
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/05/23 08:22:34.882
STEP: waiting for pod running 01/05/23 08:22:34.891
Jan  5 08:22:34.891: INFO: Waiting up to 2m0s for pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" in namespace "var-expansion-8349" to be "running"
Jan  5 08:22:34.893: INFO: Pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.765881ms
Jan  5 08:22:36.896: INFO: Pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004583042s
Jan  5 08:22:36.896: INFO: Pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" satisfied condition "running"
STEP: creating a file in subpath 01/05/23 08:22:36.896
Jan  5 08:22:36.897: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8349 PodName:var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:22:36.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:22:36.899: INFO: ExecWithOptions: Clientset creation
Jan  5 08:22:36.899: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8349/pods/var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/05/23 08:22:36.957
Jan  5 08:22:36.960: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8349 PodName:var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:22:36.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:22:36.961: INFO: ExecWithOptions: Clientset creation
Jan  5 08:22:36.961: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8349/pods/var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/05/23 08:22:37.016
Jan  5 08:22:37.526: INFO: Successfully updated pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb"
STEP: waiting for annotated pod running 01/05/23 08:22:37.526
Jan  5 08:22:37.526: INFO: Waiting up to 2m0s for pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" in namespace "var-expansion-8349" to be "running"
Jan  5 08:22:37.528: INFO: Pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb": Phase="Running", Reason="", readiness=true. Elapsed: 1.834605ms
Jan  5 08:22:37.528: INFO: Pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" satisfied condition "running"
STEP: deleting the pod gracefully 01/05/23 08:22:37.528
Jan  5 08:22:37.528: INFO: Deleting pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" in namespace "var-expansion-8349"
Jan  5 08:22:37.536: INFO: Wait up to 5m0s for pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 08:23:11.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8349" for this suite. 01/05/23 08:23:11.544
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":206,"skipped":4152,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.681 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:22:34.867
    Jan  5 08:22:34.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename var-expansion 01/05/23 08:22:34.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:22:34.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:22:34.881
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/05/23 08:22:34.882
    STEP: waiting for pod running 01/05/23 08:22:34.891
    Jan  5 08:22:34.891: INFO: Waiting up to 2m0s for pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" in namespace "var-expansion-8349" to be "running"
    Jan  5 08:22:34.893: INFO: Pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.765881ms
    Jan  5 08:22:36.896: INFO: Pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004583042s
    Jan  5 08:22:36.896: INFO: Pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" satisfied condition "running"
    STEP: creating a file in subpath 01/05/23 08:22:36.896
    Jan  5 08:22:36.897: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8349 PodName:var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:22:36.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:22:36.899: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:22:36.899: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8349/pods/var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/05/23 08:22:36.957
    Jan  5 08:22:36.960: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8349 PodName:var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:22:36.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:22:36.961: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:22:36.961: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8349/pods/var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/05/23 08:22:37.016
    Jan  5 08:22:37.526: INFO: Successfully updated pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb"
    STEP: waiting for annotated pod running 01/05/23 08:22:37.526
    Jan  5 08:22:37.526: INFO: Waiting up to 2m0s for pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" in namespace "var-expansion-8349" to be "running"
    Jan  5 08:22:37.528: INFO: Pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb": Phase="Running", Reason="", readiness=true. Elapsed: 1.834605ms
    Jan  5 08:22:37.528: INFO: Pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" satisfied condition "running"
    STEP: deleting the pod gracefully 01/05/23 08:22:37.528
    Jan  5 08:22:37.528: INFO: Deleting pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" in namespace "var-expansion-8349"
    Jan  5 08:22:37.536: INFO: Wait up to 5m0s for pod "var-expansion-1a6a12df-7990-45d6-b0e1-beb1a45396eb" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 08:23:11.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8349" for this suite. 01/05/23 08:23:11.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:23:11.55
Jan  5 08:23:11.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 08:23:11.55
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:11.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:11.565
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/05/23 08:23:11.57
Jan  5 08:23:11.575: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38" in namespace "downward-api-3485" to be "Succeeded or Failed"
Jan  5 08:23:11.578: INFO: Pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.706597ms
Jan  5 08:23:13.582: INFO: Pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006827744s
Jan  5 08:23:15.582: INFO: Pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006293495s
Jan  5 08:23:17.582: INFO: Pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006655863s
STEP: Saw pod success 01/05/23 08:23:17.582
Jan  5 08:23:17.582: INFO: Pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38" satisfied condition "Succeeded or Failed"
Jan  5 08:23:17.584: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38 container client-container: <nil>
STEP: delete the pod 01/05/23 08:23:17.587
Jan  5 08:23:17.601: INFO: Waiting for pod downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38 to disappear
Jan  5 08:23:17.603: INFO: Pod downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 08:23:17.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3485" for this suite. 01/05/23 08:23:17.605
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":207,"skipped":4216,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.063 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:23:11.55
    Jan  5 08:23:11.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 08:23:11.55
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:11.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:11.565
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/05/23 08:23:11.57
    Jan  5 08:23:11.575: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38" in namespace "downward-api-3485" to be "Succeeded or Failed"
    Jan  5 08:23:11.578: INFO: Pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.706597ms
    Jan  5 08:23:13.582: INFO: Pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006827744s
    Jan  5 08:23:15.582: INFO: Pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006293495s
    Jan  5 08:23:17.582: INFO: Pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006655863s
    STEP: Saw pod success 01/05/23 08:23:17.582
    Jan  5 08:23:17.582: INFO: Pod "downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38" satisfied condition "Succeeded or Failed"
    Jan  5 08:23:17.584: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38 container client-container: <nil>
    STEP: delete the pod 01/05/23 08:23:17.587
    Jan  5 08:23:17.601: INFO: Waiting for pod downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38 to disappear
    Jan  5 08:23:17.603: INFO: Pod downwardapi-volume-95daeff0-5cc0-4554-abbd-8b248bb4cd38 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 08:23:17.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3485" for this suite. 01/05/23 08:23:17.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:23:17.613
Jan  5 08:23:17.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:23:17.614
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:17.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:17.639
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-2e98131f-9219-4c23-8f6b-a7c88f549734 01/05/23 08:23:17.64
STEP: Creating a pod to test consume configMaps 01/05/23 08:23:17.644
Jan  5 08:23:17.655: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900" in namespace "projected-5507" to be "Succeeded or Failed"
Jan  5 08:23:17.657: INFO: Pod "pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038827ms
Jan  5 08:23:19.662: INFO: Pod "pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006636107s
Jan  5 08:23:21.660: INFO: Pod "pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004836157s
STEP: Saw pod success 01/05/23 08:23:21.66
Jan  5 08:23:21.660: INFO: Pod "pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900" satisfied condition "Succeeded or Failed"
Jan  5 08:23:21.662: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:23:21.666
Jan  5 08:23:21.682: INFO: Waiting for pod pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900 to disappear
Jan  5 08:23:21.684: INFO: Pod pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 08:23:21.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5507" for this suite. 01/05/23 08:23:21.687
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":208,"skipped":4228,"failed":0}
------------------------------
â€¢ [4.082 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:23:17.613
    Jan  5 08:23:17.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:23:17.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:17.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:17.639
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-2e98131f-9219-4c23-8f6b-a7c88f549734 01/05/23 08:23:17.64
    STEP: Creating a pod to test consume configMaps 01/05/23 08:23:17.644
    Jan  5 08:23:17.655: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900" in namespace "projected-5507" to be "Succeeded or Failed"
    Jan  5 08:23:17.657: INFO: Pod "pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038827ms
    Jan  5 08:23:19.662: INFO: Pod "pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006636107s
    Jan  5 08:23:21.660: INFO: Pod "pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004836157s
    STEP: Saw pod success 01/05/23 08:23:21.66
    Jan  5 08:23:21.660: INFO: Pod "pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900" satisfied condition "Succeeded or Failed"
    Jan  5 08:23:21.662: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:23:21.666
    Jan  5 08:23:21.682: INFO: Waiting for pod pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900 to disappear
    Jan  5 08:23:21.684: INFO: Pod pod-projected-configmaps-07fc4b67-53fc-47d1-942a-ba008e716900 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 08:23:21.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5507" for this suite. 01/05/23 08:23:21.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:23:21.696
Jan  5 08:23:21.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename security-context-test 01/05/23 08:23:21.697
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:21.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:21.715
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan  5 08:23:21.728: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a" in namespace "security-context-test-1914" to be "Succeeded or Failed"
Jan  5 08:23:21.730: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.939039ms
Jan  5 08:23:23.734: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005143361s
Jan  5 08:23:25.735: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006156631s
Jan  5 08:23:27.733: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005051384s
Jan  5 08:23:29.735: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00624943s
Jan  5 08:23:29.735: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 08:23:29.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1914" for this suite. 01/05/23 08:23:29.743
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":209,"skipped":4254,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.053 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:23:21.696
    Jan  5 08:23:21.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename security-context-test 01/05/23 08:23:21.697
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:21.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:21.715
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan  5 08:23:21.728: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a" in namespace "security-context-test-1914" to be "Succeeded or Failed"
    Jan  5 08:23:21.730: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.939039ms
    Jan  5 08:23:23.734: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005143361s
    Jan  5 08:23:25.735: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006156631s
    Jan  5 08:23:27.733: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005051384s
    Jan  5 08:23:29.735: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00624943s
    Jan  5 08:23:29.735: INFO: Pod "alpine-nnp-false-0219f986-88db-4e42-a9d9-dda3b42ec82a" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 08:23:29.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1914" for this suite. 01/05/23 08:23:29.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:23:29.749
Jan  5 08:23:29.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename replication-controller 01/05/23 08:23:29.75
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:29.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:29.769
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan  5 08:23:29.772: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/05/23 08:23:30.78
STEP: Checking rc "condition-test" has the desired failure condition set 01/05/23 08:23:30.805
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/05/23 08:23:31.811
Jan  5 08:23:31.817: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/05/23 08:23:31.817
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  5 08:23:32.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2381" for this suite. 01/05/23 08:23:32.825
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":210,"skipped":4261,"failed":0}
------------------------------
â€¢ [3.084 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:23:29.749
    Jan  5 08:23:29.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename replication-controller 01/05/23 08:23:29.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:29.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:29.769
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan  5 08:23:29.772: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/05/23 08:23:30.78
    STEP: Checking rc "condition-test" has the desired failure condition set 01/05/23 08:23:30.805
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/05/23 08:23:31.811
    Jan  5 08:23:31.817: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/05/23 08:23:31.817
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  5 08:23:32.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2381" for this suite. 01/05/23 08:23:32.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:23:32.835
Jan  5 08:23:32.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:23:32.836
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:32.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:32.864
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-1666 01/05/23 08:23:32.866
STEP: creating replication controller nodeport-test in namespace services-1666 01/05/23 08:23:32.887
I0105 08:23:32.896329      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1666, replica count: 2
I0105 08:23:35.947960      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 08:23:35.948: INFO: Creating new exec pod
Jan  5 08:23:35.952: INFO: Waiting up to 5m0s for pod "execpod826xf" in namespace "services-1666" to be "running"
Jan  5 08:23:35.953: INFO: Pod "execpod826xf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.514267ms
Jan  5 08:23:37.956: INFO: Pod "execpod826xf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004473112s
Jan  5 08:23:39.958: INFO: Pod "execpod826xf": Phase="Running", Reason="", readiness=true. Elapsed: 4.006559241s
Jan  5 08:23:39.958: INFO: Pod "execpod826xf" satisfied condition "running"
Jan  5 08:23:40.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1666 exec execpod826xf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan  5 08:23:41.075: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan  5 08:23:41.075: INFO: stdout: "nodeport-test-fptkl"
Jan  5 08:23:41.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1666 exec execpod826xf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.69.79 80'
Jan  5 08:23:41.187: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.69.79 80\nConnection to 10.96.69.79 80 port [tcp/http] succeeded!\n"
Jan  5 08:23:41.187: INFO: stdout: ""
Jan  5 08:23:42.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1666 exec execpod826xf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.69.79 80'
Jan  5 08:23:42.313: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.69.79 80\nConnection to 10.96.69.79 80 port [tcp/http] succeeded!\n"
Jan  5 08:23:42.313: INFO: stdout: "nodeport-test-lqn6p"
Jan  5 08:23:42.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1666 exec execpod826xf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.212 31250'
Jan  5 08:23:42.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.212 31250\nConnection to 16.0.14.212 31250 port [tcp/*] succeeded!\n"
Jan  5 08:23:42.440: INFO: stdout: "nodeport-test-fptkl"
Jan  5 08:23:42.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1666 exec execpod826xf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.214 31250'
Jan  5 08:23:42.567: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.214 31250\nConnection to 16.0.14.214 31250 port [tcp/*] succeeded!\n"
Jan  5 08:23:42.567: INFO: stdout: "nodeport-test-fptkl"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:23:42.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1666" for this suite. 01/05/23 08:23:42.57
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":211,"skipped":4291,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.771 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:23:32.835
    Jan  5 08:23:32.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:23:32.836
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:32.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:32.864
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-1666 01/05/23 08:23:32.866
    STEP: creating replication controller nodeport-test in namespace services-1666 01/05/23 08:23:32.887
    I0105 08:23:32.896329      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1666, replica count: 2
    I0105 08:23:35.947960      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 08:23:35.948: INFO: Creating new exec pod
    Jan  5 08:23:35.952: INFO: Waiting up to 5m0s for pod "execpod826xf" in namespace "services-1666" to be "running"
    Jan  5 08:23:35.953: INFO: Pod "execpod826xf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.514267ms
    Jan  5 08:23:37.956: INFO: Pod "execpod826xf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004473112s
    Jan  5 08:23:39.958: INFO: Pod "execpod826xf": Phase="Running", Reason="", readiness=true. Elapsed: 4.006559241s
    Jan  5 08:23:39.958: INFO: Pod "execpod826xf" satisfied condition "running"
    Jan  5 08:23:40.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1666 exec execpod826xf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan  5 08:23:41.075: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan  5 08:23:41.075: INFO: stdout: "nodeport-test-fptkl"
    Jan  5 08:23:41.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1666 exec execpod826xf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.69.79 80'
    Jan  5 08:23:41.187: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.69.79 80\nConnection to 10.96.69.79 80 port [tcp/http] succeeded!\n"
    Jan  5 08:23:41.187: INFO: stdout: ""
    Jan  5 08:23:42.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1666 exec execpod826xf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.69.79 80'
    Jan  5 08:23:42.313: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.69.79 80\nConnection to 10.96.69.79 80 port [tcp/http] succeeded!\n"
    Jan  5 08:23:42.313: INFO: stdout: "nodeport-test-lqn6p"
    Jan  5 08:23:42.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1666 exec execpod826xf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.212 31250'
    Jan  5 08:23:42.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.212 31250\nConnection to 16.0.14.212 31250 port [tcp/*] succeeded!\n"
    Jan  5 08:23:42.440: INFO: stdout: "nodeport-test-fptkl"
    Jan  5 08:23:42.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-1666 exec execpod826xf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.214 31250'
    Jan  5 08:23:42.567: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.214 31250\nConnection to 16.0.14.214 31250 port [tcp/*] succeeded!\n"
    Jan  5 08:23:42.567: INFO: stdout: "nodeport-test-fptkl"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:23:42.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1666" for this suite. 01/05/23 08:23:42.57
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:23:42.606
Jan  5 08:23:42.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 08:23:42.607
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:42.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:42.622
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/05/23 08:23:42.624
Jan  5 08:23:42.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9543 create -f -'
Jan  5 08:23:43.335: INFO: stderr: ""
Jan  5 08:23:43.335: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/05/23 08:23:43.335
Jan  5 08:23:44.339: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 08:23:44.339: INFO: Found 0 / 1
Jan  5 08:23:45.340: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 08:23:45.340: INFO: Found 1 / 1
Jan  5 08:23:45.340: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/05/23 08:23:45.34
Jan  5 08:23:45.343: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 08:23:45.343: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  5 08:23:45.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9543 patch pod agnhost-primary-c8ltp -p {"metadata":{"annotations":{"x":"y"}}}'
Jan  5 08:23:45.417: INFO: stderr: ""
Jan  5 08:23:45.417: INFO: stdout: "pod/agnhost-primary-c8ltp patched\n"
STEP: checking annotations 01/05/23 08:23:45.417
Jan  5 08:23:45.419: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 08:23:45.419: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 08:23:45.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9543" for this suite. 01/05/23 08:23:45.422
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":212,"skipped":4291,"failed":0}
------------------------------
â€¢ [2.836 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:23:42.606
    Jan  5 08:23:42.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 08:23:42.607
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:42.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:42.622
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/05/23 08:23:42.624
    Jan  5 08:23:42.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9543 create -f -'
    Jan  5 08:23:43.335: INFO: stderr: ""
    Jan  5 08:23:43.335: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/05/23 08:23:43.335
    Jan  5 08:23:44.339: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 08:23:44.339: INFO: Found 0 / 1
    Jan  5 08:23:45.340: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 08:23:45.340: INFO: Found 1 / 1
    Jan  5 08:23:45.340: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/05/23 08:23:45.34
    Jan  5 08:23:45.343: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 08:23:45.343: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  5 08:23:45.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-9543 patch pod agnhost-primary-c8ltp -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan  5 08:23:45.417: INFO: stderr: ""
    Jan  5 08:23:45.417: INFO: stdout: "pod/agnhost-primary-c8ltp patched\n"
    STEP: checking annotations 01/05/23 08:23:45.417
    Jan  5 08:23:45.419: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 08:23:45.419: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 08:23:45.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9543" for this suite. 01/05/23 08:23:45.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:23:45.444
Jan  5 08:23:45.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-runtime 01/05/23 08:23:45.445
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:45.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:45.461
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/05/23 08:23:45.464
STEP: wait for the container to reach Succeeded 01/05/23 08:23:45.474
STEP: get the container status 01/05/23 08:23:49.489
STEP: the container should be terminated 01/05/23 08:23:49.491
STEP: the termination message should be set 01/05/23 08:23:49.491
Jan  5 08:23:49.491: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/05/23 08:23:49.491
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  5 08:23:49.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7194" for this suite. 01/05/23 08:23:49.509
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":213,"skipped":4304,"failed":0}
------------------------------
â€¢ [4.074 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:23:45.444
    Jan  5 08:23:45.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-runtime 01/05/23 08:23:45.445
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:45.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:45.461
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/05/23 08:23:45.464
    STEP: wait for the container to reach Succeeded 01/05/23 08:23:45.474
    STEP: get the container status 01/05/23 08:23:49.489
    STEP: the container should be terminated 01/05/23 08:23:49.491
    STEP: the termination message should be set 01/05/23 08:23:49.491
    Jan  5 08:23:49.491: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/05/23 08:23:49.491
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  5 08:23:49.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7194" for this suite. 01/05/23 08:23:49.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:23:49.519
Jan  5 08:23:49.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 08:23:49.52
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:49.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:49.532
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/05/23 08:23:49.534
Jan  5 08:23:49.545: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75" in namespace "downward-api-3081" to be "Succeeded or Failed"
Jan  5 08:23:49.547: INFO: Pod "downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75": Phase="Pending", Reason="", readiness=false. Elapsed: 1.380731ms
Jan  5 08:23:51.551: INFO: Pod "downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005296666s
Jan  5 08:23:53.549: INFO: Pod "downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003926183s
STEP: Saw pod success 01/05/23 08:23:53.549
Jan  5 08:23:53.549: INFO: Pod "downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75" satisfied condition "Succeeded or Failed"
Jan  5 08:23:53.551: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75 container client-container: <nil>
STEP: delete the pod 01/05/23 08:23:53.555
Jan  5 08:23:53.569: INFO: Waiting for pod downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75 to disappear
Jan  5 08:23:53.570: INFO: Pod downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 08:23:53.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3081" for this suite. 01/05/23 08:23:53.572
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":214,"skipped":4353,"failed":0}
------------------------------
â€¢ [4.061 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:23:49.519
    Jan  5 08:23:49.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 08:23:49.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:49.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:49.532
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/05/23 08:23:49.534
    Jan  5 08:23:49.545: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75" in namespace "downward-api-3081" to be "Succeeded or Failed"
    Jan  5 08:23:49.547: INFO: Pod "downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75": Phase="Pending", Reason="", readiness=false. Elapsed: 1.380731ms
    Jan  5 08:23:51.551: INFO: Pod "downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005296666s
    Jan  5 08:23:53.549: INFO: Pod "downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003926183s
    STEP: Saw pod success 01/05/23 08:23:53.549
    Jan  5 08:23:53.549: INFO: Pod "downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75" satisfied condition "Succeeded or Failed"
    Jan  5 08:23:53.551: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75 container client-container: <nil>
    STEP: delete the pod 01/05/23 08:23:53.555
    Jan  5 08:23:53.569: INFO: Waiting for pod downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75 to disappear
    Jan  5 08:23:53.570: INFO: Pod downwardapi-volume-c686ac7a-0206-466d-94d1-324abb0cfe75 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 08:23:53.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3081" for this suite. 01/05/23 08:23:53.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:23:53.581
Jan  5 08:23:53.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 08:23:53.582
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:53.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:53.609
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/05/23 08:23:53.611
Jan  5 08:23:53.621: INFO: Waiting up to 5m0s for pod "downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee" in namespace "downward-api-872" to be "Succeeded or Failed"
Jan  5 08:23:53.623: INFO: Pod "downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee": Phase="Pending", Reason="", readiness=false. Elapsed: 1.391779ms
Jan  5 08:23:55.625: INFO: Pod "downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003993118s
Jan  5 08:23:57.627: INFO: Pod "downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005708759s
STEP: Saw pod success 01/05/23 08:23:57.627
Jan  5 08:23:57.627: INFO: Pod "downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee" satisfied condition "Succeeded or Failed"
Jan  5 08:23:57.630: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee container dapi-container: <nil>
STEP: delete the pod 01/05/23 08:23:57.634
Jan  5 08:23:57.647: INFO: Waiting for pod downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee to disappear
Jan  5 08:23:57.649: INFO: Pod downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  5 08:23:57.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-872" for this suite. 01/05/23 08:23:57.651
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":215,"skipped":4371,"failed":0}
------------------------------
â€¢ [4.074 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:23:53.581
    Jan  5 08:23:53.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 08:23:53.582
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:53.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:53.609
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/05/23 08:23:53.611
    Jan  5 08:23:53.621: INFO: Waiting up to 5m0s for pod "downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee" in namespace "downward-api-872" to be "Succeeded or Failed"
    Jan  5 08:23:53.623: INFO: Pod "downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee": Phase="Pending", Reason="", readiness=false. Elapsed: 1.391779ms
    Jan  5 08:23:55.625: INFO: Pod "downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003993118s
    Jan  5 08:23:57.627: INFO: Pod "downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005708759s
    STEP: Saw pod success 01/05/23 08:23:57.627
    Jan  5 08:23:57.627: INFO: Pod "downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee" satisfied condition "Succeeded or Failed"
    Jan  5 08:23:57.630: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee container dapi-container: <nil>
    STEP: delete the pod 01/05/23 08:23:57.634
    Jan  5 08:23:57.647: INFO: Waiting for pod downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee to disappear
    Jan  5 08:23:57.649: INFO: Pod downward-api-c0cd7c14-15d7-4220-aa51-7d1987d715ee no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  5 08:23:57.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-872" for this suite. 01/05/23 08:23:57.651
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:23:57.656
Jan  5 08:23:57.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename disruption 01/05/23 08:23:57.656
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:57.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:57.673
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/05/23 08:23:57.676
STEP: Waiting for the pdb to be processed 01/05/23 08:23:57.679
STEP: updating the pdb 01/05/23 08:23:59.684
STEP: Waiting for the pdb to be processed 01/05/23 08:23:59.689
STEP: patching the pdb 01/05/23 08:24:01.696
STEP: Waiting for the pdb to be processed 01/05/23 08:24:01.702
STEP: Waiting for the pdb to be deleted 01/05/23 08:24:03.721
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  5 08:24:03.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7792" for this suite. 01/05/23 08:24:03.724
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":216,"skipped":4372,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.073 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:23:57.656
    Jan  5 08:23:57.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename disruption 01/05/23 08:23:57.656
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:23:57.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:23:57.673
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/05/23 08:23:57.676
    STEP: Waiting for the pdb to be processed 01/05/23 08:23:57.679
    STEP: updating the pdb 01/05/23 08:23:59.684
    STEP: Waiting for the pdb to be processed 01/05/23 08:23:59.689
    STEP: patching the pdb 01/05/23 08:24:01.696
    STEP: Waiting for the pdb to be processed 01/05/23 08:24:01.702
    STEP: Waiting for the pdb to be deleted 01/05/23 08:24:03.721
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  5 08:24:03.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7792" for this suite. 01/05/23 08:24:03.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:03.729
Jan  5 08:24:03.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename deployment 01/05/23 08:24:03.729
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:03.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:03.746
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan  5 08:24:03.750: INFO: Creating simple deployment test-new-deployment
Jan  5 08:24:03.758: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
Jan  5 08:24:05.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 01/05/23 08:24:07.769
STEP: updating a scale subresource 01/05/23 08:24:07.77
STEP: verifying the deployment Spec.Replicas was modified 01/05/23 08:24:07.774
STEP: Patch a scale subresource 01/05/23 08:24:07.776
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 08:24:07.807: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1486  1042c6f6-2368-4603-946f-fef13eeafcdf 24850 3 2023-01-05 08:24:03 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-05 08:24:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:24:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004999618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 08:24:06 +0000 UTC,LastTransitionTime:2023-01-05 08:24:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-05 08:24:06 +0000 UTC,LastTransitionTime:2023-01-05 08:24:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 08:24:07.834: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1486  c977d925-3420-4892-8cb0-82918b4eb115 24855 3 2023-01-05 08:24:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 1042c6f6-2368-4603-946f-fef13eeafcdf 0xc004999a67 0xc004999a68}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:24:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 08:24:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1042c6f6-2368-4603-946f-fef13eeafcdf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004999af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 08:24:07.836: INFO: Pod "test-new-deployment-845c8977d9-jkrq6" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-jkrq6 test-new-deployment-845c8977d9- deployment-1486  e9a6ed36-98a6-4b0a-a253-9a153e0fd02b 24854 0 2023-01-05 08:24:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c977d925-3420-4892-8cb0-82918b4eb115 0xc004999ee7 0xc004999ee8}] [] [{kube-controller-manager Update v1 2023-01-05 08:24:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c977d925-3420-4892-8cb0-82918b4eb115\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p274k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p274k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 08:24:07.836: INFO: Pod "test-new-deployment-845c8977d9-l6h92" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-l6h92 test-new-deployment-845c8977d9- deployment-1486  cd12c199-961c-427d-9904-6bf383f6f7ae 24842 0 2023-01-05 08:24:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2aa9b38d7ecdd8bf6d41604feac81149c7da66f53c51a3a19249cf75109334f5 cni.projectcalico.org/podIP:10.244.1.98/32 cni.projectcalico.org/podIPs:10.244.1.98/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c977d925-3420-4892-8cb0-82918b4eb115 0xc002df8050 0xc002df8051}] [] [{kube-controller-manager Update v1 2023-01-05 08:24:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c977d925-3420-4892-8cb0-82918b4eb115\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:24:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:24:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9q86,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9q86,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.98,StartTime:2023-01-05 08:24:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:24:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ea39d91b07fe4ef615c7169deecc703f835d7e440a918ec4a47b40f5bb33ebd2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 08:24:07.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1486" for this suite. 01/05/23 08:24:07.848
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":217,"skipped":4378,"failed":0}
------------------------------
â€¢ [4.138 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:03.729
    Jan  5 08:24:03.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename deployment 01/05/23 08:24:03.729
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:03.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:03.746
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan  5 08:24:03.750: INFO: Creating simple deployment test-new-deployment
    Jan  5 08:24:03.758: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    Jan  5 08:24:05.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 01/05/23 08:24:07.769
    STEP: updating a scale subresource 01/05/23 08:24:07.77
    STEP: verifying the deployment Spec.Replicas was modified 01/05/23 08:24:07.774
    STEP: Patch a scale subresource 01/05/23 08:24:07.776
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 08:24:07.807: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1486  1042c6f6-2368-4603-946f-fef13eeafcdf 24850 3 2023-01-05 08:24:03 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-05 08:24:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:24:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004999618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 08:24:06 +0000 UTC,LastTransitionTime:2023-01-05 08:24:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-05 08:24:06 +0000 UTC,LastTransitionTime:2023-01-05 08:24:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 08:24:07.834: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1486  c977d925-3420-4892-8cb0-82918b4eb115 24855 3 2023-01-05 08:24:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 1042c6f6-2368-4603-946f-fef13eeafcdf 0xc004999a67 0xc004999a68}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:24:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 08:24:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1042c6f6-2368-4603-946f-fef13eeafcdf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004999af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 08:24:07.836: INFO: Pod "test-new-deployment-845c8977d9-jkrq6" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-jkrq6 test-new-deployment-845c8977d9- deployment-1486  e9a6ed36-98a6-4b0a-a253-9a153e0fd02b 24854 0 2023-01-05 08:24:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c977d925-3420-4892-8cb0-82918b4eb115 0xc004999ee7 0xc004999ee8}] [] [{kube-controller-manager Update v1 2023-01-05 08:24:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c977d925-3420-4892-8cb0-82918b4eb115\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p274k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p274k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm722.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 08:24:07.836: INFO: Pod "test-new-deployment-845c8977d9-l6h92" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-l6h92 test-new-deployment-845c8977d9- deployment-1486  cd12c199-961c-427d-9904-6bf383f6f7ae 24842 0 2023-01-05 08:24:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2aa9b38d7ecdd8bf6d41604feac81149c7da66f53c51a3a19249cf75109334f5 cni.projectcalico.org/podIP:10.244.1.98/32 cni.projectcalico.org/podIPs:10.244.1.98/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c977d925-3420-4892-8cb0-82918b4eb115 0xc002df8050 0xc002df8051}] [] [{kube-controller-manager Update v1 2023-01-05 08:24:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c977d925-3420-4892-8cb0-82918b4eb115\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:24:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:24:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9q86,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9q86,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.98,StartTime:2023-01-05 08:24:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:24:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ea39d91b07fe4ef615c7169deecc703f835d7e440a918ec4a47b40f5bb33ebd2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 08:24:07.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1486" for this suite. 01/05/23 08:24:07.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:07.868
Jan  5 08:24:07.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename disruption 01/05/23 08:24:07.868
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:07.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:07.905
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:07.908
Jan  5 08:24:07.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename disruption-2 01/05/23 08:24:07.91
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:07.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:07.93
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/05/23 08:24:07.94
STEP: Waiting for the pdb to be processed 01/05/23 08:24:09.953
STEP: Waiting for the pdb to be processed 01/05/23 08:24:09.999
STEP: listing a collection of PDBs across all namespaces 01/05/23 08:24:12.004
STEP: listing a collection of PDBs in namespace disruption-4648 01/05/23 08:24:12.005
STEP: deleting a collection of PDBs 01/05/23 08:24:12.007
STEP: Waiting for the PDB collection to be deleted 01/05/23 08:24:12.02
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan  5 08:24:12.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8370" for this suite. 01/05/23 08:24:12.024
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  5 08:24:12.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4648" for this suite. 01/05/23 08:24:12.031
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":218,"skipped":4400,"failed":0}
------------------------------
â€¢ [4.185 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:07.868
    Jan  5 08:24:07.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename disruption 01/05/23 08:24:07.868
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:07.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:07.905
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:07.908
    Jan  5 08:24:07.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename disruption-2 01/05/23 08:24:07.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:07.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:07.93
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/05/23 08:24:07.94
    STEP: Waiting for the pdb to be processed 01/05/23 08:24:09.953
    STEP: Waiting for the pdb to be processed 01/05/23 08:24:09.999
    STEP: listing a collection of PDBs across all namespaces 01/05/23 08:24:12.004
    STEP: listing a collection of PDBs in namespace disruption-4648 01/05/23 08:24:12.005
    STEP: deleting a collection of PDBs 01/05/23 08:24:12.007
    STEP: Waiting for the PDB collection to be deleted 01/05/23 08:24:12.02
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan  5 08:24:12.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-8370" for this suite. 01/05/23 08:24:12.024
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  5 08:24:12.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4648" for this suite. 01/05/23 08:24:12.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:12.053
Jan  5 08:24:12.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename deployment 01/05/23 08:24:12.054
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:12.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:12.069
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan  5 08:24:12.071: INFO: Creating deployment "test-recreate-deployment"
Jan  5 08:24:12.082: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan  5 08:24:12.088: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan  5 08:24:14.093: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan  5 08:24:14.095: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan  5 08:24:14.109: INFO: Updating deployment test-recreate-deployment
Jan  5 08:24:14.109: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 08:24:14.236: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9212  5c2eaa88-cae3-4f90-ba58-a893798d1218 24967 2 2023-01-05 08:24:12 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032d8648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 08:24:14 +0000 UTC,LastTransitionTime:2023-01-05 08:24:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-05 08:24:14 +0000 UTC,LastTransitionTime:2023-01-05 08:24:12 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan  5 08:24:14.238: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-9212  baabc1a4-ff8e-4e9e-b40a-726820e1a6e1 24965 1 2023-01-05 08:24:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5c2eaa88-cae3-4f90-ba58-a893798d1218 0xc00420b940 0xc00420b941}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c2eaa88-cae3-4f90-ba58-a893798d1218\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00420b9d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 08:24:14.238: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan  5 08:24:14.238: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-9212  327eaf13-294b-42cc-b26c-caa8e020be95 24956 2 2023-01-05 08:24:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5c2eaa88-cae3-4f90-ba58-a893798d1218 0xc00420b827 0xc00420b828}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c2eaa88-cae3-4f90-ba58-a893798d1218\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00420b8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 08:24:14.239: INFO: Pod "test-recreate-deployment-9d58999df-6zzwt" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-6zzwt test-recreate-deployment-9d58999df- deployment-9212  a83f53cb-56b4-4f24-9d17-81604d212068 24966 0 2023-01-05 08:24:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df baabc1a4-ff8e-4e9e-b40a-726820e1a6e1 0xc0032d89b0 0xc0032d89b1}] [] [{kube-controller-manager Update v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"baabc1a4-ff8e-4e9e-b40a-726820e1a6e1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42vfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42vfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 08:24:14.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9212" for this suite. 01/05/23 08:24:14.241
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":219,"skipped":4410,"failed":0}
------------------------------
â€¢ [2.191 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:12.053
    Jan  5 08:24:12.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename deployment 01/05/23 08:24:12.054
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:12.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:12.069
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan  5 08:24:12.071: INFO: Creating deployment "test-recreate-deployment"
    Jan  5 08:24:12.082: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan  5 08:24:12.088: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan  5 08:24:14.093: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan  5 08:24:14.095: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan  5 08:24:14.109: INFO: Updating deployment test-recreate-deployment
    Jan  5 08:24:14.109: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 08:24:14.236: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9212  5c2eaa88-cae3-4f90-ba58-a893798d1218 24967 2 2023-01-05 08:24:12 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032d8648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 08:24:14 +0000 UTC,LastTransitionTime:2023-01-05 08:24:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-05 08:24:14 +0000 UTC,LastTransitionTime:2023-01-05 08:24:12 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan  5 08:24:14.238: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-9212  baabc1a4-ff8e-4e9e-b40a-726820e1a6e1 24965 1 2023-01-05 08:24:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5c2eaa88-cae3-4f90-ba58-a893798d1218 0xc00420b940 0xc00420b941}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c2eaa88-cae3-4f90-ba58-a893798d1218\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00420b9d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 08:24:14.238: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan  5 08:24:14.238: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-9212  327eaf13-294b-42cc-b26c-caa8e020be95 24956 2 2023-01-05 08:24:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5c2eaa88-cae3-4f90-ba58-a893798d1218 0xc00420b827 0xc00420b828}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c2eaa88-cae3-4f90-ba58-a893798d1218\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00420b8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 08:24:14.239: INFO: Pod "test-recreate-deployment-9d58999df-6zzwt" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-6zzwt test-recreate-deployment-9d58999df- deployment-9212  a83f53cb-56b4-4f24-9d17-81604d212068 24966 0 2023-01-05 08:24:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df baabc1a4-ff8e-4e9e-b40a-726820e1a6e1 0xc0032d89b0 0xc0032d89b1}] [] [{kube-controller-manager Update v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"baabc1a4-ff8e-4e9e-b40a-726820e1a6e1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 08:24:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42vfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42vfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:,StartTime:2023-01-05 08:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 08:24:14.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9212" for this suite. 01/05/23 08:24:14.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:14.245
Jan  5 08:24:14.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename lease-test 01/05/23 08:24:14.246
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:14.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:14.279
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan  5 08:24:14.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2136" for this suite. 01/05/23 08:24:14.336
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":220,"skipped":4420,"failed":0}
------------------------------
â€¢ [0.100 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:14.245
    Jan  5 08:24:14.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename lease-test 01/05/23 08:24:14.246
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:14.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:14.279
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan  5 08:24:14.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-2136" for this suite. 01/05/23 08:24:14.336
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:14.346
Jan  5 08:24:14.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 08:24:14.347
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:14.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:14.364
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/05/23 08:24:14.378
Jan  5 08:24:14.390: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8804" to be "running and ready"
Jan  5 08:24:14.392: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.5451ms
Jan  5 08:24:14.392: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:24:16.396: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005642903s
Jan  5 08:24:16.396: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 08:24:16.396: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/05/23 08:24:16.397
Jan  5 08:24:16.401: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8804" to be "running and ready"
Jan  5 08:24:16.403: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140146ms
Jan  5 08:24:16.403: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:24:18.411: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009861315s
Jan  5 08:24:18.411: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan  5 08:24:18.411: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/05/23 08:24:18.413
STEP: delete the pod with lifecycle hook 01/05/23 08:24:18.425
Jan  5 08:24:18.429: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  5 08:24:18.431: INFO: Pod pod-with-poststart-http-hook still exists
Jan  5 08:24:20.431: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  5 08:24:20.434: INFO: Pod pod-with-poststart-http-hook still exists
Jan  5 08:24:22.432: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  5 08:24:22.436: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  5 08:24:22.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8804" for this suite. 01/05/23 08:24:22.438
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":221,"skipped":4422,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.097 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:14.346
    Jan  5 08:24:14.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 08:24:14.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:14.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:14.364
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 08:24:14.378
    Jan  5 08:24:14.390: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8804" to be "running and ready"
    Jan  5 08:24:14.392: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.5451ms
    Jan  5 08:24:14.392: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:24:16.396: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005642903s
    Jan  5 08:24:16.396: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 08:24:16.396: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/05/23 08:24:16.397
    Jan  5 08:24:16.401: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8804" to be "running and ready"
    Jan  5 08:24:16.403: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140146ms
    Jan  5 08:24:16.403: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:24:18.411: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009861315s
    Jan  5 08:24:18.411: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan  5 08:24:18.411: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/05/23 08:24:18.413
    STEP: delete the pod with lifecycle hook 01/05/23 08:24:18.425
    Jan  5 08:24:18.429: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  5 08:24:18.431: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  5 08:24:20.431: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  5 08:24:20.434: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  5 08:24:22.432: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  5 08:24:22.436: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  5 08:24:22.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8804" for this suite. 01/05/23 08:24:22.438
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:22.443
Jan  5 08:24:22.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename replicaset 01/05/23 08:24:22.444
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:22.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:22.464
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/05/23 08:24:22.471
Jan  5 08:24:22.478: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 08:24:27.481: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 08:24:27.482
STEP: getting scale subresource 01/05/23 08:24:27.482
STEP: updating a scale subresource 01/05/23 08:24:27.483
STEP: verifying the replicaset Spec.Replicas was modified 01/05/23 08:24:27.487
STEP: Patch a scale subresource 01/05/23 08:24:27.489
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 08:24:27.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9900" for this suite. 01/05/23 08:24:27.514
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":222,"skipped":4424,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.088 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:22.443
    Jan  5 08:24:22.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename replicaset 01/05/23 08:24:22.444
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:22.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:22.464
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/05/23 08:24:22.471
    Jan  5 08:24:22.478: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 08:24:27.481: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 08:24:27.482
    STEP: getting scale subresource 01/05/23 08:24:27.482
    STEP: updating a scale subresource 01/05/23 08:24:27.483
    STEP: verifying the replicaset Spec.Replicas was modified 01/05/23 08:24:27.487
    STEP: Patch a scale subresource 01/05/23 08:24:27.489
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 08:24:27.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9900" for this suite. 01/05/23 08:24:27.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:27.532
Jan  5 08:24:27.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:24:27.533
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:27.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:27.559
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:24:27.591
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:24:28.068
STEP: Deploying the webhook pod 01/05/23 08:24:28.074
STEP: Wait for the deployment to be ready 01/05/23 08:24:28.106
Jan  5 08:24:28.138: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  5 08:24:30.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/05/23 08:24:32.149
STEP: Verifying the service has paired with the endpoint 01/05/23 08:24:32.168
Jan  5 08:24:33.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/05/23 08:24:33.172
STEP: create a configmap that should be updated by the webhook 01/05/23 08:24:33.183
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:24:33.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2282" for this suite. 01/05/23 08:24:33.411
STEP: Destroying namespace "webhook-2282-markers" for this suite. 01/05/23 08:24:33.419
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":223,"skipped":4464,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.939 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:27.532
    Jan  5 08:24:27.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:24:27.533
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:27.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:27.559
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:24:27.591
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:24:28.068
    STEP: Deploying the webhook pod 01/05/23 08:24:28.074
    STEP: Wait for the deployment to be ready 01/05/23 08:24:28.106
    Jan  5 08:24:28.138: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  5 08:24:30.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/05/23 08:24:32.149
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:24:32.168
    Jan  5 08:24:33.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/05/23 08:24:33.172
    STEP: create a configmap that should be updated by the webhook 01/05/23 08:24:33.183
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:24:33.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2282" for this suite. 01/05/23 08:24:33.411
    STEP: Destroying namespace "webhook-2282-markers" for this suite. 01/05/23 08:24:33.419
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:33.474
Jan  5 08:24:33.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename svcaccounts 01/05/23 08:24:33.475
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:33.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:33.516
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan  5 08:24:33.642: INFO: created pod pod-service-account-defaultsa
Jan  5 08:24:33.642: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan  5 08:24:33.646: INFO: created pod pod-service-account-mountsa
Jan  5 08:24:33.646: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan  5 08:24:33.657: INFO: created pod pod-service-account-nomountsa
Jan  5 08:24:33.657: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan  5 08:24:33.665: INFO: created pod pod-service-account-defaultsa-mountspec
Jan  5 08:24:33.665: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan  5 08:24:33.676: INFO: created pod pod-service-account-mountsa-mountspec
Jan  5 08:24:33.676: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan  5 08:24:33.690: INFO: created pod pod-service-account-nomountsa-mountspec
Jan  5 08:24:33.690: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan  5 08:24:33.700: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan  5 08:24:33.700: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan  5 08:24:33.710: INFO: created pod pod-service-account-mountsa-nomountspec
Jan  5 08:24:33.710: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan  5 08:24:33.722: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan  5 08:24:33.722: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 08:24:33.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1105" for this suite. 01/05/23 08:24:33.745
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":224,"skipped":4549,"failed":0}
------------------------------
â€¢ [0.283 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:33.474
    Jan  5 08:24:33.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 08:24:33.475
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:33.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:33.516
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan  5 08:24:33.642: INFO: created pod pod-service-account-defaultsa
    Jan  5 08:24:33.642: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan  5 08:24:33.646: INFO: created pod pod-service-account-mountsa
    Jan  5 08:24:33.646: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan  5 08:24:33.657: INFO: created pod pod-service-account-nomountsa
    Jan  5 08:24:33.657: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan  5 08:24:33.665: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan  5 08:24:33.665: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan  5 08:24:33.676: INFO: created pod pod-service-account-mountsa-mountspec
    Jan  5 08:24:33.676: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan  5 08:24:33.690: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan  5 08:24:33.690: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan  5 08:24:33.700: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan  5 08:24:33.700: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan  5 08:24:33.710: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan  5 08:24:33.710: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan  5 08:24:33.722: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan  5 08:24:33.722: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 08:24:33.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1105" for this suite. 01/05/23 08:24:33.745
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:33.758
Jan  5 08:24:33.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename resourcequota 01/05/23 08:24:33.759
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:33.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:33.774
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/05/23 08:24:33.776
STEP: Creating a ResourceQuota 01/05/23 08:24:38.777
STEP: Ensuring resource quota status is calculated 01/05/23 08:24:38.781
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 08:24:40.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3672" for this suite. 01/05/23 08:24:40.785
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":225,"skipped":4549,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.031 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:33.758
    Jan  5 08:24:33.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename resourcequota 01/05/23 08:24:33.759
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:33.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:33.774
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/05/23 08:24:33.776
    STEP: Creating a ResourceQuota 01/05/23 08:24:38.777
    STEP: Ensuring resource quota status is calculated 01/05/23 08:24:38.781
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 08:24:40.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3672" for this suite. 01/05/23 08:24:40.785
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:40.789
Jan  5 08:24:40.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 08:24:40.79
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:40.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:40.806
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-98019479-6a85-41c3-a8b2-4917024d746a 01/05/23 08:24:40.81
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 08:24:40.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3555" for this suite. 01/05/23 08:24:40.813
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":226,"skipped":4550,"failed":0}
------------------------------
â€¢ [0.028 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:40.789
    Jan  5 08:24:40.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 08:24:40.79
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:40.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:40.806
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-98019479-6a85-41c3-a8b2-4917024d746a 01/05/23 08:24:40.81
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 08:24:40.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3555" for this suite. 01/05/23 08:24:40.813
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:40.817
Jan  5 08:24:40.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:24:40.818
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:40.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:40.856
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:24:40.877
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:24:41.296
STEP: Deploying the webhook pod 01/05/23 08:24:41.308
STEP: Wait for the deployment to be ready 01/05/23 08:24:41.325
Jan  5 08:24:41.329: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  5 08:24:43.338: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:24:45.340: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/05/23 08:24:47.341
STEP: Verifying the service has paired with the endpoint 01/05/23 08:24:47.353
Jan  5 08:24:48.354: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan  5 08:24:48.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6638-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 08:24:48.865
STEP: Creating a custom resource while v1 is storage version 01/05/23 08:24:48.875
STEP: Patching Custom Resource Definition to set v2 as storage 01/05/23 08:24:51.135
STEP: Patching the custom resource while v2 is storage version 01/05/23 08:24:51.167
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:24:51.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4781" for this suite. 01/05/23 08:24:51.74
STEP: Destroying namespace "webhook-4781-markers" for this suite. 01/05/23 08:24:51.744
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":227,"skipped":4554,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.022 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:40.817
    Jan  5 08:24:40.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:24:40.818
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:40.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:40.856
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:24:40.877
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:24:41.296
    STEP: Deploying the webhook pod 01/05/23 08:24:41.308
    STEP: Wait for the deployment to be ready 01/05/23 08:24:41.325
    Jan  5 08:24:41.329: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  5 08:24:43.338: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:24:45.340: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/05/23 08:24:47.341
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:24:47.353
    Jan  5 08:24:48.354: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan  5 08:24:48.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6638-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 08:24:48.865
    STEP: Creating a custom resource while v1 is storage version 01/05/23 08:24:48.875
    STEP: Patching Custom Resource Definition to set v2 as storage 01/05/23 08:24:51.135
    STEP: Patching the custom resource while v2 is storage version 01/05/23 08:24:51.167
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:24:51.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4781" for this suite. 01/05/23 08:24:51.74
    STEP: Destroying namespace "webhook-4781-markers" for this suite. 01/05/23 08:24:51.744
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:24:51.84
Jan  5 08:24:51.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename init-container 01/05/23 08:24:51.842
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:51.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:51.863
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/05/23 08:24:51.865
Jan  5 08:24:51.865: INFO: PodSpec: initContainers in spec.initContainers
Jan  5 08:25:37.284: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-be5b5613-bd3b-4da3-8339-1db22d0b7d94", GenerateName:"", Namespace:"init-container-7582", SelfLink:"", UID:"ab2faed6-75e3-41d5-bf4d-8335d28f54f4", ResourceVersion:"25537", Generation:0, CreationTimestamp:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"865277506"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"8df7f4d482308e6a9c8c38d0bd9ca8bde358eb6ef6807489f3c5f65ba06f1c4a", "cni.projectcalico.org/podIP":"10.244.1.107/32", "cni.projectcalico.org/podIPs":"10.244.1.107/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003199140), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 8, 24, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003199188), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 8, 25, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0031991e8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-n6cf7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000abd640), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n6cf7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n6cf7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n6cf7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0050c1b28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"mip-bd-vm724.mip.storage.hpecorp.net", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004197b90), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0050c1ba0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0050c1bc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0050c1bc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0050c1bcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e23940), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"16.0.14.214", PodIP:"10.244.1.107", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.107"}}, StartTime:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004197c70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004197ce0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://2fe07d7f6e9f8dc5dd0628acae4bfb93632d0f31cf21607864b750b0b4f33497", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000abd6e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000abd6c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0050c1c4f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 08:25:37.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7582" for this suite. 01/05/23 08:25:37.287
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":228,"skipped":4571,"failed":0}
------------------------------
â€¢ [SLOW TEST] [45.468 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:24:51.84
    Jan  5 08:24:51.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename init-container 01/05/23 08:24:51.842
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:24:51.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:24:51.863
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/05/23 08:24:51.865
    Jan  5 08:24:51.865: INFO: PodSpec: initContainers in spec.initContainers
    Jan  5 08:25:37.284: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-be5b5613-bd3b-4da3-8339-1db22d0b7d94", GenerateName:"", Namespace:"init-container-7582", SelfLink:"", UID:"ab2faed6-75e3-41d5-bf4d-8335d28f54f4", ResourceVersion:"25537", Generation:0, CreationTimestamp:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"865277506"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"8df7f4d482308e6a9c8c38d0bd9ca8bde358eb6ef6807489f3c5f65ba06f1c4a", "cni.projectcalico.org/podIP":"10.244.1.107/32", "cni.projectcalico.org/podIPs":"10.244.1.107/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003199140), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 8, 24, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003199188), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 8, 25, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0031991e8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-n6cf7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000abd640), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n6cf7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n6cf7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n6cf7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0050c1b28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"mip-bd-vm724.mip.storage.hpecorp.net", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004197b90), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0050c1ba0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0050c1bc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0050c1bc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0050c1bcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e23940), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"16.0.14.214", PodIP:"10.244.1.107", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.107"}}, StartTime:time.Date(2023, time.January, 5, 8, 24, 51, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004197c70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004197ce0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://2fe07d7f6e9f8dc5dd0628acae4bfb93632d0f31cf21607864b750b0b4f33497", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000abd6e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000abd6c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0050c1c4f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 08:25:37.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7582" for this suite. 01/05/23 08:25:37.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:25:37.309
Jan  5 08:25:37.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename job 01/05/23 08:25:37.309
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:25:37.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:25:37.328
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/05/23 08:25:37.331
STEP: Ensuring job reaches completions 01/05/23 08:25:37.342
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 08:25:51.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-474" for this suite. 01/05/23 08:25:51.347
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":229,"skipped":4595,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.042 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:25:37.309
    Jan  5 08:25:37.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename job 01/05/23 08:25:37.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:25:37.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:25:37.328
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/05/23 08:25:37.331
    STEP: Ensuring job reaches completions 01/05/23 08:25:37.342
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 08:25:51.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-474" for this suite. 01/05/23 08:25:51.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:25:51.352
Jan  5 08:25:51.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:25:51.353
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:25:51.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:25:51.367
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-8548 01/05/23 08:25:51.376
STEP: creating service affinity-nodeport-transition in namespace services-8548 01/05/23 08:25:51.376
STEP: creating replication controller affinity-nodeport-transition in namespace services-8548 01/05/23 08:25:51.391
I0105 08:25:51.403880      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8548, replica count: 3
I0105 08:25:54.454570      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 08:25:54.462: INFO: Creating new exec pod
Jan  5 08:25:54.466: INFO: Waiting up to 5m0s for pod "execpod-affinityf7l9q" in namespace "services-8548" to be "running"
Jan  5 08:25:54.467: INFO: Pod "execpod-affinityf7l9q": Phase="Pending", Reason="", readiness=false. Elapsed: 1.461679ms
Jan  5 08:25:56.479: INFO: Pod "execpod-affinityf7l9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.012723442s
Jan  5 08:25:56.479: INFO: Pod "execpod-affinityf7l9q" satisfied condition "running"
Jan  5 08:25:57.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan  5 08:25:57.611: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan  5 08:25:57.611: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:25:57.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.169.127 80'
Jan  5 08:25:57.719: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.169.127 80\nConnection to 10.111.169.127 80 port [tcp/http] succeeded!\n"
Jan  5 08:25:57.719: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:25:57.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.212 30109'
Jan  5 08:25:57.839: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.212 30109\nConnection to 16.0.14.212 30109 port [tcp/*] succeeded!\n"
Jan  5 08:25:57.839: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:25:57.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.214 30109'
Jan  5 08:25:57.956: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.214 30109\nConnection to 16.0.14.214 30109 port [tcp/*] succeeded!\n"
Jan  5 08:25:57.956: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:25:57.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://16.0.14.212:30109/ ; done'
Jan  5 08:25:58.194: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n"
Jan  5 08:25:58.194: INFO: stdout: "\naffinity-nodeport-transition-2kjcv\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-2kjcv\naffinity-nodeport-transition-t7s7j\naffinity-nodeport-transition-2kjcv\naffinity-nodeport-transition-2kjcv\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-t7s7j\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-t7s7j\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-2kjcv\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-t7s7j\naffinity-nodeport-transition-t7s7j"
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-2kjcv
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-2kjcv
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-t7s7j
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-2kjcv
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-2kjcv
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-t7s7j
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-t7s7j
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-2kjcv
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-t7s7j
Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-t7s7j
Jan  5 08:25:58.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://16.0.14.212:30109/ ; done'
Jan  5 08:25:58.421: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n"
Jan  5 08:25:58.421: INFO: stdout: "\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd"
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
Jan  5 08:25:58.421: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8548, will wait for the garbage collector to delete the pods 01/05/23 08:25:58.437
Jan  5 08:25:58.498: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.996398ms
Jan  5 08:25:58.598: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.194245ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:26:00.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8548" for this suite. 01/05/23 08:26:00.541
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":230,"skipped":4631,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.199 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:25:51.352
    Jan  5 08:25:51.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:25:51.353
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:25:51.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:25:51.367
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-8548 01/05/23 08:25:51.376
    STEP: creating service affinity-nodeport-transition in namespace services-8548 01/05/23 08:25:51.376
    STEP: creating replication controller affinity-nodeport-transition in namespace services-8548 01/05/23 08:25:51.391
    I0105 08:25:51.403880      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8548, replica count: 3
    I0105 08:25:54.454570      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 08:25:54.462: INFO: Creating new exec pod
    Jan  5 08:25:54.466: INFO: Waiting up to 5m0s for pod "execpod-affinityf7l9q" in namespace "services-8548" to be "running"
    Jan  5 08:25:54.467: INFO: Pod "execpod-affinityf7l9q": Phase="Pending", Reason="", readiness=false. Elapsed: 1.461679ms
    Jan  5 08:25:56.479: INFO: Pod "execpod-affinityf7l9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.012723442s
    Jan  5 08:25:56.479: INFO: Pod "execpod-affinityf7l9q" satisfied condition "running"
    Jan  5 08:25:57.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan  5 08:25:57.611: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan  5 08:25:57.611: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:25:57.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.169.127 80'
    Jan  5 08:25:57.719: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.169.127 80\nConnection to 10.111.169.127 80 port [tcp/http] succeeded!\n"
    Jan  5 08:25:57.719: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:25:57.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.212 30109'
    Jan  5 08:25:57.839: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.212 30109\nConnection to 16.0.14.212 30109 port [tcp/*] succeeded!\n"
    Jan  5 08:25:57.839: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:25:57.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.214 30109'
    Jan  5 08:25:57.956: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.214 30109\nConnection to 16.0.14.214 30109 port [tcp/*] succeeded!\n"
    Jan  5 08:25:57.956: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:25:57.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://16.0.14.212:30109/ ; done'
    Jan  5 08:25:58.194: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n"
    Jan  5 08:25:58.194: INFO: stdout: "\naffinity-nodeport-transition-2kjcv\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-2kjcv\naffinity-nodeport-transition-t7s7j\naffinity-nodeport-transition-2kjcv\naffinity-nodeport-transition-2kjcv\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-t7s7j\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-t7s7j\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-2kjcv\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-t7s7j\naffinity-nodeport-transition-t7s7j"
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-2kjcv
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-2kjcv
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-t7s7j
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-2kjcv
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-2kjcv
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-t7s7j
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-t7s7j
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-2kjcv
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-t7s7j
    Jan  5 08:25:58.194: INFO: Received response from host: affinity-nodeport-transition-t7s7j
    Jan  5 08:25:58.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-8548 exec execpod-affinityf7l9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://16.0.14.212:30109/ ; done'
    Jan  5 08:25:58.421: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30109/\n"
    Jan  5 08:25:58.421: INFO: stdout: "\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd\naffinity-nodeport-transition-pt9fd"
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Received response from host: affinity-nodeport-transition-pt9fd
    Jan  5 08:25:58.421: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8548, will wait for the garbage collector to delete the pods 01/05/23 08:25:58.437
    Jan  5 08:25:58.498: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.996398ms
    Jan  5 08:25:58.598: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.194245ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:26:00.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8548" for this suite. 01/05/23 08:26:00.541
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:26:00.551
Jan  5 08:26:00.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename ingressclass 01/05/23 08:26:00.552
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:00.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:00.566
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/05/23 08:26:00.568
STEP: getting /apis/networking.k8s.io 01/05/23 08:26:00.569
STEP: getting /apis/networking.k8s.iov1 01/05/23 08:26:00.569
STEP: creating 01/05/23 08:26:00.57
STEP: getting 01/05/23 08:26:00.589
STEP: listing 01/05/23 08:26:00.59
STEP: watching 01/05/23 08:26:00.592
Jan  5 08:26:00.592: INFO: starting watch
STEP: patching 01/05/23 08:26:00.592
STEP: updating 01/05/23 08:26:00.596
Jan  5 08:26:00.606: INFO: waiting for watch events with expected annotations
Jan  5 08:26:00.606: INFO: saw patched and updated annotations
STEP: deleting 01/05/23 08:26:00.606
STEP: deleting a collection 01/05/23 08:26:00.612
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan  5 08:26:00.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2176" for this suite. 01/05/23 08:26:00.631
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":231,"skipped":4632,"failed":0}
------------------------------
â€¢ [0.094 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:26:00.551
    Jan  5 08:26:00.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename ingressclass 01/05/23 08:26:00.552
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:00.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:00.566
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/05/23 08:26:00.568
    STEP: getting /apis/networking.k8s.io 01/05/23 08:26:00.569
    STEP: getting /apis/networking.k8s.iov1 01/05/23 08:26:00.569
    STEP: creating 01/05/23 08:26:00.57
    STEP: getting 01/05/23 08:26:00.589
    STEP: listing 01/05/23 08:26:00.59
    STEP: watching 01/05/23 08:26:00.592
    Jan  5 08:26:00.592: INFO: starting watch
    STEP: patching 01/05/23 08:26:00.592
    STEP: updating 01/05/23 08:26:00.596
    Jan  5 08:26:00.606: INFO: waiting for watch events with expected annotations
    Jan  5 08:26:00.606: INFO: saw patched and updated annotations
    STEP: deleting 01/05/23 08:26:00.606
    STEP: deleting a collection 01/05/23 08:26:00.612
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan  5 08:26:00.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-2176" for this suite. 01/05/23 08:26:00.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:26:00.646
Jan  5 08:26:00.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename runtimeclass 01/05/23 08:26:00.647
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:00.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:00.661
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/05/23 08:26:00.663
STEP: getting /apis/node.k8s.io 01/05/23 08:26:00.664
STEP: getting /apis/node.k8s.io/v1 01/05/23 08:26:00.665
STEP: creating 01/05/23 08:26:00.665
STEP: watching 01/05/23 08:26:00.681
Jan  5 08:26:00.681: INFO: starting watch
STEP: getting 01/05/23 08:26:00.685
STEP: listing 01/05/23 08:26:00.686
STEP: patching 01/05/23 08:26:00.688
STEP: updating 01/05/23 08:26:00.695
Jan  5 08:26:00.698: INFO: waiting for watch events with expected annotations
STEP: deleting 01/05/23 08:26:00.698
STEP: deleting a collection 01/05/23 08:26:00.708
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  5 08:26:00.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5413" for this suite. 01/05/23 08:26:00.721
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":232,"skipped":4648,"failed":0}
------------------------------
â€¢ [0.078 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:26:00.646
    Jan  5 08:26:00.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 08:26:00.647
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:00.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:00.661
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/05/23 08:26:00.663
    STEP: getting /apis/node.k8s.io 01/05/23 08:26:00.664
    STEP: getting /apis/node.k8s.io/v1 01/05/23 08:26:00.665
    STEP: creating 01/05/23 08:26:00.665
    STEP: watching 01/05/23 08:26:00.681
    Jan  5 08:26:00.681: INFO: starting watch
    STEP: getting 01/05/23 08:26:00.685
    STEP: listing 01/05/23 08:26:00.686
    STEP: patching 01/05/23 08:26:00.688
    STEP: updating 01/05/23 08:26:00.695
    Jan  5 08:26:00.698: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/05/23 08:26:00.698
    STEP: deleting a collection 01/05/23 08:26:00.708
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  5 08:26:00.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5413" for this suite. 01/05/23 08:26:00.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:26:00.725
Jan  5 08:26:00.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 08:26:00.726
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:00.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:00.738
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-2d62c367-bd83-4eaf-a646-8e0670455d79 01/05/23 08:26:00.742
STEP: Creating a pod to test consume configMaps 01/05/23 08:26:00.751
Jan  5 08:26:00.756: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696" in namespace "configmap-5714" to be "Succeeded or Failed"
Jan  5 08:26:00.758: INFO: Pod "pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581041ms
Jan  5 08:26:02.761: INFO: Pod "pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004903744s
Jan  5 08:26:04.761: INFO: Pod "pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004710144s
STEP: Saw pod success 01/05/23 08:26:04.761
Jan  5 08:26:04.761: INFO: Pod "pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696" satisfied condition "Succeeded or Failed"
Jan  5 08:26:04.762: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:26:04.773
Jan  5 08:26:04.786: INFO: Waiting for pod pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696 to disappear
Jan  5 08:26:04.788: INFO: Pod pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 08:26:04.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5714" for this suite. 01/05/23 08:26:04.789
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":233,"skipped":4655,"failed":0}
------------------------------
â€¢ [4.072 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:26:00.725
    Jan  5 08:26:00.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 08:26:00.726
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:00.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:00.738
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-2d62c367-bd83-4eaf-a646-8e0670455d79 01/05/23 08:26:00.742
    STEP: Creating a pod to test consume configMaps 01/05/23 08:26:00.751
    Jan  5 08:26:00.756: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696" in namespace "configmap-5714" to be "Succeeded or Failed"
    Jan  5 08:26:00.758: INFO: Pod "pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581041ms
    Jan  5 08:26:02.761: INFO: Pod "pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004903744s
    Jan  5 08:26:04.761: INFO: Pod "pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004710144s
    STEP: Saw pod success 01/05/23 08:26:04.761
    Jan  5 08:26:04.761: INFO: Pod "pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696" satisfied condition "Succeeded or Failed"
    Jan  5 08:26:04.762: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:26:04.773
    Jan  5 08:26:04.786: INFO: Waiting for pod pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696 to disappear
    Jan  5 08:26:04.788: INFO: Pod pod-configmaps-bb0df57f-c8a2-4c39-843a-7320249f7696 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 08:26:04.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5714" for this suite. 01/05/23 08:26:04.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:26:04.798
Jan  5 08:26:04.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename replicaset 01/05/23 08:26:04.799
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:04.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:04.815
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan  5 08:26:04.826: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 08:26:09.830: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 08:26:09.83
STEP: Scaling up "test-rs" replicaset  01/05/23 08:26:09.83
Jan  5 08:26:09.857: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/05/23 08:26:09.857
W0105 08:26:09.868107      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  5 08:26:09.869: INFO: observed ReplicaSet test-rs in namespace replicaset-3799 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 08:26:09.892: INFO: observed ReplicaSet test-rs in namespace replicaset-3799 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 08:26:09.924: INFO: observed ReplicaSet test-rs in namespace replicaset-3799 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 08:26:09.935: INFO: observed ReplicaSet test-rs in namespace replicaset-3799 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 08:26:11.349: INFO: observed ReplicaSet test-rs in namespace replicaset-3799 with ReadyReplicas 2, AvailableReplicas 2
Jan  5 08:26:12.402: INFO: observed Replicaset test-rs in namespace replicaset-3799 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 08:26:12.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3799" for this suite. 01/05/23 08:26:12.405
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":234,"skipped":4670,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.612 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:26:04.798
    Jan  5 08:26:04.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename replicaset 01/05/23 08:26:04.799
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:04.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:04.815
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan  5 08:26:04.826: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 08:26:09.830: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 08:26:09.83
    STEP: Scaling up "test-rs" replicaset  01/05/23 08:26:09.83
    Jan  5 08:26:09.857: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/05/23 08:26:09.857
    W0105 08:26:09.868107      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  5 08:26:09.869: INFO: observed ReplicaSet test-rs in namespace replicaset-3799 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 08:26:09.892: INFO: observed ReplicaSet test-rs in namespace replicaset-3799 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 08:26:09.924: INFO: observed ReplicaSet test-rs in namespace replicaset-3799 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 08:26:09.935: INFO: observed ReplicaSet test-rs in namespace replicaset-3799 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 08:26:11.349: INFO: observed ReplicaSet test-rs in namespace replicaset-3799 with ReadyReplicas 2, AvailableReplicas 2
    Jan  5 08:26:12.402: INFO: observed Replicaset test-rs in namespace replicaset-3799 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 08:26:12.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3799" for this suite. 01/05/23 08:26:12.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:26:12.41
Jan  5 08:26:12.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename namespaces 01/05/23 08:26:12.411
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:12.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:12.434
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/05/23 08:26:12.436
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:12.449
STEP: Creating a pod in the namespace 01/05/23 08:26:12.452
STEP: Waiting for the pod to have running status 01/05/23 08:26:12.465
Jan  5 08:26:12.465: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9429" to be "running"
Jan  5 08:26:12.467: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.84325ms
Jan  5 08:26:14.471: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006308698s
Jan  5 08:26:14.471: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/05/23 08:26:14.471
STEP: Waiting for the namespace to be removed. 01/05/23 08:26:14.476
STEP: Recreating the namespace 01/05/23 08:26:25.48
STEP: Verifying there are no pods in the namespace 01/05/23 08:26:25.497
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:26:25.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7815" for this suite. 01/05/23 08:26:25.501
STEP: Destroying namespace "nsdeletetest-9429" for this suite. 01/05/23 08:26:25.509
Jan  5 08:26:25.511: INFO: Namespace nsdeletetest-9429 was already deleted
STEP: Destroying namespace "nsdeletetest-4007" for this suite. 01/05/23 08:26:25.511
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":235,"skipped":4676,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.104 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:26:12.41
    Jan  5 08:26:12.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename namespaces 01/05/23 08:26:12.411
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:12.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:12.434
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/05/23 08:26:12.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:12.449
    STEP: Creating a pod in the namespace 01/05/23 08:26:12.452
    STEP: Waiting for the pod to have running status 01/05/23 08:26:12.465
    Jan  5 08:26:12.465: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9429" to be "running"
    Jan  5 08:26:12.467: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.84325ms
    Jan  5 08:26:14.471: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006308698s
    Jan  5 08:26:14.471: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/05/23 08:26:14.471
    STEP: Waiting for the namespace to be removed. 01/05/23 08:26:14.476
    STEP: Recreating the namespace 01/05/23 08:26:25.48
    STEP: Verifying there are no pods in the namespace 01/05/23 08:26:25.497
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:26:25.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-7815" for this suite. 01/05/23 08:26:25.501
    STEP: Destroying namespace "nsdeletetest-9429" for this suite. 01/05/23 08:26:25.509
    Jan  5 08:26:25.511: INFO: Namespace nsdeletetest-9429 was already deleted
    STEP: Destroying namespace "nsdeletetest-4007" for this suite. 01/05/23 08:26:25.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:26:25.515
Jan  5 08:26:25.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename gc 01/05/23 08:26:25.516
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:25.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:25.529
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/05/23 08:26:25.534
STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 08:26:25.538
STEP: delete the deployment 01/05/23 08:26:26.045
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/05/23 08:26:26.048
STEP: Gathering metrics 01/05/23 08:26:26.56
W0105 08:26:26.563045      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 08:26:26.563: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 08:26:26.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1156" for this suite. 01/05/23 08:26:26.564
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":236,"skipped":4692,"failed":0}
------------------------------
â€¢ [1.058 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:26:25.515
    Jan  5 08:26:25.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename gc 01/05/23 08:26:25.516
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:25.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:25.529
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/05/23 08:26:25.534
    STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 08:26:25.538
    STEP: delete the deployment 01/05/23 08:26:26.045
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/05/23 08:26:26.048
    STEP: Gathering metrics 01/05/23 08:26:26.56
    W0105 08:26:26.563045      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 08:26:26.563: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 08:26:26.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1156" for this suite. 01/05/23 08:26:26.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:26:26.574
Jan  5 08:26:26.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sched-pred 01/05/23 08:26:26.575
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:26.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:26.589
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  5 08:26:26.591: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 08:26:26.595: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 08:26:26.596: INFO: 
Logging pods the apiserver thinks is on node mip-bd-vm722.mip.storage.hpecorp.net before test
Jan  5 08:26:26.599: INFO: csi-hostpathplugin-0 from default started at 2023-01-05 07:12:21 +0000 UTC (8 container statuses recorded)
Jan  5 08:26:26.599: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  5 08:26:26.599: INFO: 	Container csi-external-health-monitor-controller ready: true, restart count 1
Jan  5 08:26:26.599: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  5 08:26:26.599: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  5 08:26:26.599: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  5 08:26:26.599: INFO: 	Container hostpath ready: true, restart count 0
Jan  5 08:26:26.599: INFO: 	Container liveness-probe ready: true, restart count 0
Jan  5 08:26:26.599: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan  5 08:26:26.599: INFO: simpletest.deployment-5db9f64cbf-b22sv from gc-1156 started at 2023-01-05 08:26:25 +0000 UTC (1 container statuses recorded)
Jan  5 08:26:26.599: INFO: 	Container nginx ready: false, restart count 0
Jan  5 08:26:26.599: INFO: canal-6z7xb from kube-system started at 2023-01-05 07:12:02 +0000 UTC (2 container statuses recorded)
Jan  5 08:26:26.599: INFO: 	Container calico-node ready: true, restart count 0
Jan  5 08:26:26.599: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  5 08:26:26.599: INFO: coredns-564fd8c776-nwmff from kube-system started at 2023-01-05 07:12:21 +0000 UTC (1 container statuses recorded)
Jan  5 08:26:26.599: INFO: 	Container coredns ready: true, restart count 0
Jan  5 08:26:26.599: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2p88t from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
Jan  5 08:26:26.599: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 08:26:26.599: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 08:26:26.599: INFO: 
Logging pods the apiserver thinks is on node mip-bd-vm724.mip.storage.hpecorp.net before test
Jan  5 08:26:26.603: INFO: simpletest.deployment-5db9f64cbf-k72pv from gc-1156 started at 2023-01-05 08:26:25 +0000 UTC (1 container statuses recorded)
Jan  5 08:26:26.603: INFO: 	Container nginx ready: false, restart count 0
Jan  5 08:26:26.603: INFO: canal-6sfgp from kube-system started at 2023-01-05 07:21:40 +0000 UTC (2 container statuses recorded)
Jan  5 08:26:26.603: INFO: 	Container calico-node ready: true, restart count 0
Jan  5 08:26:26.603: INFO: 	Container kube-flannel ready: true, restart count 0
Jan  5 08:26:26.603: INFO: sonobuoy from sonobuoy started at 2023-01-05 07:22:09 +0000 UTC (1 container statuses recorded)
Jan  5 08:26:26.603: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 08:26:26.603: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2qcx8 from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
Jan  5 08:26:26.603: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 08:26:26.603: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 08:26:26.603: INFO: pod-service-account-defaultsa from svcaccounts-1105 started at 2023-01-05 08:24:33 +0000 UTC (1 container statuses recorded)
Jan  5 08:26:26.603: INFO: 	Container token-test ready: false, restart count 0
Jan  5 08:26:26.603: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-1105 started at 2023-01-05 08:24:33 +0000 UTC (1 container statuses recorded)
Jan  5 08:26:26.603: INFO: 	Container token-test ready: false, restart count 0
Jan  5 08:26:26.603: INFO: pod-service-account-mountsa from svcaccounts-1105 started at 2023-01-05 08:24:33 +0000 UTC (1 container statuses recorded)
Jan  5 08:26:26.603: INFO: 	Container token-test ready: false, restart count 0
Jan  5 08:26:26.603: INFO: pod-service-account-mountsa-mountspec from svcaccounts-1105 started at 2023-01-05 08:24:33 +0000 UTC (1 container statuses recorded)
Jan  5 08:26:26.603: INFO: 	Container token-test ready: false, restart count 0
Jan  5 08:26:26.603: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-1105 started at 2023-01-05 08:24:33 +0000 UTC (1 container statuses recorded)
Jan  5 08:26:26.603: INFO: 	Container token-test ready: false, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 08:26:26.603
Jan  5 08:26:26.612: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2407" to be "running"
Jan  5 08:26:26.614: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.628964ms
Jan  5 08:26:28.616: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004332575s
Jan  5 08:26:28.616: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 08:26:28.618
STEP: Trying to apply a random label on the found node. 01/05/23 08:26:28.635
STEP: verifying the node has the label kubernetes.io/e2e-f88123d4-c3d1-4e60-bd83-903297ceadeb 95 01/05/23 08:26:28.645
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/05/23 08:26:28.647
Jan  5 08:26:28.651: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2407" to be "not pending"
Jan  5 08:26:28.653: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489412ms
Jan  5 08:26:30.658: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006492764s
Jan  5 08:26:30.658: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 16.0.14.214 on the node which pod4 resides and expect not scheduled 01/05/23 08:26:30.658
Jan  5 08:26:30.667: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2407" to be "not pending"
Jan  5 08:26:30.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.586485ms
Jan  5 08:26:32.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005093929s
Jan  5 08:26:34.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004749126s
Jan  5 08:26:36.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005844435s
Jan  5 08:26:38.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004401555s
Jan  5 08:26:40.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005815247s
Jan  5 08:26:42.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00465101s
Jan  5 08:26:44.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004435783s
Jan  5 08:26:46.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006335713s
Jan  5 08:26:48.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006195507s
Jan  5 08:26:50.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005138594s
Jan  5 08:26:52.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006143293s
Jan  5 08:26:54.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005852588s
Jan  5 08:26:56.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004598191s
Jan  5 08:26:58.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005552213s
Jan  5 08:27:00.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004425267s
Jan  5 08:27:02.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004960417s
Jan  5 08:27:04.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005171389s
Jan  5 08:27:06.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006448158s
Jan  5 08:27:08.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006728816s
Jan  5 08:27:10.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005418173s
Jan  5 08:27:12.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006034354s
Jan  5 08:27:14.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005092566s
Jan  5 08:27:16.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006042384s
Jan  5 08:27:18.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00617407s
Jan  5 08:27:20.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004795415s
Jan  5 08:27:22.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004990553s
Jan  5 08:27:24.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004751507s
Jan  5 08:27:26.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005636686s
Jan  5 08:27:28.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.003837664s
Jan  5 08:27:30.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005278781s
Jan  5 08:27:32.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005631142s
Jan  5 08:27:34.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004892205s
Jan  5 08:27:36.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006872806s
Jan  5 08:27:38.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005718229s
Jan  5 08:27:40.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004633299s
Jan  5 08:27:42.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006775664s
Jan  5 08:27:44.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004348382s
Jan  5 08:27:46.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006184923s
Jan  5 08:27:48.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006914239s
Jan  5 08:27:50.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005835081s
Jan  5 08:27:52.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004860088s
Jan  5 08:27:54.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005111756s
Jan  5 08:27:56.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005084649s
Jan  5 08:27:58.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004674838s
Jan  5 08:28:00.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006087815s
Jan  5 08:28:02.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.00595317s
Jan  5 08:28:04.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005563804s
Jan  5 08:28:06.678: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011392141s
Jan  5 08:28:08.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00596642s
Jan  5 08:28:10.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005097684s
Jan  5 08:28:12.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006673492s
Jan  5 08:28:14.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004805583s
Jan  5 08:28:16.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004885748s
Jan  5 08:28:18.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005952933s
Jan  5 08:28:20.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004564558s
Jan  5 08:28:22.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005627441s
Jan  5 08:28:24.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00445075s
Jan  5 08:28:26.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006366267s
Jan  5 08:28:28.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005287092s
Jan  5 08:28:30.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.0061205s
Jan  5 08:28:32.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005870183s
Jan  5 08:28:34.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005018356s
Jan  5 08:28:36.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006191418s
Jan  5 08:28:38.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.006100468s
Jan  5 08:28:40.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004716407s
Jan  5 08:28:42.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006115326s
Jan  5 08:28:44.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005313681s
Jan  5 08:28:46.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004829057s
Jan  5 08:28:48.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005300598s
Jan  5 08:28:50.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.005036118s
Jan  5 08:28:52.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005438736s
Jan  5 08:28:54.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005522258s
Jan  5 08:28:56.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006322394s
Jan  5 08:28:58.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004538982s
Jan  5 08:29:00.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004713907s
Jan  5 08:29:02.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005419118s
Jan  5 08:29:04.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005037406s
Jan  5 08:29:06.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.005830135s
Jan  5 08:29:08.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.005029066s
Jan  5 08:29:10.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.004915166s
Jan  5 08:29:12.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.004898866s
Jan  5 08:29:14.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.005107143s
Jan  5 08:29:16.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005404216s
Jan  5 08:29:18.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.006470322s
Jan  5 08:29:20.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004803854s
Jan  5 08:29:22.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.00506884s
Jan  5 08:29:24.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.005450727s
Jan  5 08:29:26.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.006535273s
Jan  5 08:29:28.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.00516841s
Jan  5 08:29:30.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.005339015s
Jan  5 08:29:32.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005858948s
Jan  5 08:29:34.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.005072867s
Jan  5 08:29:36.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005835874s
Jan  5 08:29:38.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.005598523s
Jan  5 08:29:40.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.004985943s
Jan  5 08:29:42.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005022469s
Jan  5 08:29:44.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006142433s
Jan  5 08:29:46.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006687485s
Jan  5 08:29:48.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007039361s
Jan  5 08:29:50.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005028576s
Jan  5 08:29:52.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004746857s
Jan  5 08:29:54.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005184271s
Jan  5 08:29:56.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.005782013s
Jan  5 08:29:58.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.004263172s
Jan  5 08:30:00.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.005307053s
Jan  5 08:30:02.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005569571s
Jan  5 08:30:04.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.003962514s
Jan  5 08:30:06.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.00611192s
Jan  5 08:30:08.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.005460957s
Jan  5 08:30:10.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004277566s
Jan  5 08:30:12.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.004500294s
Jan  5 08:30:14.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004039057s
Jan  5 08:30:16.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.004189308s
Jan  5 08:30:18.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.004367789s
Jan  5 08:30:20.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00551754s
Jan  5 08:30:22.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.005002394s
Jan  5 08:30:24.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.004989153s
Jan  5 08:30:26.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.005137508s
Jan  5 08:30:28.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004085113s
Jan  5 08:30:30.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.004427749s
Jan  5 08:30:32.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007014297s
Jan  5 08:30:34.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005922529s
Jan  5 08:30:36.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005675283s
Jan  5 08:30:38.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006135833s
Jan  5 08:30:40.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.005843115s
Jan  5 08:30:42.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.005322767s
Jan  5 08:30:44.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.005605693s
Jan  5 08:30:46.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006152933s
Jan  5 08:30:48.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.0056206s
Jan  5 08:30:50.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005936625s
Jan  5 08:30:52.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.0047214s
Jan  5 08:30:54.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005209128s
Jan  5 08:30:56.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.006113516s
Jan  5 08:30:58.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.004408848s
Jan  5 08:31:00.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005764295s
Jan  5 08:31:02.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005610733s
Jan  5 08:31:04.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005257554s
Jan  5 08:31:06.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007312388s
Jan  5 08:31:08.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.004800204s
Jan  5 08:31:10.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004500872s
Jan  5 08:31:12.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006470531s
Jan  5 08:31:14.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.005022503s
Jan  5 08:31:16.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.005268729s
Jan  5 08:31:18.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006525656s
Jan  5 08:31:20.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.004314679s
Jan  5 08:31:22.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005368097s
Jan  5 08:31:24.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.006530988s
Jan  5 08:31:26.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.005954715s
Jan  5 08:31:28.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.005261948s
Jan  5 08:31:30.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005932551s
Jan  5 08:31:30.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007902395s
STEP: removing the label kubernetes.io/e2e-f88123d4-c3d1-4e60-bd83-903297ceadeb off the node mip-bd-vm724.mip.storage.hpecorp.net 01/05/23 08:31:30.675
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f88123d4-c3d1-4e60-bd83-903297ceadeb 01/05/23 08:31:30.683
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:31:30.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2407" for this suite. 01/05/23 08:31:30.689
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":237,"skipped":4710,"failed":0}
------------------------------
â€¢ [SLOW TEST] [304.126 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:26:26.574
    Jan  5 08:26:26.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sched-pred 01/05/23 08:26:26.575
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:26:26.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:26:26.589
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  5 08:26:26.591: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 08:26:26.595: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 08:26:26.596: INFO: 
    Logging pods the apiserver thinks is on node mip-bd-vm722.mip.storage.hpecorp.net before test
    Jan  5 08:26:26.599: INFO: csi-hostpathplugin-0 from default started at 2023-01-05 07:12:21 +0000 UTC (8 container statuses recorded)
    Jan  5 08:26:26.599: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: 	Container csi-external-health-monitor-controller ready: true, restart count 1
    Jan  5 08:26:26.599: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: 	Container hostpath ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: simpletest.deployment-5db9f64cbf-b22sv from gc-1156 started at 2023-01-05 08:26:25 +0000 UTC (1 container statuses recorded)
    Jan  5 08:26:26.599: INFO: 	Container nginx ready: false, restart count 0
    Jan  5 08:26:26.599: INFO: canal-6z7xb from kube-system started at 2023-01-05 07:12:02 +0000 UTC (2 container statuses recorded)
    Jan  5 08:26:26.599: INFO: 	Container calico-node ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: coredns-564fd8c776-nwmff from kube-system started at 2023-01-05 07:12:21 +0000 UTC (1 container statuses recorded)
    Jan  5 08:26:26.599: INFO: 	Container coredns ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2p88t from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
    Jan  5 08:26:26.599: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 08:26:26.599: INFO: 
    Logging pods the apiserver thinks is on node mip-bd-vm724.mip.storage.hpecorp.net before test
    Jan  5 08:26:26.603: INFO: simpletest.deployment-5db9f64cbf-k72pv from gc-1156 started at 2023-01-05 08:26:25 +0000 UTC (1 container statuses recorded)
    Jan  5 08:26:26.603: INFO: 	Container nginx ready: false, restart count 0
    Jan  5 08:26:26.603: INFO: canal-6sfgp from kube-system started at 2023-01-05 07:21:40 +0000 UTC (2 container statuses recorded)
    Jan  5 08:26:26.603: INFO: 	Container calico-node ready: true, restart count 0
    Jan  5 08:26:26.603: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan  5 08:26:26.603: INFO: sonobuoy from sonobuoy started at 2023-01-05 07:22:09 +0000 UTC (1 container statuses recorded)
    Jan  5 08:26:26.603: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 08:26:26.603: INFO: sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2qcx8 from sonobuoy started at 2023-01-05 07:22:28 +0000 UTC (2 container statuses recorded)
    Jan  5 08:26:26.603: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 08:26:26.603: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 08:26:26.603: INFO: pod-service-account-defaultsa from svcaccounts-1105 started at 2023-01-05 08:24:33 +0000 UTC (1 container statuses recorded)
    Jan  5 08:26:26.603: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 08:26:26.603: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-1105 started at 2023-01-05 08:24:33 +0000 UTC (1 container statuses recorded)
    Jan  5 08:26:26.603: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 08:26:26.603: INFO: pod-service-account-mountsa from svcaccounts-1105 started at 2023-01-05 08:24:33 +0000 UTC (1 container statuses recorded)
    Jan  5 08:26:26.603: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 08:26:26.603: INFO: pod-service-account-mountsa-mountspec from svcaccounts-1105 started at 2023-01-05 08:24:33 +0000 UTC (1 container statuses recorded)
    Jan  5 08:26:26.603: INFO: 	Container token-test ready: false, restart count 0
    Jan  5 08:26:26.603: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-1105 started at 2023-01-05 08:24:33 +0000 UTC (1 container statuses recorded)
    Jan  5 08:26:26.603: INFO: 	Container token-test ready: false, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 08:26:26.603
    Jan  5 08:26:26.612: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2407" to be "running"
    Jan  5 08:26:26.614: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.628964ms
    Jan  5 08:26:28.616: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004332575s
    Jan  5 08:26:28.616: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 08:26:28.618
    STEP: Trying to apply a random label on the found node. 01/05/23 08:26:28.635
    STEP: verifying the node has the label kubernetes.io/e2e-f88123d4-c3d1-4e60-bd83-903297ceadeb 95 01/05/23 08:26:28.645
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/05/23 08:26:28.647
    Jan  5 08:26:28.651: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2407" to be "not pending"
    Jan  5 08:26:28.653: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489412ms
    Jan  5 08:26:30.658: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006492764s
    Jan  5 08:26:30.658: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 16.0.14.214 on the node which pod4 resides and expect not scheduled 01/05/23 08:26:30.658
    Jan  5 08:26:30.667: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2407" to be "not pending"
    Jan  5 08:26:30.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.586485ms
    Jan  5 08:26:32.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005093929s
    Jan  5 08:26:34.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004749126s
    Jan  5 08:26:36.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005844435s
    Jan  5 08:26:38.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004401555s
    Jan  5 08:26:40.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005815247s
    Jan  5 08:26:42.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00465101s
    Jan  5 08:26:44.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004435783s
    Jan  5 08:26:46.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006335713s
    Jan  5 08:26:48.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006195507s
    Jan  5 08:26:50.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005138594s
    Jan  5 08:26:52.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006143293s
    Jan  5 08:26:54.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005852588s
    Jan  5 08:26:56.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004598191s
    Jan  5 08:26:58.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005552213s
    Jan  5 08:27:00.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004425267s
    Jan  5 08:27:02.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004960417s
    Jan  5 08:27:04.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005171389s
    Jan  5 08:27:06.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006448158s
    Jan  5 08:27:08.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006728816s
    Jan  5 08:27:10.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005418173s
    Jan  5 08:27:12.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006034354s
    Jan  5 08:27:14.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005092566s
    Jan  5 08:27:16.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006042384s
    Jan  5 08:27:18.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00617407s
    Jan  5 08:27:20.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004795415s
    Jan  5 08:27:22.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004990553s
    Jan  5 08:27:24.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004751507s
    Jan  5 08:27:26.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005636686s
    Jan  5 08:27:28.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.003837664s
    Jan  5 08:27:30.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005278781s
    Jan  5 08:27:32.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005631142s
    Jan  5 08:27:34.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004892205s
    Jan  5 08:27:36.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006872806s
    Jan  5 08:27:38.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005718229s
    Jan  5 08:27:40.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004633299s
    Jan  5 08:27:42.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006775664s
    Jan  5 08:27:44.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004348382s
    Jan  5 08:27:46.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006184923s
    Jan  5 08:27:48.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006914239s
    Jan  5 08:27:50.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005835081s
    Jan  5 08:27:52.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004860088s
    Jan  5 08:27:54.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005111756s
    Jan  5 08:27:56.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005084649s
    Jan  5 08:27:58.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004674838s
    Jan  5 08:28:00.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006087815s
    Jan  5 08:28:02.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.00595317s
    Jan  5 08:28:04.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005563804s
    Jan  5 08:28:06.678: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011392141s
    Jan  5 08:28:08.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00596642s
    Jan  5 08:28:10.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005097684s
    Jan  5 08:28:12.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006673492s
    Jan  5 08:28:14.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004805583s
    Jan  5 08:28:16.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004885748s
    Jan  5 08:28:18.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005952933s
    Jan  5 08:28:20.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004564558s
    Jan  5 08:28:22.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005627441s
    Jan  5 08:28:24.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00445075s
    Jan  5 08:28:26.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006366267s
    Jan  5 08:28:28.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005287092s
    Jan  5 08:28:30.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.0061205s
    Jan  5 08:28:32.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005870183s
    Jan  5 08:28:34.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005018356s
    Jan  5 08:28:36.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006191418s
    Jan  5 08:28:38.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.006100468s
    Jan  5 08:28:40.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004716407s
    Jan  5 08:28:42.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006115326s
    Jan  5 08:28:44.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005313681s
    Jan  5 08:28:46.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004829057s
    Jan  5 08:28:48.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005300598s
    Jan  5 08:28:50.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.005036118s
    Jan  5 08:28:52.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005438736s
    Jan  5 08:28:54.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005522258s
    Jan  5 08:28:56.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006322394s
    Jan  5 08:28:58.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004538982s
    Jan  5 08:29:00.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004713907s
    Jan  5 08:29:02.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005419118s
    Jan  5 08:29:04.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005037406s
    Jan  5 08:29:06.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.005830135s
    Jan  5 08:29:08.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.005029066s
    Jan  5 08:29:10.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.004915166s
    Jan  5 08:29:12.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.004898866s
    Jan  5 08:29:14.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.005107143s
    Jan  5 08:29:16.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005404216s
    Jan  5 08:29:18.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.006470322s
    Jan  5 08:29:20.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004803854s
    Jan  5 08:29:22.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.00506884s
    Jan  5 08:29:24.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.005450727s
    Jan  5 08:29:26.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.006535273s
    Jan  5 08:29:28.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.00516841s
    Jan  5 08:29:30.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.005339015s
    Jan  5 08:29:32.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005858948s
    Jan  5 08:29:34.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.005072867s
    Jan  5 08:29:36.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005835874s
    Jan  5 08:29:38.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.005598523s
    Jan  5 08:29:40.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.004985943s
    Jan  5 08:29:42.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005022469s
    Jan  5 08:29:44.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006142433s
    Jan  5 08:29:46.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006687485s
    Jan  5 08:29:48.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007039361s
    Jan  5 08:29:50.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005028576s
    Jan  5 08:29:52.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004746857s
    Jan  5 08:29:54.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005184271s
    Jan  5 08:29:56.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.005782013s
    Jan  5 08:29:58.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.004263172s
    Jan  5 08:30:00.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.005307053s
    Jan  5 08:30:02.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005569571s
    Jan  5 08:30:04.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.003962514s
    Jan  5 08:30:06.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.00611192s
    Jan  5 08:30:08.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.005460957s
    Jan  5 08:30:10.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004277566s
    Jan  5 08:30:12.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.004500294s
    Jan  5 08:30:14.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004039057s
    Jan  5 08:30:16.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.004189308s
    Jan  5 08:30:18.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.004367789s
    Jan  5 08:30:20.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00551754s
    Jan  5 08:30:22.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.005002394s
    Jan  5 08:30:24.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.004989153s
    Jan  5 08:30:26.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.005137508s
    Jan  5 08:30:28.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004085113s
    Jan  5 08:30:30.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.004427749s
    Jan  5 08:30:32.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007014297s
    Jan  5 08:30:34.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005922529s
    Jan  5 08:30:36.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005675283s
    Jan  5 08:30:38.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006135833s
    Jan  5 08:30:40.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.005843115s
    Jan  5 08:30:42.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.005322767s
    Jan  5 08:30:44.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.005605693s
    Jan  5 08:30:46.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006152933s
    Jan  5 08:30:48.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.0056206s
    Jan  5 08:30:50.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005936625s
    Jan  5 08:30:52.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.0047214s
    Jan  5 08:30:54.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005209128s
    Jan  5 08:30:56.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.006113516s
    Jan  5 08:30:58.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.004408848s
    Jan  5 08:31:00.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005764295s
    Jan  5 08:31:02.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005610733s
    Jan  5 08:31:04.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005257554s
    Jan  5 08:31:06.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007312388s
    Jan  5 08:31:08.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.004800204s
    Jan  5 08:31:10.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004500872s
    Jan  5 08:31:12.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006470531s
    Jan  5 08:31:14.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.005022503s
    Jan  5 08:31:16.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.005268729s
    Jan  5 08:31:18.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006525656s
    Jan  5 08:31:20.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.004314679s
    Jan  5 08:31:22.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005368097s
    Jan  5 08:31:24.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.006530988s
    Jan  5 08:31:26.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.005954715s
    Jan  5 08:31:28.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.005261948s
    Jan  5 08:31:30.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005932551s
    Jan  5 08:31:30.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007902395s
    STEP: removing the label kubernetes.io/e2e-f88123d4-c3d1-4e60-bd83-903297ceadeb off the node mip-bd-vm724.mip.storage.hpecorp.net 01/05/23 08:31:30.675
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f88123d4-c3d1-4e60-bd83-903297ceadeb 01/05/23 08:31:30.683
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:31:30.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2407" for this suite. 01/05/23 08:31:30.689
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:31:30.7
Jan  5 08:31:30.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename dns 01/05/23 08:31:30.701
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:30.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:30.726
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/05/23 08:31:30.729
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8037.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8037.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 137.153.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.153.137_udp@PTR;check="$$(dig +tcp +noall +answer +search 137.153.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.153.137_tcp@PTR;sleep 1; done
 01/05/23 08:31:30.761
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8037.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8037.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 137.153.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.153.137_udp@PTR;check="$$(dig +tcp +noall +answer +search 137.153.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.153.137_tcp@PTR;sleep 1; done
 01/05/23 08:31:30.761
STEP: creating a pod to probe DNS 01/05/23 08:31:30.761
STEP: submitting the pod to kubernetes 01/05/23 08:31:30.762
Jan  5 08:31:30.781: INFO: Waiting up to 15m0s for pod "dns-test-059dd166-0f65-47b1-8994-66020d026857" in namespace "dns-8037" to be "running"
Jan  5 08:31:30.783: INFO: Pod "dns-test-059dd166-0f65-47b1-8994-66020d026857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.417321ms
Jan  5 08:31:32.786: INFO: Pod "dns-test-059dd166-0f65-47b1-8994-66020d026857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005324459s
Jan  5 08:31:34.787: INFO: Pod "dns-test-059dd166-0f65-47b1-8994-66020d026857": Phase="Running", Reason="", readiness=true. Elapsed: 4.006573094s
Jan  5 08:31:34.787: INFO: Pod "dns-test-059dd166-0f65-47b1-8994-66020d026857" satisfied condition "running"
STEP: retrieving the pod 01/05/23 08:31:34.787
STEP: looking for the results for each expected name from probers 01/05/23 08:31:34.791
Jan  5 08:31:34.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
Jan  5 08:31:34.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
Jan  5 08:31:34.798: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
Jan  5 08:31:34.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
Jan  5 08:31:34.808: INFO: Unable to read jessie_udp@dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
Jan  5 08:31:34.810: INFO: Unable to read jessie_tcp@dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
Jan  5 08:31:34.811: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
Jan  5 08:31:34.813: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
Jan  5 08:31:34.819: INFO: Lookups using dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857 failed for: [wheezy_udp@dns-test-service.dns-8037.svc.cluster.local wheezy_tcp@dns-test-service.dns-8037.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local jessie_udp@dns-test-service.dns-8037.svc.cluster.local jessie_tcp@dns-test-service.dns-8037.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local]

Jan  5 08:31:39.858: INFO: DNS probes using dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857 succeeded

STEP: deleting the pod 01/05/23 08:31:39.858
STEP: deleting the test service 01/05/23 08:31:39.885
STEP: deleting the test headless service 01/05/23 08:31:39.914
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 08:31:39.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8037" for this suite. 01/05/23 08:31:39.94
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":238,"skipped":4725,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.248 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:31:30.7
    Jan  5 08:31:30.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename dns 01/05/23 08:31:30.701
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:30.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:30.726
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/05/23 08:31:30.729
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8037.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8037.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 137.153.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.153.137_udp@PTR;check="$$(dig +tcp +noall +answer +search 137.153.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.153.137_tcp@PTR;sleep 1; done
     01/05/23 08:31:30.761
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8037.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8037.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8037.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8037.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8037.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 137.153.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.153.137_udp@PTR;check="$$(dig +tcp +noall +answer +search 137.153.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.153.137_tcp@PTR;sleep 1; done
     01/05/23 08:31:30.761
    STEP: creating a pod to probe DNS 01/05/23 08:31:30.761
    STEP: submitting the pod to kubernetes 01/05/23 08:31:30.762
    Jan  5 08:31:30.781: INFO: Waiting up to 15m0s for pod "dns-test-059dd166-0f65-47b1-8994-66020d026857" in namespace "dns-8037" to be "running"
    Jan  5 08:31:30.783: INFO: Pod "dns-test-059dd166-0f65-47b1-8994-66020d026857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.417321ms
    Jan  5 08:31:32.786: INFO: Pod "dns-test-059dd166-0f65-47b1-8994-66020d026857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005324459s
    Jan  5 08:31:34.787: INFO: Pod "dns-test-059dd166-0f65-47b1-8994-66020d026857": Phase="Running", Reason="", readiness=true. Elapsed: 4.006573094s
    Jan  5 08:31:34.787: INFO: Pod "dns-test-059dd166-0f65-47b1-8994-66020d026857" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 08:31:34.787
    STEP: looking for the results for each expected name from probers 01/05/23 08:31:34.791
    Jan  5 08:31:34.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
    Jan  5 08:31:34.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
    Jan  5 08:31:34.798: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
    Jan  5 08:31:34.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
    Jan  5 08:31:34.808: INFO: Unable to read jessie_udp@dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
    Jan  5 08:31:34.810: INFO: Unable to read jessie_tcp@dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
    Jan  5 08:31:34.811: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
    Jan  5 08:31:34.813: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local from pod dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857: the server could not find the requested resource (get pods dns-test-059dd166-0f65-47b1-8994-66020d026857)
    Jan  5 08:31:34.819: INFO: Lookups using dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857 failed for: [wheezy_udp@dns-test-service.dns-8037.svc.cluster.local wheezy_tcp@dns-test-service.dns-8037.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local jessie_udp@dns-test-service.dns-8037.svc.cluster.local jessie_tcp@dns-test-service.dns-8037.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8037.svc.cluster.local]

    Jan  5 08:31:39.858: INFO: DNS probes using dns-8037/dns-test-059dd166-0f65-47b1-8994-66020d026857 succeeded

    STEP: deleting the pod 01/05/23 08:31:39.858
    STEP: deleting the test service 01/05/23 08:31:39.885
    STEP: deleting the test headless service 01/05/23 08:31:39.914
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 08:31:39.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8037" for this suite. 01/05/23 08:31:39.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:31:39.949
Jan  5 08:31:39.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 08:31:39.95
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:39.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:39.964
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan  5 08:31:39.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 create -f -'
Jan  5 08:31:40.752: INFO: stderr: ""
Jan  5 08:31:40.753: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan  5 08:31:40.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 create -f -'
Jan  5 08:31:40.963: INFO: stderr: ""
Jan  5 08:31:40.963: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/05/23 08:31:40.963
Jan  5 08:31:41.966: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 08:31:41.966: INFO: Found 0 / 1
Jan  5 08:31:42.967: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 08:31:42.967: INFO: Found 0 / 1
Jan  5 08:31:43.966: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 08:31:43.966: INFO: Found 1 / 1
Jan  5 08:31:43.966: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  5 08:31:43.968: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 08:31:43.968: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  5 08:31:43.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 describe pod agnhost-primary-tftjq'
Jan  5 08:31:44.033: INFO: stderr: ""
Jan  5 08:31:44.033: INFO: stdout: "Name:             agnhost-primary-tftjq\nNamespace:        kubectl-3713\nPriority:         0\nService Account:  default\nNode:             mip-bd-vm724.mip.storage.hpecorp.net/16.0.14.214\nStart Time:       Thu, 05 Jan 2023 08:31:40 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 2ca0b93db2308ac82d4123234c09d0b7662d84a0090b0965e6b9a6a3155e7a88\n                  cni.projectcalico.org/podIP: 10.244.1.123/32\n                  cni.projectcalico.org/podIPs: 10.244.1.123/32\nStatus:           Running\nIP:               10.244.1.123\nIPs:\n  IP:           10.244.1.123\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://455466e66e9e0eb79c890dfface3d8906ccddc620ed70257376151f15887dc06\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Jan 2023 08:31:42 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xl65r (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-xl65r:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-3713/agnhost-primary-tftjq to mip-bd-vm724.mip.storage.hpecorp.net\n  Normal  Pulling    3s    kubelet            Pulling image \"registry.k8s.io/e2e-test-images/agnhost:2.40\"\n  Normal  Pulled     3s    kubelet            Successfully pulled image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" in 678.262114ms\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan  5 08:31:44.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 describe rc agnhost-primary'
Jan  5 08:31:44.113: INFO: stderr: ""
Jan  5 08:31:44.113: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3713\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-tftjq\n"
Jan  5 08:31:44.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 describe service agnhost-primary'
Jan  5 08:31:44.181: INFO: stderr: ""
Jan  5 08:31:44.181: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3713\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.106.61.16\nIPs:               10.106.61.16\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.123:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan  5 08:31:44.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 describe node mip-bd-vm721.mip.storage.hpecorp.net'
Jan  5 08:31:44.260: INFO: stderr: ""
Jan  5 08:31:44.260: INFO: stdout: "Name:               mip-bd-vm721.mip.storage.hpecorp.net\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=mip-bd-vm721.mip.storage.hpecorp.net\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.ecp.ezkube/dns=mip-bd-vm721.mip.storage.hpecorp.net\n                    node.ecp.ezkube/ipaddress=16.0.14.211\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"9e:62:4b:c5:6a:c1\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 16.0.14.211\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 16.0.14.211/22\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 05 Jan 2023 07:08:23 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  mip-bd-vm721.mip.storage.hpecorp.net\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 05 Jan 2023 08:31:37 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 05 Jan 2023 07:09:08 +0000   Thu, 05 Jan 2023 07:09:08 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Thu, 05 Jan 2023 08:29:47 +0000   Thu, 05 Jan 2023 07:08:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 05 Jan 2023 08:29:47 +0000   Thu, 05 Jan 2023 07:08:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 05 Jan 2023 08:29:47 +0000   Thu, 05 Jan 2023 07:08:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 05 Jan 2023 08:29:47 +0000   Thu, 05 Jan 2023 07:08:54 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  16.0.14.211\n  Hostname:    mip-bd-vm721.mip.storage.hpecorp.net\nCapacity:\n  cpu:                8\n  ephemeral-storage:  493566Mi\n  hugepages-2Mi:      0\n  memory:             32877192Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  465787315044\n  hugepages-2Mi:      0\n  memory:             32774792Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 3b754a699ae24b86b8f3416cee18e712\n  System UUID:                c0ec1142-de76-206b-c210-27fdd73f48c4\n  Boot ID:                    1b796813-c269-40be-a71a-45a047777b6f\n  Kernel Version:             5.3.18-57-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP3\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.12-hpe1\n  Kubelet Version:            v1.25.5-hpe1\n  Kube-Proxy Version:         v1.25.5-hpe1\nPodCIDR:                      10.244.0.0/25\nPodCIDRs:                     10.244.0.0/25\nNon-terminated Pods:          (5 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ezctl                       ezctl-app-6667d4d559-6qmjn                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 calico-kube-controllers-798cc86c47-rtpl2                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\n  kube-system                 canal-xz7k4                                                250m (3%)     0 (0%)      0 (0%)           0 (0%)         83m\n  sonobuoy                    sonobuoy-e2e-job-1a057e0a7db64ab3                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2knqs    0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                250m (3%)  0 (0%)\n  memory             0 (0%)     0 (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:              <none>\n"
Jan  5 08:31:44.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 describe namespace kubectl-3713'
Jan  5 08:31:44.323: INFO: stderr: ""
Jan  5 08:31:44.323: INFO: stdout: "Name:         kubectl-3713\nLabels:       e2e-framework=kubectl\n              e2e-run=77228e4c-8e3b-412e-b6c7-acee0a58df34\n              kubernetes.io/metadata.name=kubectl-3713\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 08:31:44.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3713" for this suite. 01/05/23 08:31:44.327
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":239,"skipped":4736,"failed":0}
------------------------------
â€¢ [4.382 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:31:39.949
    Jan  5 08:31:39.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 08:31:39.95
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:39.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:39.964
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan  5 08:31:39.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 create -f -'
    Jan  5 08:31:40.752: INFO: stderr: ""
    Jan  5 08:31:40.753: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan  5 08:31:40.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 create -f -'
    Jan  5 08:31:40.963: INFO: stderr: ""
    Jan  5 08:31:40.963: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/05/23 08:31:40.963
    Jan  5 08:31:41.966: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 08:31:41.966: INFO: Found 0 / 1
    Jan  5 08:31:42.967: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 08:31:42.967: INFO: Found 0 / 1
    Jan  5 08:31:43.966: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 08:31:43.966: INFO: Found 1 / 1
    Jan  5 08:31:43.966: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  5 08:31:43.968: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 08:31:43.968: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  5 08:31:43.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 describe pod agnhost-primary-tftjq'
    Jan  5 08:31:44.033: INFO: stderr: ""
    Jan  5 08:31:44.033: INFO: stdout: "Name:             agnhost-primary-tftjq\nNamespace:        kubectl-3713\nPriority:         0\nService Account:  default\nNode:             mip-bd-vm724.mip.storage.hpecorp.net/16.0.14.214\nStart Time:       Thu, 05 Jan 2023 08:31:40 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 2ca0b93db2308ac82d4123234c09d0b7662d84a0090b0965e6b9a6a3155e7a88\n                  cni.projectcalico.org/podIP: 10.244.1.123/32\n                  cni.projectcalico.org/podIPs: 10.244.1.123/32\nStatus:           Running\nIP:               10.244.1.123\nIPs:\n  IP:           10.244.1.123\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://455466e66e9e0eb79c890dfface3d8906ccddc620ed70257376151f15887dc06\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Jan 2023 08:31:42 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xl65r (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-xl65r:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-3713/agnhost-primary-tftjq to mip-bd-vm724.mip.storage.hpecorp.net\n  Normal  Pulling    3s    kubelet            Pulling image \"registry.k8s.io/e2e-test-images/agnhost:2.40\"\n  Normal  Pulled     3s    kubelet            Successfully pulled image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" in 678.262114ms\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Jan  5 08:31:44.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 describe rc agnhost-primary'
    Jan  5 08:31:44.113: INFO: stderr: ""
    Jan  5 08:31:44.113: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3713\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-tftjq\n"
    Jan  5 08:31:44.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 describe service agnhost-primary'
    Jan  5 08:31:44.181: INFO: stderr: ""
    Jan  5 08:31:44.181: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3713\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.106.61.16\nIPs:               10.106.61.16\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.123:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan  5 08:31:44.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 describe node mip-bd-vm721.mip.storage.hpecorp.net'
    Jan  5 08:31:44.260: INFO: stderr: ""
    Jan  5 08:31:44.260: INFO: stdout: "Name:               mip-bd-vm721.mip.storage.hpecorp.net\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=mip-bd-vm721.mip.storage.hpecorp.net\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.ecp.ezkube/dns=mip-bd-vm721.mip.storage.hpecorp.net\n                    node.ecp.ezkube/ipaddress=16.0.14.211\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"9e:62:4b:c5:6a:c1\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 16.0.14.211\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 16.0.14.211/22\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 05 Jan 2023 07:08:23 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  mip-bd-vm721.mip.storage.hpecorp.net\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 05 Jan 2023 08:31:37 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 05 Jan 2023 07:09:08 +0000   Thu, 05 Jan 2023 07:09:08 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Thu, 05 Jan 2023 08:29:47 +0000   Thu, 05 Jan 2023 07:08:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 05 Jan 2023 08:29:47 +0000   Thu, 05 Jan 2023 07:08:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 05 Jan 2023 08:29:47 +0000   Thu, 05 Jan 2023 07:08:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 05 Jan 2023 08:29:47 +0000   Thu, 05 Jan 2023 07:08:54 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  16.0.14.211\n  Hostname:    mip-bd-vm721.mip.storage.hpecorp.net\nCapacity:\n  cpu:                8\n  ephemeral-storage:  493566Mi\n  hugepages-2Mi:      0\n  memory:             32877192Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  465787315044\n  hugepages-2Mi:      0\n  memory:             32774792Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 3b754a699ae24b86b8f3416cee18e712\n  System UUID:                c0ec1142-de76-206b-c210-27fdd73f48c4\n  Boot ID:                    1b796813-c269-40be-a71a-45a047777b6f\n  Kernel Version:             5.3.18-57-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP3\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.12-hpe1\n  Kubelet Version:            v1.25.5-hpe1\n  Kube-Proxy Version:         v1.25.5-hpe1\nPodCIDR:                      10.244.0.0/25\nPodCIDRs:                     10.244.0.0/25\nNon-terminated Pods:          (5 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ezctl                       ezctl-app-6667d4d559-6qmjn                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 calico-kube-controllers-798cc86c47-rtpl2                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\n  kube-system                 canal-xz7k4                                                250m (3%)     0 (0%)      0 (0%)           0 (0%)         83m\n  sonobuoy                    sonobuoy-e2e-job-1a057e0a7db64ab3                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-b6a786db62124987-2knqs    0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                250m (3%)  0 (0%)\n  memory             0 (0%)     0 (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:              <none>\n"
    Jan  5 08:31:44.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-3713 describe namespace kubectl-3713'
    Jan  5 08:31:44.323: INFO: stderr: ""
    Jan  5 08:31:44.323: INFO: stdout: "Name:         kubectl-3713\nLabels:       e2e-framework=kubectl\n              e2e-run=77228e4c-8e3b-412e-b6c7-acee0a58df34\n              kubernetes.io/metadata.name=kubectl-3713\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 08:31:44.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3713" for this suite. 01/05/23 08:31:44.327
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:31:44.332
Jan  5 08:31:44.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:31:44.334
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:44.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:44.349
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/05/23 08:31:44.352
Jan  5 08:31:44.358: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d" in namespace "projected-7421" to be "Succeeded or Failed"
Jan  5 08:31:44.360: INFO: Pod "downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.219335ms
Jan  5 08:31:46.364: INFO: Pod "downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006576843s
Jan  5 08:31:48.364: INFO: Pod "downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006750988s
STEP: Saw pod success 01/05/23 08:31:48.364
Jan  5 08:31:48.364: INFO: Pod "downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d" satisfied condition "Succeeded or Failed"
Jan  5 08:31:48.367: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d container client-container: <nil>
STEP: delete the pod 01/05/23 08:31:48.388
Jan  5 08:31:48.430: INFO: Waiting for pod downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d to disappear
Jan  5 08:31:48.432: INFO: Pod downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 08:31:48.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7421" for this suite. 01/05/23 08:31:48.434
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":240,"skipped":4738,"failed":0}
------------------------------
â€¢ [4.134 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:31:44.332
    Jan  5 08:31:44.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:31:44.334
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:44.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:44.349
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/05/23 08:31:44.352
    Jan  5 08:31:44.358: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d" in namespace "projected-7421" to be "Succeeded or Failed"
    Jan  5 08:31:44.360: INFO: Pod "downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.219335ms
    Jan  5 08:31:46.364: INFO: Pod "downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006576843s
    Jan  5 08:31:48.364: INFO: Pod "downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006750988s
    STEP: Saw pod success 01/05/23 08:31:48.364
    Jan  5 08:31:48.364: INFO: Pod "downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d" satisfied condition "Succeeded or Failed"
    Jan  5 08:31:48.367: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d container client-container: <nil>
    STEP: delete the pod 01/05/23 08:31:48.388
    Jan  5 08:31:48.430: INFO: Waiting for pod downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d to disappear
    Jan  5 08:31:48.432: INFO: Pod downwardapi-volume-7ebfdce2-b9b6-4393-9945-46cf0a6cf42d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 08:31:48.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7421" for this suite. 01/05/23 08:31:48.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:31:48.467
Jan  5 08:31:48.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename gc 01/05/23 08:31:48.468
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:48.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:48.49
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/05/23 08:31:48.493
STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 08:31:48.497
STEP: delete the deployment 01/05/23 08:31:49.002
STEP: wait for all rs to be garbage collected 01/05/23 08:31:49.015
STEP: expected 0 rs, got 1 rs 01/05/23 08:31:49.018
STEP: expected 0 pods, got 2 pods 01/05/23 08:31:49.026
STEP: Gathering metrics 01/05/23 08:31:49.531
W0105 08:31:49.534471      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 08:31:49.534: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 08:31:49.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3403" for this suite. 01/05/23 08:31:49.536
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":241,"skipped":4745,"failed":0}
------------------------------
â€¢ [1.073 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:31:48.467
    Jan  5 08:31:48.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename gc 01/05/23 08:31:48.468
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:48.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:48.49
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/05/23 08:31:48.493
    STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 08:31:48.497
    STEP: delete the deployment 01/05/23 08:31:49.002
    STEP: wait for all rs to be garbage collected 01/05/23 08:31:49.015
    STEP: expected 0 rs, got 1 rs 01/05/23 08:31:49.018
    STEP: expected 0 pods, got 2 pods 01/05/23 08:31:49.026
    STEP: Gathering metrics 01/05/23 08:31:49.531
    W0105 08:31:49.534471      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 08:31:49.534: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 08:31:49.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3403" for this suite. 01/05/23 08:31:49.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:31:49.54
Jan  5 08:31:49.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:31:49.541
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:49.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:49.556
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:31:49.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8293" for this suite. 01/05/23 08:31:49.569
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":242,"skipped":4750,"failed":0}
------------------------------
â€¢ [0.043 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:31:49.54
    Jan  5 08:31:49.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:31:49.541
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:49.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:49.556
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:31:49.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8293" for this suite. 01/05/23 08:31:49.569
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:31:49.585
Jan  5 08:31:49.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename resourcequota 01/05/23 08:31:49.585
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:49.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:49.606
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/05/23 08:31:49.607
STEP: Counting existing ResourceQuota 01/05/23 08:31:54.631
STEP: Creating a ResourceQuota 01/05/23 08:31:59.634
STEP: Ensuring resource quota status is calculated 01/05/23 08:31:59.637
STEP: Creating a Secret 01/05/23 08:32:01.641
STEP: Ensuring resource quota status captures secret creation 01/05/23 08:32:01.657
STEP: Deleting a secret 01/05/23 08:32:03.66
STEP: Ensuring resource quota status released usage 01/05/23 08:32:03.664
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 08:32:05.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8680" for this suite. 01/05/23 08:32:05.672
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":243,"skipped":4798,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.091 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:31:49.585
    Jan  5 08:31:49.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename resourcequota 01/05/23 08:31:49.585
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:31:49.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:31:49.606
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/05/23 08:31:49.607
    STEP: Counting existing ResourceQuota 01/05/23 08:31:54.631
    STEP: Creating a ResourceQuota 01/05/23 08:31:59.634
    STEP: Ensuring resource quota status is calculated 01/05/23 08:31:59.637
    STEP: Creating a Secret 01/05/23 08:32:01.641
    STEP: Ensuring resource quota status captures secret creation 01/05/23 08:32:01.657
    STEP: Deleting a secret 01/05/23 08:32:03.66
    STEP: Ensuring resource quota status released usage 01/05/23 08:32:03.664
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 08:32:05.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8680" for this suite. 01/05/23 08:32:05.672
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:32:05.676
Jan  5 08:32:05.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-probe 01/05/23 08:32:05.677
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:05.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:05.691
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jan  5 08:32:05.702: INFO: Waiting up to 5m0s for pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea" in namespace "container-probe-9866" to be "running and ready"
Jan  5 08:32:05.704: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.810944ms
Jan  5 08:32:05.704: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:32:07.709: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 2.00636728s
Jan  5 08:32:07.709: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
Jan  5 08:32:09.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 4.005790239s
Jan  5 08:32:09.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
Jan  5 08:32:11.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 6.005606766s
Jan  5 08:32:11.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
Jan  5 08:32:13.710: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 8.007377282s
Jan  5 08:32:13.710: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
Jan  5 08:32:15.707: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 10.004951452s
Jan  5 08:32:15.707: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
Jan  5 08:32:17.709: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 12.006623468s
Jan  5 08:32:17.709: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
Jan  5 08:32:19.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 14.005355633s
Jan  5 08:32:19.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
Jan  5 08:32:21.710: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 16.007624894s
Jan  5 08:32:21.710: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
Jan  5 08:32:23.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 18.005778085s
Jan  5 08:32:23.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
Jan  5 08:32:25.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 20.005413394s
Jan  5 08:32:25.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
Jan  5 08:32:27.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=true. Elapsed: 22.005947584s
Jan  5 08:32:27.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = true)
Jan  5 08:32:27.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea" satisfied condition "running and ready"
Jan  5 08:32:27.711: INFO: Container started at 2023-01-05 08:32:06 +0000 UTC, pod became ready at 2023-01-05 08:32:26 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 08:32:27.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9866" for this suite. 01/05/23 08:32:27.714
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":244,"skipped":4799,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.043 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:32:05.676
    Jan  5 08:32:05.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-probe 01/05/23 08:32:05.677
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:05.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:05.691
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jan  5 08:32:05.702: INFO: Waiting up to 5m0s for pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea" in namespace "container-probe-9866" to be "running and ready"
    Jan  5 08:32:05.704: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.810944ms
    Jan  5 08:32:05.704: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:32:07.709: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 2.00636728s
    Jan  5 08:32:07.709: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
    Jan  5 08:32:09.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 4.005790239s
    Jan  5 08:32:09.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
    Jan  5 08:32:11.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 6.005606766s
    Jan  5 08:32:11.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
    Jan  5 08:32:13.710: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 8.007377282s
    Jan  5 08:32:13.710: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
    Jan  5 08:32:15.707: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 10.004951452s
    Jan  5 08:32:15.707: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
    Jan  5 08:32:17.709: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 12.006623468s
    Jan  5 08:32:17.709: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
    Jan  5 08:32:19.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 14.005355633s
    Jan  5 08:32:19.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
    Jan  5 08:32:21.710: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 16.007624894s
    Jan  5 08:32:21.710: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
    Jan  5 08:32:23.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 18.005778085s
    Jan  5 08:32:23.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
    Jan  5 08:32:25.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=false. Elapsed: 20.005413394s
    Jan  5 08:32:25.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = false)
    Jan  5 08:32:27.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea": Phase="Running", Reason="", readiness=true. Elapsed: 22.005947584s
    Jan  5 08:32:27.708: INFO: The phase of Pod test-webserver-391f613e-1368-427c-8f98-ba7950a94cea is Running (Ready = true)
    Jan  5 08:32:27.708: INFO: Pod "test-webserver-391f613e-1368-427c-8f98-ba7950a94cea" satisfied condition "running and ready"
    Jan  5 08:32:27.711: INFO: Container started at 2023-01-05 08:32:06 +0000 UTC, pod became ready at 2023-01-05 08:32:26 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 08:32:27.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9866" for this suite. 01/05/23 08:32:27.714
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:32:27.719
Jan  5 08:32:27.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename endpointslicemirroring 01/05/23 08:32:27.72
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:27.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:27.737
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/05/23 08:32:27.752
Jan  5 08:32:27.761: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/05/23 08:32:29.765
Jan  5 08:32:29.773: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/05/23 08:32:31.776
Jan  5 08:32:31.784: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan  5 08:32:33.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-1266" for this suite. 01/05/23 08:32:33.79
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":245,"skipped":4803,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.076 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:32:27.719
    Jan  5 08:32:27.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename endpointslicemirroring 01/05/23 08:32:27.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:27.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:27.737
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/05/23 08:32:27.752
    Jan  5 08:32:27.761: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/05/23 08:32:29.765
    Jan  5 08:32:29.773: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/05/23 08:32:31.776
    Jan  5 08:32:31.784: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan  5 08:32:33.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-1266" for this suite. 01/05/23 08:32:33.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:32:33.795
Jan  5 08:32:33.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 08:32:33.796
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:33.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:33.811
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan  5 08:32:33.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 08:32:35.875
Jan  5 08:32:35.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-771 --namespace=crd-publish-openapi-771 create -f -'
Jan  5 08:32:36.453: INFO: stderr: ""
Jan  5 08:32:36.453: INFO: stdout: "e2e-test-crd-publish-openapi-4580-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  5 08:32:36.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-771 --namespace=crd-publish-openapi-771 delete e2e-test-crd-publish-openapi-4580-crds test-cr'
Jan  5 08:32:36.521: INFO: stderr: ""
Jan  5 08:32:36.521: INFO: stdout: "e2e-test-crd-publish-openapi-4580-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan  5 08:32:36.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-771 --namespace=crd-publish-openapi-771 apply -f -'
Jan  5 08:32:36.713: INFO: stderr: ""
Jan  5 08:32:36.713: INFO: stdout: "e2e-test-crd-publish-openapi-4580-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  5 08:32:36.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-771 --namespace=crd-publish-openapi-771 delete e2e-test-crd-publish-openapi-4580-crds test-cr'
Jan  5 08:32:36.786: INFO: stderr: ""
Jan  5 08:32:36.786: INFO: stdout: "e2e-test-crd-publish-openapi-4580-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/05/23 08:32:36.786
Jan  5 08:32:36.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-771 explain e2e-test-crd-publish-openapi-4580-crds'
Jan  5 08:32:37.014: INFO: stderr: ""
Jan  5 08:32:37.014: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4580-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:32:38.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-771" for this suite. 01/05/23 08:32:38.978
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":246,"skipped":4811,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.189 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:32:33.795
    Jan  5 08:32:33.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 08:32:33.796
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:33.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:33.811
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan  5 08:32:33.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 08:32:35.875
    Jan  5 08:32:35.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-771 --namespace=crd-publish-openapi-771 create -f -'
    Jan  5 08:32:36.453: INFO: stderr: ""
    Jan  5 08:32:36.453: INFO: stdout: "e2e-test-crd-publish-openapi-4580-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  5 08:32:36.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-771 --namespace=crd-publish-openapi-771 delete e2e-test-crd-publish-openapi-4580-crds test-cr'
    Jan  5 08:32:36.521: INFO: stderr: ""
    Jan  5 08:32:36.521: INFO: stdout: "e2e-test-crd-publish-openapi-4580-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan  5 08:32:36.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-771 --namespace=crd-publish-openapi-771 apply -f -'
    Jan  5 08:32:36.713: INFO: stderr: ""
    Jan  5 08:32:36.713: INFO: stdout: "e2e-test-crd-publish-openapi-4580-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  5 08:32:36.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-771 --namespace=crd-publish-openapi-771 delete e2e-test-crd-publish-openapi-4580-crds test-cr'
    Jan  5 08:32:36.786: INFO: stderr: ""
    Jan  5 08:32:36.786: INFO: stdout: "e2e-test-crd-publish-openapi-4580-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/05/23 08:32:36.786
    Jan  5 08:32:36.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-771 explain e2e-test-crd-publish-openapi-4580-crds'
    Jan  5 08:32:37.014: INFO: stderr: ""
    Jan  5 08:32:37.014: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4580-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:32:38.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-771" for this suite. 01/05/23 08:32:38.978
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:32:38.985
Jan  5 08:32:38.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename resourcequota 01/05/23 08:32:38.985
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:38.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:38.999
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/05/23 08:32:39.001
STEP: Creating a ResourceQuota 01/05/23 08:32:44.003
STEP: Ensuring resource quota status is calculated 01/05/23 08:32:44.007
STEP: Creating a Pod that fits quota 01/05/23 08:32:46.011
STEP: Ensuring ResourceQuota status captures the pod usage 01/05/23 08:32:46.027
STEP: Not allowing a pod to be created that exceeds remaining quota 01/05/23 08:32:48.031
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/05/23 08:32:48.032
STEP: Ensuring a pod cannot update its resource requirements 01/05/23 08:32:48.034
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/05/23 08:32:48.037
STEP: Deleting the pod 01/05/23 08:32:50.04
STEP: Ensuring resource quota status released the pod usage 01/05/23 08:32:50.057
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 08:32:52.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1025" for this suite. 01/05/23 08:32:52.065
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":247,"skipped":4813,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.085 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:32:38.985
    Jan  5 08:32:38.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename resourcequota 01/05/23 08:32:38.985
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:38.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:38.999
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/05/23 08:32:39.001
    STEP: Creating a ResourceQuota 01/05/23 08:32:44.003
    STEP: Ensuring resource quota status is calculated 01/05/23 08:32:44.007
    STEP: Creating a Pod that fits quota 01/05/23 08:32:46.011
    STEP: Ensuring ResourceQuota status captures the pod usage 01/05/23 08:32:46.027
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/05/23 08:32:48.031
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/05/23 08:32:48.032
    STEP: Ensuring a pod cannot update its resource requirements 01/05/23 08:32:48.034
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/05/23 08:32:48.037
    STEP: Deleting the pod 01/05/23 08:32:50.04
    STEP: Ensuring resource quota status released the pod usage 01/05/23 08:32:50.057
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 08:32:52.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1025" for this suite. 01/05/23 08:32:52.065
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:32:52.07
Jan  5 08:32:52.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pods 01/05/23 08:32:52.071
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:52.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:52.088
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/05/23 08:32:52.091
STEP: submitting the pod to kubernetes 01/05/23 08:32:52.091
STEP: verifying QOS class is set on the pod 01/05/23 08:32:52.097
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan  5 08:32:52.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-957" for this suite. 01/05/23 08:32:52.101
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":248,"skipped":4814,"failed":0}
------------------------------
â€¢ [0.041 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:32:52.07
    Jan  5 08:32:52.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pods 01/05/23 08:32:52.071
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:52.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:52.088
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/05/23 08:32:52.091
    STEP: submitting the pod to kubernetes 01/05/23 08:32:52.091
    STEP: verifying QOS class is set on the pod 01/05/23 08:32:52.097
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan  5 08:32:52.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-957" for this suite. 01/05/23 08:32:52.101
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:32:52.111
Jan  5 08:32:52.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pods 01/05/23 08:32:52.112
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:52.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:52.138
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan  5 08:32:52.181: INFO: Waiting up to 5m0s for pod "server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907" in namespace "pods-8916" to be "running and ready"
Jan  5 08:32:52.183: INFO: Pod "server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907": Phase="Pending", Reason="", readiness=false. Elapsed: 2.457506ms
Jan  5 08:32:52.183: INFO: The phase of Pod server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:32:54.188: INFO: Pod "server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907": Phase="Running", Reason="", readiness=true. Elapsed: 2.007405666s
Jan  5 08:32:54.188: INFO: The phase of Pod server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907 is Running (Ready = true)
Jan  5 08:32:54.188: INFO: Pod "server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907" satisfied condition "running and ready"
Jan  5 08:32:54.225: INFO: Waiting up to 5m0s for pod "client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d" in namespace "pods-8916" to be "Succeeded or Failed"
Jan  5 08:32:54.244: INFO: Pod "client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.495723ms
Jan  5 08:32:56.249: INFO: Pod "client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024123034s
Jan  5 08:32:58.247: INFO: Pod "client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022766558s
STEP: Saw pod success 01/05/23 08:32:58.247
Jan  5 08:32:58.247: INFO: Pod "client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d" satisfied condition "Succeeded or Failed"
Jan  5 08:32:58.249: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d container env3cont: <nil>
STEP: delete the pod 01/05/23 08:32:58.253
Jan  5 08:32:58.266: INFO: Waiting for pod client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d to disappear
Jan  5 08:32:58.269: INFO: Pod client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 08:32:58.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8916" for this suite. 01/05/23 08:32:58.271
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":249,"skipped":4814,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.168 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:32:52.111
    Jan  5 08:32:52.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pods 01/05/23 08:32:52.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:52.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:52.138
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan  5 08:32:52.181: INFO: Waiting up to 5m0s for pod "server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907" in namespace "pods-8916" to be "running and ready"
    Jan  5 08:32:52.183: INFO: Pod "server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907": Phase="Pending", Reason="", readiness=false. Elapsed: 2.457506ms
    Jan  5 08:32:52.183: INFO: The phase of Pod server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:32:54.188: INFO: Pod "server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907": Phase="Running", Reason="", readiness=true. Elapsed: 2.007405666s
    Jan  5 08:32:54.188: INFO: The phase of Pod server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907 is Running (Ready = true)
    Jan  5 08:32:54.188: INFO: Pod "server-envvars-2a78a804-1d9b-4ad3-ade5-86e7cd48b907" satisfied condition "running and ready"
    Jan  5 08:32:54.225: INFO: Waiting up to 5m0s for pod "client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d" in namespace "pods-8916" to be "Succeeded or Failed"
    Jan  5 08:32:54.244: INFO: Pod "client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.495723ms
    Jan  5 08:32:56.249: INFO: Pod "client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024123034s
    Jan  5 08:32:58.247: INFO: Pod "client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022766558s
    STEP: Saw pod success 01/05/23 08:32:58.247
    Jan  5 08:32:58.247: INFO: Pod "client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d" satisfied condition "Succeeded or Failed"
    Jan  5 08:32:58.249: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d container env3cont: <nil>
    STEP: delete the pod 01/05/23 08:32:58.253
    Jan  5 08:32:58.266: INFO: Waiting for pod client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d to disappear
    Jan  5 08:32:58.269: INFO: Pod client-envvars-fde5d8f0-61d2-4c1e-8805-b7553343037d no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 08:32:58.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8916" for this suite. 01/05/23 08:32:58.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:32:58.281
Jan  5 08:32:58.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 08:32:58.282
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:58.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:58.296
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/05/23 08:32:58.297
Jan  5 08:32:58.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4767 api-versions'
Jan  5 08:32:58.348: INFO: stderr: ""
Jan  5 08:32:58.348: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 08:32:58.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4767" for this suite. 01/05/23 08:32:58.351
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":250,"skipped":4822,"failed":0}
------------------------------
â€¢ [0.079 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:32:58.281
    Jan  5 08:32:58.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 08:32:58.282
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:58.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:58.296
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/05/23 08:32:58.297
    Jan  5 08:32:58.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-4767 api-versions'
    Jan  5 08:32:58.348: INFO: stderr: ""
    Jan  5 08:32:58.348: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 08:32:58.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4767" for this suite. 01/05/23 08:32:58.351
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:32:58.36
Jan  5 08:32:58.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 08:32:58.361
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:58.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:58.397
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 08:32:58.4
Jan  5 08:32:58.411: INFO: Waiting up to 5m0s for pod "pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6" in namespace "emptydir-996" to be "Succeeded or Failed"
Jan  5 08:32:58.412: INFO: Pod "pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.449946ms
Jan  5 08:33:00.416: INFO: Pod "pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005047432s
Jan  5 08:33:02.416: INFO: Pod "pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00506465s
STEP: Saw pod success 01/05/23 08:33:02.416
Jan  5 08:33:02.416: INFO: Pod "pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6" satisfied condition "Succeeded or Failed"
Jan  5 08:33:02.418: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6 container test-container: <nil>
STEP: delete the pod 01/05/23 08:33:02.421
Jan  5 08:33:02.439: INFO: Waiting for pod pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6 to disappear
Jan  5 08:33:02.441: INFO: Pod pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 08:33:02.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-996" for this suite. 01/05/23 08:33:02.443
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":251,"skipped":4826,"failed":0}
------------------------------
â€¢ [4.090 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:32:58.36
    Jan  5 08:32:58.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 08:32:58.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:32:58.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:32:58.397
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 08:32:58.4
    Jan  5 08:32:58.411: INFO: Waiting up to 5m0s for pod "pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6" in namespace "emptydir-996" to be "Succeeded or Failed"
    Jan  5 08:32:58.412: INFO: Pod "pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.449946ms
    Jan  5 08:33:00.416: INFO: Pod "pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005047432s
    Jan  5 08:33:02.416: INFO: Pod "pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00506465s
    STEP: Saw pod success 01/05/23 08:33:02.416
    Jan  5 08:33:02.416: INFO: Pod "pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6" satisfied condition "Succeeded or Failed"
    Jan  5 08:33:02.418: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6 container test-container: <nil>
    STEP: delete the pod 01/05/23 08:33:02.421
    Jan  5 08:33:02.439: INFO: Waiting for pod pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6 to disappear
    Jan  5 08:33:02.441: INFO: Pod pod-5cfb994a-5b5c-47e6-a202-b342e6919eb6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 08:33:02.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-996" for this suite. 01/05/23 08:33:02.443
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:02.45
Jan  5 08:33:02.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 08:33:02.451
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:02.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:02.469
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/05/23 08:33:02.471
Jan  5 08:33:02.482: INFO: Waiting up to 5m0s for pod "pod-709d709c-6e33-4011-bcf8-e27c5fabdd65" in namespace "emptydir-9089" to be "Succeeded or Failed"
Jan  5 08:33:02.484: INFO: Pod "pod-709d709c-6e33-4011-bcf8-e27c5fabdd65": Phase="Pending", Reason="", readiness=false. Elapsed: 1.311625ms
Jan  5 08:33:04.487: INFO: Pod "pod-709d709c-6e33-4011-bcf8-e27c5fabdd65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004510428s
Jan  5 08:33:06.488: INFO: Pod "pod-709d709c-6e33-4011-bcf8-e27c5fabdd65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005904922s
STEP: Saw pod success 01/05/23 08:33:06.488
Jan  5 08:33:06.489: INFO: Pod "pod-709d709c-6e33-4011-bcf8-e27c5fabdd65" satisfied condition "Succeeded or Failed"
Jan  5 08:33:06.491: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-709d709c-6e33-4011-bcf8-e27c5fabdd65 container test-container: <nil>
STEP: delete the pod 01/05/23 08:33:06.495
Jan  5 08:33:06.509: INFO: Waiting for pod pod-709d709c-6e33-4011-bcf8-e27c5fabdd65 to disappear
Jan  5 08:33:06.510: INFO: Pod pod-709d709c-6e33-4011-bcf8-e27c5fabdd65 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 08:33:06.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9089" for this suite. 01/05/23 08:33:06.513
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":252,"skipped":4830,"failed":0}
------------------------------
â€¢ [4.066 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:02.45
    Jan  5 08:33:02.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 08:33:02.451
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:02.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:02.469
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/05/23 08:33:02.471
    Jan  5 08:33:02.482: INFO: Waiting up to 5m0s for pod "pod-709d709c-6e33-4011-bcf8-e27c5fabdd65" in namespace "emptydir-9089" to be "Succeeded or Failed"
    Jan  5 08:33:02.484: INFO: Pod "pod-709d709c-6e33-4011-bcf8-e27c5fabdd65": Phase="Pending", Reason="", readiness=false. Elapsed: 1.311625ms
    Jan  5 08:33:04.487: INFO: Pod "pod-709d709c-6e33-4011-bcf8-e27c5fabdd65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004510428s
    Jan  5 08:33:06.488: INFO: Pod "pod-709d709c-6e33-4011-bcf8-e27c5fabdd65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005904922s
    STEP: Saw pod success 01/05/23 08:33:06.488
    Jan  5 08:33:06.489: INFO: Pod "pod-709d709c-6e33-4011-bcf8-e27c5fabdd65" satisfied condition "Succeeded or Failed"
    Jan  5 08:33:06.491: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-709d709c-6e33-4011-bcf8-e27c5fabdd65 container test-container: <nil>
    STEP: delete the pod 01/05/23 08:33:06.495
    Jan  5 08:33:06.509: INFO: Waiting for pod pod-709d709c-6e33-4011-bcf8-e27c5fabdd65 to disappear
    Jan  5 08:33:06.510: INFO: Pod pod-709d709c-6e33-4011-bcf8-e27c5fabdd65 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 08:33:06.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9089" for this suite. 01/05/23 08:33:06.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:06.517
Jan  5 08:33:06.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename deployment 01/05/23 08:33:06.518
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:06.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:06.541
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/05/23 08:33:06.545
Jan  5 08:33:06.546: INFO: Creating simple deployment test-deployment-kq9r6
Jan  5 08:33:06.560: INFO: deployment "test-deployment-kq9r6" doesn't have the required revision set
STEP: Getting /status 01/05/23 08:33:08.566
Jan  5 08:33:08.568: INFO: Deployment test-deployment-kq9r6 has Conditions: [{Available True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kq9r6-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/05/23 08:33:08.568
Jan  5 08:33:08.573: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 33, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 33, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 33, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 33, 6, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-kq9r6-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/05/23 08:33:08.573
Jan  5 08:33:08.574: INFO: Observed &Deployment event: ADDED
Jan  5 08:33:08.574: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kq9r6-777898ffcc"}
Jan  5 08:33:08.574: INFO: Observed &Deployment event: MODIFIED
Jan  5 08:33:08.574: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kq9r6-777898ffcc"}
Jan  5 08:33:08.574: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 08:33:08.575: INFO: Observed &Deployment event: MODIFIED
Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-kq9r6-777898ffcc" is progressing.}
Jan  5 08:33:08.575: INFO: Observed &Deployment event: MODIFIED
Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kq9r6-777898ffcc" has successfully progressed.}
Jan  5 08:33:08.575: INFO: Observed &Deployment event: MODIFIED
Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kq9r6-777898ffcc" has successfully progressed.}
Jan  5 08:33:08.575: INFO: Found Deployment test-deployment-kq9r6 in namespace deployment-8607 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 08:33:08.575: INFO: Deployment test-deployment-kq9r6 has an updated status
STEP: patching the Statefulset Status 01/05/23 08:33:08.575
Jan  5 08:33:08.575: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  5 08:33:08.594: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/05/23 08:33:08.594
Jan  5 08:33:08.595: INFO: Observed &Deployment event: ADDED
Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kq9r6-777898ffcc"}
Jan  5 08:33:08.595: INFO: Observed &Deployment event: MODIFIED
Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kq9r6-777898ffcc"}
Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 08:33:08.595: INFO: Observed &Deployment event: MODIFIED
Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-kq9r6-777898ffcc" is progressing.}
Jan  5 08:33:08.595: INFO: Observed &Deployment event: MODIFIED
Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kq9r6-777898ffcc" has successfully progressed.}
Jan  5 08:33:08.595: INFO: Observed &Deployment event: MODIFIED
Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kq9r6-777898ffcc" has successfully progressed.}
Jan  5 08:33:08.596: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 08:33:08.596: INFO: Observed &Deployment event: MODIFIED
Jan  5 08:33:08.596: INFO: Found deployment test-deployment-kq9r6 in namespace deployment-8607 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan  5 08:33:08.596: INFO: Deployment test-deployment-kq9r6 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 08:33:08.597: INFO: Deployment "test-deployment-kq9r6":
&Deployment{ObjectMeta:{test-deployment-kq9r6  deployment-8607  f33c6f29-9777-4795-8a89-01949df775d1 27326 1 2023-01-05 08:33:06 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-05 08:33:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-05 08:33:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-05 08:33:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bedb78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 08:33:08.600: INFO: New ReplicaSet "test-deployment-kq9r6-777898ffcc" of Deployment "test-deployment-kq9r6":
&ReplicaSet{ObjectMeta:{test-deployment-kq9r6-777898ffcc  deployment-8607  9eb40a50-af1b-4b94-bdb3-d395b8098370 27323 1 2023-01-05 08:33:06 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-kq9r6 f33c6f29-9777-4795-8a89-01949df775d1 0xc003342dd0 0xc003342dd1}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:33:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f33c6f29-9777-4795-8a89-01949df775d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:33:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003342e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 08:33:08.602: INFO: Pod "test-deployment-kq9r6-777898ffcc-77rkz" is available:
&Pod{ObjectMeta:{test-deployment-kq9r6-777898ffcc-77rkz test-deployment-kq9r6-777898ffcc- deployment-8607  2d8f38db-69dd-4888-8777-d5c8d1e5dc65 27322 0 2023-01-05 08:33:06 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:a36c440eb74f76040595c5c2cd20332b21eb1cc56ce11bbe6c1dff9bf9177987 cni.projectcalico.org/podIP:10.244.1.7/32 cni.projectcalico.org/podIPs:10.244.1.7/32] [{apps/v1 ReplicaSet test-deployment-kq9r6-777898ffcc 9eb40a50-af1b-4b94-bdb3-d395b8098370 0xc004c1a160 0xc004c1a161}] [] [{kube-controller-manager Update v1 2023-01-05 08:33:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9eb40a50-af1b-4b94-bdb3-d395b8098370\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:33:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:33:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6b4jm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6b4jm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:33:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:33:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.7,StartTime:2023-01-05 08:33:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:33:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a1ac03339c076dbacd858af1dafe643000d8b955b1d22498e8e8ee71a4f24cfc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 08:33:08.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8607" for this suite. 01/05/23 08:33:08.604
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":253,"skipped":4836,"failed":0}
------------------------------
â€¢ [2.104 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:06.517
    Jan  5 08:33:06.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename deployment 01/05/23 08:33:06.518
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:06.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:06.541
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/05/23 08:33:06.545
    Jan  5 08:33:06.546: INFO: Creating simple deployment test-deployment-kq9r6
    Jan  5 08:33:06.560: INFO: deployment "test-deployment-kq9r6" doesn't have the required revision set
    STEP: Getting /status 01/05/23 08:33:08.566
    Jan  5 08:33:08.568: INFO: Deployment test-deployment-kq9r6 has Conditions: [{Available True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kq9r6-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/05/23 08:33:08.568
    Jan  5 08:33:08.573: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 33, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 33, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 33, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 33, 6, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-kq9r6-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/05/23 08:33:08.573
    Jan  5 08:33:08.574: INFO: Observed &Deployment event: ADDED
    Jan  5 08:33:08.574: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kq9r6-777898ffcc"}
    Jan  5 08:33:08.574: INFO: Observed &Deployment event: MODIFIED
    Jan  5 08:33:08.574: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kq9r6-777898ffcc"}
    Jan  5 08:33:08.574: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 08:33:08.575: INFO: Observed &Deployment event: MODIFIED
    Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-kq9r6-777898ffcc" is progressing.}
    Jan  5 08:33:08.575: INFO: Observed &Deployment event: MODIFIED
    Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kq9r6-777898ffcc" has successfully progressed.}
    Jan  5 08:33:08.575: INFO: Observed &Deployment event: MODIFIED
    Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 08:33:08.575: INFO: Observed Deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kq9r6-777898ffcc" has successfully progressed.}
    Jan  5 08:33:08.575: INFO: Found Deployment test-deployment-kq9r6 in namespace deployment-8607 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 08:33:08.575: INFO: Deployment test-deployment-kq9r6 has an updated status
    STEP: patching the Statefulset Status 01/05/23 08:33:08.575
    Jan  5 08:33:08.575: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  5 08:33:08.594: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/05/23 08:33:08.594
    Jan  5 08:33:08.595: INFO: Observed &Deployment event: ADDED
    Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kq9r6-777898ffcc"}
    Jan  5 08:33:08.595: INFO: Observed &Deployment event: MODIFIED
    Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kq9r6-777898ffcc"}
    Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 08:33:08.595: INFO: Observed &Deployment event: MODIFIED
    Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:06 +0000 UTC 2023-01-05 08:33:06 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-kq9r6-777898ffcc" is progressing.}
    Jan  5 08:33:08.595: INFO: Observed &Deployment event: MODIFIED
    Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kq9r6-777898ffcc" has successfully progressed.}
    Jan  5 08:33:08.595: INFO: Observed &Deployment event: MODIFIED
    Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 08:33:08.595: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 08:33:08 +0000 UTC 2023-01-05 08:33:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kq9r6-777898ffcc" has successfully progressed.}
    Jan  5 08:33:08.596: INFO: Observed deployment test-deployment-kq9r6 in namespace deployment-8607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 08:33:08.596: INFO: Observed &Deployment event: MODIFIED
    Jan  5 08:33:08.596: INFO: Found deployment test-deployment-kq9r6 in namespace deployment-8607 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan  5 08:33:08.596: INFO: Deployment test-deployment-kq9r6 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 08:33:08.597: INFO: Deployment "test-deployment-kq9r6":
    &Deployment{ObjectMeta:{test-deployment-kq9r6  deployment-8607  f33c6f29-9777-4795-8a89-01949df775d1 27326 1 2023-01-05 08:33:06 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-05 08:33:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-05 08:33:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-05 08:33:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bedb78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 08:33:08.600: INFO: New ReplicaSet "test-deployment-kq9r6-777898ffcc" of Deployment "test-deployment-kq9r6":
    &ReplicaSet{ObjectMeta:{test-deployment-kq9r6-777898ffcc  deployment-8607  9eb40a50-af1b-4b94-bdb3-d395b8098370 27323 1 2023-01-05 08:33:06 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-kq9r6 f33c6f29-9777-4795-8a89-01949df775d1 0xc003342dd0 0xc003342dd1}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:33:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f33c6f29-9777-4795-8a89-01949df775d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:33:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003342e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 08:33:08.602: INFO: Pod "test-deployment-kq9r6-777898ffcc-77rkz" is available:
    &Pod{ObjectMeta:{test-deployment-kq9r6-777898ffcc-77rkz test-deployment-kq9r6-777898ffcc- deployment-8607  2d8f38db-69dd-4888-8777-d5c8d1e5dc65 27322 0 2023-01-05 08:33:06 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:a36c440eb74f76040595c5c2cd20332b21eb1cc56ce11bbe6c1dff9bf9177987 cni.projectcalico.org/podIP:10.244.1.7/32 cni.projectcalico.org/podIPs:10.244.1.7/32] [{apps/v1 ReplicaSet test-deployment-kq9r6-777898ffcc 9eb40a50-af1b-4b94-bdb3-d395b8098370 0xc004c1a160 0xc004c1a161}] [] [{kube-controller-manager Update v1 2023-01-05 08:33:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9eb40a50-af1b-4b94-bdb3-d395b8098370\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:33:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:33:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6b4jm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6b4jm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:33:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:33:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.7,StartTime:2023-01-05 08:33:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:33:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a1ac03339c076dbacd858af1dafe643000d8b955b1d22498e8e8ee71a4f24cfc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 08:33:08.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8607" for this suite. 01/05/23 08:33:08.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:08.622
Jan  5 08:33:08.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:33:08.623
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:08.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:08.643
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-9573 01/05/23 08:33:08.645
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9573 to expose endpoints map[] 01/05/23 08:33:08.702
Jan  5 08:33:08.704: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan  5 08:33:09.712: INFO: successfully validated that service multi-endpoint-test in namespace services-9573 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9573 01/05/23 08:33:09.712
Jan  5 08:33:09.716: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9573" to be "running and ready"
Jan  5 08:33:09.717: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.261414ms
Jan  5 08:33:09.717: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:33:11.721: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004858458s
Jan  5 08:33:11.721: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  5 08:33:11.721: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9573 to expose endpoints map[pod1:[100]] 01/05/23 08:33:11.723
Jan  5 08:33:11.734: INFO: successfully validated that service multi-endpoint-test in namespace services-9573 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9573 01/05/23 08:33:11.734
Jan  5 08:33:11.737: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9573" to be "running and ready"
Jan  5 08:33:11.739: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.378101ms
Jan  5 08:33:11.739: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:33:13.742: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004551742s
Jan  5 08:33:13.742: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  5 08:33:13.742: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9573 to expose endpoints map[pod1:[100] pod2:[101]] 01/05/23 08:33:13.747
Jan  5 08:33:13.760: INFO: successfully validated that service multi-endpoint-test in namespace services-9573 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/05/23 08:33:13.76
Jan  5 08:33:13.760: INFO: Creating new exec pod
Jan  5 08:33:13.770: INFO: Waiting up to 5m0s for pod "execpod8zhvs" in namespace "services-9573" to be "running"
Jan  5 08:33:13.772: INFO: Pod "execpod8zhvs": Phase="Pending", Reason="", readiness=false. Elapsed: 1.633695ms
Jan  5 08:33:15.774: INFO: Pod "execpod8zhvs": Phase="Running", Reason="", readiness=true. Elapsed: 2.00441s
Jan  5 08:33:15.774: INFO: Pod "execpod8zhvs" satisfied condition "running"
Jan  5 08:33:16.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9573 exec execpod8zhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan  5 08:33:16.888: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan  5 08:33:16.888: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:33:16.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9573 exec execpod8zhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.89.34 80'
Jan  5 08:33:17.013: INFO: stderr: "+ nc -v -t -w 2 10.97.89.34 80\n+ echo hostName\nConnection to 10.97.89.34 80 port [tcp/http] succeeded!\n"
Jan  5 08:33:17.013: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:33:17.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9573 exec execpod8zhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan  5 08:33:17.131: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan  5 08:33:17.131: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:33:17.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9573 exec execpod8zhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.89.34 81'
Jan  5 08:33:17.237: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.89.34 81\nConnection to 10.97.89.34 81 port [tcp/*] succeeded!\n"
Jan  5 08:33:17.237: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9573 01/05/23 08:33:17.237
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9573 to expose endpoints map[pod2:[101]] 01/05/23 08:33:17.251
Jan  5 08:33:17.263: INFO: successfully validated that service multi-endpoint-test in namespace services-9573 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9573 01/05/23 08:33:17.263
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9573 to expose endpoints map[] 01/05/23 08:33:17.281
Jan  5 08:33:17.284: INFO: successfully validated that service multi-endpoint-test in namespace services-9573 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:33:17.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9573" for this suite. 01/05/23 08:33:17.312
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":254,"skipped":4849,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.694 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:08.622
    Jan  5 08:33:08.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:33:08.623
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:08.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:08.643
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-9573 01/05/23 08:33:08.645
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9573 to expose endpoints map[] 01/05/23 08:33:08.702
    Jan  5 08:33:08.704: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan  5 08:33:09.712: INFO: successfully validated that service multi-endpoint-test in namespace services-9573 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9573 01/05/23 08:33:09.712
    Jan  5 08:33:09.716: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9573" to be "running and ready"
    Jan  5 08:33:09.717: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.261414ms
    Jan  5 08:33:09.717: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:33:11.721: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004858458s
    Jan  5 08:33:11.721: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  5 08:33:11.721: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9573 to expose endpoints map[pod1:[100]] 01/05/23 08:33:11.723
    Jan  5 08:33:11.734: INFO: successfully validated that service multi-endpoint-test in namespace services-9573 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9573 01/05/23 08:33:11.734
    Jan  5 08:33:11.737: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9573" to be "running and ready"
    Jan  5 08:33:11.739: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.378101ms
    Jan  5 08:33:11.739: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:33:13.742: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004551742s
    Jan  5 08:33:13.742: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  5 08:33:13.742: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9573 to expose endpoints map[pod1:[100] pod2:[101]] 01/05/23 08:33:13.747
    Jan  5 08:33:13.760: INFO: successfully validated that service multi-endpoint-test in namespace services-9573 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/05/23 08:33:13.76
    Jan  5 08:33:13.760: INFO: Creating new exec pod
    Jan  5 08:33:13.770: INFO: Waiting up to 5m0s for pod "execpod8zhvs" in namespace "services-9573" to be "running"
    Jan  5 08:33:13.772: INFO: Pod "execpod8zhvs": Phase="Pending", Reason="", readiness=false. Elapsed: 1.633695ms
    Jan  5 08:33:15.774: INFO: Pod "execpod8zhvs": Phase="Running", Reason="", readiness=true. Elapsed: 2.00441s
    Jan  5 08:33:15.774: INFO: Pod "execpod8zhvs" satisfied condition "running"
    Jan  5 08:33:16.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9573 exec execpod8zhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan  5 08:33:16.888: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan  5 08:33:16.888: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:33:16.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9573 exec execpod8zhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.89.34 80'
    Jan  5 08:33:17.013: INFO: stderr: "+ nc -v -t -w 2 10.97.89.34 80\n+ echo hostName\nConnection to 10.97.89.34 80 port [tcp/http] succeeded!\n"
    Jan  5 08:33:17.013: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:33:17.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9573 exec execpod8zhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan  5 08:33:17.131: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan  5 08:33:17.131: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:33:17.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-9573 exec execpod8zhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.89.34 81'
    Jan  5 08:33:17.237: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.89.34 81\nConnection to 10.97.89.34 81 port [tcp/*] succeeded!\n"
    Jan  5 08:33:17.237: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-9573 01/05/23 08:33:17.237
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9573 to expose endpoints map[pod2:[101]] 01/05/23 08:33:17.251
    Jan  5 08:33:17.263: INFO: successfully validated that service multi-endpoint-test in namespace services-9573 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9573 01/05/23 08:33:17.263
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9573 to expose endpoints map[] 01/05/23 08:33:17.281
    Jan  5 08:33:17.284: INFO: successfully validated that service multi-endpoint-test in namespace services-9573 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:33:17.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9573" for this suite. 01/05/23 08:33:17.312
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:17.316
Jan  5 08:33:17.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename init-container 01/05/23 08:33:17.317
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:17.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:17.338
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/05/23 08:33:17.34
Jan  5 08:33:17.340: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 08:33:22.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3909" for this suite. 01/05/23 08:33:22.25
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":255,"skipped":4857,"failed":0}
------------------------------
â€¢ [4.944 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:17.316
    Jan  5 08:33:17.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename init-container 01/05/23 08:33:17.317
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:17.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:17.338
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/05/23 08:33:17.34
    Jan  5 08:33:17.340: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 08:33:22.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3909" for this suite. 01/05/23 08:33:22.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:22.261
Jan  5 08:33:22.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:33:22.263
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:22.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:22.287
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-e45bfa8e-9b54-439a-a968-fb69e9d8c69e 01/05/23 08:33:22.288
STEP: Creating a pod to test consume secrets 01/05/23 08:33:22.291
Jan  5 08:33:22.301: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb" in namespace "projected-709" to be "Succeeded or Failed"
Jan  5 08:33:22.303: INFO: Pod "pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.428897ms
Jan  5 08:33:24.308: INFO: Pod "pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006680504s
Jan  5 08:33:26.306: INFO: Pod "pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005240248s
STEP: Saw pod success 01/05/23 08:33:26.306
Jan  5 08:33:26.306: INFO: Pod "pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb" satisfied condition "Succeeded or Failed"
Jan  5 08:33:26.308: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 08:33:26.312
Jan  5 08:33:26.387: INFO: Waiting for pod pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb to disappear
Jan  5 08:33:26.389: INFO: Pod pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 08:33:26.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-709" for this suite. 01/05/23 08:33:26.392
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":256,"skipped":4891,"failed":0}
------------------------------
â€¢ [4.135 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:22.261
    Jan  5 08:33:22.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:33:22.263
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:22.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:22.287
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-e45bfa8e-9b54-439a-a968-fb69e9d8c69e 01/05/23 08:33:22.288
    STEP: Creating a pod to test consume secrets 01/05/23 08:33:22.291
    Jan  5 08:33:22.301: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb" in namespace "projected-709" to be "Succeeded or Failed"
    Jan  5 08:33:22.303: INFO: Pod "pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.428897ms
    Jan  5 08:33:24.308: INFO: Pod "pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006680504s
    Jan  5 08:33:26.306: INFO: Pod "pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005240248s
    STEP: Saw pod success 01/05/23 08:33:26.306
    Jan  5 08:33:26.306: INFO: Pod "pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb" satisfied condition "Succeeded or Failed"
    Jan  5 08:33:26.308: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 08:33:26.312
    Jan  5 08:33:26.387: INFO: Waiting for pod pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb to disappear
    Jan  5 08:33:26.389: INFO: Pod pod-projected-secrets-d025e4c6-2503-4fe1-a0d5-8cc5c21534cb no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 08:33:26.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-709" for this suite. 01/05/23 08:33:26.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:26.396
Jan  5 08:33:26.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename security-context-test 01/05/23 08:33:26.397
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:26.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:26.416
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan  5 08:33:26.422: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2a642119-3587-4654-aa5d-0dd615351c3f" in namespace "security-context-test-3388" to be "Succeeded or Failed"
Jan  5 08:33:26.424: INFO: Pod "busybox-user-65534-2a642119-3587-4654-aa5d-0dd615351c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.902644ms
Jan  5 08:33:28.426: INFO: Pod "busybox-user-65534-2a642119-3587-4654-aa5d-0dd615351c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004156869s
Jan  5 08:33:30.427: INFO: Pod "busybox-user-65534-2a642119-3587-4654-aa5d-0dd615351c3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005640235s
Jan  5 08:33:30.427: INFO: Pod "busybox-user-65534-2a642119-3587-4654-aa5d-0dd615351c3f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 08:33:30.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3388" for this suite. 01/05/23 08:33:30.43
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":257,"skipped":4896,"failed":0}
------------------------------
â€¢ [4.038 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:26.396
    Jan  5 08:33:26.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename security-context-test 01/05/23 08:33:26.397
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:26.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:26.416
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan  5 08:33:26.422: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2a642119-3587-4654-aa5d-0dd615351c3f" in namespace "security-context-test-3388" to be "Succeeded or Failed"
    Jan  5 08:33:26.424: INFO: Pod "busybox-user-65534-2a642119-3587-4654-aa5d-0dd615351c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.902644ms
    Jan  5 08:33:28.426: INFO: Pod "busybox-user-65534-2a642119-3587-4654-aa5d-0dd615351c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004156869s
    Jan  5 08:33:30.427: INFO: Pod "busybox-user-65534-2a642119-3587-4654-aa5d-0dd615351c3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005640235s
    Jan  5 08:33:30.427: INFO: Pod "busybox-user-65534-2a642119-3587-4654-aa5d-0dd615351c3f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 08:33:30.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3388" for this suite. 01/05/23 08:33:30.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:30.437
Jan  5 08:33:30.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 08:33:30.437
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:30.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:30.463
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 08:33:30.464
Jan  5 08:33:30.470: INFO: Waiting up to 5m0s for pod "pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a" in namespace "emptydir-8215" to be "Succeeded or Failed"
Jan  5 08:33:30.472: INFO: Pod "pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.615363ms
Jan  5 08:33:32.474: INFO: Pod "pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00396491s
Jan  5 08:33:34.475: INFO: Pod "pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005476807s
STEP: Saw pod success 01/05/23 08:33:34.475
Jan  5 08:33:34.475: INFO: Pod "pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a" satisfied condition "Succeeded or Failed"
Jan  5 08:33:34.477: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a container test-container: <nil>
STEP: delete the pod 01/05/23 08:33:34.481
Jan  5 08:33:34.497: INFO: Waiting for pod pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a to disappear
Jan  5 08:33:34.498: INFO: Pod pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 08:33:34.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8215" for this suite. 01/05/23 08:33:34.5
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":258,"skipped":4980,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:30.437
    Jan  5 08:33:30.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 08:33:30.437
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:30.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:30.463
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 08:33:30.464
    Jan  5 08:33:30.470: INFO: Waiting up to 5m0s for pod "pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a" in namespace "emptydir-8215" to be "Succeeded or Failed"
    Jan  5 08:33:30.472: INFO: Pod "pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.615363ms
    Jan  5 08:33:32.474: INFO: Pod "pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00396491s
    Jan  5 08:33:34.475: INFO: Pod "pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005476807s
    STEP: Saw pod success 01/05/23 08:33:34.475
    Jan  5 08:33:34.475: INFO: Pod "pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a" satisfied condition "Succeeded or Failed"
    Jan  5 08:33:34.477: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a container test-container: <nil>
    STEP: delete the pod 01/05/23 08:33:34.481
    Jan  5 08:33:34.497: INFO: Waiting for pod pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a to disappear
    Jan  5 08:33:34.498: INFO: Pod pod-e9d7d933-d181-42b7-9ad3-c9ed7ff17c9a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 08:33:34.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8215" for this suite. 01/05/23 08:33:34.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:34.508
Jan  5 08:33:34.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:33:34.508
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:34.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:34.534
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:33:34.548
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:33:34.997
STEP: Deploying the webhook pod 01/05/23 08:33:35.005
STEP: Wait for the deployment to be ready 01/05/23 08:33:35.036
Jan  5 08:33:35.047: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  5 08:33:37.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 33, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 33, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 33, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 33, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/05/23 08:33:39.058
STEP: Verifying the service has paired with the endpoint 01/05/23 08:33:39.079
Jan  5 08:33:40.080: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 08:33:40.082
STEP: create a pod that should be denied by the webhook 01/05/23 08:33:40.096
STEP: create a pod that causes the webhook to hang 01/05/23 08:33:40.11
STEP: create a configmap that should be denied by the webhook 01/05/23 08:33:50.115
STEP: create a configmap that should be admitted by the webhook 01/05/23 08:33:50.134
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 08:33:50.144
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 08:33:50.149
STEP: create a namespace that bypass the webhook 01/05/23 08:33:50.153
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/05/23 08:33:50.163
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:33:50.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-836" for this suite. 01/05/23 08:33:50.196
STEP: Destroying namespace "webhook-836-markers" for this suite. 01/05/23 08:33:50.207
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":259,"skipped":4993,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.758 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:34.508
    Jan  5 08:33:34.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:33:34.508
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:34.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:34.534
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:33:34.548
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:33:34.997
    STEP: Deploying the webhook pod 01/05/23 08:33:35.005
    STEP: Wait for the deployment to be ready 01/05/23 08:33:35.036
    Jan  5 08:33:35.047: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  5 08:33:37.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 33, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 33, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 33, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 33, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/05/23 08:33:39.058
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:33:39.079
    Jan  5 08:33:40.080: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 08:33:40.082
    STEP: create a pod that should be denied by the webhook 01/05/23 08:33:40.096
    STEP: create a pod that causes the webhook to hang 01/05/23 08:33:40.11
    STEP: create a configmap that should be denied by the webhook 01/05/23 08:33:50.115
    STEP: create a configmap that should be admitted by the webhook 01/05/23 08:33:50.134
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 08:33:50.144
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 08:33:50.149
    STEP: create a namespace that bypass the webhook 01/05/23 08:33:50.153
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/05/23 08:33:50.163
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:33:50.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-836" for this suite. 01/05/23 08:33:50.196
    STEP: Destroying namespace "webhook-836-markers" for this suite. 01/05/23 08:33:50.207
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:50.266
Jan  5 08:33:50.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename cronjob 01/05/23 08:33:50.267
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:50.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:50.303
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/05/23 08:33:50.309
STEP: creating 01/05/23 08:33:50.309
STEP: getting 01/05/23 08:33:50.313
STEP: listing 01/05/23 08:33:50.318
STEP: watching 01/05/23 08:33:50.321
Jan  5 08:33:50.321: INFO: starting watch
STEP: cluster-wide listing 01/05/23 08:33:50.322
STEP: cluster-wide watching 01/05/23 08:33:50.324
Jan  5 08:33:50.324: INFO: starting watch
STEP: patching 01/05/23 08:33:50.325
STEP: updating 01/05/23 08:33:50.336
Jan  5 08:33:50.344: INFO: waiting for watch events with expected annotations
Jan  5 08:33:50.344: INFO: saw patched and updated annotations
STEP: patching /status 01/05/23 08:33:50.344
STEP: updating /status 01/05/23 08:33:50.355
STEP: get /status 01/05/23 08:33:50.359
STEP: deleting 01/05/23 08:33:50.361
STEP: deleting a collection 01/05/23 08:33:50.389
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  5 08:33:50.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5150" for this suite. 01/05/23 08:33:50.405
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":260,"skipped":4998,"failed":0}
------------------------------
â€¢ [0.160 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:50.266
    Jan  5 08:33:50.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename cronjob 01/05/23 08:33:50.267
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:50.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:50.303
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/05/23 08:33:50.309
    STEP: creating 01/05/23 08:33:50.309
    STEP: getting 01/05/23 08:33:50.313
    STEP: listing 01/05/23 08:33:50.318
    STEP: watching 01/05/23 08:33:50.321
    Jan  5 08:33:50.321: INFO: starting watch
    STEP: cluster-wide listing 01/05/23 08:33:50.322
    STEP: cluster-wide watching 01/05/23 08:33:50.324
    Jan  5 08:33:50.324: INFO: starting watch
    STEP: patching 01/05/23 08:33:50.325
    STEP: updating 01/05/23 08:33:50.336
    Jan  5 08:33:50.344: INFO: waiting for watch events with expected annotations
    Jan  5 08:33:50.344: INFO: saw patched and updated annotations
    STEP: patching /status 01/05/23 08:33:50.344
    STEP: updating /status 01/05/23 08:33:50.355
    STEP: get /status 01/05/23 08:33:50.359
    STEP: deleting 01/05/23 08:33:50.361
    STEP: deleting a collection 01/05/23 08:33:50.389
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  5 08:33:50.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5150" for this suite. 01/05/23 08:33:50.405
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:50.426
Jan  5 08:33:50.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-runtime 01/05/23 08:33:50.427
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:50.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:50.454
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/05/23 08:33:50.456
STEP: wait for the container to reach Failed 01/05/23 08:33:50.469
STEP: get the container status 01/05/23 08:33:54.494
STEP: the container should be terminated 01/05/23 08:33:54.496
STEP: the termination message should be set 01/05/23 08:33:54.496
Jan  5 08:33:54.496: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/05/23 08:33:54.496
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  5 08:33:54.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-323" for this suite. 01/05/23 08:33:54.524
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":261,"skipped":4999,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:50.426
    Jan  5 08:33:50.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-runtime 01/05/23 08:33:50.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:50.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:50.454
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/05/23 08:33:50.456
    STEP: wait for the container to reach Failed 01/05/23 08:33:50.469
    STEP: get the container status 01/05/23 08:33:54.494
    STEP: the container should be terminated 01/05/23 08:33:54.496
    STEP: the termination message should be set 01/05/23 08:33:54.496
    Jan  5 08:33:54.496: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/05/23 08:33:54.496
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  5 08:33:54.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-323" for this suite. 01/05/23 08:33:54.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:54.529
Jan  5 08:33:54.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename proxy 01/05/23 08:33:54.529
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:54.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:54.549
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan  5 08:33:54.551: INFO: Creating pod...
Jan  5 08:33:54.574: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2137" to be "running"
Jan  5 08:33:54.577: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.420107ms
Jan  5 08:33:56.580: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005410565s
Jan  5 08:33:56.580: INFO: Pod "agnhost" satisfied condition "running"
Jan  5 08:33:56.580: INFO: Creating service...
Jan  5 08:33:56.591: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/DELETE
Jan  5 08:33:56.595: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 08:33:56.595: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/GET
Jan  5 08:33:56.598: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  5 08:33:56.598: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/HEAD
Jan  5 08:33:56.600: INFO: http.Client request:HEAD | StatusCode:200
Jan  5 08:33:56.600: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/OPTIONS
Jan  5 08:33:56.603: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 08:33:56.603: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/PATCH
Jan  5 08:33:56.604: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 08:33:56.604: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/POST
Jan  5 08:33:56.606: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 08:33:56.606: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/PUT
Jan  5 08:33:56.607: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  5 08:33:56.607: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/DELETE
Jan  5 08:33:56.609: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 08:33:56.609: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/GET
Jan  5 08:33:56.610: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  5 08:33:56.610: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/HEAD
Jan  5 08:33:56.612: INFO: http.Client request:HEAD | StatusCode:200
Jan  5 08:33:56.612: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/OPTIONS
Jan  5 08:33:56.613: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 08:33:56.613: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/PATCH
Jan  5 08:33:56.615: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 08:33:56.615: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/POST
Jan  5 08:33:56.617: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 08:33:56.617: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/PUT
Jan  5 08:33:56.619: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan  5 08:33:56.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2137" for this suite. 01/05/23 08:33:56.626
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":262,"skipped":5018,"failed":0}
------------------------------
â€¢ [2.100 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:54.529
    Jan  5 08:33:54.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename proxy 01/05/23 08:33:54.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:54.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:54.549
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan  5 08:33:54.551: INFO: Creating pod...
    Jan  5 08:33:54.574: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2137" to be "running"
    Jan  5 08:33:54.577: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.420107ms
    Jan  5 08:33:56.580: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005410565s
    Jan  5 08:33:56.580: INFO: Pod "agnhost" satisfied condition "running"
    Jan  5 08:33:56.580: INFO: Creating service...
    Jan  5 08:33:56.591: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/DELETE
    Jan  5 08:33:56.595: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 08:33:56.595: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/GET
    Jan  5 08:33:56.598: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  5 08:33:56.598: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/HEAD
    Jan  5 08:33:56.600: INFO: http.Client request:HEAD | StatusCode:200
    Jan  5 08:33:56.600: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan  5 08:33:56.603: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 08:33:56.603: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/PATCH
    Jan  5 08:33:56.604: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 08:33:56.604: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/POST
    Jan  5 08:33:56.606: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 08:33:56.606: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/pods/agnhost/proxy/some/path/with/PUT
    Jan  5 08:33:56.607: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  5 08:33:56.607: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/DELETE
    Jan  5 08:33:56.609: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 08:33:56.609: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/GET
    Jan  5 08:33:56.610: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  5 08:33:56.610: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/HEAD
    Jan  5 08:33:56.612: INFO: http.Client request:HEAD | StatusCode:200
    Jan  5 08:33:56.612: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/OPTIONS
    Jan  5 08:33:56.613: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 08:33:56.613: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/PATCH
    Jan  5 08:33:56.615: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 08:33:56.615: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/POST
    Jan  5 08:33:56.617: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 08:33:56.617: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2137/services/test-service/proxy/some/path/with/PUT
    Jan  5 08:33:56.619: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan  5 08:33:56.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2137" for this suite. 01/05/23 08:33:56.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:33:56.634
Jan  5 08:33:56.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename replication-controller 01/05/23 08:33:56.635
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:56.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:56.65
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/05/23 08:33:56.654
STEP: waiting for RC to be added 01/05/23 08:33:56.658
STEP: waiting for available Replicas 01/05/23 08:33:56.659
STEP: patching ReplicationController 01/05/23 08:33:58.348
STEP: waiting for RC to be modified 01/05/23 08:33:58.358
STEP: patching ReplicationController status 01/05/23 08:33:58.358
STEP: waiting for RC to be modified 01/05/23 08:33:58.366
STEP: waiting for available Replicas 01/05/23 08:33:58.366
STEP: fetching ReplicationController status 01/05/23 08:33:58.369
STEP: patching ReplicationController scale 01/05/23 08:33:58.371
STEP: waiting for RC to be modified 01/05/23 08:33:58.381
STEP: waiting for ReplicationController's scale to be the max amount 01/05/23 08:33:58.381
STEP: fetching ReplicationController; ensuring that it's patched 01/05/23 08:34:00.297
STEP: updating ReplicationController status 01/05/23 08:34:00.299
STEP: waiting for RC to be modified 01/05/23 08:34:00.303
STEP: listing all ReplicationControllers 01/05/23 08:34:00.303
STEP: checking that ReplicationController has expected values 01/05/23 08:34:00.304
STEP: deleting ReplicationControllers by collection 01/05/23 08:34:00.304
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/05/23 08:34:00.314
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  5 08:34:00.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2326" for this suite. 01/05/23 08:34:00.357
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":263,"skipped":5079,"failed":0}
------------------------------
â€¢ [3.731 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:33:56.634
    Jan  5 08:33:56.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename replication-controller 01/05/23 08:33:56.635
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:33:56.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:33:56.65
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/05/23 08:33:56.654
    STEP: waiting for RC to be added 01/05/23 08:33:56.658
    STEP: waiting for available Replicas 01/05/23 08:33:56.659
    STEP: patching ReplicationController 01/05/23 08:33:58.348
    STEP: waiting for RC to be modified 01/05/23 08:33:58.358
    STEP: patching ReplicationController status 01/05/23 08:33:58.358
    STEP: waiting for RC to be modified 01/05/23 08:33:58.366
    STEP: waiting for available Replicas 01/05/23 08:33:58.366
    STEP: fetching ReplicationController status 01/05/23 08:33:58.369
    STEP: patching ReplicationController scale 01/05/23 08:33:58.371
    STEP: waiting for RC to be modified 01/05/23 08:33:58.381
    STEP: waiting for ReplicationController's scale to be the max amount 01/05/23 08:33:58.381
    STEP: fetching ReplicationController; ensuring that it's patched 01/05/23 08:34:00.297
    STEP: updating ReplicationController status 01/05/23 08:34:00.299
    STEP: waiting for RC to be modified 01/05/23 08:34:00.303
    STEP: listing all ReplicationControllers 01/05/23 08:34:00.303
    STEP: checking that ReplicationController has expected values 01/05/23 08:34:00.304
    STEP: deleting ReplicationControllers by collection 01/05/23 08:34:00.304
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/05/23 08:34:00.314
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  5 08:34:00.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2326" for this suite. 01/05/23 08:34:00.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:34:00.366
Jan  5 08:34:00.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename job 01/05/23 08:34:00.367
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:00.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:00.391
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/05/23 08:34:00.393
STEP: Ensuring active pods == parallelism 01/05/23 08:34:00.417
STEP: delete a job 01/05/23 08:34:04.421
STEP: deleting Job.batch foo in namespace job-2383, will wait for the garbage collector to delete the pods 01/05/23 08:34:04.421
Jan  5 08:34:04.478: INFO: Deleting Job.batch foo took: 4.46597ms
Jan  5 08:34:04.578: INFO: Terminating Job.batch foo pods took: 100.630262ms
STEP: Ensuring job was deleted 01/05/23 08:34:36.479
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 08:34:36.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2383" for this suite. 01/05/23 08:34:36.484
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":264,"skipped":5108,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.128 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:34:00.366
    Jan  5 08:34:00.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename job 01/05/23 08:34:00.367
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:00.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:00.391
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/05/23 08:34:00.393
    STEP: Ensuring active pods == parallelism 01/05/23 08:34:00.417
    STEP: delete a job 01/05/23 08:34:04.421
    STEP: deleting Job.batch foo in namespace job-2383, will wait for the garbage collector to delete the pods 01/05/23 08:34:04.421
    Jan  5 08:34:04.478: INFO: Deleting Job.batch foo took: 4.46597ms
    Jan  5 08:34:04.578: INFO: Terminating Job.batch foo pods took: 100.630262ms
    STEP: Ensuring job was deleted 01/05/23 08:34:36.479
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 08:34:36.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2383" for this suite. 01/05/23 08:34:36.484
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:34:36.494
Jan  5 08:34:36.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 08:34:36.495
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:36.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:36.527
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 08:34:36.528
Jan  5 08:34:36.538: INFO: Waiting up to 5m0s for pod "pod-567f5ee7-2e8e-406a-9826-c519de0333a6" in namespace "emptydir-6342" to be "Succeeded or Failed"
Jan  5 08:34:36.540: INFO: Pod "pod-567f5ee7-2e8e-406a-9826-c519de0333a6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.510298ms
Jan  5 08:34:38.544: INFO: Pod "pod-567f5ee7-2e8e-406a-9826-c519de0333a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005756461s
Jan  5 08:34:40.544: INFO: Pod "pod-567f5ee7-2e8e-406a-9826-c519de0333a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005503295s
STEP: Saw pod success 01/05/23 08:34:40.544
Jan  5 08:34:40.544: INFO: Pod "pod-567f5ee7-2e8e-406a-9826-c519de0333a6" satisfied condition "Succeeded or Failed"
Jan  5 08:34:40.546: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-567f5ee7-2e8e-406a-9826-c519de0333a6 container test-container: <nil>
STEP: delete the pod 01/05/23 08:34:40.551
Jan  5 08:34:40.570: INFO: Waiting for pod pod-567f5ee7-2e8e-406a-9826-c519de0333a6 to disappear
Jan  5 08:34:40.571: INFO: Pod pod-567f5ee7-2e8e-406a-9826-c519de0333a6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 08:34:40.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6342" for this suite. 01/05/23 08:34:40.573
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":265,"skipped":5110,"failed":0}
------------------------------
â€¢ [4.093 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:34:36.494
    Jan  5 08:34:36.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 08:34:36.495
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:36.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:36.527
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 08:34:36.528
    Jan  5 08:34:36.538: INFO: Waiting up to 5m0s for pod "pod-567f5ee7-2e8e-406a-9826-c519de0333a6" in namespace "emptydir-6342" to be "Succeeded or Failed"
    Jan  5 08:34:36.540: INFO: Pod "pod-567f5ee7-2e8e-406a-9826-c519de0333a6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.510298ms
    Jan  5 08:34:38.544: INFO: Pod "pod-567f5ee7-2e8e-406a-9826-c519de0333a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005756461s
    Jan  5 08:34:40.544: INFO: Pod "pod-567f5ee7-2e8e-406a-9826-c519de0333a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005503295s
    STEP: Saw pod success 01/05/23 08:34:40.544
    Jan  5 08:34:40.544: INFO: Pod "pod-567f5ee7-2e8e-406a-9826-c519de0333a6" satisfied condition "Succeeded or Failed"
    Jan  5 08:34:40.546: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-567f5ee7-2e8e-406a-9826-c519de0333a6 container test-container: <nil>
    STEP: delete the pod 01/05/23 08:34:40.551
    Jan  5 08:34:40.570: INFO: Waiting for pod pod-567f5ee7-2e8e-406a-9826-c519de0333a6 to disappear
    Jan  5 08:34:40.571: INFO: Pod pod-567f5ee7-2e8e-406a-9826-c519de0333a6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 08:34:40.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6342" for this suite. 01/05/23 08:34:40.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:34:40.588
Jan  5 08:34:40.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename proxy 01/05/23 08:34:40.589
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:40.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:40.606
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/05/23 08:34:40.62
STEP: creating replication controller proxy-service-k8fbh in namespace proxy-9116 01/05/23 08:34:40.62
I0105 08:34:40.641624      23 runners.go:193] Created replication controller with name: proxy-service-k8fbh, namespace: proxy-9116, replica count: 1
I0105 08:34:41.692566      23 runners.go:193] proxy-service-k8fbh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0105 08:34:42.693673      23 runners.go:193] proxy-service-k8fbh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0105 08:34:43.693854      23 runners.go:193] proxy-service-k8fbh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 08:34:43.696: INFO: setup took 3.088301283s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/05/23 08:34:43.696
Jan  5 08:34:43.700: INFO: (0) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.175222ms)
Jan  5 08:34:43.701: INFO: (0) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.480264ms)
Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 5.769195ms)
Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 5.816427ms)
Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 5.951775ms)
Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 5.800908ms)
Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 6.074298ms)
Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 6.002428ms)
Jan  5 08:34:43.703: INFO: (0) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 6.578477ms)
Jan  5 08:34:43.703: INFO: (0) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 6.696345ms)
Jan  5 08:34:43.703: INFO: (0) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 6.930187ms)
Jan  5 08:34:43.705: INFO: (0) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 8.649032ms)
Jan  5 08:34:43.705: INFO: (0) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 8.831075ms)
Jan  5 08:34:43.707: INFO: (0) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 10.538408ms)
Jan  5 08:34:43.707: INFO: (0) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 10.842046ms)
Jan  5 08:34:43.708: INFO: (0) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 11.260822ms)
Jan  5 08:34:43.712: INFO: (1) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.192868ms)
Jan  5 08:34:43.712: INFO: (1) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 4.358635ms)
Jan  5 08:34:43.712: INFO: (1) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.91846ms)
Jan  5 08:34:43.712: INFO: (1) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.612177ms)
Jan  5 08:34:43.712: INFO: (1) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 3.56705ms)
Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.649182ms)
Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 5.214038ms)
Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.762553ms)
Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.719196ms)
Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 5.100911ms)
Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 5.3157ms)
Jan  5 08:34:43.714: INFO: (1) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 5.221522ms)
Jan  5 08:34:43.714: INFO: (1) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 5.795072ms)
Jan  5 08:34:43.714: INFO: (1) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 6.307214ms)
Jan  5 08:34:43.714: INFO: (1) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 5.839052ms)
Jan  5 08:34:43.714: INFO: (1) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 5.888002ms)
Jan  5 08:34:43.717: INFO: (2) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.86327ms)
Jan  5 08:34:43.717: INFO: (2) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 3.02977ms)
Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 3.71131ms)
Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.768005ms)
Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.922119ms)
Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.802898ms)
Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.89181ms)
Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.112398ms)
Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.12742ms)
Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.176164ms)
Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.394362ms)
Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 4.567854ms)
Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.686187ms)
Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 4.768874ms)
Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 5.095651ms)
Jan  5 08:34:43.720: INFO: (2) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 5.81917ms)
Jan  5 08:34:43.725: INFO: (3) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.640844ms)
Jan  5 08:34:43.725: INFO: (3) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 4.799017ms)
Jan  5 08:34:43.725: INFO: (3) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 5.026888ms)
Jan  5 08:34:43.725: INFO: (3) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 5.087056ms)
Jan  5 08:34:43.726: INFO: (3) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 5.251065ms)
Jan  5 08:34:43.726: INFO: (3) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 5.924811ms)
Jan  5 08:34:43.726: INFO: (3) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 5.950542ms)
Jan  5 08:34:43.726: INFO: (3) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 6.054646ms)
Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 6.241231ms)
Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 6.584211ms)
Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 6.548875ms)
Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 6.706906ms)
Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 6.693224ms)
Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 6.529275ms)
Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 6.843049ms)
Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 6.936903ms)
Jan  5 08:34:43.729: INFO: (4) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 1.360937ms)
Jan  5 08:34:43.730: INFO: (4) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.309756ms)
Jan  5 08:34:43.730: INFO: (4) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.547172ms)
Jan  5 08:34:43.731: INFO: (4) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.481878ms)
Jan  5 08:34:43.731: INFO: (4) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 3.897631ms)
Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.126133ms)
Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.147112ms)
Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 4.210798ms)
Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.367863ms)
Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.360497ms)
Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.367825ms)
Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.498163ms)
Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.708914ms)
Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 4.83802ms)
Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.963699ms)
Jan  5 08:34:43.733: INFO: (4) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 5.198617ms)
Jan  5 08:34:43.735: INFO: (5) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.060607ms)
Jan  5 08:34:43.735: INFO: (5) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.368881ms)
Jan  5 08:34:43.735: INFO: (5) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.438014ms)
Jan  5 08:34:43.735: INFO: (5) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.612288ms)
Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.995188ms)
Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 2.953448ms)
Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.00129ms)
Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.023702ms)
Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 3.330558ms)
Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.597708ms)
Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.614996ms)
Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 3.727791ms)
Jan  5 08:34:43.737: INFO: (5) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 3.89393ms)
Jan  5 08:34:43.737: INFO: (5) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.881476ms)
Jan  5 08:34:43.737: INFO: (5) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.160207ms)
Jan  5 08:34:43.737: INFO: (5) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.079498ms)
Jan  5 08:34:43.739: INFO: (6) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.342394ms)
Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.448059ms)
Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.499866ms)
Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.712578ms)
Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.794371ms)
Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 2.915516ms)
Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.063313ms)
Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 3.208584ms)
Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 3.244888ms)
Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.32026ms)
Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.256142ms)
Jan  5 08:34:43.741: INFO: (6) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 3.537658ms)
Jan  5 08:34:43.741: INFO: (6) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 3.628668ms)
Jan  5 08:34:43.741: INFO: (6) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 3.66179ms)
Jan  5 08:34:43.741: INFO: (6) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.608817ms)
Jan  5 08:34:43.741: INFO: (6) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.670639ms)
Jan  5 08:34:43.744: INFO: (7) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.098568ms)
Jan  5 08:34:43.744: INFO: (7) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.046941ms)
Jan  5 08:34:43.744: INFO: (7) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.165061ms)
Jan  5 08:34:43.744: INFO: (7) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.367998ms)
Jan  5 08:34:43.744: INFO: (7) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.43268ms)
Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.730599ms)
Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 4.154857ms)
Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.160708ms)
Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.130868ms)
Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.293435ms)
Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 4.197573ms)
Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 4.273546ms)
Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.431392ms)
Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.444135ms)
Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.448936ms)
Jan  5 08:34:43.746: INFO: (7) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.780002ms)
Jan  5 08:34:43.748: INFO: (8) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 1.690602ms)
Jan  5 08:34:43.748: INFO: (8) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 1.795182ms)
Jan  5 08:34:43.748: INFO: (8) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 1.875462ms)
Jan  5 08:34:43.748: INFO: (8) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.145102ms)
Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 2.739372ms)
Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.650354ms)
Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 2.91299ms)
Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.587564ms)
Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 3.357013ms)
Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 3.586354ms)
Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.12728ms)
Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.019269ms)
Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.021066ms)
Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.392944ms)
Jan  5 08:34:43.750: INFO: (8) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 3.222336ms)
Jan  5 08:34:43.750: INFO: (8) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.373329ms)
Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 4.184966ms)
Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.940183ms)
Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 4.012094ms)
Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.184066ms)
Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.155366ms)
Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.313285ms)
Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.530717ms)
Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 4.146091ms)
Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.151101ms)
Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.423277ms)
Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.630049ms)
Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.704684ms)
Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.505141ms)
Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.616299ms)
Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.846152ms)
Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 5.387388ms)
Jan  5 08:34:43.757: INFO: (10) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 1.524642ms)
Jan  5 08:34:43.762: INFO: (10) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 5.922733ms)
Jan  5 08:34:43.762: INFO: (10) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 6.183073ms)
Jan  5 08:34:43.764: INFO: (10) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 8.468334ms)
Jan  5 08:34:43.764: INFO: (10) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 8.602385ms)
Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 8.882644ms)
Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 9.085682ms)
Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 8.969021ms)
Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 9.122384ms)
Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 9.373739ms)
Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 9.137015ms)
Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 9.1267ms)
Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 9.543156ms)
Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 9.387268ms)
Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 9.524267ms)
Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 9.577728ms)
Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.136562ms)
Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.530705ms)
Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.55504ms)
Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.712107ms)
Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 3.012638ms)
Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 2.649115ms)
Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.765271ms)
Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 2.892191ms)
Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 2.881276ms)
Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.246492ms)
Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.960448ms)
Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 3.081323ms)
Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.474997ms)
Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.278444ms)
Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 3.399798ms)
Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.249911ms)
Jan  5 08:34:43.771: INFO: (12) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.355995ms)
Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.210022ms)
Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.780323ms)
Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.663524ms)
Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.92289ms)
Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 2.701752ms)
Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.783102ms)
Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.79504ms)
Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.101201ms)
Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.306286ms)
Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.546727ms)
Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.106606ms)
Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.545354ms)
Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.182372ms)
Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.351091ms)
Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.484898ms)
Jan  5 08:34:43.777: INFO: (13) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 2.9418ms)
Jan  5 08:34:43.777: INFO: (13) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.08824ms)
Jan  5 08:34:43.777: INFO: (13) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.273903ms)
Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 3.95503ms)
Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.098917ms)
Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 4.192231ms)
Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 4.285386ms)
Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.126296ms)
Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.578287ms)
Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 4.126449ms)
Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 4.2031ms)
Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.427922ms)
Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.405008ms)
Jan  5 08:34:43.779: INFO: (13) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 4.564996ms)
Jan  5 08:34:43.779: INFO: (13) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.781379ms)
Jan  5 08:34:43.779: INFO: (13) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 5.00754ms)
Jan  5 08:34:43.781: INFO: (14) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.497135ms)
Jan  5 08:34:43.781: INFO: (14) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 2.728823ms)
Jan  5 08:34:43.781: INFO: (14) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 2.830194ms)
Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.800596ms)
Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.446198ms)
Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.739069ms)
Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 2.857611ms)
Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.967991ms)
Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.074944ms)
Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 3.041044ms)
Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.213985ms)
Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.959065ms)
Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.448532ms)
Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.46749ms)
Jan  5 08:34:43.783: INFO: (14) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 3.445388ms)
Jan  5 08:34:43.783: INFO: (14) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 3.590265ms)
Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 2.580966ms)
Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.100181ms)
Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 3.369893ms)
Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.169628ms)
Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.147937ms)
Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 3.338575ms)
Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 3.642192ms)
Jan  5 08:34:43.787: INFO: (15) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.613756ms)
Jan  5 08:34:43.787: INFO: (15) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.444215ms)
Jan  5 08:34:43.787: INFO: (15) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.811157ms)
Jan  5 08:34:43.787: INFO: (15) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.5175ms)
Jan  5 08:34:43.787: INFO: (15) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.270538ms)
Jan  5 08:34:43.788: INFO: (15) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 5.146902ms)
Jan  5 08:34:43.788: INFO: (15) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 5.001752ms)
Jan  5 08:34:43.788: INFO: (15) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 5.331498ms)
Jan  5 08:34:43.788: INFO: (15) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 5.344471ms)
Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.303738ms)
Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.969564ms)
Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.805827ms)
Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 2.845712ms)
Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.07792ms)
Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.844725ms)
Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.049022ms)
Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 3.474562ms)
Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 3.346599ms)
Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.548544ms)
Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.749752ms)
Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.586145ms)
Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 3.613977ms)
Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 3.749122ms)
Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 3.768124ms)
Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.77022ms)
Jan  5 08:34:43.795: INFO: (17) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.271681ms)
Jan  5 08:34:43.795: INFO: (17) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.287232ms)
Jan  5 08:34:43.795: INFO: (17) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.490763ms)
Jan  5 08:34:43.796: INFO: (17) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.932612ms)
Jan  5 08:34:43.796: INFO: (17) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.152969ms)
Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.139331ms)
Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 4.170202ms)
Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.201008ms)
Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.245192ms)
Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.357911ms)
Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 4.467882ms)
Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.622756ms)
Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.705782ms)
Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.803435ms)
Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.833333ms)
Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.993014ms)
Jan  5 08:34:43.801: INFO: (18) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.372058ms)
Jan  5 08:34:43.801: INFO: (18) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.438867ms)
Jan  5 08:34:43.801: INFO: (18) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.165918ms)
Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.871049ms)
Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.077543ms)
Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.121704ms)
Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 3.863683ms)
Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.107374ms)
Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.199361ms)
Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.566071ms)
Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.55103ms)
Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.802576ms)
Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.57219ms)
Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 4.563919ms)
Jan  5 08:34:43.803: INFO: (18) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.836433ms)
Jan  5 08:34:43.803: INFO: (18) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 5.744787ms)
Jan  5 08:34:43.805: INFO: (19) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 1.539257ms)
Jan  5 08:34:43.806: INFO: (19) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.783325ms)
Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.79833ms)
Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.007599ms)
Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.126402ms)
Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.298044ms)
Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.375957ms)
Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.944863ms)
Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.126681ms)
Jan  5 08:34:43.808: INFO: (19) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.422253ms)
Jan  5 08:34:43.808: INFO: (19) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.677129ms)
Jan  5 08:34:43.808: INFO: (19) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.630095ms)
Jan  5 08:34:43.808: INFO: (19) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.542278ms)
Jan  5 08:34:43.808: INFO: (19) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.894743ms)
Jan  5 08:34:43.809: INFO: (19) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.682686ms)
Jan  5 08:34:43.809: INFO: (19) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 4.969653ms)
STEP: deleting ReplicationController proxy-service-k8fbh in namespace proxy-9116, will wait for the garbage collector to delete the pods 01/05/23 08:34:43.809
Jan  5 08:34:43.865: INFO: Deleting ReplicationController proxy-service-k8fbh took: 4.156602ms
Jan  5 08:34:43.966: INFO: Terminating ReplicationController proxy-service-k8fbh pods took: 101.113389ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan  5 08:34:46.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9116" for this suite. 01/05/23 08:34:46.57
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":266,"skipped":5123,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.987 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:34:40.588
    Jan  5 08:34:40.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename proxy 01/05/23 08:34:40.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:40.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:40.606
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/05/23 08:34:40.62
    STEP: creating replication controller proxy-service-k8fbh in namespace proxy-9116 01/05/23 08:34:40.62
    I0105 08:34:40.641624      23 runners.go:193] Created replication controller with name: proxy-service-k8fbh, namespace: proxy-9116, replica count: 1
    I0105 08:34:41.692566      23 runners.go:193] proxy-service-k8fbh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0105 08:34:42.693673      23 runners.go:193] proxy-service-k8fbh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0105 08:34:43.693854      23 runners.go:193] proxy-service-k8fbh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 08:34:43.696: INFO: setup took 3.088301283s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/05/23 08:34:43.696
    Jan  5 08:34:43.700: INFO: (0) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.175222ms)
    Jan  5 08:34:43.701: INFO: (0) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.480264ms)
    Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 5.769195ms)
    Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 5.816427ms)
    Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 5.951775ms)
    Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 5.800908ms)
    Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 6.074298ms)
    Jan  5 08:34:43.702: INFO: (0) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 6.002428ms)
    Jan  5 08:34:43.703: INFO: (0) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 6.578477ms)
    Jan  5 08:34:43.703: INFO: (0) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 6.696345ms)
    Jan  5 08:34:43.703: INFO: (0) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 6.930187ms)
    Jan  5 08:34:43.705: INFO: (0) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 8.649032ms)
    Jan  5 08:34:43.705: INFO: (0) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 8.831075ms)
    Jan  5 08:34:43.707: INFO: (0) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 10.538408ms)
    Jan  5 08:34:43.707: INFO: (0) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 10.842046ms)
    Jan  5 08:34:43.708: INFO: (0) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 11.260822ms)
    Jan  5 08:34:43.712: INFO: (1) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.192868ms)
    Jan  5 08:34:43.712: INFO: (1) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 4.358635ms)
    Jan  5 08:34:43.712: INFO: (1) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.91846ms)
    Jan  5 08:34:43.712: INFO: (1) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.612177ms)
    Jan  5 08:34:43.712: INFO: (1) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 3.56705ms)
    Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.649182ms)
    Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 5.214038ms)
    Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.762553ms)
    Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.719196ms)
    Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 5.100911ms)
    Jan  5 08:34:43.713: INFO: (1) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 5.3157ms)
    Jan  5 08:34:43.714: INFO: (1) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 5.221522ms)
    Jan  5 08:34:43.714: INFO: (1) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 5.795072ms)
    Jan  5 08:34:43.714: INFO: (1) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 6.307214ms)
    Jan  5 08:34:43.714: INFO: (1) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 5.839052ms)
    Jan  5 08:34:43.714: INFO: (1) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 5.888002ms)
    Jan  5 08:34:43.717: INFO: (2) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.86327ms)
    Jan  5 08:34:43.717: INFO: (2) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 3.02977ms)
    Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 3.71131ms)
    Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.768005ms)
    Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.922119ms)
    Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.802898ms)
    Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.89181ms)
    Jan  5 08:34:43.718: INFO: (2) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.112398ms)
    Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.12742ms)
    Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.176164ms)
    Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.394362ms)
    Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 4.567854ms)
    Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.686187ms)
    Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 4.768874ms)
    Jan  5 08:34:43.719: INFO: (2) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 5.095651ms)
    Jan  5 08:34:43.720: INFO: (2) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 5.81917ms)
    Jan  5 08:34:43.725: INFO: (3) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.640844ms)
    Jan  5 08:34:43.725: INFO: (3) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 4.799017ms)
    Jan  5 08:34:43.725: INFO: (3) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 5.026888ms)
    Jan  5 08:34:43.725: INFO: (3) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 5.087056ms)
    Jan  5 08:34:43.726: INFO: (3) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 5.251065ms)
    Jan  5 08:34:43.726: INFO: (3) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 5.924811ms)
    Jan  5 08:34:43.726: INFO: (3) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 5.950542ms)
    Jan  5 08:34:43.726: INFO: (3) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 6.054646ms)
    Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 6.241231ms)
    Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 6.584211ms)
    Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 6.548875ms)
    Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 6.706906ms)
    Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 6.693224ms)
    Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 6.529275ms)
    Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 6.843049ms)
    Jan  5 08:34:43.727: INFO: (3) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 6.936903ms)
    Jan  5 08:34:43.729: INFO: (4) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 1.360937ms)
    Jan  5 08:34:43.730: INFO: (4) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.309756ms)
    Jan  5 08:34:43.730: INFO: (4) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.547172ms)
    Jan  5 08:34:43.731: INFO: (4) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.481878ms)
    Jan  5 08:34:43.731: INFO: (4) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 3.897631ms)
    Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.126133ms)
    Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.147112ms)
    Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 4.210798ms)
    Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.367863ms)
    Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.360497ms)
    Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.367825ms)
    Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.498163ms)
    Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.708914ms)
    Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 4.83802ms)
    Jan  5 08:34:43.732: INFO: (4) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.963699ms)
    Jan  5 08:34:43.733: INFO: (4) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 5.198617ms)
    Jan  5 08:34:43.735: INFO: (5) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.060607ms)
    Jan  5 08:34:43.735: INFO: (5) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.368881ms)
    Jan  5 08:34:43.735: INFO: (5) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.438014ms)
    Jan  5 08:34:43.735: INFO: (5) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.612288ms)
    Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.995188ms)
    Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 2.953448ms)
    Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.00129ms)
    Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.023702ms)
    Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 3.330558ms)
    Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.597708ms)
    Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.614996ms)
    Jan  5 08:34:43.736: INFO: (5) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 3.727791ms)
    Jan  5 08:34:43.737: INFO: (5) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 3.89393ms)
    Jan  5 08:34:43.737: INFO: (5) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.881476ms)
    Jan  5 08:34:43.737: INFO: (5) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.160207ms)
    Jan  5 08:34:43.737: INFO: (5) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.079498ms)
    Jan  5 08:34:43.739: INFO: (6) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.342394ms)
    Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.448059ms)
    Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.499866ms)
    Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.712578ms)
    Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.794371ms)
    Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 2.915516ms)
    Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.063313ms)
    Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 3.208584ms)
    Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 3.244888ms)
    Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.32026ms)
    Jan  5 08:34:43.740: INFO: (6) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.256142ms)
    Jan  5 08:34:43.741: INFO: (6) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 3.537658ms)
    Jan  5 08:34:43.741: INFO: (6) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 3.628668ms)
    Jan  5 08:34:43.741: INFO: (6) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 3.66179ms)
    Jan  5 08:34:43.741: INFO: (6) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.608817ms)
    Jan  5 08:34:43.741: INFO: (6) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.670639ms)
    Jan  5 08:34:43.744: INFO: (7) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.098568ms)
    Jan  5 08:34:43.744: INFO: (7) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.046941ms)
    Jan  5 08:34:43.744: INFO: (7) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.165061ms)
    Jan  5 08:34:43.744: INFO: (7) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.367998ms)
    Jan  5 08:34:43.744: INFO: (7) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.43268ms)
    Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.730599ms)
    Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 4.154857ms)
    Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.160708ms)
    Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.130868ms)
    Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.293435ms)
    Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 4.197573ms)
    Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 4.273546ms)
    Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.431392ms)
    Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.444135ms)
    Jan  5 08:34:43.745: INFO: (7) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.448936ms)
    Jan  5 08:34:43.746: INFO: (7) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.780002ms)
    Jan  5 08:34:43.748: INFO: (8) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 1.690602ms)
    Jan  5 08:34:43.748: INFO: (8) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 1.795182ms)
    Jan  5 08:34:43.748: INFO: (8) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 1.875462ms)
    Jan  5 08:34:43.748: INFO: (8) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.145102ms)
    Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 2.739372ms)
    Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.650354ms)
    Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 2.91299ms)
    Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.587564ms)
    Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 3.357013ms)
    Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 3.586354ms)
    Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.12728ms)
    Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.019269ms)
    Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.021066ms)
    Jan  5 08:34:43.749: INFO: (8) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.392944ms)
    Jan  5 08:34:43.750: INFO: (8) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 3.222336ms)
    Jan  5 08:34:43.750: INFO: (8) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.373329ms)
    Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 4.184966ms)
    Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.940183ms)
    Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 4.012094ms)
    Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.184066ms)
    Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.155366ms)
    Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.313285ms)
    Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.530717ms)
    Jan  5 08:34:43.754: INFO: (9) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 4.146091ms)
    Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.151101ms)
    Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.423277ms)
    Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.630049ms)
    Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.704684ms)
    Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.505141ms)
    Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.616299ms)
    Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.846152ms)
    Jan  5 08:34:43.755: INFO: (9) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 5.387388ms)
    Jan  5 08:34:43.757: INFO: (10) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 1.524642ms)
    Jan  5 08:34:43.762: INFO: (10) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 5.922733ms)
    Jan  5 08:34:43.762: INFO: (10) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 6.183073ms)
    Jan  5 08:34:43.764: INFO: (10) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 8.468334ms)
    Jan  5 08:34:43.764: INFO: (10) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 8.602385ms)
    Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 8.882644ms)
    Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 9.085682ms)
    Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 8.969021ms)
    Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 9.122384ms)
    Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 9.373739ms)
    Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 9.137015ms)
    Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 9.1267ms)
    Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 9.543156ms)
    Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 9.387268ms)
    Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 9.524267ms)
    Jan  5 08:34:43.765: INFO: (10) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 9.577728ms)
    Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.136562ms)
    Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.530705ms)
    Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.55504ms)
    Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.712107ms)
    Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 3.012638ms)
    Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 2.649115ms)
    Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.765271ms)
    Jan  5 08:34:43.768: INFO: (11) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 2.892191ms)
    Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 2.881276ms)
    Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.246492ms)
    Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.960448ms)
    Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 3.081323ms)
    Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.474997ms)
    Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.278444ms)
    Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 3.399798ms)
    Jan  5 08:34:43.769: INFO: (11) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.249911ms)
    Jan  5 08:34:43.771: INFO: (12) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.355995ms)
    Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.210022ms)
    Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.780323ms)
    Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.663524ms)
    Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.92289ms)
    Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 2.701752ms)
    Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.783102ms)
    Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.79504ms)
    Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.101201ms)
    Jan  5 08:34:43.772: INFO: (12) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.306286ms)
    Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.546727ms)
    Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.106606ms)
    Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.545354ms)
    Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.182372ms)
    Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.351091ms)
    Jan  5 08:34:43.773: INFO: (12) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.484898ms)
    Jan  5 08:34:43.777: INFO: (13) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 2.9418ms)
    Jan  5 08:34:43.777: INFO: (13) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.08824ms)
    Jan  5 08:34:43.777: INFO: (13) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.273903ms)
    Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 3.95503ms)
    Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.098917ms)
    Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 4.192231ms)
    Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 4.285386ms)
    Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.126296ms)
    Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.578287ms)
    Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 4.126449ms)
    Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 4.2031ms)
    Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.427922ms)
    Jan  5 08:34:43.778: INFO: (13) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.405008ms)
    Jan  5 08:34:43.779: INFO: (13) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 4.564996ms)
    Jan  5 08:34:43.779: INFO: (13) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.781379ms)
    Jan  5 08:34:43.779: INFO: (13) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 5.00754ms)
    Jan  5 08:34:43.781: INFO: (14) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.497135ms)
    Jan  5 08:34:43.781: INFO: (14) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 2.728823ms)
    Jan  5 08:34:43.781: INFO: (14) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 2.830194ms)
    Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.800596ms)
    Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.446198ms)
    Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.739069ms)
    Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 2.857611ms)
    Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.967991ms)
    Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.074944ms)
    Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 3.041044ms)
    Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.213985ms)
    Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.959065ms)
    Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.448532ms)
    Jan  5 08:34:43.782: INFO: (14) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.46749ms)
    Jan  5 08:34:43.783: INFO: (14) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 3.445388ms)
    Jan  5 08:34:43.783: INFO: (14) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 3.590265ms)
    Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 2.580966ms)
    Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.100181ms)
    Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 3.369893ms)
    Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.169628ms)
    Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.147937ms)
    Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 3.338575ms)
    Jan  5 08:34:43.786: INFO: (15) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 3.642192ms)
    Jan  5 08:34:43.787: INFO: (15) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.613756ms)
    Jan  5 08:34:43.787: INFO: (15) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.444215ms)
    Jan  5 08:34:43.787: INFO: (15) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.811157ms)
    Jan  5 08:34:43.787: INFO: (15) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.5175ms)
    Jan  5 08:34:43.787: INFO: (15) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.270538ms)
    Jan  5 08:34:43.788: INFO: (15) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 5.146902ms)
    Jan  5 08:34:43.788: INFO: (15) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 5.001752ms)
    Jan  5 08:34:43.788: INFO: (15) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 5.331498ms)
    Jan  5 08:34:43.788: INFO: (15) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 5.344471ms)
    Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.303738ms)
    Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.969564ms)
    Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 2.805827ms)
    Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 2.845712ms)
    Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.07792ms)
    Jan  5 08:34:43.791: INFO: (16) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.844725ms)
    Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.049022ms)
    Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 3.474562ms)
    Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 3.346599ms)
    Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.548544ms)
    Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.749752ms)
    Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 3.586145ms)
    Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 3.613977ms)
    Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 3.749122ms)
    Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 3.768124ms)
    Jan  5 08:34:43.792: INFO: (16) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.77022ms)
    Jan  5 08:34:43.795: INFO: (17) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 2.271681ms)
    Jan  5 08:34:43.795: INFO: (17) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.287232ms)
    Jan  5 08:34:43.795: INFO: (17) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 2.490763ms)
    Jan  5 08:34:43.796: INFO: (17) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 3.932612ms)
    Jan  5 08:34:43.796: INFO: (17) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.152969ms)
    Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.139331ms)
    Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 4.170202ms)
    Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.201008ms)
    Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.245192ms)
    Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.357911ms)
    Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 4.467882ms)
    Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.622756ms)
    Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.705782ms)
    Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.803435ms)
    Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.833333ms)
    Jan  5 08:34:43.797: INFO: (17) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.993014ms)
    Jan  5 08:34:43.801: INFO: (18) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.372058ms)
    Jan  5 08:34:43.801: INFO: (18) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.438867ms)
    Jan  5 08:34:43.801: INFO: (18) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.165918ms)
    Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.871049ms)
    Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 4.077543ms)
    Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 4.121704ms)
    Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 3.863683ms)
    Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.107374ms)
    Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.199361ms)
    Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.566071ms)
    Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.55103ms)
    Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.802576ms)
    Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 4.57219ms)
    Jan  5 08:34:43.802: INFO: (18) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 4.563919ms)
    Jan  5 08:34:43.803: INFO: (18) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.836433ms)
    Jan  5 08:34:43.803: INFO: (18) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 5.744787ms)
    Jan  5 08:34:43.805: INFO: (19) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:462/proxy/: tls qux (200; 1.539257ms)
    Jan  5 08:34:43.806: INFO: (19) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:443/proxy/tlsrewritem... (200; 2.783325ms)
    Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 2.79833ms)
    Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/https:proxy-service-k8fbh-vhktm:460/proxy/: tls baz (200; 3.007599ms)
    Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">test<... (200; 3.126402ms)
    Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 3.298044ms)
    Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm/proxy/rewriteme">test</a> (200; 3.375957ms)
    Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:160/proxy/: foo (200; 2.944863ms)
    Jan  5 08:34:43.807: INFO: (19) /api/v1/namespaces/proxy-9116/pods/proxy-service-k8fbh-vhktm:162/proxy/: bar (200; 3.126681ms)
    Jan  5 08:34:43.808: INFO: (19) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname2/proxy/: tls qux (200; 4.422253ms)
    Jan  5 08:34:43.808: INFO: (19) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname1/proxy/: foo (200; 4.677129ms)
    Jan  5 08:34:43.808: INFO: (19) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname2/proxy/: bar (200; 4.630095ms)
    Jan  5 08:34:43.808: INFO: (19) /api/v1/namespaces/proxy-9116/services/http:proxy-service-k8fbh:portname2/proxy/: bar (200; 4.542278ms)
    Jan  5 08:34:43.808: INFO: (19) /api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9116/pods/http:proxy-service-k8fbh-vhktm:1080/proxy/rewriteme">... (200; 4.894743ms)
    Jan  5 08:34:43.809: INFO: (19) /api/v1/namespaces/proxy-9116/services/https:proxy-service-k8fbh:tlsportname1/proxy/: tls baz (200; 4.682686ms)
    Jan  5 08:34:43.809: INFO: (19) /api/v1/namespaces/proxy-9116/services/proxy-service-k8fbh:portname1/proxy/: foo (200; 4.969653ms)
    STEP: deleting ReplicationController proxy-service-k8fbh in namespace proxy-9116, will wait for the garbage collector to delete the pods 01/05/23 08:34:43.809
    Jan  5 08:34:43.865: INFO: Deleting ReplicationController proxy-service-k8fbh took: 4.156602ms
    Jan  5 08:34:43.966: INFO: Terminating ReplicationController proxy-service-k8fbh pods took: 101.113389ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan  5 08:34:46.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-9116" for this suite. 01/05/23 08:34:46.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:34:46.576
Jan  5 08:34:46.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename events 01/05/23 08:34:46.577
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:46.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:46.596
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/05/23 08:34:46.598
STEP: listing all events in all namespaces 01/05/23 08:34:46.602
STEP: patching the test event 01/05/23 08:34:46.604
STEP: fetching the test event 01/05/23 08:34:46.615
STEP: updating the test event 01/05/23 08:34:46.623
STEP: getting the test event 01/05/23 08:34:46.628
STEP: deleting the test event 01/05/23 08:34:46.63
STEP: listing all events in all namespaces 01/05/23 08:34:46.638
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan  5 08:34:46.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4673" for this suite. 01/05/23 08:34:46.642
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":267,"skipped":5156,"failed":0}
------------------------------
â€¢ [0.084 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:34:46.576
    Jan  5 08:34:46.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename events 01/05/23 08:34:46.577
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:46.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:46.596
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/05/23 08:34:46.598
    STEP: listing all events in all namespaces 01/05/23 08:34:46.602
    STEP: patching the test event 01/05/23 08:34:46.604
    STEP: fetching the test event 01/05/23 08:34:46.615
    STEP: updating the test event 01/05/23 08:34:46.623
    STEP: getting the test event 01/05/23 08:34:46.628
    STEP: deleting the test event 01/05/23 08:34:46.63
    STEP: listing all events in all namespaces 01/05/23 08:34:46.638
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan  5 08:34:46.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4673" for this suite. 01/05/23 08:34:46.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:34:46.66
Jan  5 08:34:46.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:34:46.661
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:46.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:46.686
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:34:46.702
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:34:47.452
STEP: Deploying the webhook pod 01/05/23 08:34:47.457
STEP: Wait for the deployment to be ready 01/05/23 08:34:47.469
Jan  5 08:34:47.477: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 08:34:49.494
STEP: Verifying the service has paired with the endpoint 01/05/23 08:34:49.506
Jan  5 08:34:50.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/05/23 08:34:50.576
STEP: Creating a configMap that should be mutated 01/05/23 08:34:50.584
STEP: Deleting the collection of validation webhooks 01/05/23 08:34:50.816
STEP: Creating a configMap that should not be mutated 01/05/23 08:34:50.858
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:34:50.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-43" for this suite. 01/05/23 08:34:50.87
STEP: Destroying namespace "webhook-43-markers" for this suite. 01/05/23 08:34:50.878
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":268,"skipped":5169,"failed":0}
------------------------------
â€¢ [4.268 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:34:46.66
    Jan  5 08:34:46.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:34:46.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:46.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:46.686
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:34:46.702
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:34:47.452
    STEP: Deploying the webhook pod 01/05/23 08:34:47.457
    STEP: Wait for the deployment to be ready 01/05/23 08:34:47.469
    Jan  5 08:34:47.477: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 08:34:49.494
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:34:49.506
    Jan  5 08:34:50.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/05/23 08:34:50.576
    STEP: Creating a configMap that should be mutated 01/05/23 08:34:50.584
    STEP: Deleting the collection of validation webhooks 01/05/23 08:34:50.816
    STEP: Creating a configMap that should not be mutated 01/05/23 08:34:50.858
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:34:50.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-43" for this suite. 01/05/23 08:34:50.87
    STEP: Destroying namespace "webhook-43-markers" for this suite. 01/05/23 08:34:50.878
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:34:50.929
Jan  5 08:34:50.929: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 08:34:50.93
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:50.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:50.954
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/05/23 08:34:50.955
Jan  5 08:34:50.964: INFO: Waiting up to 5m0s for pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f" in namespace "downward-api-6633" to be "running and ready"
Jan  5 08:34:50.965: INFO: Pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.615365ms
Jan  5 08:34:50.965: INFO: The phase of Pod annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:34:52.969: INFO: Pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005020707s
Jan  5 08:34:52.969: INFO: The phase of Pod annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:34:54.969: INFO: Pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f": Phase="Running", Reason="", readiness=true. Elapsed: 4.005840905s
Jan  5 08:34:54.969: INFO: The phase of Pod annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f is Running (Ready = true)
Jan  5 08:34:54.969: INFO: Pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f" satisfied condition "running and ready"
Jan  5 08:34:55.486: INFO: Successfully updated pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 08:34:57.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6633" for this suite. 01/05/23 08:34:57.497
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":269,"skipped":5176,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.573 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:34:50.929
    Jan  5 08:34:50.929: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 08:34:50.93
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:50.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:50.954
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/05/23 08:34:50.955
    Jan  5 08:34:50.964: INFO: Waiting up to 5m0s for pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f" in namespace "downward-api-6633" to be "running and ready"
    Jan  5 08:34:50.965: INFO: Pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.615365ms
    Jan  5 08:34:50.965: INFO: The phase of Pod annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:34:52.969: INFO: Pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005020707s
    Jan  5 08:34:52.969: INFO: The phase of Pod annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:34:54.969: INFO: Pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f": Phase="Running", Reason="", readiness=true. Elapsed: 4.005840905s
    Jan  5 08:34:54.969: INFO: The phase of Pod annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f is Running (Ready = true)
    Jan  5 08:34:54.969: INFO: Pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f" satisfied condition "running and ready"
    Jan  5 08:34:55.486: INFO: Successfully updated pod "annotationupdate591ba689-1b9d-4514-84bd-7ec44e3bab8f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 08:34:57.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6633" for this suite. 01/05/23 08:34:57.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:34:57.503
Jan  5 08:34:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sysctl 01/05/23 08:34:57.504
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:57.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:57.53
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/05/23 08:34:57.533
STEP: Watching for error events or started pod 01/05/23 08:34:57.538
STEP: Waiting for pod completion 01/05/23 08:35:01.542
Jan  5 08:35:01.542: INFO: Waiting up to 3m0s for pod "sysctl-fdb932c0-1dc0-4b85-8e1f-126daba29d28" in namespace "sysctl-9720" to be "completed"
Jan  5 08:35:01.544: INFO: Pod "sysctl-fdb932c0-1dc0-4b85-8e1f-126daba29d28": Phase="Pending", Reason="", readiness=false. Elapsed: 1.926558ms
Jan  5 08:35:03.547: INFO: Pod "sysctl-fdb932c0-1dc0-4b85-8e1f-126daba29d28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00492832s
Jan  5 08:35:03.547: INFO: Pod "sysctl-fdb932c0-1dc0-4b85-8e1f-126daba29d28" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/05/23 08:35:03.549
STEP: Getting logs from the pod 01/05/23 08:35:03.549
STEP: Checking that the sysctl is actually updated 01/05/23 08:35:03.553
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 08:35:03.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9720" for this suite. 01/05/23 08:35:03.556
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":270,"skipped":5195,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.057 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:34:57.503
    Jan  5 08:34:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sysctl 01/05/23 08:34:57.504
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:34:57.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:34:57.53
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/05/23 08:34:57.533
    STEP: Watching for error events or started pod 01/05/23 08:34:57.538
    STEP: Waiting for pod completion 01/05/23 08:35:01.542
    Jan  5 08:35:01.542: INFO: Waiting up to 3m0s for pod "sysctl-fdb932c0-1dc0-4b85-8e1f-126daba29d28" in namespace "sysctl-9720" to be "completed"
    Jan  5 08:35:01.544: INFO: Pod "sysctl-fdb932c0-1dc0-4b85-8e1f-126daba29d28": Phase="Pending", Reason="", readiness=false. Elapsed: 1.926558ms
    Jan  5 08:35:03.547: INFO: Pod "sysctl-fdb932c0-1dc0-4b85-8e1f-126daba29d28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00492832s
    Jan  5 08:35:03.547: INFO: Pod "sysctl-fdb932c0-1dc0-4b85-8e1f-126daba29d28" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/05/23 08:35:03.549
    STEP: Getting logs from the pod 01/05/23 08:35:03.549
    STEP: Checking that the sysctl is actually updated 01/05/23 08:35:03.553
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 08:35:03.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-9720" for this suite. 01/05/23 08:35:03.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:35:03.561
Jan  5 08:35:03.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename containers 01/05/23 08:35:03.562
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:35:03.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:35:03.583
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/05/23 08:35:03.584
Jan  5 08:35:03.590: INFO: Waiting up to 5m0s for pod "client-containers-525532f5-976a-492b-9271-6a10816fba0c" in namespace "containers-9419" to be "Succeeded or Failed"
Jan  5 08:35:03.592: INFO: Pod "client-containers-525532f5-976a-492b-9271-6a10816fba0c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.752473ms
Jan  5 08:35:05.594: INFO: Pod "client-containers-525532f5-976a-492b-9271-6a10816fba0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004359436s
Jan  5 08:35:07.595: INFO: Pod "client-containers-525532f5-976a-492b-9271-6a10816fba0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005070193s
STEP: Saw pod success 01/05/23 08:35:07.595
Jan  5 08:35:07.595: INFO: Pod "client-containers-525532f5-976a-492b-9271-6a10816fba0c" satisfied condition "Succeeded or Failed"
Jan  5 08:35:07.597: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod client-containers-525532f5-976a-492b-9271-6a10816fba0c container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:35:07.6
Jan  5 08:35:07.616: INFO: Waiting for pod client-containers-525532f5-976a-492b-9271-6a10816fba0c to disappear
Jan  5 08:35:07.617: INFO: Pod client-containers-525532f5-976a-492b-9271-6a10816fba0c no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  5 08:35:07.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9419" for this suite. 01/05/23 08:35:07.619
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":271,"skipped":5204,"failed":0}
------------------------------
â€¢ [4.069 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:35:03.561
    Jan  5 08:35:03.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename containers 01/05/23 08:35:03.562
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:35:03.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:35:03.583
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/05/23 08:35:03.584
    Jan  5 08:35:03.590: INFO: Waiting up to 5m0s for pod "client-containers-525532f5-976a-492b-9271-6a10816fba0c" in namespace "containers-9419" to be "Succeeded or Failed"
    Jan  5 08:35:03.592: INFO: Pod "client-containers-525532f5-976a-492b-9271-6a10816fba0c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.752473ms
    Jan  5 08:35:05.594: INFO: Pod "client-containers-525532f5-976a-492b-9271-6a10816fba0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004359436s
    Jan  5 08:35:07.595: INFO: Pod "client-containers-525532f5-976a-492b-9271-6a10816fba0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005070193s
    STEP: Saw pod success 01/05/23 08:35:07.595
    Jan  5 08:35:07.595: INFO: Pod "client-containers-525532f5-976a-492b-9271-6a10816fba0c" satisfied condition "Succeeded or Failed"
    Jan  5 08:35:07.597: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod client-containers-525532f5-976a-492b-9271-6a10816fba0c container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:35:07.6
    Jan  5 08:35:07.616: INFO: Waiting for pod client-containers-525532f5-976a-492b-9271-6a10816fba0c to disappear
    Jan  5 08:35:07.617: INFO: Pod client-containers-525532f5-976a-492b-9271-6a10816fba0c no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  5 08:35:07.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9419" for this suite. 01/05/23 08:35:07.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:35:07.63
Jan  5 08:35:07.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename kubectl 01/05/23 08:35:07.631
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:35:07.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:35:07.649
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/05/23 08:35:07.651
Jan  5 08:35:07.652: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5098 proxy --unix-socket=/tmp/kubectl-proxy-unix1226645612/test'
STEP: retrieving proxy /api/ output 01/05/23 08:35:07.703
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  5 08:35:07.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5098" for this suite. 01/05/23 08:35:07.707
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":272,"skipped":5217,"failed":0}
------------------------------
â€¢ [0.081 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:35:07.63
    Jan  5 08:35:07.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename kubectl 01/05/23 08:35:07.631
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:35:07.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:35:07.649
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/05/23 08:35:07.651
    Jan  5 08:35:07.652: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=kubectl-5098 proxy --unix-socket=/tmp/kubectl-proxy-unix1226645612/test'
    STEP: retrieving proxy /api/ output 01/05/23 08:35:07.703
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  5 08:35:07.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5098" for this suite. 01/05/23 08:35:07.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:35:07.712
Jan  5 08:35:07.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 08:35:07.713
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:35:07.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:35:07.731
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-be5e9df5-18df-47ae-86f1-e87d2b94a0f5 01/05/23 08:35:07.733
STEP: Creating a pod to test consume secrets 01/05/23 08:35:07.738
Jan  5 08:35:07.761: INFO: Waiting up to 5m0s for pod "pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93" in namespace "secrets-2925" to be "Succeeded or Failed"
Jan  5 08:35:07.763: INFO: Pod "pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93": Phase="Pending", Reason="", readiness=false. Elapsed: 1.451888ms
Jan  5 08:35:09.765: INFO: Pod "pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003965148s
Jan  5 08:35:11.766: INFO: Pod "pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005040104s
STEP: Saw pod success 01/05/23 08:35:11.766
Jan  5 08:35:11.766: INFO: Pod "pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93" satisfied condition "Succeeded or Failed"
Jan  5 08:35:11.768: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 08:35:11.772
Jan  5 08:35:11.782: INFO: Waiting for pod pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93 to disappear
Jan  5 08:35:11.784: INFO: Pod pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 08:35:11.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2925" for this suite. 01/05/23 08:35:11.786
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":273,"skipped":5248,"failed":0}
------------------------------
â€¢ [4.081 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:35:07.712
    Jan  5 08:35:07.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 08:35:07.713
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:35:07.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:35:07.731
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-be5e9df5-18df-47ae-86f1-e87d2b94a0f5 01/05/23 08:35:07.733
    STEP: Creating a pod to test consume secrets 01/05/23 08:35:07.738
    Jan  5 08:35:07.761: INFO: Waiting up to 5m0s for pod "pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93" in namespace "secrets-2925" to be "Succeeded or Failed"
    Jan  5 08:35:07.763: INFO: Pod "pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93": Phase="Pending", Reason="", readiness=false. Elapsed: 1.451888ms
    Jan  5 08:35:09.765: INFO: Pod "pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003965148s
    Jan  5 08:35:11.766: INFO: Pod "pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005040104s
    STEP: Saw pod success 01/05/23 08:35:11.766
    Jan  5 08:35:11.766: INFO: Pod "pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93" satisfied condition "Succeeded or Failed"
    Jan  5 08:35:11.768: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 08:35:11.772
    Jan  5 08:35:11.782: INFO: Waiting for pod pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93 to disappear
    Jan  5 08:35:11.784: INFO: Pod pod-secrets-f0e365e1-3eda-4216-b88a-c5326ddb7e93 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 08:35:11.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2925" for this suite. 01/05/23 08:35:11.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:35:11.794
Jan  5 08:35:11.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:35:11.794
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:35:11.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:35:11.809
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:35:11.821
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:35:12.224
STEP: Deploying the webhook pod 01/05/23 08:35:12.228
STEP: Wait for the deployment to be ready 01/05/23 08:35:12.239
Jan  5 08:35:12.246: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  5 08:35:14.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 35, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 35, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 35, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 35, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/05/23 08:35:16.254
STEP: Verifying the service has paired with the endpoint 01/05/23 08:35:16.265
Jan  5 08:35:17.266: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 08:35:17.268
STEP: create a pod 01/05/23 08:35:17.28
Jan  5 08:35:17.293: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6103" to be "running"
Jan  5 08:35:17.295: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.766748ms
Jan  5 08:35:19.300: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006961316s
Jan  5 08:35:21.299: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005405277s
Jan  5 08:35:21.299: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/05/23 08:35:21.299
Jan  5 08:35:21.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=webhook-6103 attach --namespace=webhook-6103 to-be-attached-pod -i -c=container1'
Jan  5 08:35:21.375: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:35:21.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6103" for this suite. 01/05/23 08:35:21.382
STEP: Destroying namespace "webhook-6103-markers" for this suite. 01/05/23 08:35:21.39
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":274,"skipped":5274,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.653 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:35:11.794
    Jan  5 08:35:11.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:35:11.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:35:11.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:35:11.809
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:35:11.821
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:35:12.224
    STEP: Deploying the webhook pod 01/05/23 08:35:12.228
    STEP: Wait for the deployment to be ready 01/05/23 08:35:12.239
    Jan  5 08:35:12.246: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  5 08:35:14.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 35, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 35, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 35, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 35, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/05/23 08:35:16.254
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:35:16.265
    Jan  5 08:35:17.266: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 08:35:17.268
    STEP: create a pod 01/05/23 08:35:17.28
    Jan  5 08:35:17.293: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6103" to be "running"
    Jan  5 08:35:17.295: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.766748ms
    Jan  5 08:35:19.300: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006961316s
    Jan  5 08:35:21.299: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005405277s
    Jan  5 08:35:21.299: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/05/23 08:35:21.299
    Jan  5 08:35:21.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=webhook-6103 attach --namespace=webhook-6103 to-be-attached-pod -i -c=container1'
    Jan  5 08:35:21.375: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:35:21.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6103" for this suite. 01/05/23 08:35:21.382
    STEP: Destroying namespace "webhook-6103-markers" for this suite. 01/05/23 08:35:21.39
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:35:21.446
Jan  5 08:35:21.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-probe 01/05/23 08:35:21.447
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:35:21.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:35:21.466
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 in namespace container-probe-5157 01/05/23 08:35:21.468
Jan  5 08:35:21.477: INFO: Waiting up to 5m0s for pod "liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3" in namespace "container-probe-5157" to be "not pending"
Jan  5 08:35:21.479: INFO: Pod "liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.875551ms
Jan  5 08:35:23.483: INFO: Pod "liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005151442s
Jan  5 08:35:25.483: INFO: Pod "liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.005563654s
Jan  5 08:35:25.483: INFO: Pod "liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3" satisfied condition "not pending"
Jan  5 08:35:25.483: INFO: Started pod liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 in namespace container-probe-5157
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:35:25.483
Jan  5 08:35:25.485: INFO: Initial restart count of pod liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is 0
Jan  5 08:35:43.522: INFO: Restart count of pod container-probe-5157/liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is now 1 (18.037029981s elapsed)
Jan  5 08:36:05.572: INFO: Restart count of pod container-probe-5157/liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is now 2 (40.087111093s elapsed)
Jan  5 08:36:25.606: INFO: Restart count of pod container-probe-5157/liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is now 3 (1m0.120124741s elapsed)
Jan  5 08:36:45.640: INFO: Restart count of pod container-probe-5157/liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is now 4 (1m20.154274074s elapsed)
Jan  5 08:37:57.776: INFO: Restart count of pod container-probe-5157/liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is now 5 (2m32.290538941s elapsed)
STEP: deleting the pod 01/05/23 08:37:57.776
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 08:37:57.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5157" for this suite. 01/05/23 08:37:57.789
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":275,"skipped":5276,"failed":0}
------------------------------
â€¢ [SLOW TEST] [156.348 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:35:21.446
    Jan  5 08:35:21.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-probe 01/05/23 08:35:21.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:35:21.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:35:21.466
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 in namespace container-probe-5157 01/05/23 08:35:21.468
    Jan  5 08:35:21.477: INFO: Waiting up to 5m0s for pod "liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3" in namespace "container-probe-5157" to be "not pending"
    Jan  5 08:35:21.479: INFO: Pod "liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.875551ms
    Jan  5 08:35:23.483: INFO: Pod "liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005151442s
    Jan  5 08:35:25.483: INFO: Pod "liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.005563654s
    Jan  5 08:35:25.483: INFO: Pod "liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3" satisfied condition "not pending"
    Jan  5 08:35:25.483: INFO: Started pod liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 in namespace container-probe-5157
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:35:25.483
    Jan  5 08:35:25.485: INFO: Initial restart count of pod liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is 0
    Jan  5 08:35:43.522: INFO: Restart count of pod container-probe-5157/liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is now 1 (18.037029981s elapsed)
    Jan  5 08:36:05.572: INFO: Restart count of pod container-probe-5157/liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is now 2 (40.087111093s elapsed)
    Jan  5 08:36:25.606: INFO: Restart count of pod container-probe-5157/liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is now 3 (1m0.120124741s elapsed)
    Jan  5 08:36:45.640: INFO: Restart count of pod container-probe-5157/liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is now 4 (1m20.154274074s elapsed)
    Jan  5 08:37:57.776: INFO: Restart count of pod container-probe-5157/liveness-072af069-ce14-4dd2-ae11-fdc04e73b0d3 is now 5 (2m32.290538941s elapsed)
    STEP: deleting the pod 01/05/23 08:37:57.776
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 08:37:57.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5157" for this suite. 01/05/23 08:37:57.789
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:37:57.794
Jan  5 08:37:57.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:37:57.795
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:37:57.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:37:57.81
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/05/23 08:37:57.814
Jan  5 08:37:57.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61" in namespace "projected-4875" to be "Succeeded or Failed"
Jan  5 08:37:57.823: INFO: Pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.275055ms
Jan  5 08:37:59.826: INFO: Pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005798853s
Jan  5 08:38:01.827: INFO: Pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006562805s
Jan  5 08:38:03.826: INFO: Pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005764874s
STEP: Saw pod success 01/05/23 08:38:03.826
Jan  5 08:38:03.826: INFO: Pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61" satisfied condition "Succeeded or Failed"
Jan  5 08:38:03.828: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61 container client-container: <nil>
STEP: delete the pod 01/05/23 08:38:03.842
Jan  5 08:38:03.877: INFO: Waiting for pod downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61 to disappear
Jan  5 08:38:03.879: INFO: Pod downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 08:38:03.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4875" for this suite. 01/05/23 08:38:03.881
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":276,"skipped":5277,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.092 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:37:57.794
    Jan  5 08:37:57.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:37:57.795
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:37:57.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:37:57.81
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/05/23 08:37:57.814
    Jan  5 08:37:57.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61" in namespace "projected-4875" to be "Succeeded or Failed"
    Jan  5 08:37:57.823: INFO: Pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.275055ms
    Jan  5 08:37:59.826: INFO: Pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005798853s
    Jan  5 08:38:01.827: INFO: Pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006562805s
    Jan  5 08:38:03.826: INFO: Pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005764874s
    STEP: Saw pod success 01/05/23 08:38:03.826
    Jan  5 08:38:03.826: INFO: Pod "downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61" satisfied condition "Succeeded or Failed"
    Jan  5 08:38:03.828: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61 container client-container: <nil>
    STEP: delete the pod 01/05/23 08:38:03.842
    Jan  5 08:38:03.877: INFO: Waiting for pod downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61 to disappear
    Jan  5 08:38:03.879: INFO: Pod downwardapi-volume-f8a81c93-5108-4090-85e3-4cbe26309a61 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 08:38:03.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4875" for this suite. 01/05/23 08:38:03.881
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:38:03.887
Jan  5 08:38:03.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sched-preemption 01/05/23 08:38:03.888
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:38:03.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:38:03.908
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  5 08:38:03.925: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 08:39:03.939: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/05/23 08:39:03.941
Jan  5 08:39:03.962: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  5 08:39:03.972: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  5 08:39:04.006: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  5 08:39:04.021: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/05/23 08:39:04.021
Jan  5 08:39:04.021: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5097" to be "running"
Jan  5 08:39:04.030: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.231036ms
Jan  5 08:39:06.034: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012548041s
Jan  5 08:39:06.034: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  5 08:39:06.034: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5097" to be "running"
Jan  5 08:39:06.035: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.505388ms
Jan  5 08:39:08.039: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005194834s
Jan  5 08:39:08.039: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 08:39:08.039: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5097" to be "running"
Jan  5 08:39:08.041: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.244994ms
Jan  5 08:39:08.041: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 08:39:08.041: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5097" to be "running"
Jan  5 08:39:08.044: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.213235ms
Jan  5 08:39:08.044: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/05/23 08:39:08.044
Jan  5 08:39:08.050: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan  5 08:39:08.052: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.347885ms
Jan  5 08:39:10.054: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003492598s
Jan  5 08:39:12.056: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006005201s
Jan  5 08:39:12.056: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:39:12.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5097" for this suite. 01/05/23 08:39:12.086
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":277,"skipped":5280,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.274 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:38:03.887
    Jan  5 08:38:03.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 08:38:03.888
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:38:03.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:38:03.908
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  5 08:38:03.925: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 08:39:03.939: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/05/23 08:39:03.941
    Jan  5 08:39:03.962: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  5 08:39:03.972: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  5 08:39:04.006: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  5 08:39:04.021: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/05/23 08:39:04.021
    Jan  5 08:39:04.021: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5097" to be "running"
    Jan  5 08:39:04.030: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.231036ms
    Jan  5 08:39:06.034: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012548041s
    Jan  5 08:39:06.034: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  5 08:39:06.034: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5097" to be "running"
    Jan  5 08:39:06.035: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.505388ms
    Jan  5 08:39:08.039: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005194834s
    Jan  5 08:39:08.039: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 08:39:08.039: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5097" to be "running"
    Jan  5 08:39:08.041: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.244994ms
    Jan  5 08:39:08.041: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 08:39:08.041: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5097" to be "running"
    Jan  5 08:39:08.044: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.213235ms
    Jan  5 08:39:08.044: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/05/23 08:39:08.044
    Jan  5 08:39:08.050: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan  5 08:39:08.052: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.347885ms
    Jan  5 08:39:10.054: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003492598s
    Jan  5 08:39:12.056: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006005201s
    Jan  5 08:39:12.056: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:39:12.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5097" for this suite. 01/05/23 08:39:12.086
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:39:12.165
Jan  5 08:39:12.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 08:39:12.165
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:12.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:12.197
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/05/23 08:39:12.203
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/05/23 08:39:12.209
STEP: patching the secret 01/05/23 08:39:12.21
STEP: deleting the secret using a LabelSelector 01/05/23 08:39:12.224
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/05/23 08:39:12.228
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  5 08:39:12.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9888" for this suite. 01/05/23 08:39:12.234
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":278,"skipped":5315,"failed":0}
------------------------------
â€¢ [0.081 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:39:12.165
    Jan  5 08:39:12.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 08:39:12.165
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:12.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:12.197
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/05/23 08:39:12.203
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/05/23 08:39:12.209
    STEP: patching the secret 01/05/23 08:39:12.21
    STEP: deleting the secret using a LabelSelector 01/05/23 08:39:12.224
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/05/23 08:39:12.228
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 08:39:12.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9888" for this suite. 01/05/23 08:39:12.234
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:39:12.245
Jan  5 08:39:12.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename statefulset 01/05/23 08:39:12.246
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:12.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:12.279
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-684 01/05/23 08:39:12.282
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jan  5 08:39:12.306: INFO: Found 0 stateful pods, waiting for 1
Jan  5 08:39:22.310: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/05/23 08:39:22.314
W0105 08:39:22.319258      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  5 08:39:22.322: INFO: Found 1 stateful pods, waiting for 2
Jan  5 08:39:32.328: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 08:39:32.328: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/05/23 08:39:32.332
STEP: Delete all of the StatefulSets 01/05/23 08:39:32.334
STEP: Verify that StatefulSets have been deleted 01/05/23 08:39:32.339
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 08:39:32.342: INFO: Deleting all statefulset in ns statefulset-684
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 08:39:32.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-684" for this suite. 01/05/23 08:39:32.351
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":279,"skipped":5318,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.122 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:39:12.245
    Jan  5 08:39:12.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename statefulset 01/05/23 08:39:12.246
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:12.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:12.279
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-684 01/05/23 08:39:12.282
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jan  5 08:39:12.306: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 08:39:22.310: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/05/23 08:39:22.314
    W0105 08:39:22.319258      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  5 08:39:22.322: INFO: Found 1 stateful pods, waiting for 2
    Jan  5 08:39:32.328: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 08:39:32.328: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/05/23 08:39:32.332
    STEP: Delete all of the StatefulSets 01/05/23 08:39:32.334
    STEP: Verify that StatefulSets have been deleted 01/05/23 08:39:32.339
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 08:39:32.342: INFO: Deleting all statefulset in ns statefulset-684
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 08:39:32.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-684" for this suite. 01/05/23 08:39:32.351
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:39:32.368
Jan  5 08:39:32.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 08:39:32.369
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:32.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:32.397
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 08:39:32.398
Jan  5 08:39:32.404: INFO: Waiting up to 5m0s for pod "pod-5328ffa9-2912-470d-bc60-f2e478c23ed1" in namespace "emptydir-5777" to be "Succeeded or Failed"
Jan  5 08:39:32.406: INFO: Pod "pod-5328ffa9-2912-470d-bc60-f2e478c23ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.302789ms
Jan  5 08:39:34.411: INFO: Pod "pod-5328ffa9-2912-470d-bc60-f2e478c23ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007030316s
Jan  5 08:39:36.411: INFO: Pod "pod-5328ffa9-2912-470d-bc60-f2e478c23ed1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006674589s
STEP: Saw pod success 01/05/23 08:39:36.411
Jan  5 08:39:36.411: INFO: Pod "pod-5328ffa9-2912-470d-bc60-f2e478c23ed1" satisfied condition "Succeeded or Failed"
Jan  5 08:39:36.413: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-5328ffa9-2912-470d-bc60-f2e478c23ed1 container test-container: <nil>
STEP: delete the pod 01/05/23 08:39:36.424
Jan  5 08:39:36.440: INFO: Waiting for pod pod-5328ffa9-2912-470d-bc60-f2e478c23ed1 to disappear
Jan  5 08:39:36.448: INFO: Pod pod-5328ffa9-2912-470d-bc60-f2e478c23ed1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 08:39:36.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5777" for this suite. 01/05/23 08:39:36.45
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":280,"skipped":5321,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:39:32.368
    Jan  5 08:39:32.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 08:39:32.369
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:32.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:32.397
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 08:39:32.398
    Jan  5 08:39:32.404: INFO: Waiting up to 5m0s for pod "pod-5328ffa9-2912-470d-bc60-f2e478c23ed1" in namespace "emptydir-5777" to be "Succeeded or Failed"
    Jan  5 08:39:32.406: INFO: Pod "pod-5328ffa9-2912-470d-bc60-f2e478c23ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.302789ms
    Jan  5 08:39:34.411: INFO: Pod "pod-5328ffa9-2912-470d-bc60-f2e478c23ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007030316s
    Jan  5 08:39:36.411: INFO: Pod "pod-5328ffa9-2912-470d-bc60-f2e478c23ed1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006674589s
    STEP: Saw pod success 01/05/23 08:39:36.411
    Jan  5 08:39:36.411: INFO: Pod "pod-5328ffa9-2912-470d-bc60-f2e478c23ed1" satisfied condition "Succeeded or Failed"
    Jan  5 08:39:36.413: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-5328ffa9-2912-470d-bc60-f2e478c23ed1 container test-container: <nil>
    STEP: delete the pod 01/05/23 08:39:36.424
    Jan  5 08:39:36.440: INFO: Waiting for pod pod-5328ffa9-2912-470d-bc60-f2e478c23ed1 to disappear
    Jan  5 08:39:36.448: INFO: Pod pod-5328ffa9-2912-470d-bc60-f2e478c23ed1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 08:39:36.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5777" for this suite. 01/05/23 08:39:36.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:39:36.456
Jan  5 08:39:36.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 08:39:36.456
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:36.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:36.472
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/05/23 08:39:36.477
Jan  5 08:39:36.485: INFO: Waiting up to 5m0s for pod "downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f" in namespace "downward-api-8369" to be "Succeeded or Failed"
Jan  5 08:39:36.499: INFO: Pod "downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.105682ms
Jan  5 08:39:38.501: INFO: Pod "downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016660214s
Jan  5 08:39:40.503: INFO: Pod "downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018119867s
STEP: Saw pod success 01/05/23 08:39:40.503
Jan  5 08:39:40.503: INFO: Pod "downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f" satisfied condition "Succeeded or Failed"
Jan  5 08:39:40.505: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f container dapi-container: <nil>
STEP: delete the pod 01/05/23 08:39:40.509
Jan  5 08:39:40.541: INFO: Waiting for pod downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f to disappear
Jan  5 08:39:40.543: INFO: Pod downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  5 08:39:40.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8369" for this suite. 01/05/23 08:39:40.545
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":281,"skipped":5351,"failed":0}
------------------------------
â€¢ [4.092 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:39:36.456
    Jan  5 08:39:36.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 08:39:36.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:36.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:36.472
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/05/23 08:39:36.477
    Jan  5 08:39:36.485: INFO: Waiting up to 5m0s for pod "downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f" in namespace "downward-api-8369" to be "Succeeded or Failed"
    Jan  5 08:39:36.499: INFO: Pod "downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.105682ms
    Jan  5 08:39:38.501: INFO: Pod "downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016660214s
    Jan  5 08:39:40.503: INFO: Pod "downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018119867s
    STEP: Saw pod success 01/05/23 08:39:40.503
    Jan  5 08:39:40.503: INFO: Pod "downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f" satisfied condition "Succeeded or Failed"
    Jan  5 08:39:40.505: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f container dapi-container: <nil>
    STEP: delete the pod 01/05/23 08:39:40.509
    Jan  5 08:39:40.541: INFO: Waiting for pod downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f to disappear
    Jan  5 08:39:40.543: INFO: Pod downward-api-542e1798-3dbd-4d0a-a219-9e8abf251d7f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  5 08:39:40.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8369" for this suite. 01/05/23 08:39:40.545
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:39:40.548
Jan  5 08:39:40.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename dns 01/05/23 08:39:40.549
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:40.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:40.595
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/05/23 08:39:40.598
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/05/23 08:39:40.598
STEP: creating a pod to probe DNS 01/05/23 08:39:40.598
STEP: submitting the pod to kubernetes 01/05/23 08:39:40.598
Jan  5 08:39:40.604: INFO: Waiting up to 15m0s for pod "dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098" in namespace "dns-5095" to be "running"
Jan  5 08:39:40.607: INFO: Pod "dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.656458ms
Jan  5 08:39:42.611: INFO: Pod "dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006497714s
Jan  5 08:39:44.610: INFO: Pod "dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098": Phase="Running", Reason="", readiness=true. Elapsed: 4.005562325s
Jan  5 08:39:44.610: INFO: Pod "dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098" satisfied condition "running"
STEP: retrieving the pod 01/05/23 08:39:44.61
STEP: looking for the results for each expected name from probers 01/05/23 08:39:44.611
Jan  5 08:39:44.619: INFO: DNS probes using dns-5095/dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098 succeeded

STEP: deleting the pod 01/05/23 08:39:44.619
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 08:39:44.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5095" for this suite. 01/05/23 08:39:44.648
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":282,"skipped":5351,"failed":0}
------------------------------
â€¢ [4.110 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:39:40.548
    Jan  5 08:39:40.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename dns 01/05/23 08:39:40.549
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:40.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:40.595
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/05/23 08:39:40.598
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/05/23 08:39:40.598
    STEP: creating a pod to probe DNS 01/05/23 08:39:40.598
    STEP: submitting the pod to kubernetes 01/05/23 08:39:40.598
    Jan  5 08:39:40.604: INFO: Waiting up to 15m0s for pod "dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098" in namespace "dns-5095" to be "running"
    Jan  5 08:39:40.607: INFO: Pod "dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.656458ms
    Jan  5 08:39:42.611: INFO: Pod "dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006497714s
    Jan  5 08:39:44.610: INFO: Pod "dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098": Phase="Running", Reason="", readiness=true. Elapsed: 4.005562325s
    Jan  5 08:39:44.610: INFO: Pod "dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 08:39:44.61
    STEP: looking for the results for each expected name from probers 01/05/23 08:39:44.611
    Jan  5 08:39:44.619: INFO: DNS probes using dns-5095/dns-test-5a6cccbf-7ffc-48db-a7e2-0057a1726098 succeeded

    STEP: deleting the pod 01/05/23 08:39:44.619
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 08:39:44.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5095" for this suite. 01/05/23 08:39:44.648
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:39:44.659
Jan  5 08:39:44.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename podtemplate 01/05/23 08:39:44.66
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:44.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:44.678
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan  5 08:39:44.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3370" for this suite. 01/05/23 08:39:44.719
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":283,"skipped":5351,"failed":0}
------------------------------
â€¢ [0.064 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:39:44.659
    Jan  5 08:39:44.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename podtemplate 01/05/23 08:39:44.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:44.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:44.678
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan  5 08:39:44.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3370" for this suite. 01/05/23 08:39:44.719
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:39:44.724
Jan  5 08:39:44.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-probe 01/05/23 08:39:44.725
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:44.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:44.741
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a in namespace container-probe-845 01/05/23 08:39:44.756
Jan  5 08:39:44.760: INFO: Waiting up to 5m0s for pod "busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a" in namespace "container-probe-845" to be "not pending"
Jan  5 08:39:44.761: INFO: Pod "busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.476147ms
Jan  5 08:39:46.765: INFO: Pod "busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004965193s
Jan  5 08:39:48.765: INFO: Pod "busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a": Phase="Running", Reason="", readiness=true. Elapsed: 4.004766388s
Jan  5 08:39:48.765: INFO: Pod "busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a" satisfied condition "not pending"
Jan  5 08:39:48.765: INFO: Started pod busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a in namespace container-probe-845
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:39:48.765
Jan  5 08:39:48.767: INFO: Initial restart count of pod busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a is 0
STEP: deleting the pod 01/05/23 08:43:49.243
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 08:43:49.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-845" for this suite. 01/05/23 08:43:49.271
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":284,"skipped":5352,"failed":0}
------------------------------
â€¢ [SLOW TEST] [244.551 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:39:44.724
    Jan  5 08:39:44.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-probe 01/05/23 08:39:44.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:39:44.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:39:44.741
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a in namespace container-probe-845 01/05/23 08:39:44.756
    Jan  5 08:39:44.760: INFO: Waiting up to 5m0s for pod "busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a" in namespace "container-probe-845" to be "not pending"
    Jan  5 08:39:44.761: INFO: Pod "busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.476147ms
    Jan  5 08:39:46.765: INFO: Pod "busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004965193s
    Jan  5 08:39:48.765: INFO: Pod "busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a": Phase="Running", Reason="", readiness=true. Elapsed: 4.004766388s
    Jan  5 08:39:48.765: INFO: Pod "busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a" satisfied condition "not pending"
    Jan  5 08:39:48.765: INFO: Started pod busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a in namespace container-probe-845
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:39:48.765
    Jan  5 08:39:48.767: INFO: Initial restart count of pod busybox-7811ce3d-e124-4b78-a3c8-e91d71add87a is 0
    STEP: deleting the pod 01/05/23 08:43:49.243
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 08:43:49.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-845" for this suite. 01/05/23 08:43:49.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:43:49.275
Jan  5 08:43:49.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:43:49.276
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:43:49.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:43:49.295
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-be86904a-5bdc-41be-b8b5-6c36ba88f854 01/05/23 08:43:49.298
STEP: Creating a pod to test consume secrets 01/05/23 08:43:49.327
Jan  5 08:43:49.348: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b" in namespace "projected-4641" to be "Succeeded or Failed"
Jan  5 08:43:49.350: INFO: Pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.764179ms
Jan  5 08:43:51.354: INFO: Pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006281241s
Jan  5 08:43:53.355: INFO: Pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007048159s
Jan  5 08:43:55.353: INFO: Pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005516697s
STEP: Saw pod success 01/05/23 08:43:55.353
Jan  5 08:43:55.354: INFO: Pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b" satisfied condition "Succeeded or Failed"
Jan  5 08:43:55.355: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 08:43:55.368
Jan  5 08:43:55.381: INFO: Waiting for pod pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b to disappear
Jan  5 08:43:55.382: INFO: Pod pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 08:43:55.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4641" for this suite. 01/05/23 08:43:55.384
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":285,"skipped":5373,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.119 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:43:49.275
    Jan  5 08:43:49.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:43:49.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:43:49.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:43:49.295
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-be86904a-5bdc-41be-b8b5-6c36ba88f854 01/05/23 08:43:49.298
    STEP: Creating a pod to test consume secrets 01/05/23 08:43:49.327
    Jan  5 08:43:49.348: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b" in namespace "projected-4641" to be "Succeeded or Failed"
    Jan  5 08:43:49.350: INFO: Pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.764179ms
    Jan  5 08:43:51.354: INFO: Pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006281241s
    Jan  5 08:43:53.355: INFO: Pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007048159s
    Jan  5 08:43:55.353: INFO: Pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005516697s
    STEP: Saw pod success 01/05/23 08:43:55.353
    Jan  5 08:43:55.354: INFO: Pod "pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b" satisfied condition "Succeeded or Failed"
    Jan  5 08:43:55.355: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 08:43:55.368
    Jan  5 08:43:55.381: INFO: Waiting for pod pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b to disappear
    Jan  5 08:43:55.382: INFO: Pod pod-projected-secrets-fcf97270-8a55-4e9f-8adc-1726cdcb861b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 08:43:55.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4641" for this suite. 01/05/23 08:43:55.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:43:55.396
Jan  5 08:43:55.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename replicaset 01/05/23 08:43:55.397
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:43:55.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:43:55.424
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/05/23 08:43:55.427
STEP: Verify that the required pods have come up. 01/05/23 08:43:55.438
Jan  5 08:43:55.441: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 08:44:00.447: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 08:44:00.447
STEP: Getting /status 01/05/23 08:44:00.447
Jan  5 08:44:00.474: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/05/23 08:44:00.474
Jan  5 08:44:00.484: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/05/23 08:44:00.484
Jan  5 08:44:00.485: INFO: Observed &ReplicaSet event: ADDED
Jan  5 08:44:00.485: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 08:44:00.486: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 08:44:00.486: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 08:44:00.486: INFO: Found replicaset test-rs in namespace replicaset-2345 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 08:44:00.486: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/05/23 08:44:00.486
Jan  5 08:44:00.486: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  5 08:44:00.496: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/05/23 08:44:00.496
Jan  5 08:44:00.498: INFO: Observed &ReplicaSet event: ADDED
Jan  5 08:44:00.498: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 08:44:00.498: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 08:44:00.498: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 08:44:00.499: INFO: Observed replicaset test-rs in namespace replicaset-2345 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 08:44:00.499: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 08:44:00.499: INFO: Found replicaset test-rs in namespace replicaset-2345 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan  5 08:44:00.499: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 08:44:00.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2345" for this suite. 01/05/23 08:44:00.509
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":286,"skipped":5381,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.126 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:43:55.396
    Jan  5 08:43:55.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename replicaset 01/05/23 08:43:55.397
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:43:55.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:43:55.424
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/05/23 08:43:55.427
    STEP: Verify that the required pods have come up. 01/05/23 08:43:55.438
    Jan  5 08:43:55.441: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 08:44:00.447: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 08:44:00.447
    STEP: Getting /status 01/05/23 08:44:00.447
    Jan  5 08:44:00.474: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/05/23 08:44:00.474
    Jan  5 08:44:00.484: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/05/23 08:44:00.484
    Jan  5 08:44:00.485: INFO: Observed &ReplicaSet event: ADDED
    Jan  5 08:44:00.485: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 08:44:00.486: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 08:44:00.486: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 08:44:00.486: INFO: Found replicaset test-rs in namespace replicaset-2345 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 08:44:00.486: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/05/23 08:44:00.486
    Jan  5 08:44:00.486: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  5 08:44:00.496: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/05/23 08:44:00.496
    Jan  5 08:44:00.498: INFO: Observed &ReplicaSet event: ADDED
    Jan  5 08:44:00.498: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 08:44:00.498: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 08:44:00.498: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 08:44:00.499: INFO: Observed replicaset test-rs in namespace replicaset-2345 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 08:44:00.499: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 08:44:00.499: INFO: Found replicaset test-rs in namespace replicaset-2345 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan  5 08:44:00.499: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 08:44:00.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2345" for this suite. 01/05/23 08:44:00.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:00.523
Jan  5 08:44:00.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 08:44:00.524
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:00.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:00.541
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/05/23 08:44:00.543
Jan  5 08:44:00.554: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215" in namespace "emptydir-4786" to be "running"
Jan  5 08:44:00.557: INFO: Pod "pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215": Phase="Pending", Reason="", readiness=false. Elapsed: 3.049107ms
Jan  5 08:44:02.561: INFO: Pod "pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00677254s
Jan  5 08:44:04.562: INFO: Pod "pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215": Phase="Running", Reason="", readiness=false. Elapsed: 4.008050191s
Jan  5 08:44:04.562: INFO: Pod "pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/05/23 08:44:04.562
Jan  5 08:44:04.562: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4786 PodName:pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:44:04.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:44:04.562: INFO: ExecWithOptions: Clientset creation
Jan  5 08:44:04.562: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-4786/pods/pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan  5 08:44:04.628: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 08:44:04.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4786" for this suite. 01/05/23 08:44:04.631
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":287,"skipped":5396,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:00.523
    Jan  5 08:44:00.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 08:44:00.524
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:00.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:00.541
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/05/23 08:44:00.543
    Jan  5 08:44:00.554: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215" in namespace "emptydir-4786" to be "running"
    Jan  5 08:44:00.557: INFO: Pod "pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215": Phase="Pending", Reason="", readiness=false. Elapsed: 3.049107ms
    Jan  5 08:44:02.561: INFO: Pod "pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00677254s
    Jan  5 08:44:04.562: INFO: Pod "pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215": Phase="Running", Reason="", readiness=false. Elapsed: 4.008050191s
    Jan  5 08:44:04.562: INFO: Pod "pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/05/23 08:44:04.562
    Jan  5 08:44:04.562: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4786 PodName:pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:44:04.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:44:04.562: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:44:04.562: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-4786/pods/pod-sharedvolume-184550cf-036e-4731-8bfd-e81f18d2b215/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan  5 08:44:04.628: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 08:44:04.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4786" for this suite. 01/05/23 08:44:04.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:04.636
Jan  5 08:44:04.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 08:44:04.637
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:04.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:04.651
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 08:44:04.655
Jan  5 08:44:04.661: INFO: Waiting up to 5m0s for pod "pod-99c08736-c162-41b7-9969-ed6239203ecf" in namespace "emptydir-2203" to be "Succeeded or Failed"
Jan  5 08:44:04.663: INFO: Pod "pod-99c08736-c162-41b7-9969-ed6239203ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368958ms
Jan  5 08:44:06.667: INFO: Pod "pod-99c08736-c162-41b7-9969-ed6239203ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005982817s
Jan  5 08:44:08.666: INFO: Pod "pod-99c08736-c162-41b7-9969-ed6239203ecf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005574839s
STEP: Saw pod success 01/05/23 08:44:08.666
Jan  5 08:44:08.666: INFO: Pod "pod-99c08736-c162-41b7-9969-ed6239203ecf" satisfied condition "Succeeded or Failed"
Jan  5 08:44:08.668: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-99c08736-c162-41b7-9969-ed6239203ecf container test-container: <nil>
STEP: delete the pod 01/05/23 08:44:08.671
Jan  5 08:44:08.683: INFO: Waiting for pod pod-99c08736-c162-41b7-9969-ed6239203ecf to disappear
Jan  5 08:44:08.685: INFO: Pod pod-99c08736-c162-41b7-9969-ed6239203ecf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 08:44:08.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2203" for this suite. 01/05/23 08:44:08.687
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":288,"skipped":5411,"failed":0}
------------------------------
â€¢ [4.058 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:04.636
    Jan  5 08:44:04.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 08:44:04.637
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:04.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:04.651
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 08:44:04.655
    Jan  5 08:44:04.661: INFO: Waiting up to 5m0s for pod "pod-99c08736-c162-41b7-9969-ed6239203ecf" in namespace "emptydir-2203" to be "Succeeded or Failed"
    Jan  5 08:44:04.663: INFO: Pod "pod-99c08736-c162-41b7-9969-ed6239203ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368958ms
    Jan  5 08:44:06.667: INFO: Pod "pod-99c08736-c162-41b7-9969-ed6239203ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005982817s
    Jan  5 08:44:08.666: INFO: Pod "pod-99c08736-c162-41b7-9969-ed6239203ecf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005574839s
    STEP: Saw pod success 01/05/23 08:44:08.666
    Jan  5 08:44:08.666: INFO: Pod "pod-99c08736-c162-41b7-9969-ed6239203ecf" satisfied condition "Succeeded or Failed"
    Jan  5 08:44:08.668: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-99c08736-c162-41b7-9969-ed6239203ecf container test-container: <nil>
    STEP: delete the pod 01/05/23 08:44:08.671
    Jan  5 08:44:08.683: INFO: Waiting for pod pod-99c08736-c162-41b7-9969-ed6239203ecf to disappear
    Jan  5 08:44:08.685: INFO: Pod pod-99c08736-c162-41b7-9969-ed6239203ecf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 08:44:08.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2203" for this suite. 01/05/23 08:44:08.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:08.696
Jan  5 08:44:08.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 08:44:08.696
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:08.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:08.709
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/05/23 08:44:08.711
Jan  5 08:44:08.720: INFO: Waiting up to 5m0s for pod "pod-0731f9c0-84ef-4036-82b4-dc40ca058613" in namespace "emptydir-1599" to be "Succeeded or Failed"
Jan  5 08:44:08.721: INFO: Pod "pod-0731f9c0-84ef-4036-82b4-dc40ca058613": Phase="Pending", Reason="", readiness=false. Elapsed: 1.299137ms
Jan  5 08:44:10.724: INFO: Pod "pod-0731f9c0-84ef-4036-82b4-dc40ca058613": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003904666s
Jan  5 08:44:12.725: INFO: Pod "pod-0731f9c0-84ef-4036-82b4-dc40ca058613": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004883681s
STEP: Saw pod success 01/05/23 08:44:12.725
Jan  5 08:44:12.725: INFO: Pod "pod-0731f9c0-84ef-4036-82b4-dc40ca058613" satisfied condition "Succeeded or Failed"
Jan  5 08:44:12.727: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-0731f9c0-84ef-4036-82b4-dc40ca058613 container test-container: <nil>
STEP: delete the pod 01/05/23 08:44:12.732
Jan  5 08:44:12.746: INFO: Waiting for pod pod-0731f9c0-84ef-4036-82b4-dc40ca058613 to disappear
Jan  5 08:44:12.748: INFO: Pod pod-0731f9c0-84ef-4036-82b4-dc40ca058613 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 08:44:12.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1599" for this suite. 01/05/23 08:44:12.75
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":289,"skipped":5438,"failed":0}
------------------------------
â€¢ [4.059 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:08.696
    Jan  5 08:44:08.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 08:44:08.696
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:08.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:08.709
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/05/23 08:44:08.711
    Jan  5 08:44:08.720: INFO: Waiting up to 5m0s for pod "pod-0731f9c0-84ef-4036-82b4-dc40ca058613" in namespace "emptydir-1599" to be "Succeeded or Failed"
    Jan  5 08:44:08.721: INFO: Pod "pod-0731f9c0-84ef-4036-82b4-dc40ca058613": Phase="Pending", Reason="", readiness=false. Elapsed: 1.299137ms
    Jan  5 08:44:10.724: INFO: Pod "pod-0731f9c0-84ef-4036-82b4-dc40ca058613": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003904666s
    Jan  5 08:44:12.725: INFO: Pod "pod-0731f9c0-84ef-4036-82b4-dc40ca058613": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004883681s
    STEP: Saw pod success 01/05/23 08:44:12.725
    Jan  5 08:44:12.725: INFO: Pod "pod-0731f9c0-84ef-4036-82b4-dc40ca058613" satisfied condition "Succeeded or Failed"
    Jan  5 08:44:12.727: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-0731f9c0-84ef-4036-82b4-dc40ca058613 container test-container: <nil>
    STEP: delete the pod 01/05/23 08:44:12.732
    Jan  5 08:44:12.746: INFO: Waiting for pod pod-0731f9c0-84ef-4036-82b4-dc40ca058613 to disappear
    Jan  5 08:44:12.748: INFO: Pod pod-0731f9c0-84ef-4036-82b4-dc40ca058613 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 08:44:12.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1599" for this suite. 01/05/23 08:44:12.75
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:12.754
Jan  5 08:44:12.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename deployment 01/05/23 08:44:12.755
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:12.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:12.771
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan  5 08:44:12.780: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan  5 08:44:17.784: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 08:44:17.784
Jan  5 08:44:17.784: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/05/23 08:44:17.792
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 08:44:17.804: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3963  532182ff-2ccc-4b80-b31c-73c746f7b150 29960 1 2023-01-05 08:44:17 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-05 08:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00478b408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan  5 08:44:17.806: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan  5 08:44:17.806: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan  5 08:44:17.807: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3963  1b04509f-a6f5-4b82-897a-13812bc84095 29962 1 2023-01-05 08:44:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 532182ff-2ccc-4b80-b31c-73c746f7b150 0xc004c1ad07 0xc004c1ad08}] [] [{e2e.test Update apps/v1 2023-01-05 08:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:44:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 08:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"532182ff-2ccc-4b80-b31c-73c746f7b150\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004c1add8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 08:44:17.813: INFO: Pod "test-cleanup-controller-7pkfw" is available:
&Pod{ObjectMeta:{test-cleanup-controller-7pkfw test-cleanup-controller- deployment-3963  949cf531-7d1e-4e49-a1e4-a5b04cf1ef97 29949 0 2023-01-05 08:44:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:3a3a4a5a2a92c9189fc2aa4d0796159ab67f7c784cfaacf76379d3aeebc776b9 cni.projectcalico.org/podIP:10.244.1.44/32 cni.projectcalico.org/podIPs:10.244.1.44/32] [{apps/v1 ReplicaSet test-cleanup-controller 1b04509f-a6f5-4b82-897a-13812bc84095 0xc0038cc337 0xc0038cc338}] [] [{kube-controller-manager Update v1 2023-01-05 08:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b04509f-a6f5-4b82-897a-13812bc84095\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:44:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kc6j2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kc6j2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.44,StartTime:2023-01-05 08:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2d983dd2ffe56df7df29fe967b0816e91d27d681ecaf30c866134ad8728f05e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 08:44:17.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3963" for this suite. 01/05/23 08:44:17.816
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":290,"skipped":5439,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.080 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:12.754
    Jan  5 08:44:12.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename deployment 01/05/23 08:44:12.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:12.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:12.771
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan  5 08:44:12.780: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan  5 08:44:17.784: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 08:44:17.784
    Jan  5 08:44:17.784: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/05/23 08:44:17.792
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 08:44:17.804: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3963  532182ff-2ccc-4b80-b31c-73c746f7b150 29960 1 2023-01-05 08:44:17 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-05 08:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00478b408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan  5 08:44:17.806: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan  5 08:44:17.806: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan  5 08:44:17.807: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3963  1b04509f-a6f5-4b82-897a-13812bc84095 29962 1 2023-01-05 08:44:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 532182ff-2ccc-4b80-b31c-73c746f7b150 0xc004c1ad07 0xc004c1ad08}] [] [{e2e.test Update apps/v1 2023-01-05 08:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:44:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 08:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"532182ff-2ccc-4b80-b31c-73c746f7b150\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004c1add8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 08:44:17.813: INFO: Pod "test-cleanup-controller-7pkfw" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-7pkfw test-cleanup-controller- deployment-3963  949cf531-7d1e-4e49-a1e4-a5b04cf1ef97 29949 0 2023-01-05 08:44:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:3a3a4a5a2a92c9189fc2aa4d0796159ab67f7c784cfaacf76379d3aeebc776b9 cni.projectcalico.org/podIP:10.244.1.44/32 cni.projectcalico.org/podIPs:10.244.1.44/32] [{apps/v1 ReplicaSet test-cleanup-controller 1b04509f-a6f5-4b82-897a-13812bc84095 0xc0038cc337 0xc0038cc338}] [] [{kube-controller-manager Update v1 2023-01-05 08:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b04509f-a6f5-4b82-897a-13812bc84095\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:44:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kc6j2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kc6j2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.44,StartTime:2023-01-05 08:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2d983dd2ffe56df7df29fe967b0816e91d27d681ecaf30c866134ad8728f05e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 08:44:17.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3963" for this suite. 01/05/23 08:44:17.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:17.835
Jan  5 08:44:17.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 08:44:17.836
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:17.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:17.86
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 08:44:17.861
Jan  5 08:44:17.900: INFO: Waiting up to 5m0s for pod "pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17" in namespace "emptydir-2446" to be "Succeeded or Failed"
Jan  5 08:44:17.901: INFO: Pod "pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17": Phase="Pending", Reason="", readiness=false. Elapsed: 1.458041ms
Jan  5 08:44:19.904: INFO: Pod "pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004220402s
Jan  5 08:44:21.906: INFO: Pod "pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00592138s
STEP: Saw pod success 01/05/23 08:44:21.906
Jan  5 08:44:21.906: INFO: Pod "pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17" satisfied condition "Succeeded or Failed"
Jan  5 08:44:21.909: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17 container test-container: <nil>
STEP: delete the pod 01/05/23 08:44:21.913
Jan  5 08:44:21.930: INFO: Waiting for pod pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17 to disappear
Jan  5 08:44:21.939: INFO: Pod pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 08:44:21.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2446" for this suite. 01/05/23 08:44:21.941
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":291,"skipped":5447,"failed":0}
------------------------------
â€¢ [4.110 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:17.835
    Jan  5 08:44:17.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 08:44:17.836
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:17.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:17.86
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 08:44:17.861
    Jan  5 08:44:17.900: INFO: Waiting up to 5m0s for pod "pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17" in namespace "emptydir-2446" to be "Succeeded or Failed"
    Jan  5 08:44:17.901: INFO: Pod "pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17": Phase="Pending", Reason="", readiness=false. Elapsed: 1.458041ms
    Jan  5 08:44:19.904: INFO: Pod "pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004220402s
    Jan  5 08:44:21.906: INFO: Pod "pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00592138s
    STEP: Saw pod success 01/05/23 08:44:21.906
    Jan  5 08:44:21.906: INFO: Pod "pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17" satisfied condition "Succeeded or Failed"
    Jan  5 08:44:21.909: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17 container test-container: <nil>
    STEP: delete the pod 01/05/23 08:44:21.913
    Jan  5 08:44:21.930: INFO: Waiting for pod pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17 to disappear
    Jan  5 08:44:21.939: INFO: Pod pod-6d5d6fd6-ad96-4bdd-8414-ede754a94e17 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 08:44:21.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2446" for this suite. 01/05/23 08:44:21.941
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:21.945
Jan  5 08:44:21.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 08:44:21.946
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:21.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:21.963
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan  5 08:44:21.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:44:22.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6448" for this suite. 01/05/23 08:44:22.526
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":292,"skipped":5451,"failed":0}
------------------------------
â€¢ [0.588 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:21.945
    Jan  5 08:44:21.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 08:44:21.946
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:21.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:21.963
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan  5 08:44:21.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:44:22.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6448" for this suite. 01/05/23 08:44:22.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:22.534
Jan  5 08:44:22.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:44:22.535
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:22.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:22.564
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-f1ad3b5c-05c2-435a-8c08-9df1a1601cfc 01/05/23 08:44:22.566
STEP: Creating a pod to test consume configMaps 01/05/23 08:44:22.574
Jan  5 08:44:22.578: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0" in namespace "projected-7221" to be "Succeeded or Failed"
Jan  5 08:44:22.580: INFO: Pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645485ms
Jan  5 08:44:24.584: INFO: Pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005487183s
Jan  5 08:44:26.583: INFO: Pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0048325s
Jan  5 08:44:28.583: INFO: Pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005012312s
STEP: Saw pod success 01/05/23 08:44:28.583
Jan  5 08:44:28.583: INFO: Pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0" satisfied condition "Succeeded or Failed"
Jan  5 08:44:28.585: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:44:28.588
Jan  5 08:44:28.600: INFO: Waiting for pod pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0 to disappear
Jan  5 08:44:28.602: INFO: Pod pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  5 08:44:28.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7221" for this suite. 01/05/23 08:44:28.604
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":293,"skipped":5460,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.080 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:22.534
    Jan  5 08:44:22.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:44:22.535
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:22.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:22.564
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-f1ad3b5c-05c2-435a-8c08-9df1a1601cfc 01/05/23 08:44:22.566
    STEP: Creating a pod to test consume configMaps 01/05/23 08:44:22.574
    Jan  5 08:44:22.578: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0" in namespace "projected-7221" to be "Succeeded or Failed"
    Jan  5 08:44:22.580: INFO: Pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645485ms
    Jan  5 08:44:24.584: INFO: Pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005487183s
    Jan  5 08:44:26.583: INFO: Pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0048325s
    Jan  5 08:44:28.583: INFO: Pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005012312s
    STEP: Saw pod success 01/05/23 08:44:28.583
    Jan  5 08:44:28.583: INFO: Pod "pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0" satisfied condition "Succeeded or Failed"
    Jan  5 08:44:28.585: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:44:28.588
    Jan  5 08:44:28.600: INFO: Waiting for pod pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0 to disappear
    Jan  5 08:44:28.602: INFO: Pod pod-projected-configmaps-bba0fac7-0894-4d02-86db-74d26c25ebb0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  5 08:44:28.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7221" for this suite. 01/05/23 08:44:28.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:28.615
Jan  5 08:44:28.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pods 01/05/23 08:44:28.616
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:28.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:28.633
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/05/23 08:44:28.635
STEP: setting up watch 01/05/23 08:44:28.635
STEP: submitting the pod to kubernetes 01/05/23 08:44:28.744
STEP: verifying the pod is in kubernetes 01/05/23 08:44:28.753
STEP: verifying pod creation was observed 01/05/23 08:44:28.755
Jan  5 08:44:28.756: INFO: Waiting up to 5m0s for pod "pod-submit-remove-afaad97b-1532-48ce-a80b-8c519caebcc7" in namespace "pods-6145" to be "running"
Jan  5 08:44:28.761: INFO: Pod "pod-submit-remove-afaad97b-1532-48ce-a80b-8c519caebcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.494552ms
Jan  5 08:44:30.764: INFO: Pod "pod-submit-remove-afaad97b-1532-48ce-a80b-8c519caebcc7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008734434s
Jan  5 08:44:30.764: INFO: Pod "pod-submit-remove-afaad97b-1532-48ce-a80b-8c519caebcc7" satisfied condition "running"
STEP: deleting the pod gracefully 01/05/23 08:44:30.766
STEP: verifying pod deletion was observed 01/05/23 08:44:30.77
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 08:44:33.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6145" for this suite. 01/05/23 08:44:33.703
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":294,"skipped":5468,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.101 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:28.615
    Jan  5 08:44:28.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pods 01/05/23 08:44:28.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:28.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:28.633
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/05/23 08:44:28.635
    STEP: setting up watch 01/05/23 08:44:28.635
    STEP: submitting the pod to kubernetes 01/05/23 08:44:28.744
    STEP: verifying the pod is in kubernetes 01/05/23 08:44:28.753
    STEP: verifying pod creation was observed 01/05/23 08:44:28.755
    Jan  5 08:44:28.756: INFO: Waiting up to 5m0s for pod "pod-submit-remove-afaad97b-1532-48ce-a80b-8c519caebcc7" in namespace "pods-6145" to be "running"
    Jan  5 08:44:28.761: INFO: Pod "pod-submit-remove-afaad97b-1532-48ce-a80b-8c519caebcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.494552ms
    Jan  5 08:44:30.764: INFO: Pod "pod-submit-remove-afaad97b-1532-48ce-a80b-8c519caebcc7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008734434s
    Jan  5 08:44:30.764: INFO: Pod "pod-submit-remove-afaad97b-1532-48ce-a80b-8c519caebcc7" satisfied condition "running"
    STEP: deleting the pod gracefully 01/05/23 08:44:30.766
    STEP: verifying pod deletion was observed 01/05/23 08:44:30.77
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 08:44:33.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6145" for this suite. 01/05/23 08:44:33.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:33.717
Jan  5 08:44:33.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename ephemeral-containers-test 01/05/23 08:44:33.717
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:33.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:33.734
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/05/23 08:44:33.736
Jan  5 08:44:33.748: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4009" to be "running and ready"
Jan  5 08:44:33.750: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.723408ms
Jan  5 08:44:33.750: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:44:35.753: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005226749s
Jan  5 08:44:35.753: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan  5 08:44:35.753: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/05/23 08:44:35.755
Jan  5 08:44:35.763: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4009" to be "container debugger running"
Jan  5 08:44:35.764: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.392137ms
Jan  5 08:44:37.768: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004818287s
Jan  5 08:44:39.768: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004665986s
Jan  5 08:44:39.768: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/05/23 08:44:39.768
Jan  5 08:44:39.768: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4009 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:44:39.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:44:39.768: INFO: ExecWithOptions: Clientset creation
Jan  5 08:44:39.768: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4009/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan  5 08:44:39.828: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 08:44:39.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-4009" for this suite. 01/05/23 08:44:39.836
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":295,"skipped":5476,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.124 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:33.717
    Jan  5 08:44:33.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/05/23 08:44:33.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:33.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:33.734
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/05/23 08:44:33.736
    Jan  5 08:44:33.748: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4009" to be "running and ready"
    Jan  5 08:44:33.750: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.723408ms
    Jan  5 08:44:33.750: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:44:35.753: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005226749s
    Jan  5 08:44:35.753: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan  5 08:44:35.753: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/05/23 08:44:35.755
    Jan  5 08:44:35.763: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4009" to be "container debugger running"
    Jan  5 08:44:35.764: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.392137ms
    Jan  5 08:44:37.768: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004818287s
    Jan  5 08:44:39.768: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004665986s
    Jan  5 08:44:39.768: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/05/23 08:44:39.768
    Jan  5 08:44:39.768: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4009 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:44:39.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:44:39.768: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:44:39.768: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4009/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan  5 08:44:39.828: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 08:44:39.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-4009" for this suite. 01/05/23 08:44:39.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:39.841
Jan  5 08:44:39.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:44:39.842
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:39.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:39.867
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-7352 01/05/23 08:44:39.869
STEP: creating service affinity-nodeport in namespace services-7352 01/05/23 08:44:39.869
STEP: creating replication controller affinity-nodeport in namespace services-7352 01/05/23 08:44:39.889
I0105 08:44:39.899121      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7352, replica count: 3
I0105 08:44:42.950181      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 08:44:42.956: INFO: Creating new exec pod
Jan  5 08:44:42.960: INFO: Waiting up to 5m0s for pod "execpod-affinity7422n" in namespace "services-7352" to be "running"
Jan  5 08:44:42.961: INFO: Pod "execpod-affinity7422n": Phase="Pending", Reason="", readiness=false. Elapsed: 1.531793ms
Jan  5 08:44:44.964: INFO: Pod "execpod-affinity7422n": Phase="Running", Reason="", readiness=true. Elapsed: 2.003830864s
Jan  5 08:44:44.964: INFO: Pod "execpod-affinity7422n" satisfied condition "running"
Jan  5 08:44:45.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7352 exec execpod-affinity7422n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan  5 08:44:46.093: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan  5 08:44:46.093: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:44:46.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7352 exec execpod-affinity7422n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.48.59 80'
Jan  5 08:44:46.215: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.48.59 80\nConnection to 10.102.48.59 80 port [tcp/http] succeeded!\n"
Jan  5 08:44:46.215: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:44:46.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7352 exec execpod-affinity7422n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.212 30124'
Jan  5 08:44:46.338: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.212 30124\nConnection to 16.0.14.212 30124 port [tcp/*] succeeded!\n"
Jan  5 08:44:46.339: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:44:46.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7352 exec execpod-affinity7422n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.214 30124'
Jan  5 08:44:46.463: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.214 30124\nConnection to 16.0.14.214 30124 port [tcp/*] succeeded!\n"
Jan  5 08:44:46.463: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  5 08:44:46.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7352 exec execpod-affinity7422n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://16.0.14.212:30124/ ; done'
Jan  5 08:44:46.686: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n"
Jan  5 08:44:46.686: INFO: stdout: "\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j"
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
Jan  5 08:44:46.686: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7352, will wait for the garbage collector to delete the pods 01/05/23 08:44:46.704
Jan  5 08:44:46.766: INFO: Deleting ReplicationController affinity-nodeport took: 9.844729ms
Jan  5 08:44:46.867: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.808956ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:44:48.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7352" for this suite. 01/05/23 08:44:48.901
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":296,"skipped":5489,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.071 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:39.841
    Jan  5 08:44:39.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:44:39.842
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:39.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:39.867
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-7352 01/05/23 08:44:39.869
    STEP: creating service affinity-nodeport in namespace services-7352 01/05/23 08:44:39.869
    STEP: creating replication controller affinity-nodeport in namespace services-7352 01/05/23 08:44:39.889
    I0105 08:44:39.899121      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7352, replica count: 3
    I0105 08:44:42.950181      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 08:44:42.956: INFO: Creating new exec pod
    Jan  5 08:44:42.960: INFO: Waiting up to 5m0s for pod "execpod-affinity7422n" in namespace "services-7352" to be "running"
    Jan  5 08:44:42.961: INFO: Pod "execpod-affinity7422n": Phase="Pending", Reason="", readiness=false. Elapsed: 1.531793ms
    Jan  5 08:44:44.964: INFO: Pod "execpod-affinity7422n": Phase="Running", Reason="", readiness=true. Elapsed: 2.003830864s
    Jan  5 08:44:44.964: INFO: Pod "execpod-affinity7422n" satisfied condition "running"
    Jan  5 08:44:45.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7352 exec execpod-affinity7422n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan  5 08:44:46.093: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan  5 08:44:46.093: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:44:46.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7352 exec execpod-affinity7422n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.48.59 80'
    Jan  5 08:44:46.215: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.48.59 80\nConnection to 10.102.48.59 80 port [tcp/http] succeeded!\n"
    Jan  5 08:44:46.215: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:44:46.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7352 exec execpod-affinity7422n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.212 30124'
    Jan  5 08:44:46.338: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.212 30124\nConnection to 16.0.14.212 30124 port [tcp/*] succeeded!\n"
    Jan  5 08:44:46.339: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:44:46.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7352 exec execpod-affinity7422n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 16.0.14.214 30124'
    Jan  5 08:44:46.463: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 16.0.14.214 30124\nConnection to 16.0.14.214 30124 port [tcp/*] succeeded!\n"
    Jan  5 08:44:46.463: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  5 08:44:46.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7352 exec execpod-affinity7422n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://16.0.14.212:30124/ ; done'
    Jan  5 08:44:46.686: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n+ echo\n+ curl -q -s --connect-timeout 2 http://16.0.14.212:30124/\n"
    Jan  5 08:44:46.686: INFO: stdout: "\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j\naffinity-nodeport-qqw5j"
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Received response from host: affinity-nodeport-qqw5j
    Jan  5 08:44:46.686: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-7352, will wait for the garbage collector to delete the pods 01/05/23 08:44:46.704
    Jan  5 08:44:46.766: INFO: Deleting ReplicationController affinity-nodeport took: 9.844729ms
    Jan  5 08:44:46.867: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.808956ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:44:48.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7352" for this suite. 01/05/23 08:44:48.901
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:48.913
Jan  5 08:44:48.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 08:44:48.914
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:48.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:48.931
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-7e16afd9-d812-4db7-9ab3-8b5b6c5547cb 01/05/23 08:44:48.932
STEP: Creating a pod to test consume secrets 01/05/23 08:44:48.939
Jan  5 08:44:48.945: INFO: Waiting up to 5m0s for pod "pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789" in namespace "secrets-675" to be "Succeeded or Failed"
Jan  5 08:44:48.947: INFO: Pod "pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091446ms
Jan  5 08:44:50.950: INFO: Pod "pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004895856s
Jan  5 08:44:52.951: INFO: Pod "pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006047727s
STEP: Saw pod success 01/05/23 08:44:52.951
Jan  5 08:44:52.951: INFO: Pod "pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789" satisfied condition "Succeeded or Failed"
Jan  5 08:44:52.953: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 08:44:52.957
Jan  5 08:44:52.971: INFO: Waiting for pod pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789 to disappear
Jan  5 08:44:52.972: INFO: Pod pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 08:44:52.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-675" for this suite. 01/05/23 08:44:52.975
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":297,"skipped":5494,"failed":0}
------------------------------
â€¢ [4.073 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:48.913
    Jan  5 08:44:48.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 08:44:48.914
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:48.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:48.931
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-7e16afd9-d812-4db7-9ab3-8b5b6c5547cb 01/05/23 08:44:48.932
    STEP: Creating a pod to test consume secrets 01/05/23 08:44:48.939
    Jan  5 08:44:48.945: INFO: Waiting up to 5m0s for pod "pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789" in namespace "secrets-675" to be "Succeeded or Failed"
    Jan  5 08:44:48.947: INFO: Pod "pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091446ms
    Jan  5 08:44:50.950: INFO: Pod "pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004895856s
    Jan  5 08:44:52.951: INFO: Pod "pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006047727s
    STEP: Saw pod success 01/05/23 08:44:52.951
    Jan  5 08:44:52.951: INFO: Pod "pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789" satisfied condition "Succeeded or Failed"
    Jan  5 08:44:52.953: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 08:44:52.957
    Jan  5 08:44:52.971: INFO: Waiting for pod pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789 to disappear
    Jan  5 08:44:52.972: INFO: Pod pod-secrets-f020c23c-3384-490d-82d6-8c5536a1a789 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 08:44:52.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-675" for this suite. 01/05/23 08:44:52.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:52.988
Jan  5 08:44:52.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename certificates 01/05/23 08:44:52.989
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:53.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:53.033
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/05/23 08:44:53.692
STEP: getting /apis/certificates.k8s.io 01/05/23 08:44:53.693
STEP: getting /apis/certificates.k8s.io/v1 01/05/23 08:44:53.694
STEP: creating 01/05/23 08:44:53.695
STEP: getting 01/05/23 08:44:53.717
STEP: listing 01/05/23 08:44:53.719
STEP: watching 01/05/23 08:44:53.721
Jan  5 08:44:53.721: INFO: starting watch
STEP: patching 01/05/23 08:44:53.722
STEP: updating 01/05/23 08:44:53.729
Jan  5 08:44:53.738: INFO: waiting for watch events with expected annotations
Jan  5 08:44:53.738: INFO: saw patched and updated annotations
STEP: getting /approval 01/05/23 08:44:53.738
STEP: patching /approval 01/05/23 08:44:53.739
STEP: updating /approval 01/05/23 08:44:53.753
STEP: getting /status 01/05/23 08:44:53.757
STEP: patching /status 01/05/23 08:44:53.759
STEP: updating /status 01/05/23 08:44:53.77
STEP: deleting 01/05/23 08:44:53.775
STEP: deleting a collection 01/05/23 08:44:53.785
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:44:53.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7892" for this suite. 01/05/23 08:44:53.798
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":298,"skipped":5526,"failed":0}
------------------------------
â€¢ [0.814 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:52.988
    Jan  5 08:44:52.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename certificates 01/05/23 08:44:52.989
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:53.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:53.033
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/05/23 08:44:53.692
    STEP: getting /apis/certificates.k8s.io 01/05/23 08:44:53.693
    STEP: getting /apis/certificates.k8s.io/v1 01/05/23 08:44:53.694
    STEP: creating 01/05/23 08:44:53.695
    STEP: getting 01/05/23 08:44:53.717
    STEP: listing 01/05/23 08:44:53.719
    STEP: watching 01/05/23 08:44:53.721
    Jan  5 08:44:53.721: INFO: starting watch
    STEP: patching 01/05/23 08:44:53.722
    STEP: updating 01/05/23 08:44:53.729
    Jan  5 08:44:53.738: INFO: waiting for watch events with expected annotations
    Jan  5 08:44:53.738: INFO: saw patched and updated annotations
    STEP: getting /approval 01/05/23 08:44:53.738
    STEP: patching /approval 01/05/23 08:44:53.739
    STEP: updating /approval 01/05/23 08:44:53.753
    STEP: getting /status 01/05/23 08:44:53.757
    STEP: patching /status 01/05/23 08:44:53.759
    STEP: updating /status 01/05/23 08:44:53.77
    STEP: deleting 01/05/23 08:44:53.775
    STEP: deleting a collection 01/05/23 08:44:53.785
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:44:53.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-7892" for this suite. 01/05/23 08:44:53.798
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:53.802
Jan  5 08:44:53.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename var-expansion 01/05/23 08:44:53.803
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:53.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:53.817
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jan  5 08:44:53.826: INFO: Waiting up to 2m0s for pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0" in namespace "var-expansion-2566" to be "container 0 failed with reason CreateContainerConfigError"
Jan  5 08:44:53.827: INFO: Pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.57095ms
Jan  5 08:44:55.831: INFO: Pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004979132s
Jan  5 08:44:55.831: INFO: Pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  5 08:44:55.831: INFO: Deleting pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0" in namespace "var-expansion-2566"
Jan  5 08:44:55.835: INFO: Wait up to 5m0s for pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 08:44:57.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2566" for this suite. 01/05/23 08:44:57.843
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":299,"skipped":5529,"failed":0}
------------------------------
â€¢ [4.046 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:53.802
    Jan  5 08:44:53.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename var-expansion 01/05/23 08:44:53.803
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:53.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:53.817
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jan  5 08:44:53.826: INFO: Waiting up to 2m0s for pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0" in namespace "var-expansion-2566" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  5 08:44:53.827: INFO: Pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.57095ms
    Jan  5 08:44:55.831: INFO: Pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004979132s
    Jan  5 08:44:55.831: INFO: Pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  5 08:44:55.831: INFO: Deleting pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0" in namespace "var-expansion-2566"
    Jan  5 08:44:55.835: INFO: Wait up to 5m0s for pod "var-expansion-b1a86c7a-2f18-4c92-bd41-6938120844f0" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 08:44:57.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2566" for this suite. 01/05/23 08:44:57.843
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:44:57.848
Jan  5 08:44:57.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-probe 01/05/23 08:44:57.849
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:57.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:57.863
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-17c237d5-789f-407e-8e60-d990568e1ca3 in namespace container-probe-4395 01/05/23 08:44:57.869
Jan  5 08:44:57.877: INFO: Waiting up to 5m0s for pod "liveness-17c237d5-789f-407e-8e60-d990568e1ca3" in namespace "container-probe-4395" to be "not pending"
Jan  5 08:44:57.884: INFO: Pod "liveness-17c237d5-789f-407e-8e60-d990568e1ca3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.333826ms
Jan  5 08:44:59.889: INFO: Pod "liveness-17c237d5-789f-407e-8e60-d990568e1ca3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011469499s
Jan  5 08:44:59.889: INFO: Pod "liveness-17c237d5-789f-407e-8e60-d990568e1ca3" satisfied condition "not pending"
Jan  5 08:44:59.889: INFO: Started pod liveness-17c237d5-789f-407e-8e60-d990568e1ca3 in namespace container-probe-4395
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:44:59.889
Jan  5 08:44:59.890: INFO: Initial restart count of pod liveness-17c237d5-789f-407e-8e60-d990568e1ca3 is 0
STEP: deleting the pod 01/05/23 08:49:00.335
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 08:49:00.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4395" for this suite. 01/05/23 08:49:00.369
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":300,"skipped":5531,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.544 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:44:57.848
    Jan  5 08:44:57.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-probe 01/05/23 08:44:57.849
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:44:57.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:44:57.863
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-17c237d5-789f-407e-8e60-d990568e1ca3 in namespace container-probe-4395 01/05/23 08:44:57.869
    Jan  5 08:44:57.877: INFO: Waiting up to 5m0s for pod "liveness-17c237d5-789f-407e-8e60-d990568e1ca3" in namespace "container-probe-4395" to be "not pending"
    Jan  5 08:44:57.884: INFO: Pod "liveness-17c237d5-789f-407e-8e60-d990568e1ca3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.333826ms
    Jan  5 08:44:59.889: INFO: Pod "liveness-17c237d5-789f-407e-8e60-d990568e1ca3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011469499s
    Jan  5 08:44:59.889: INFO: Pod "liveness-17c237d5-789f-407e-8e60-d990568e1ca3" satisfied condition "not pending"
    Jan  5 08:44:59.889: INFO: Started pod liveness-17c237d5-789f-407e-8e60-d990568e1ca3 in namespace container-probe-4395
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:44:59.889
    Jan  5 08:44:59.890: INFO: Initial restart count of pod liveness-17c237d5-789f-407e-8e60-d990568e1ca3 is 0
    STEP: deleting the pod 01/05/23 08:49:00.335
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 08:49:00.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4395" for this suite. 01/05/23 08:49:00.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:49:00.392
Jan  5 08:49:00.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename resourcequota 01/05/23 08:49:00.393
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:00.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:00.408
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/05/23 08:49:00.413
STEP: Creating a ResourceQuota 01/05/23 08:49:05.415
STEP: Ensuring resource quota status is calculated 01/05/23 08:49:05.418
STEP: Creating a Service 01/05/23 08:49:07.421
STEP: Creating a NodePort Service 01/05/23 08:49:07.442
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/05/23 08:49:07.474
STEP: Ensuring resource quota status captures service creation 01/05/23 08:49:07.503
STEP: Deleting Services 01/05/23 08:49:09.507
STEP: Ensuring resource quota status released usage 01/05/23 08:49:09.568
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 08:49:11.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1489" for this suite. 01/05/23 08:49:11.574
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":301,"skipped":5548,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.185 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:49:00.392
    Jan  5 08:49:00.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename resourcequota 01/05/23 08:49:00.393
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:00.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:00.408
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/05/23 08:49:00.413
    STEP: Creating a ResourceQuota 01/05/23 08:49:05.415
    STEP: Ensuring resource quota status is calculated 01/05/23 08:49:05.418
    STEP: Creating a Service 01/05/23 08:49:07.421
    STEP: Creating a NodePort Service 01/05/23 08:49:07.442
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/05/23 08:49:07.474
    STEP: Ensuring resource quota status captures service creation 01/05/23 08:49:07.503
    STEP: Deleting Services 01/05/23 08:49:09.507
    STEP: Ensuring resource quota status released usage 01/05/23 08:49:09.568
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 08:49:11.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1489" for this suite. 01/05/23 08:49:11.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:49:11.579
Jan  5 08:49:11.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 08:49:11.58
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:11.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:11.598
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/05/23 08:49:11.601
Jan  5 08:49:11.606: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9822" to be "running and ready"
Jan  5 08:49:11.607: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.446282ms
Jan  5 08:49:11.607: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:49:13.610: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004289667s
Jan  5 08:49:13.610: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 08:49:13.610: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/05/23 08:49:13.612
Jan  5 08:49:13.616: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9822" to be "running and ready"
Jan  5 08:49:13.618: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.561184ms
Jan  5 08:49:13.618: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:49:15.622: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005308009s
Jan  5 08:49:15.622: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan  5 08:49:15.622: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/05/23 08:49:15.624
Jan  5 08:49:15.629: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  5 08:49:15.632: INFO: Pod pod-with-prestop-http-hook still exists
Jan  5 08:49:17.633: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  5 08:49:17.636: INFO: Pod pod-with-prestop-http-hook still exists
Jan  5 08:49:19.633: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  5 08:49:19.636: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/05/23 08:49:19.636
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  5 08:49:19.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9822" for this suite. 01/05/23 08:49:19.651
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":302,"skipped":5599,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.076 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:49:11.579
    Jan  5 08:49:11.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 08:49:11.58
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:11.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:11.598
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 08:49:11.601
    Jan  5 08:49:11.606: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9822" to be "running and ready"
    Jan  5 08:49:11.607: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.446282ms
    Jan  5 08:49:11.607: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:49:13.610: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004289667s
    Jan  5 08:49:13.610: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 08:49:13.610: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/05/23 08:49:13.612
    Jan  5 08:49:13.616: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9822" to be "running and ready"
    Jan  5 08:49:13.618: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.561184ms
    Jan  5 08:49:13.618: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:49:15.622: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005308009s
    Jan  5 08:49:15.622: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan  5 08:49:15.622: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/05/23 08:49:15.624
    Jan  5 08:49:15.629: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  5 08:49:15.632: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  5 08:49:17.633: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  5 08:49:17.636: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  5 08:49:19.633: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  5 08:49:19.636: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/05/23 08:49:19.636
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  5 08:49:19.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9822" for this suite. 01/05/23 08:49:19.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:49:19.657
Jan  5 08:49:19.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename svcaccounts 01/05/23 08:49:19.658
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:19.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:19.695
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan  5 08:49:19.730: INFO: created pod
Jan  5 08:49:19.730: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-9788" to be "Succeeded or Failed"
Jan  5 08:49:19.732: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.829837ms
Jan  5 08:49:21.736: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005603045s
Jan  5 08:49:23.734: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004484405s
STEP: Saw pod success 01/05/23 08:49:23.734
Jan  5 08:49:23.735: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan  5 08:49:53.738: INFO: polling logs
Jan  5 08:49:53.753: INFO: Pod logs: 
I0105 08:49:21.052444       1 log.go:195] OK: Got token
I0105 08:49:21.052479       1 log.go:195] validating with in-cluster discovery
I0105 08:49:21.052739       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0105 08:49:21.052763       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9788:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672909159, NotBefore:1672908559, IssuedAt:1672908559, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9788", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"66b14260-6c2b-4c8b-9a1a-96e10b40a531"}}}
I0105 08:49:21.064406       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0105 08:49:21.069053       1 log.go:195] OK: Validated signature on JWT
I0105 08:49:21.069118       1 log.go:195] OK: Got valid claims from token!
I0105 08:49:21.069141       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9788:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672909159, NotBefore:1672908559, IssuedAt:1672908559, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9788", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"66b14260-6c2b-4c8b-9a1a-96e10b40a531"}}}

Jan  5 08:49:53.753: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  5 08:49:53.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9788" for this suite. 01/05/23 08:49:53.76
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":303,"skipped":5650,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.114 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:49:19.657
    Jan  5 08:49:19.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 08:49:19.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:19.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:19.695
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan  5 08:49:19.730: INFO: created pod
    Jan  5 08:49:19.730: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-9788" to be "Succeeded or Failed"
    Jan  5 08:49:19.732: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.829837ms
    Jan  5 08:49:21.736: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005603045s
    Jan  5 08:49:23.734: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004484405s
    STEP: Saw pod success 01/05/23 08:49:23.734
    Jan  5 08:49:23.735: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan  5 08:49:53.738: INFO: polling logs
    Jan  5 08:49:53.753: INFO: Pod logs: 
    I0105 08:49:21.052444       1 log.go:195] OK: Got token
    I0105 08:49:21.052479       1 log.go:195] validating with in-cluster discovery
    I0105 08:49:21.052739       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0105 08:49:21.052763       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9788:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672909159, NotBefore:1672908559, IssuedAt:1672908559, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9788", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"66b14260-6c2b-4c8b-9a1a-96e10b40a531"}}}
    I0105 08:49:21.064406       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0105 08:49:21.069053       1 log.go:195] OK: Validated signature on JWT
    I0105 08:49:21.069118       1 log.go:195] OK: Got valid claims from token!
    I0105 08:49:21.069141       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9788:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672909159, NotBefore:1672908559, IssuedAt:1672908559, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9788", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"66b14260-6c2b-4c8b-9a1a-96e10b40a531"}}}

    Jan  5 08:49:53.753: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  5 08:49:53.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9788" for this suite. 01/05/23 08:49:53.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:49:53.772
Jan  5 08:49:53.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename containers 01/05/23 08:49:53.773
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:53.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:53.792
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan  5 08:49:53.811: INFO: Waiting up to 5m0s for pod "client-containers-79b97183-e897-438f-aea3-054f09abec9e" in namespace "containers-7614" to be "running"
Jan  5 08:49:53.813: INFO: Pod "client-containers-79b97183-e897-438f-aea3-054f09abec9e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.768996ms
Jan  5 08:49:55.817: INFO: Pod "client-containers-79b97183-e897-438f-aea3-054f09abec9e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005551909s
Jan  5 08:49:55.817: INFO: Pod "client-containers-79b97183-e897-438f-aea3-054f09abec9e" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  5 08:49:55.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7614" for this suite. 01/05/23 08:49:55.823
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":304,"skipped":5684,"failed":0}
------------------------------
â€¢ [2.054 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:49:53.772
    Jan  5 08:49:53.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename containers 01/05/23 08:49:53.773
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:53.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:53.792
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan  5 08:49:53.811: INFO: Waiting up to 5m0s for pod "client-containers-79b97183-e897-438f-aea3-054f09abec9e" in namespace "containers-7614" to be "running"
    Jan  5 08:49:53.813: INFO: Pod "client-containers-79b97183-e897-438f-aea3-054f09abec9e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.768996ms
    Jan  5 08:49:55.817: INFO: Pod "client-containers-79b97183-e897-438f-aea3-054f09abec9e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005551909s
    Jan  5 08:49:55.817: INFO: Pod "client-containers-79b97183-e897-438f-aea3-054f09abec9e" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  5 08:49:55.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7614" for this suite. 01/05/23 08:49:55.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:49:55.827
Jan  5 08:49:55.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename downward-api 01/05/23 08:49:55.828
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:55.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:55.849
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/05/23 08:49:55.851
Jan  5 08:49:55.858: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311" in namespace "downward-api-121" to be "Succeeded or Failed"
Jan  5 08:49:55.860: INFO: Pod "downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311": Phase="Pending", Reason="", readiness=false. Elapsed: 1.756615ms
Jan  5 08:49:57.863: INFO: Pod "downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005426502s
Jan  5 08:49:59.862: INFO: Pod "downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004585592s
STEP: Saw pod success 01/05/23 08:49:59.862
Jan  5 08:49:59.863: INFO: Pod "downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311" satisfied condition "Succeeded or Failed"
Jan  5 08:49:59.864: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311 container client-container: <nil>
STEP: delete the pod 01/05/23 08:49:59.869
Jan  5 08:49:59.882: INFO: Waiting for pod downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311 to disappear
Jan  5 08:49:59.884: INFO: Pod downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  5 08:49:59.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-121" for this suite. 01/05/23 08:49:59.886
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":305,"skipped":5692,"failed":0}
------------------------------
â€¢ [4.067 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:49:55.827
    Jan  5 08:49:55.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename downward-api 01/05/23 08:49:55.828
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:55.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:55.849
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/05/23 08:49:55.851
    Jan  5 08:49:55.858: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311" in namespace "downward-api-121" to be "Succeeded or Failed"
    Jan  5 08:49:55.860: INFO: Pod "downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311": Phase="Pending", Reason="", readiness=false. Elapsed: 1.756615ms
    Jan  5 08:49:57.863: INFO: Pod "downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005426502s
    Jan  5 08:49:59.862: INFO: Pod "downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004585592s
    STEP: Saw pod success 01/05/23 08:49:59.862
    Jan  5 08:49:59.863: INFO: Pod "downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311" satisfied condition "Succeeded or Failed"
    Jan  5 08:49:59.864: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311 container client-container: <nil>
    STEP: delete the pod 01/05/23 08:49:59.869
    Jan  5 08:49:59.882: INFO: Waiting for pod downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311 to disappear
    Jan  5 08:49:59.884: INFO: Pod downwardapi-volume-f0055655-78d5-4c47-9764-073b9cab3311 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  5 08:49:59.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-121" for this suite. 01/05/23 08:49:59.886
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:49:59.894
Jan  5 08:49:59.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename runtimeclass 01/05/23 08:49:59.895
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:59.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:59.911
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-962-delete-me 01/05/23 08:49:59.931
STEP: Waiting for the RuntimeClass to disappear 01/05/23 08:49:59.935
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  5 08:49:59.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-962" for this suite. 01/05/23 08:49:59.941
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":306,"skipped":5694,"failed":0}
------------------------------
â€¢ [0.056 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:49:59.894
    Jan  5 08:49:59.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 08:49:59.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:59.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:59.911
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-962-delete-me 01/05/23 08:49:59.931
    STEP: Waiting for the RuntimeClass to disappear 01/05/23 08:49:59.935
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  5 08:49:59.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-962" for this suite. 01/05/23 08:49:59.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:49:59.951
Jan  5 08:49:59.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename secrets 01/05/23 08:49:59.952
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:59.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:59.967
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-68277178-d6cf-4d0f-96fe-4bbc73c9e3df 01/05/23 08:49:59.969
STEP: Creating a pod to test consume secrets 01/05/23 08:49:59.978
Jan  5 08:49:59.983: INFO: Waiting up to 5m0s for pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc" in namespace "secrets-1399" to be "Succeeded or Failed"
Jan  5 08:49:59.985: INFO: Pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.789986ms
Jan  5 08:50:01.988: INFO: Pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004550948s
Jan  5 08:50:03.988: INFO: Pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005256901s
Jan  5 08:50:05.988: INFO: Pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004763467s
STEP: Saw pod success 01/05/23 08:50:05.988
Jan  5 08:50:05.988: INFO: Pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc" satisfied condition "Succeeded or Failed"
Jan  5 08:50:05.990: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 08:50:05.993
Jan  5 08:50:06.005: INFO: Waiting for pod pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc to disappear
Jan  5 08:50:06.007: INFO: Pod pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  5 08:50:06.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1399" for this suite. 01/05/23 08:50:06.011
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":307,"skipped":5708,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.064 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:49:59.951
    Jan  5 08:49:59.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename secrets 01/05/23 08:49:59.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:49:59.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:49:59.967
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-68277178-d6cf-4d0f-96fe-4bbc73c9e3df 01/05/23 08:49:59.969
    STEP: Creating a pod to test consume secrets 01/05/23 08:49:59.978
    Jan  5 08:49:59.983: INFO: Waiting up to 5m0s for pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc" in namespace "secrets-1399" to be "Succeeded or Failed"
    Jan  5 08:49:59.985: INFO: Pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.789986ms
    Jan  5 08:50:01.988: INFO: Pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004550948s
    Jan  5 08:50:03.988: INFO: Pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005256901s
    Jan  5 08:50:05.988: INFO: Pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004763467s
    STEP: Saw pod success 01/05/23 08:50:05.988
    Jan  5 08:50:05.988: INFO: Pod "pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc" satisfied condition "Succeeded or Failed"
    Jan  5 08:50:05.990: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 08:50:05.993
    Jan  5 08:50:06.005: INFO: Waiting for pod pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc to disappear
    Jan  5 08:50:06.007: INFO: Pod pod-secrets-7c057bcf-db53-4336-95be-9b8cd016f5cc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  5 08:50:06.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1399" for this suite. 01/05/23 08:50:06.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:50:06.017
Jan  5 08:50:06.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename resourcequota 01/05/23 08:50:06.017
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:06.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:06.048
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/05/23 08:50:06.05
STEP: Ensuring ResourceQuota status is calculated 01/05/23 08:50:06.06
STEP: Creating a ResourceQuota with not terminating scope 01/05/23 08:50:08.062
STEP: Ensuring ResourceQuota status is calculated 01/05/23 08:50:08.08
STEP: Creating a long running pod 01/05/23 08:50:10.083
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/05/23 08:50:10.097
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/05/23 08:50:12.1
STEP: Deleting the pod 01/05/23 08:50:14.105
STEP: Ensuring resource quota status released the pod usage 01/05/23 08:50:14.119
STEP: Creating a terminating pod 01/05/23 08:50:16.123
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/05/23 08:50:16.136
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/05/23 08:50:18.139
STEP: Deleting the pod 01/05/23 08:50:20.143
STEP: Ensuring resource quota status released the pod usage 01/05/23 08:50:20.168
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 08:50:22.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-490" for this suite. 01/05/23 08:50:22.174
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":308,"skipped":5788,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.176 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:50:06.017
    Jan  5 08:50:06.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename resourcequota 01/05/23 08:50:06.017
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:06.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:06.048
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/05/23 08:50:06.05
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 08:50:06.06
    STEP: Creating a ResourceQuota with not terminating scope 01/05/23 08:50:08.062
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 08:50:08.08
    STEP: Creating a long running pod 01/05/23 08:50:10.083
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/05/23 08:50:10.097
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/05/23 08:50:12.1
    STEP: Deleting the pod 01/05/23 08:50:14.105
    STEP: Ensuring resource quota status released the pod usage 01/05/23 08:50:14.119
    STEP: Creating a terminating pod 01/05/23 08:50:16.123
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/05/23 08:50:16.136
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/05/23 08:50:18.139
    STEP: Deleting the pod 01/05/23 08:50:20.143
    STEP: Ensuring resource quota status released the pod usage 01/05/23 08:50:20.168
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 08:50:22.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-490" for this suite. 01/05/23 08:50:22.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:50:22.194
Jan  5 08:50:22.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename security-context-test 01/05/23 08:50:22.195
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:22.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:22.212
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan  5 08:50:22.221: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb" in namespace "security-context-test-2260" to be "Succeeded or Failed"
Jan  5 08:50:22.223: INFO: Pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.433477ms
Jan  5 08:50:24.227: INFO: Pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005098542s
Jan  5 08:50:26.226: INFO: Pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004199185s
Jan  5 08:50:28.228: INFO: Pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006333788s
Jan  5 08:50:28.228: INFO: Pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  5 08:50:28.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2260" for this suite. 01/05/23 08:50:28.23
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":309,"skipped":5817,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.040 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:50:22.194
    Jan  5 08:50:22.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename security-context-test 01/05/23 08:50:22.195
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:22.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:22.212
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan  5 08:50:22.221: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb" in namespace "security-context-test-2260" to be "Succeeded or Failed"
    Jan  5 08:50:22.223: INFO: Pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.433477ms
    Jan  5 08:50:24.227: INFO: Pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005098542s
    Jan  5 08:50:26.226: INFO: Pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004199185s
    Jan  5 08:50:28.228: INFO: Pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006333788s
    Jan  5 08:50:28.228: INFO: Pod "busybox-readonly-false-1353135a-61f6-49b1-9ae9-45a977a27adb" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  5 08:50:28.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2260" for this suite. 01/05/23 08:50:28.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:50:28.235
Jan  5 08:50:28.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename runtimeclass 01/05/23 08:50:28.235
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:28.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:28.258
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  5 08:50:28.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8812" for this suite. 01/05/23 08:50:28.267
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":310,"skipped":5840,"failed":0}
------------------------------
â€¢ [0.036 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:50:28.235
    Jan  5 08:50:28.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 08:50:28.235
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:28.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:28.258
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  5 08:50:28.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8812" for this suite. 01/05/23 08:50:28.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:50:28.272
Jan  5 08:50:28.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename limitrange 01/05/23 08:50:28.273
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:28.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:28.311
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/05/23 08:50:28.313
STEP: Setting up watch 01/05/23 08:50:28.313
STEP: Submitting a LimitRange 01/05/23 08:50:28.415
STEP: Verifying LimitRange creation was observed 01/05/23 08:50:28.42
STEP: Fetching the LimitRange to ensure it has proper values 01/05/23 08:50:28.42
Jan  5 08:50:28.422: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  5 08:50:28.422: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/05/23 08:50:28.422
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/05/23 08:50:28.431
Jan  5 08:50:28.433: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  5 08:50:28.433: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/05/23 08:50:28.434
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/05/23 08:50:28.444
Jan  5 08:50:28.447: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan  5 08:50:28.447: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/05/23 08:50:28.447
STEP: Failing to create a Pod with more than max resources 01/05/23 08:50:28.449
STEP: Updating a LimitRange 01/05/23 08:50:28.45
STEP: Verifying LimitRange updating is effective 01/05/23 08:50:28.457
STEP: Creating a Pod with less than former min resources 01/05/23 08:50:30.461
STEP: Failing to create a Pod with more than max resources 01/05/23 08:50:30.467
STEP: Deleting a LimitRange 01/05/23 08:50:30.471
STEP: Verifying the LimitRange was deleted 01/05/23 08:50:30.487
Jan  5 08:50:35.489: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/05/23 08:50:35.489
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan  5 08:50:35.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3184" for this suite. 01/05/23 08:50:35.496
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":311,"skipped":5877,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.235 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:50:28.272
    Jan  5 08:50:28.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename limitrange 01/05/23 08:50:28.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:28.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:28.311
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/05/23 08:50:28.313
    STEP: Setting up watch 01/05/23 08:50:28.313
    STEP: Submitting a LimitRange 01/05/23 08:50:28.415
    STEP: Verifying LimitRange creation was observed 01/05/23 08:50:28.42
    STEP: Fetching the LimitRange to ensure it has proper values 01/05/23 08:50:28.42
    Jan  5 08:50:28.422: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  5 08:50:28.422: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/05/23 08:50:28.422
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/05/23 08:50:28.431
    Jan  5 08:50:28.433: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  5 08:50:28.433: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/05/23 08:50:28.434
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/05/23 08:50:28.444
    Jan  5 08:50:28.447: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan  5 08:50:28.447: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/05/23 08:50:28.447
    STEP: Failing to create a Pod with more than max resources 01/05/23 08:50:28.449
    STEP: Updating a LimitRange 01/05/23 08:50:28.45
    STEP: Verifying LimitRange updating is effective 01/05/23 08:50:28.457
    STEP: Creating a Pod with less than former min resources 01/05/23 08:50:30.461
    STEP: Failing to create a Pod with more than max resources 01/05/23 08:50:30.467
    STEP: Deleting a LimitRange 01/05/23 08:50:30.471
    STEP: Verifying the LimitRange was deleted 01/05/23 08:50:30.487
    Jan  5 08:50:35.489: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/05/23 08:50:35.489
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan  5 08:50:35.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-3184" for this suite. 01/05/23 08:50:35.496
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:50:35.508
Jan  5 08:50:35.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:50:35.508
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:35.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:35.521
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:50:35.552
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:50:36.006
STEP: Deploying the webhook pod 01/05/23 08:50:36.011
STEP: Wait for the deployment to be ready 01/05/23 08:50:36.032
Jan  5 08:50:36.042: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 08:50:38.048
STEP: Verifying the service has paired with the endpoint 01/05/23 08:50:38.09
Jan  5 08:50:39.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/05/23 08:50:39.095
STEP: create a namespace for the webhook 01/05/23 08:50:39.125
STEP: create a configmap should be unconditionally rejected by the webhook 01/05/23 08:50:39.129
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:50:39.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-725" for this suite. 01/05/23 08:50:39.155
STEP: Destroying namespace "webhook-725-markers" for this suite. 01/05/23 08:50:39.159
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":312,"skipped":5878,"failed":0}
------------------------------
â€¢ [3.724 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:50:35.508
    Jan  5 08:50:35.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:50:35.508
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:35.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:35.521
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:50:35.552
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:50:36.006
    STEP: Deploying the webhook pod 01/05/23 08:50:36.011
    STEP: Wait for the deployment to be ready 01/05/23 08:50:36.032
    Jan  5 08:50:36.042: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 08:50:38.048
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:50:38.09
    Jan  5 08:50:39.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/05/23 08:50:39.095
    STEP: create a namespace for the webhook 01/05/23 08:50:39.125
    STEP: create a configmap should be unconditionally rejected by the webhook 01/05/23 08:50:39.129
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:50:39.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-725" for this suite. 01/05/23 08:50:39.155
    STEP: Destroying namespace "webhook-725-markers" for this suite. 01/05/23 08:50:39.159
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:50:39.234
Jan  5 08:50:39.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename var-expansion 01/05/23 08:50:39.236
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:39.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:39.252
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/05/23 08:50:39.254
Jan  5 08:50:39.263: INFO: Waiting up to 5m0s for pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c" in namespace "var-expansion-7865" to be "Succeeded or Failed"
Jan  5 08:50:39.265: INFO: Pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.555778ms
Jan  5 08:50:41.269: INFO: Pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005515901s
Jan  5 08:50:43.270: INFO: Pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006581624s
Jan  5 08:50:45.269: INFO: Pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005837332s
STEP: Saw pod success 01/05/23 08:50:45.269
Jan  5 08:50:45.269: INFO: Pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c" satisfied condition "Succeeded or Failed"
Jan  5 08:50:45.272: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod var-expansion-7630ce70-ba77-4271-a885-45996d470f2c container dapi-container: <nil>
STEP: delete the pod 01/05/23 08:50:45.277
Jan  5 08:50:45.295: INFO: Waiting for pod var-expansion-7630ce70-ba77-4271-a885-45996d470f2c to disappear
Jan  5 08:50:45.296: INFO: Pod var-expansion-7630ce70-ba77-4271-a885-45996d470f2c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  5 08:50:45.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7865" for this suite. 01/05/23 08:50:45.298
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":313,"skipped":5903,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.072 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:50:39.234
    Jan  5 08:50:39.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename var-expansion 01/05/23 08:50:39.236
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:39.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:39.252
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/05/23 08:50:39.254
    Jan  5 08:50:39.263: INFO: Waiting up to 5m0s for pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c" in namespace "var-expansion-7865" to be "Succeeded or Failed"
    Jan  5 08:50:39.265: INFO: Pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.555778ms
    Jan  5 08:50:41.269: INFO: Pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005515901s
    Jan  5 08:50:43.270: INFO: Pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006581624s
    Jan  5 08:50:45.269: INFO: Pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005837332s
    STEP: Saw pod success 01/05/23 08:50:45.269
    Jan  5 08:50:45.269: INFO: Pod "var-expansion-7630ce70-ba77-4271-a885-45996d470f2c" satisfied condition "Succeeded or Failed"
    Jan  5 08:50:45.272: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod var-expansion-7630ce70-ba77-4271-a885-45996d470f2c container dapi-container: <nil>
    STEP: delete the pod 01/05/23 08:50:45.277
    Jan  5 08:50:45.295: INFO: Waiting for pod var-expansion-7630ce70-ba77-4271-a885-45996d470f2c to disappear
    Jan  5 08:50:45.296: INFO: Pod var-expansion-7630ce70-ba77-4271-a885-45996d470f2c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  5 08:50:45.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7865" for this suite. 01/05/23 08:50:45.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:50:45.307
Jan  5 08:50:45.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename resourcequota 01/05/23 08:50:45.308
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:45.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:45.321
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/05/23 08:50:45.323
STEP: Creating a ResourceQuota 01/05/23 08:50:50.325
STEP: Ensuring resource quota status is calculated 01/05/23 08:50:50.328
STEP: Creating a ReplicaSet 01/05/23 08:50:52.332
STEP: Ensuring resource quota status captures replicaset creation 01/05/23 08:50:52.346
STEP: Deleting a ReplicaSet 01/05/23 08:50:54.35
STEP: Ensuring resource quota status released usage 01/05/23 08:50:54.355
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 08:50:56.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1535" for this suite. 01/05/23 08:50:56.361
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":314,"skipped":5941,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.059 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:50:45.307
    Jan  5 08:50:45.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename resourcequota 01/05/23 08:50:45.308
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:45.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:45.321
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/05/23 08:50:45.323
    STEP: Creating a ResourceQuota 01/05/23 08:50:50.325
    STEP: Ensuring resource quota status is calculated 01/05/23 08:50:50.328
    STEP: Creating a ReplicaSet 01/05/23 08:50:52.332
    STEP: Ensuring resource quota status captures replicaset creation 01/05/23 08:50:52.346
    STEP: Deleting a ReplicaSet 01/05/23 08:50:54.35
    STEP: Ensuring resource quota status released usage 01/05/23 08:50:54.355
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 08:50:56.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1535" for this suite. 01/05/23 08:50:56.361
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:50:56.367
Jan  5 08:50:56.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pods 01/05/23 08:50:56.367
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:56.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:56.393
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan  5 08:50:56.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: creating the pod 01/05/23 08:50:56.397
STEP: submitting the pod to kubernetes 01/05/23 08:50:56.397
Jan  5 08:50:56.405: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9" in namespace "pods-4800" to be "running and ready"
Jan  5 08:50:56.408: INFO: Pod "pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.550748ms
Jan  5 08:50:56.408: INFO: The phase of Pod pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:50:58.411: INFO: Pod "pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00575899s
Jan  5 08:50:58.411: INFO: The phase of Pod pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:51:00.411: INFO: Pod "pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9": Phase="Running", Reason="", readiness=true. Elapsed: 4.006240566s
Jan  5 08:51:00.411: INFO: The phase of Pod pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9 is Running (Ready = true)
Jan  5 08:51:00.411: INFO: Pod "pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 08:51:00.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4800" for this suite. 01/05/23 08:51:00.425
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":315,"skipped":5943,"failed":0}
------------------------------
â€¢ [4.061 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:50:56.367
    Jan  5 08:50:56.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pods 01/05/23 08:50:56.367
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:50:56.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:50:56.393
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan  5 08:50:56.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: creating the pod 01/05/23 08:50:56.397
    STEP: submitting the pod to kubernetes 01/05/23 08:50:56.397
    Jan  5 08:50:56.405: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9" in namespace "pods-4800" to be "running and ready"
    Jan  5 08:50:56.408: INFO: Pod "pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.550748ms
    Jan  5 08:50:56.408: INFO: The phase of Pod pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:50:58.411: INFO: Pod "pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00575899s
    Jan  5 08:50:58.411: INFO: The phase of Pod pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:51:00.411: INFO: Pod "pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9": Phase="Running", Reason="", readiness=true. Elapsed: 4.006240566s
    Jan  5 08:51:00.411: INFO: The phase of Pod pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9 is Running (Ready = true)
    Jan  5 08:51:00.411: INFO: Pod "pod-logs-websocket-a9c08e00-016f-4556-80da-1c1b8050e6f9" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 08:51:00.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4800" for this suite. 01/05/23 08:51:00.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:51:00.429
Jan  5 08:51:00.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 08:51:00.43
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:00.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:00.46
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-e78ce49b-e117-4aa3-a1f4-a51477efd752 01/05/23 08:51:00.462
STEP: Creating a pod to test consume configMaps 01/05/23 08:51:00.466
Jan  5 08:51:00.477: INFO: Waiting up to 5m0s for pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb" in namespace "configmap-4199" to be "Succeeded or Failed"
Jan  5 08:51:00.480: INFO: Pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315662ms
Jan  5 08:51:02.482: INFO: Pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004599336s
Jan  5 08:51:04.483: INFO: Pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005375227s
Jan  5 08:51:06.484: INFO: Pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006492477s
STEP: Saw pod success 01/05/23 08:51:06.484
Jan  5 08:51:06.484: INFO: Pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb" satisfied condition "Succeeded or Failed"
Jan  5 08:51:06.487: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:51:06.491
Jan  5 08:51:06.507: INFO: Waiting for pod pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb to disappear
Jan  5 08:51:06.509: INFO: Pod pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 08:51:06.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4199" for this suite. 01/05/23 08:51:06.511
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":316,"skipped":5985,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.087 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:51:00.429
    Jan  5 08:51:00.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 08:51:00.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:00.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:00.46
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-e78ce49b-e117-4aa3-a1f4-a51477efd752 01/05/23 08:51:00.462
    STEP: Creating a pod to test consume configMaps 01/05/23 08:51:00.466
    Jan  5 08:51:00.477: INFO: Waiting up to 5m0s for pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb" in namespace "configmap-4199" to be "Succeeded or Failed"
    Jan  5 08:51:00.480: INFO: Pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315662ms
    Jan  5 08:51:02.482: INFO: Pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004599336s
    Jan  5 08:51:04.483: INFO: Pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005375227s
    Jan  5 08:51:06.484: INFO: Pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006492477s
    STEP: Saw pod success 01/05/23 08:51:06.484
    Jan  5 08:51:06.484: INFO: Pod "pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb" satisfied condition "Succeeded or Failed"
    Jan  5 08:51:06.487: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:51:06.491
    Jan  5 08:51:06.507: INFO: Waiting for pod pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb to disappear
    Jan  5 08:51:06.509: INFO: Pod pod-configmaps-24127271-2834-4a1d-9d26-a8bf3c6bbaeb no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 08:51:06.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4199" for this suite. 01/05/23 08:51:06.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:51:06.517
Jan  5 08:51:06.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:51:06.518
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:06.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:06.537
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/05/23 08:51:06.541
Jan  5 08:51:06.541: INFO: Creating e2e-svc-a-655tk
Jan  5 08:51:06.557: INFO: Creating e2e-svc-b-8fm4q
Jan  5 08:51:06.570: INFO: Creating e2e-svc-c-vmgw4
STEP: deleting service collection 01/05/23 08:51:06.588
Jan  5 08:51:06.618: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:51:06.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3234" for this suite. 01/05/23 08:51:06.62
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":317,"skipped":6029,"failed":0}
------------------------------
â€¢ [0.114 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:51:06.517
    Jan  5 08:51:06.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:51:06.518
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:06.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:06.537
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/05/23 08:51:06.541
    Jan  5 08:51:06.541: INFO: Creating e2e-svc-a-655tk
    Jan  5 08:51:06.557: INFO: Creating e2e-svc-b-8fm4q
    Jan  5 08:51:06.570: INFO: Creating e2e-svc-c-vmgw4
    STEP: deleting service collection 01/05/23 08:51:06.588
    Jan  5 08:51:06.618: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:51:06.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3234" for this suite. 01/05/23 08:51:06.62
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:51:06.632
Jan  5 08:51:06.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename init-container 01/05/23 08:51:06.633
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:06.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:06.647
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/05/23 08:51:06.648
Jan  5 08:51:06.648: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  5 08:51:10.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8974" for this suite. 01/05/23 08:51:10.506
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":318,"skipped":6063,"failed":0}
------------------------------
â€¢ [3.892 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:51:06.632
    Jan  5 08:51:06.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename init-container 01/05/23 08:51:06.633
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:06.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:06.647
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/05/23 08:51:06.648
    Jan  5 08:51:06.648: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  5 08:51:10.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-8974" for this suite. 01/05/23 08:51:10.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:51:10.524
Jan  5 08:51:10.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename aggregator 01/05/23 08:51:10.525
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:10.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:10.539
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan  5 08:51:10.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/05/23 08:51:10.548
Jan  5 08:51:11.084: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan  5 08:51:13.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:15.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:17.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:19.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:21.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:23.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:25.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:27.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:29.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:31.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:33.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:35.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:51:37.254: INFO: Waited 118.407875ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/05/23 08:51:37.307
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/05/23 08:51:37.309
STEP: List APIServices 01/05/23 08:51:37.314
Jan  5 08:51:37.320: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan  5 08:51:37.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2603" for this suite. 01/05/23 08:51:37.505
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":319,"skipped":6069,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.023 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:51:10.524
    Jan  5 08:51:10.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename aggregator 01/05/23 08:51:10.525
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:10.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:10.539
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan  5 08:51:10.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/05/23 08:51:10.548
    Jan  5 08:51:11.084: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan  5 08:51:13.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:15.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:17.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:19.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:21.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:23.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:25.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:27.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:29.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:31.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:33.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:35.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:51:37.254: INFO: Waited 118.407875ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/05/23 08:51:37.307
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/05/23 08:51:37.309
    STEP: List APIServices 01/05/23 08:51:37.314
    Jan  5 08:51:37.320: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan  5 08:51:37.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-2603" for this suite. 01/05/23 08:51:37.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:51:37.547
Jan  5 08:51:37.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename gc 01/05/23 08:51:37.548
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:37.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:37.571
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan  5 08:51:37.635: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"73025dc9-2719-471d-b17f-dfd8c166f7f8", Controller:(*bool)(0xc000d8eefe), BlockOwnerDeletion:(*bool)(0xc000d8eeff)}}
Jan  5 08:51:37.645: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8eeb17ba-3f56-4f74-8946-45d10fdb3a6f", Controller:(*bool)(0xc000d8f546), BlockOwnerDeletion:(*bool)(0xc000d8f547)}}
Jan  5 08:51:37.656: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1c6ace26-289d-40d1-aa41-5ab46f37fb15", Controller:(*bool)(0xc000d8faee), BlockOwnerDeletion:(*bool)(0xc000d8faef)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 08:51:42.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2129" for this suite. 01/05/23 08:51:42.671
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":320,"skipped":6074,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.132 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:51:37.547
    Jan  5 08:51:37.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename gc 01/05/23 08:51:37.548
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:37.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:37.571
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan  5 08:51:37.635: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"73025dc9-2719-471d-b17f-dfd8c166f7f8", Controller:(*bool)(0xc000d8eefe), BlockOwnerDeletion:(*bool)(0xc000d8eeff)}}
    Jan  5 08:51:37.645: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8eeb17ba-3f56-4f74-8946-45d10fdb3a6f", Controller:(*bool)(0xc000d8f546), BlockOwnerDeletion:(*bool)(0xc000d8f547)}}
    Jan  5 08:51:37.656: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1c6ace26-289d-40d1-aa41-5ab46f37fb15", Controller:(*bool)(0xc000d8faee), BlockOwnerDeletion:(*bool)(0xc000d8faef)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 08:51:42.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2129" for this suite. 01/05/23 08:51:42.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:51:42.682
Jan  5 08:51:42.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename statefulset 01/05/23 08:51:42.683
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:42.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:42.709
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8883 01/05/23 08:51:42.712
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/05/23 08:51:42.716
Jan  5 08:51:42.729: INFO: Found 0 stateful pods, waiting for 3
Jan  5 08:51:52.732: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 08:51:52.732: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 08:51:52.732: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 08:51:52.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-8883 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 08:51:52.905: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 08:51:52.905: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 08:51:52.905: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/05/23 08:52:02.919
Jan  5 08:52:02.936: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/05/23 08:52:02.936
STEP: Updating Pods in reverse ordinal order 01/05/23 08:52:12.955
Jan  5 08:52:12.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-8883 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 08:52:13.076: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 08:52:13.076: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 08:52:13.076: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 01/05/23 08:52:23.092
Jan  5 08:52:23.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-8883 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 08:52:23.227: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 08:52:23.227: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 08:52:23.227: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 08:52:33.259: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/05/23 08:52:43.28
Jan  5 08:52:43.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-8883 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 08:52:43.409: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 08:52:43.409: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 08:52:43.409: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  5 08:52:53.423: INFO: Deleting all statefulset in ns statefulset-8883
Jan  5 08:52:53.425: INFO: Scaling statefulset ss2 to 0
Jan  5 08:53:03.434: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 08:53:03.436: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  5 08:53:03.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8883" for this suite. 01/05/23 08:53:03.448
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":321,"skipped":6146,"failed":0}
------------------------------
â€¢ [SLOW TEST] [80.777 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:51:42.682
    Jan  5 08:51:42.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename statefulset 01/05/23 08:51:42.683
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:51:42.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:51:42.709
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8883 01/05/23 08:51:42.712
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/05/23 08:51:42.716
    Jan  5 08:51:42.729: INFO: Found 0 stateful pods, waiting for 3
    Jan  5 08:51:52.732: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 08:51:52.732: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 08:51:52.732: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 08:51:52.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-8883 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 08:51:52.905: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 08:51:52.905: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 08:51:52.905: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/05/23 08:52:02.919
    Jan  5 08:52:02.936: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/05/23 08:52:02.936
    STEP: Updating Pods in reverse ordinal order 01/05/23 08:52:12.955
    Jan  5 08:52:12.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-8883 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 08:52:13.076: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 08:52:13.076: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 08:52:13.076: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 01/05/23 08:52:23.092
    Jan  5 08:52:23.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-8883 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 08:52:23.227: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 08:52:23.227: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 08:52:23.227: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 08:52:33.259: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/05/23 08:52:43.28
    Jan  5 08:52:43.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=statefulset-8883 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 08:52:43.409: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 08:52:43.409: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 08:52:43.409: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  5 08:52:53.423: INFO: Deleting all statefulset in ns statefulset-8883
    Jan  5 08:52:53.425: INFO: Scaling statefulset ss2 to 0
    Jan  5 08:53:03.434: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 08:53:03.436: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  5 08:53:03.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8883" for this suite. 01/05/23 08:53:03.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:53:03.461
Jan  5 08:53:03.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 08:53:03.461
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:03.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:03.476
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan  5 08:53:03.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:53:06.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3169" for this suite. 01/05/23 08:53:06.69
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":322,"skipped":6166,"failed":0}
------------------------------
â€¢ [3.234 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:53:03.461
    Jan  5 08:53:03.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 08:53:03.461
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:03.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:03.476
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan  5 08:53:03.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:53:06.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3169" for this suite. 01/05/23 08:53:06.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:53:06.694
Jan  5 08:53:06.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:53:06.695
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:06.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:06.715
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3630 01/05/23 08:53:06.717
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 08:53:06.733
STEP: creating service externalsvc in namespace services-3630 01/05/23 08:53:06.733
STEP: creating replication controller externalsvc in namespace services-3630 01/05/23 08:53:06.75
I0105 08:53:06.762373      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3630, replica count: 2
I0105 08:53:09.812553      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/05/23 08:53:09.814
Jan  5 08:53:09.831: INFO: Creating new exec pod
Jan  5 08:53:09.844: INFO: Waiting up to 5m0s for pod "execpodls4kc" in namespace "services-3630" to be "running"
Jan  5 08:53:09.859: INFO: Pod "execpodls4kc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.986298ms
Jan  5 08:53:11.863: INFO: Pod "execpodls4kc": Phase="Running", Reason="", readiness=true. Elapsed: 2.018425776s
Jan  5 08:53:11.863: INFO: Pod "execpodls4kc" satisfied condition "running"
Jan  5 08:53:11.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3630 exec execpodls4kc -- /bin/sh -x -c nslookup clusterip-service.services-3630.svc.cluster.local'
Jan  5 08:53:12.010: INFO: stderr: "+ nslookup clusterip-service.services-3630.svc.cluster.local\n"
Jan  5 08:53:12.010: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3630.svc.cluster.local\tcanonical name = externalsvc.services-3630.svc.cluster.local.\nName:\texternalsvc.services-3630.svc.cluster.local\nAddress: 10.107.36.122\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3630, will wait for the garbage collector to delete the pods 01/05/23 08:53:12.01
Jan  5 08:53:12.067: INFO: Deleting ReplicationController externalsvc took: 4.249674ms
Jan  5 08:53:12.168: INFO: Terminating ReplicationController externalsvc pods took: 100.991331ms
Jan  5 08:53:13.895: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:53:13.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3630" for this suite. 01/05/23 08:53:13.908
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":323,"skipped":6172,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.225 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:53:06.694
    Jan  5 08:53:06.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:53:06.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:06.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:06.715
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3630 01/05/23 08:53:06.717
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 08:53:06.733
    STEP: creating service externalsvc in namespace services-3630 01/05/23 08:53:06.733
    STEP: creating replication controller externalsvc in namespace services-3630 01/05/23 08:53:06.75
    I0105 08:53:06.762373      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3630, replica count: 2
    I0105 08:53:09.812553      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/05/23 08:53:09.814
    Jan  5 08:53:09.831: INFO: Creating new exec pod
    Jan  5 08:53:09.844: INFO: Waiting up to 5m0s for pod "execpodls4kc" in namespace "services-3630" to be "running"
    Jan  5 08:53:09.859: INFO: Pod "execpodls4kc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.986298ms
    Jan  5 08:53:11.863: INFO: Pod "execpodls4kc": Phase="Running", Reason="", readiness=true. Elapsed: 2.018425776s
    Jan  5 08:53:11.863: INFO: Pod "execpodls4kc" satisfied condition "running"
    Jan  5 08:53:11.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-3630 exec execpodls4kc -- /bin/sh -x -c nslookup clusterip-service.services-3630.svc.cluster.local'
    Jan  5 08:53:12.010: INFO: stderr: "+ nslookup clusterip-service.services-3630.svc.cluster.local\n"
    Jan  5 08:53:12.010: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3630.svc.cluster.local\tcanonical name = externalsvc.services-3630.svc.cluster.local.\nName:\texternalsvc.services-3630.svc.cluster.local\nAddress: 10.107.36.122\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3630, will wait for the garbage collector to delete the pods 01/05/23 08:53:12.01
    Jan  5 08:53:12.067: INFO: Deleting ReplicationController externalsvc took: 4.249674ms
    Jan  5 08:53:12.168: INFO: Terminating ReplicationController externalsvc pods took: 100.991331ms
    Jan  5 08:53:13.895: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:53:13.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3630" for this suite. 01/05/23 08:53:13.908
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:53:13.92
Jan  5 08:53:13.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:53:13.92
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:13.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:13.942
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:53:13.972
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:53:14.691
STEP: Deploying the webhook pod 01/05/23 08:53:14.697
STEP: Wait for the deployment to be ready 01/05/23 08:53:14.711
Jan  5 08:53:14.714: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan  5 08:53:16.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 53, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 53, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 53, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 53, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/05/23 08:53:18.724
STEP: Verifying the service has paired with the endpoint 01/05/23 08:53:18.737
Jan  5 08:53:19.737: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/05/23 08:53:19.74
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 08:53:19.75
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/05/23 08:53:19.756
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 08:53:19.766
STEP: Patching a validating webhook configuration's rules to include the create operation 01/05/23 08:53:19.78
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 08:53:19.785
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:53:19.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1919" for this suite. 01/05/23 08:53:20.002
STEP: Destroying namespace "webhook-1919-markers" for this suite. 01/05/23 08:53:20.014
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":324,"skipped":6178,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.160 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:53:13.92
    Jan  5 08:53:13.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:53:13.92
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:13.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:13.942
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:53:13.972
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:53:14.691
    STEP: Deploying the webhook pod 01/05/23 08:53:14.697
    STEP: Wait for the deployment to be ready 01/05/23 08:53:14.711
    Jan  5 08:53:14.714: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan  5 08:53:16.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 53, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 53, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 53, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 53, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/05/23 08:53:18.724
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:53:18.737
    Jan  5 08:53:19.737: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/05/23 08:53:19.74
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 08:53:19.75
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/05/23 08:53:19.756
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 08:53:19.766
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/05/23 08:53:19.78
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 08:53:19.785
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:53:19.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1919" for this suite. 01/05/23 08:53:20.002
    STEP: Destroying namespace "webhook-1919-markers" for this suite. 01/05/23 08:53:20.014
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:53:20.08
Jan  5 08:53:20.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename dns 01/05/23 08:53:20.081
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:20.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:20.104
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3872.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3872.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/05/23 08:53:20.105
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3872.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3872.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/05/23 08:53:20.105
STEP: creating a pod to probe /etc/hosts 01/05/23 08:53:20.105
STEP: submitting the pod to kubernetes 01/05/23 08:53:20.105
Jan  5 08:53:20.114: INFO: Waiting up to 15m0s for pod "dns-test-8a730a00-8a52-4d25-8553-94917413eb57" in namespace "dns-3872" to be "running"
Jan  5 08:53:20.116: INFO: Pod "dns-test-8a730a00-8a52-4d25-8553-94917413eb57": Phase="Pending", Reason="", readiness=false. Elapsed: 1.319191ms
Jan  5 08:53:22.120: INFO: Pod "dns-test-8a730a00-8a52-4d25-8553-94917413eb57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005773138s
Jan  5 08:53:24.119: INFO: Pod "dns-test-8a730a00-8a52-4d25-8553-94917413eb57": Phase="Running", Reason="", readiness=true. Elapsed: 4.004636444s
Jan  5 08:53:24.119: INFO: Pod "dns-test-8a730a00-8a52-4d25-8553-94917413eb57" satisfied condition "running"
STEP: retrieving the pod 01/05/23 08:53:24.119
STEP: looking for the results for each expected name from probers 01/05/23 08:53:24.121
Jan  5 08:53:24.131: INFO: DNS probes using dns-3872/dns-test-8a730a00-8a52-4d25-8553-94917413eb57 succeeded

STEP: deleting the pod 01/05/23 08:53:24.131
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 08:53:24.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3872" for this suite. 01/05/23 08:53:24.155
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":325,"skipped":6196,"failed":0}
------------------------------
â€¢ [4.083 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:53:20.08
    Jan  5 08:53:20.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename dns 01/05/23 08:53:20.081
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:20.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:20.104
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3872.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3872.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/05/23 08:53:20.105
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3872.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3872.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/05/23 08:53:20.105
    STEP: creating a pod to probe /etc/hosts 01/05/23 08:53:20.105
    STEP: submitting the pod to kubernetes 01/05/23 08:53:20.105
    Jan  5 08:53:20.114: INFO: Waiting up to 15m0s for pod "dns-test-8a730a00-8a52-4d25-8553-94917413eb57" in namespace "dns-3872" to be "running"
    Jan  5 08:53:20.116: INFO: Pod "dns-test-8a730a00-8a52-4d25-8553-94917413eb57": Phase="Pending", Reason="", readiness=false. Elapsed: 1.319191ms
    Jan  5 08:53:22.120: INFO: Pod "dns-test-8a730a00-8a52-4d25-8553-94917413eb57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005773138s
    Jan  5 08:53:24.119: INFO: Pod "dns-test-8a730a00-8a52-4d25-8553-94917413eb57": Phase="Running", Reason="", readiness=true. Elapsed: 4.004636444s
    Jan  5 08:53:24.119: INFO: Pod "dns-test-8a730a00-8a52-4d25-8553-94917413eb57" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 08:53:24.119
    STEP: looking for the results for each expected name from probers 01/05/23 08:53:24.121
    Jan  5 08:53:24.131: INFO: DNS probes using dns-3872/dns-test-8a730a00-8a52-4d25-8553-94917413eb57 succeeded

    STEP: deleting the pod 01/05/23 08:53:24.131
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 08:53:24.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3872" for this suite. 01/05/23 08:53:24.155
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:53:24.164
Jan  5 08:53:24.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 08:53:24.166
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:24.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:24.183
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-9916/configmap-test-a6cb5776-09a0-49e7-9bd3-66459220efb1 01/05/23 08:53:24.185
STEP: Creating a pod to test consume configMaps 01/05/23 08:53:24.203
Jan  5 08:53:24.209: INFO: Waiting up to 5m0s for pod "pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94" in namespace "configmap-9916" to be "Succeeded or Failed"
Jan  5 08:53:24.211: INFO: Pod "pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.494658ms
Jan  5 08:53:26.214: INFO: Pod "pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005299524s
Jan  5 08:53:28.216: INFO: Pod "pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007335956s
STEP: Saw pod success 01/05/23 08:53:28.216
Jan  5 08:53:28.216: INFO: Pod "pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94" satisfied condition "Succeeded or Failed"
Jan  5 08:53:28.218: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94 container env-test: <nil>
STEP: delete the pod 01/05/23 08:53:28.229
Jan  5 08:53:28.243: INFO: Waiting for pod pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94 to disappear
Jan  5 08:53:28.245: INFO: Pod pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 08:53:28.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9916" for this suite. 01/05/23 08:53:28.248
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":326,"skipped":6200,"failed":0}
------------------------------
â€¢ [4.096 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:53:24.164
    Jan  5 08:53:24.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 08:53:24.166
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:24.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:24.183
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-9916/configmap-test-a6cb5776-09a0-49e7-9bd3-66459220efb1 01/05/23 08:53:24.185
    STEP: Creating a pod to test consume configMaps 01/05/23 08:53:24.203
    Jan  5 08:53:24.209: INFO: Waiting up to 5m0s for pod "pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94" in namespace "configmap-9916" to be "Succeeded or Failed"
    Jan  5 08:53:24.211: INFO: Pod "pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.494658ms
    Jan  5 08:53:26.214: INFO: Pod "pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005299524s
    Jan  5 08:53:28.216: INFO: Pod "pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007335956s
    STEP: Saw pod success 01/05/23 08:53:28.216
    Jan  5 08:53:28.216: INFO: Pod "pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94" satisfied condition "Succeeded or Failed"
    Jan  5 08:53:28.218: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94 container env-test: <nil>
    STEP: delete the pod 01/05/23 08:53:28.229
    Jan  5 08:53:28.243: INFO: Waiting for pod pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94 to disappear
    Jan  5 08:53:28.245: INFO: Pod pod-configmaps-9819d172-159e-4ca9-bd23-b55bdb191b94 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 08:53:28.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9916" for this suite. 01/05/23 08:53:28.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:53:28.262
Jan  5 08:53:28.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sched-preemption 01/05/23 08:53:28.263
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:28.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:28.279
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  5 08:53:28.301: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 08:54:28.323: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:54:28.325
Jan  5 08:54:28.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 08:54:28.326
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:54:28.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:54:28.345
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan  5 08:54:28.365: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan  5 08:54:28.367: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan  5 08:54:28.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2759" for this suite. 01/05/23 08:54:28.397
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:54:28.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4377" for this suite. 01/05/23 08:54:28.427
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":327,"skipped":6232,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.212 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:53:28.262
    Jan  5 08:53:28.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 08:53:28.263
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:53:28.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:53:28.279
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  5 08:53:28.301: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 08:54:28.323: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:54:28.325
    Jan  5 08:54:28.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 08:54:28.326
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:54:28.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:54:28.345
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan  5 08:54:28.365: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan  5 08:54:28.367: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan  5 08:54:28.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-2759" for this suite. 01/05/23 08:54:28.397
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:54:28.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4377" for this suite. 01/05/23 08:54:28.427
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:54:28.474
Jan  5 08:54:28.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:54:28.475
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:54:28.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:54:28.503
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:54:28.532
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:54:28.859
STEP: Deploying the webhook pod 01/05/23 08:54:28.868
STEP: Wait for the deployment to be ready 01/05/23 08:54:28.882
Jan  5 08:54:28.898: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  5 08:54:30.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 54, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/05/23 08:54:32.911
STEP: Verifying the service has paired with the endpoint 01/05/23 08:54:32.929
Jan  5 08:54:33.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/05/23 08:54:33.932
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/05/23 08:54:33.933
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 08:54:33.933
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/05/23 08:54:33.933
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/05/23 08:54:33.934
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 08:54:33.934
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 08:54:33.934
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:54:33.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6526" for this suite. 01/05/23 08:54:33.936
STEP: Destroying namespace "webhook-6526-markers" for this suite. 01/05/23 08:54:33.94
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":328,"skipped":6241,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.520 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:54:28.474
    Jan  5 08:54:28.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:54:28.475
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:54:28.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:54:28.503
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:54:28.532
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:54:28.859
    STEP: Deploying the webhook pod 01/05/23 08:54:28.868
    STEP: Wait for the deployment to be ready 01/05/23 08:54:28.882
    Jan  5 08:54:28.898: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  5 08:54:30.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 8, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 54, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/05/23 08:54:32.911
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:54:32.929
    Jan  5 08:54:33.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/05/23 08:54:33.932
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/05/23 08:54:33.933
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 08:54:33.933
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/05/23 08:54:33.933
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/05/23 08:54:33.934
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 08:54:33.934
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 08:54:33.934
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:54:33.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6526" for this suite. 01/05/23 08:54:33.936
    STEP: Destroying namespace "webhook-6526-markers" for this suite. 01/05/23 08:54:33.94
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:54:33.995
Jan  5 08:54:33.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename taint-single-pod 01/05/23 08:54:33.996
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:54:34.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:54:34.023
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan  5 08:54:34.025: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 08:55:34.034: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jan  5 08:55:34.036: INFO: Starting informer...
STEP: Starting pod... 01/05/23 08:55:34.036
Jan  5 08:55:34.246: INFO: Pod is running on mip-bd-vm724.mip.storage.hpecorp.net. Tainting Node
STEP: Trying to apply a taint on the Node 01/05/23 08:55:34.246
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 08:55:34.258
STEP: Waiting short time to make sure Pod is queued for deletion 01/05/23 08:55:34.261
Jan  5 08:55:34.261: INFO: Pod wasn't evicted. Proceeding
Jan  5 08:55:34.261: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 08:55:34.278
STEP: Waiting some time to make sure that toleration time passed. 01/05/23 08:55:34.28
Jan  5 08:56:49.281: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:56:49.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7118" for this suite. 01/05/23 08:56:49.284
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":329,"skipped":6252,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.294 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:54:33.995
    Jan  5 08:54:33.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename taint-single-pod 01/05/23 08:54:33.996
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:54:34.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:54:34.023
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jan  5 08:54:34.025: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 08:55:34.034: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jan  5 08:55:34.036: INFO: Starting informer...
    STEP: Starting pod... 01/05/23 08:55:34.036
    Jan  5 08:55:34.246: INFO: Pod is running on mip-bd-vm724.mip.storage.hpecorp.net. Tainting Node
    STEP: Trying to apply a taint on the Node 01/05/23 08:55:34.246
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 08:55:34.258
    STEP: Waiting short time to make sure Pod is queued for deletion 01/05/23 08:55:34.261
    Jan  5 08:55:34.261: INFO: Pod wasn't evicted. Proceeding
    Jan  5 08:55:34.261: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 08:55:34.278
    STEP: Waiting some time to make sure that toleration time passed. 01/05/23 08:55:34.28
    Jan  5 08:56:49.281: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:56:49.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-7118" for this suite. 01/05/23 08:56:49.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:56:49.292
Jan  5 08:56:49.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 08:56:49.293
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:56:49.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:56:49.319
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-56d5ff95-7ff6-4b38-98bb-4db7c31dc32e 01/05/23 08:56:49.322
STEP: Creating a pod to test consume configMaps 01/05/23 08:56:49.327
Jan  5 08:56:49.341: INFO: Waiting up to 5m0s for pod "pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184" in namespace "configmap-50" to be "Succeeded or Failed"
Jan  5 08:56:49.342: INFO: Pod "pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184": Phase="Pending", Reason="", readiness=false. Elapsed: 1.737545ms
Jan  5 08:56:51.347: INFO: Pod "pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006042182s
Jan  5 08:56:53.347: INFO: Pod "pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006206242s
STEP: Saw pod success 01/05/23 08:56:53.347
Jan  5 08:56:53.347: INFO: Pod "pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184" satisfied condition "Succeeded or Failed"
Jan  5 08:56:53.349: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:56:53.365
Jan  5 08:56:53.375: INFO: Waiting for pod pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184 to disappear
Jan  5 08:56:53.377: INFO: Pod pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 08:56:53.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-50" for this suite. 01/05/23 08:56:53.38
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":330,"skipped":6296,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:56:49.292
    Jan  5 08:56:49.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 08:56:49.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:56:49.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:56:49.319
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-56d5ff95-7ff6-4b38-98bb-4db7c31dc32e 01/05/23 08:56:49.322
    STEP: Creating a pod to test consume configMaps 01/05/23 08:56:49.327
    Jan  5 08:56:49.341: INFO: Waiting up to 5m0s for pod "pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184" in namespace "configmap-50" to be "Succeeded or Failed"
    Jan  5 08:56:49.342: INFO: Pod "pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184": Phase="Pending", Reason="", readiness=false. Elapsed: 1.737545ms
    Jan  5 08:56:51.347: INFO: Pod "pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006042182s
    Jan  5 08:56:53.347: INFO: Pod "pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006206242s
    STEP: Saw pod success 01/05/23 08:56:53.347
    Jan  5 08:56:53.347: INFO: Pod "pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184" satisfied condition "Succeeded or Failed"
    Jan  5 08:56:53.349: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:56:53.365
    Jan  5 08:56:53.375: INFO: Waiting for pod pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184 to disappear
    Jan  5 08:56:53.377: INFO: Pod pod-configmaps-fba77459-cc98-40dd-9bdd-d5f1170bb184 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 08:56:53.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-50" for this suite. 01/05/23 08:56:53.38
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:56:53.39
Jan  5 08:56:53.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename dns 01/05/23 08:56:53.391
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:56:53.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:56:53.408
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/05/23 08:56:53.411
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
 01/05/23 08:56:53.434
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
 01/05/23 08:56:53.434
STEP: creating a pod to probe DNS 01/05/23 08:56:53.434
STEP: submitting the pod to kubernetes 01/05/23 08:56:53.435
Jan  5 08:56:53.442: INFO: Waiting up to 15m0s for pod "dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe" in namespace "dns-1607" to be "running"
Jan  5 08:56:53.444: INFO: Pod "dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.283587ms
Jan  5 08:56:55.448: INFO: Pod "dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006616841s
Jan  5 08:56:57.448: INFO: Pod "dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe": Phase="Running", Reason="", readiness=true. Elapsed: 4.005926021s
Jan  5 08:56:57.448: INFO: Pod "dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe" satisfied condition "running"
STEP: retrieving the pod 01/05/23 08:56:57.448
STEP: looking for the results for each expected name from probers 01/05/23 08:56:57.45
Jan  5 08:56:57.457: INFO: DNS probes using dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe succeeded

STEP: deleting the pod 01/05/23 08:56:57.457
STEP: changing the externalName to bar.example.com 01/05/23 08:56:57.484
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
 01/05/23 08:56:57.499
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
 01/05/23 08:56:57.499
STEP: creating a second pod to probe DNS 01/05/23 08:56:57.499
STEP: submitting the pod to kubernetes 01/05/23 08:56:57.499
Jan  5 08:56:57.505: INFO: Waiting up to 15m0s for pod "dns-test-46677809-4f18-4c95-93af-d665713f34f8" in namespace "dns-1607" to be "running"
Jan  5 08:56:57.508: INFO: Pod "dns-test-46677809-4f18-4c95-93af-d665713f34f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992433ms
Jan  5 08:56:59.510: INFO: Pod "dns-test-46677809-4f18-4c95-93af-d665713f34f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005678569s
Jan  5 08:57:01.511: INFO: Pod "dns-test-46677809-4f18-4c95-93af-d665713f34f8": Phase="Running", Reason="", readiness=true. Elapsed: 4.005880218s
Jan  5 08:57:01.511: INFO: Pod "dns-test-46677809-4f18-4c95-93af-d665713f34f8" satisfied condition "running"
STEP: retrieving the pod 01/05/23 08:57:01.511
STEP: looking for the results for each expected name from probers 01/05/23 08:57:01.513
Jan  5 08:57:01.516: INFO: File wheezy_udp@dns-test-service-3.dns-1607.svc.cluster.local from pod  dns-1607/dns-test-46677809-4f18-4c95-93af-d665713f34f8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 08:57:01.518: INFO: File jessie_udp@dns-test-service-3.dns-1607.svc.cluster.local from pod  dns-1607/dns-test-46677809-4f18-4c95-93af-d665713f34f8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  5 08:57:01.518: INFO: Lookups using dns-1607/dns-test-46677809-4f18-4c95-93af-d665713f34f8 failed for: [wheezy_udp@dns-test-service-3.dns-1607.svc.cluster.local jessie_udp@dns-test-service-3.dns-1607.svc.cluster.local]

Jan  5 08:57:06.526: INFO: DNS probes using dns-test-46677809-4f18-4c95-93af-d665713f34f8 succeeded

STEP: deleting the pod 01/05/23 08:57:06.526
STEP: changing the service to type=ClusterIP 01/05/23 08:57:06.541
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
 01/05/23 08:57:06.563
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
 01/05/23 08:57:06.564
STEP: creating a third pod to probe DNS 01/05/23 08:57:06.564
STEP: submitting the pod to kubernetes 01/05/23 08:57:06.574
Jan  5 08:57:06.579: INFO: Waiting up to 15m0s for pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051" in namespace "dns-1607" to be "running"
Jan  5 08:57:06.581: INFO: Pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.169331ms
Jan  5 08:57:08.585: INFO: Pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006154188s
Jan  5 08:57:10.584: INFO: Pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005696393s
Jan  5 08:57:12.584: INFO: Pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051": Phase="Running", Reason="", readiness=true. Elapsed: 6.005787223s
Jan  5 08:57:12.584: INFO: Pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051" satisfied condition "running"
STEP: retrieving the pod 01/05/23 08:57:12.584
STEP: looking for the results for each expected name from probers 01/05/23 08:57:12.587
Jan  5 08:57:12.592: INFO: DNS probes using dns-test-aec8942e-de8c-459c-9bec-49770ae6a051 succeeded

STEP: deleting the pod 01/05/23 08:57:12.592
STEP: deleting the test externalName service 01/05/23 08:57:12.62
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 08:57:12.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1607" for this suite. 01/05/23 08:57:12.66
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":331,"skipped":6298,"failed":0}
------------------------------
â€¢ [SLOW TEST] [19.274 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:56:53.39
    Jan  5 08:56:53.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename dns 01/05/23 08:56:53.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:56:53.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:56:53.408
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/05/23 08:56:53.411
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
     01/05/23 08:56:53.434
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
     01/05/23 08:56:53.434
    STEP: creating a pod to probe DNS 01/05/23 08:56:53.434
    STEP: submitting the pod to kubernetes 01/05/23 08:56:53.435
    Jan  5 08:56:53.442: INFO: Waiting up to 15m0s for pod "dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe" in namespace "dns-1607" to be "running"
    Jan  5 08:56:53.444: INFO: Pod "dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.283587ms
    Jan  5 08:56:55.448: INFO: Pod "dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006616841s
    Jan  5 08:56:57.448: INFO: Pod "dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe": Phase="Running", Reason="", readiness=true. Elapsed: 4.005926021s
    Jan  5 08:56:57.448: INFO: Pod "dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 08:56:57.448
    STEP: looking for the results for each expected name from probers 01/05/23 08:56:57.45
    Jan  5 08:56:57.457: INFO: DNS probes using dns-test-1748631b-cf1d-43d9-bcae-a929c07f63fe succeeded

    STEP: deleting the pod 01/05/23 08:56:57.457
    STEP: changing the externalName to bar.example.com 01/05/23 08:56:57.484
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
     01/05/23 08:56:57.499
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
     01/05/23 08:56:57.499
    STEP: creating a second pod to probe DNS 01/05/23 08:56:57.499
    STEP: submitting the pod to kubernetes 01/05/23 08:56:57.499
    Jan  5 08:56:57.505: INFO: Waiting up to 15m0s for pod "dns-test-46677809-4f18-4c95-93af-d665713f34f8" in namespace "dns-1607" to be "running"
    Jan  5 08:56:57.508: INFO: Pod "dns-test-46677809-4f18-4c95-93af-d665713f34f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992433ms
    Jan  5 08:56:59.510: INFO: Pod "dns-test-46677809-4f18-4c95-93af-d665713f34f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005678569s
    Jan  5 08:57:01.511: INFO: Pod "dns-test-46677809-4f18-4c95-93af-d665713f34f8": Phase="Running", Reason="", readiness=true. Elapsed: 4.005880218s
    Jan  5 08:57:01.511: INFO: Pod "dns-test-46677809-4f18-4c95-93af-d665713f34f8" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 08:57:01.511
    STEP: looking for the results for each expected name from probers 01/05/23 08:57:01.513
    Jan  5 08:57:01.516: INFO: File wheezy_udp@dns-test-service-3.dns-1607.svc.cluster.local from pod  dns-1607/dns-test-46677809-4f18-4c95-93af-d665713f34f8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 08:57:01.518: INFO: File jessie_udp@dns-test-service-3.dns-1607.svc.cluster.local from pod  dns-1607/dns-test-46677809-4f18-4c95-93af-d665713f34f8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  5 08:57:01.518: INFO: Lookups using dns-1607/dns-test-46677809-4f18-4c95-93af-d665713f34f8 failed for: [wheezy_udp@dns-test-service-3.dns-1607.svc.cluster.local jessie_udp@dns-test-service-3.dns-1607.svc.cluster.local]

    Jan  5 08:57:06.526: INFO: DNS probes using dns-test-46677809-4f18-4c95-93af-d665713f34f8 succeeded

    STEP: deleting the pod 01/05/23 08:57:06.526
    STEP: changing the service to type=ClusterIP 01/05/23 08:57:06.541
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
     01/05/23 08:57:06.563
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1607.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1607.svc.cluster.local; sleep 1; done
     01/05/23 08:57:06.564
    STEP: creating a third pod to probe DNS 01/05/23 08:57:06.564
    STEP: submitting the pod to kubernetes 01/05/23 08:57:06.574
    Jan  5 08:57:06.579: INFO: Waiting up to 15m0s for pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051" in namespace "dns-1607" to be "running"
    Jan  5 08:57:06.581: INFO: Pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.169331ms
    Jan  5 08:57:08.585: INFO: Pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006154188s
    Jan  5 08:57:10.584: INFO: Pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005696393s
    Jan  5 08:57:12.584: INFO: Pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051": Phase="Running", Reason="", readiness=true. Elapsed: 6.005787223s
    Jan  5 08:57:12.584: INFO: Pod "dns-test-aec8942e-de8c-459c-9bec-49770ae6a051" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 08:57:12.584
    STEP: looking for the results for each expected name from probers 01/05/23 08:57:12.587
    Jan  5 08:57:12.592: INFO: DNS probes using dns-test-aec8942e-de8c-459c-9bec-49770ae6a051 succeeded

    STEP: deleting the pod 01/05/23 08:57:12.592
    STEP: deleting the test externalName service 01/05/23 08:57:12.62
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 08:57:12.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1607" for this suite. 01/05/23 08:57:12.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:57:12.664
Jan  5 08:57:12.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename emptydir 01/05/23 08:57:12.665
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:12.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:12.679
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 08:57:12.681
Jan  5 08:57:12.693: INFO: Waiting up to 5m0s for pod "pod-610df458-d00b-45a3-ae7c-da3168f1769b" in namespace "emptydir-20" to be "Succeeded or Failed"
Jan  5 08:57:12.695: INFO: Pod "pod-610df458-d00b-45a3-ae7c-da3168f1769b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.512579ms
Jan  5 08:57:14.698: INFO: Pod "pod-610df458-d00b-45a3-ae7c-da3168f1769b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004443838s
Jan  5 08:57:16.699: INFO: Pod "pod-610df458-d00b-45a3-ae7c-da3168f1769b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006031619s
STEP: Saw pod success 01/05/23 08:57:16.699
Jan  5 08:57:16.699: INFO: Pod "pod-610df458-d00b-45a3-ae7c-da3168f1769b" satisfied condition "Succeeded or Failed"
Jan  5 08:57:16.702: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-610df458-d00b-45a3-ae7c-da3168f1769b container test-container: <nil>
STEP: delete the pod 01/05/23 08:57:16.707
Jan  5 08:57:16.723: INFO: Waiting for pod pod-610df458-d00b-45a3-ae7c-da3168f1769b to disappear
Jan  5 08:57:16.725: INFO: Pod pod-610df458-d00b-45a3-ae7c-da3168f1769b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  5 08:57:16.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-20" for this suite. 01/05/23 08:57:16.727
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":332,"skipped":6312,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:57:12.664
    Jan  5 08:57:12.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename emptydir 01/05/23 08:57:12.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:12.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:12.679
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 08:57:12.681
    Jan  5 08:57:12.693: INFO: Waiting up to 5m0s for pod "pod-610df458-d00b-45a3-ae7c-da3168f1769b" in namespace "emptydir-20" to be "Succeeded or Failed"
    Jan  5 08:57:12.695: INFO: Pod "pod-610df458-d00b-45a3-ae7c-da3168f1769b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.512579ms
    Jan  5 08:57:14.698: INFO: Pod "pod-610df458-d00b-45a3-ae7c-da3168f1769b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004443838s
    Jan  5 08:57:16.699: INFO: Pod "pod-610df458-d00b-45a3-ae7c-da3168f1769b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006031619s
    STEP: Saw pod success 01/05/23 08:57:16.699
    Jan  5 08:57:16.699: INFO: Pod "pod-610df458-d00b-45a3-ae7c-da3168f1769b" satisfied condition "Succeeded or Failed"
    Jan  5 08:57:16.702: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-610df458-d00b-45a3-ae7c-da3168f1769b container test-container: <nil>
    STEP: delete the pod 01/05/23 08:57:16.707
    Jan  5 08:57:16.723: INFO: Waiting for pod pod-610df458-d00b-45a3-ae7c-da3168f1769b to disappear
    Jan  5 08:57:16.725: INFO: Pod pod-610df458-d00b-45a3-ae7c-da3168f1769b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  5 08:57:16.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-20" for this suite. 01/05/23 08:57:16.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:57:16.735
Jan  5 08:57:16.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-probe 01/05/23 08:57:16.736
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:16.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:16.769
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c in namespace container-probe-392 01/05/23 08:57:16.772
Jan  5 08:57:16.783: INFO: Waiting up to 5m0s for pod "liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c" in namespace "container-probe-392" to be "not pending"
Jan  5 08:57:16.785: INFO: Pod "liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076541ms
Jan  5 08:57:18.791: INFO: Pod "liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007810515s
Jan  5 08:57:18.791: INFO: Pod "liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c" satisfied condition "not pending"
Jan  5 08:57:18.791: INFO: Started pod liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c in namespace container-probe-392
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:57:18.791
Jan  5 08:57:18.793: INFO: Initial restart count of pod liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c is 0
Jan  5 08:57:38.833: INFO: Restart count of pod container-probe-392/liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c is now 1 (20.039665179s elapsed)
STEP: deleting the pod 01/05/23 08:57:38.833
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  5 08:57:38.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-392" for this suite. 01/05/23 08:57:38.852
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":333,"skipped":6324,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.121 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:57:16.735
    Jan  5 08:57:16.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-probe 01/05/23 08:57:16.736
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:16.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:16.769
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c in namespace container-probe-392 01/05/23 08:57:16.772
    Jan  5 08:57:16.783: INFO: Waiting up to 5m0s for pod "liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c" in namespace "container-probe-392" to be "not pending"
    Jan  5 08:57:16.785: INFO: Pod "liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076541ms
    Jan  5 08:57:18.791: INFO: Pod "liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007810515s
    Jan  5 08:57:18.791: INFO: Pod "liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c" satisfied condition "not pending"
    Jan  5 08:57:18.791: INFO: Started pod liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c in namespace container-probe-392
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 08:57:18.791
    Jan  5 08:57:18.793: INFO: Initial restart count of pod liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c is 0
    Jan  5 08:57:38.833: INFO: Restart count of pod container-probe-392/liveness-6e4e9435-3e3d-40a5-8ef9-c91a63067c1c is now 1 (20.039665179s elapsed)
    STEP: deleting the pod 01/05/23 08:57:38.833
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  5 08:57:38.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-392" for this suite. 01/05/23 08:57:38.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:57:38.857
Jan  5 08:57:38.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename replicaset 01/05/23 08:57:38.858
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:38.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:38.872
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/05/23 08:57:38.878
STEP: Verify that the required pods have come up 01/05/23 08:57:38.882
Jan  5 08:57:38.883: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan  5 08:57:43.887: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/05/23 08:57:43.887
Jan  5 08:57:43.888: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/05/23 08:57:43.888
STEP: DeleteCollection of the ReplicaSets 01/05/23 08:57:43.891
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/05/23 08:57:43.903
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 08:57:43.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3880" for this suite. 01/05/23 08:57:43.91
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":334,"skipped":6340,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.074 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:57:38.857
    Jan  5 08:57:38.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename replicaset 01/05/23 08:57:38.858
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:38.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:38.872
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/05/23 08:57:38.878
    STEP: Verify that the required pods have come up 01/05/23 08:57:38.882
    Jan  5 08:57:38.883: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan  5 08:57:43.887: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/05/23 08:57:43.887
    Jan  5 08:57:43.888: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/05/23 08:57:43.888
    STEP: DeleteCollection of the ReplicaSets 01/05/23 08:57:43.891
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/05/23 08:57:43.903
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 08:57:43.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3880" for this suite. 01/05/23 08:57:43.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:57:43.931
Jan  5 08:57:43.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pods 01/05/23 08:57:43.932
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:43.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:43.968
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan  5 08:57:43.970: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: creating the pod 01/05/23 08:57:43.97
STEP: submitting the pod to kubernetes 01/05/23 08:57:43.97
Jan  5 08:57:43.977: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db" in namespace "pods-1471" to be "running and ready"
Jan  5 08:57:43.983: INFO: Pod "pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072453ms
Jan  5 08:57:43.983: INFO: The phase of Pod pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:57:45.987: INFO: Pod "pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db": Phase="Running", Reason="", readiness=true. Elapsed: 2.010426328s
Jan  5 08:57:45.987: INFO: The phase of Pod pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db is Running (Ready = true)
Jan  5 08:57:45.987: INFO: Pod "pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 08:57:46.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1471" for this suite. 01/05/23 08:57:46.079
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":335,"skipped":6347,"failed":0}
------------------------------
â€¢ [2.158 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:57:43.931
    Jan  5 08:57:43.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pods 01/05/23 08:57:43.932
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:43.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:43.968
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan  5 08:57:43.970: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: creating the pod 01/05/23 08:57:43.97
    STEP: submitting the pod to kubernetes 01/05/23 08:57:43.97
    Jan  5 08:57:43.977: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db" in namespace "pods-1471" to be "running and ready"
    Jan  5 08:57:43.983: INFO: Pod "pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072453ms
    Jan  5 08:57:43.983: INFO: The phase of Pod pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:57:45.987: INFO: Pod "pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db": Phase="Running", Reason="", readiness=true. Elapsed: 2.010426328s
    Jan  5 08:57:45.987: INFO: The phase of Pod pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db is Running (Ready = true)
    Jan  5 08:57:45.987: INFO: Pod "pod-exec-websocket-107358fc-c665-44ff-9795-2bcf9dfe05db" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 08:57:46.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1471" for this suite. 01/05/23 08:57:46.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:57:46.09
Jan  5 08:57:46.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename server-version 01/05/23 08:57:46.091
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:46.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:46.11
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/05/23 08:57:46.115
STEP: Confirm major version 01/05/23 08:57:46.116
Jan  5 08:57:46.116: INFO: Major version: 1
STEP: Confirm minor version 01/05/23 08:57:46.116
Jan  5 08:57:46.116: INFO: cleanMinorVersion: 25
Jan  5 08:57:46.116: INFO: Minor version: 25+
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan  5 08:57:46.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-0" for this suite. 01/05/23 08:57:46.118
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":336,"skipped":6355,"failed":0}
------------------------------
â€¢ [0.032 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:57:46.09
    Jan  5 08:57:46.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename server-version 01/05/23 08:57:46.091
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:46.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:46.11
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/05/23 08:57:46.115
    STEP: Confirm major version 01/05/23 08:57:46.116
    Jan  5 08:57:46.116: INFO: Major version: 1
    STEP: Confirm minor version 01/05/23 08:57:46.116
    Jan  5 08:57:46.116: INFO: cleanMinorVersion: 25
    Jan  5 08:57:46.116: INFO: Minor version: 25+
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan  5 08:57:46.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-0" for this suite. 01/05/23 08:57:46.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:57:46.122
Jan  5 08:57:46.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 08:57:46.123
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:46.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:46.148
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/05/23 08:57:46.152
STEP: watching for the Service to be added 01/05/23 08:57:46.165
Jan  5 08:57:46.166: INFO: Found Service test-service-cfxbz in namespace services-1409 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan  5 08:57:46.166: INFO: Service test-service-cfxbz created
STEP: Getting /status 01/05/23 08:57:46.166
Jan  5 08:57:46.168: INFO: Service test-service-cfxbz has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/05/23 08:57:46.168
STEP: watching for the Service to be patched 01/05/23 08:57:46.187
Jan  5 08:57:46.188: INFO: observed Service test-service-cfxbz in namespace services-1409 with annotations: map[] & LoadBalancer: {[]}
Jan  5 08:57:46.188: INFO: Found Service test-service-cfxbz in namespace services-1409 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan  5 08:57:46.188: INFO: Service test-service-cfxbz has service status patched
STEP: updating the ServiceStatus 01/05/23 08:57:46.188
Jan  5 08:57:46.198: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/05/23 08:57:46.198
Jan  5 08:57:46.200: INFO: Observed Service test-service-cfxbz in namespace services-1409 with annotations: map[] & Conditions: {[]}
Jan  5 08:57:46.200: INFO: Observed event: &Service{ObjectMeta:{test-service-cfxbz  services-1409  6cd0b222-b3e8-46bf-9354-26eab56d0816 33525 0 2023-01-05 08:57:46 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-05 08:57:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-05 08:57:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.107.12.13,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.107.12.13],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan  5 08:57:46.200: INFO: Found Service test-service-cfxbz in namespace services-1409 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 08:57:46.200: INFO: Service test-service-cfxbz has service status updated
STEP: patching the service 01/05/23 08:57:46.2
STEP: watching for the Service to be patched 01/05/23 08:57:46.228
Jan  5 08:57:46.229: INFO: observed Service test-service-cfxbz in namespace services-1409 with labels: map[test-service-static:true]
Jan  5 08:57:46.229: INFO: observed Service test-service-cfxbz in namespace services-1409 with labels: map[test-service-static:true]
Jan  5 08:57:46.229: INFO: observed Service test-service-cfxbz in namespace services-1409 with labels: map[test-service-static:true]
Jan  5 08:57:46.230: INFO: Found Service test-service-cfxbz in namespace services-1409 with labels: map[test-service:patched test-service-static:true]
Jan  5 08:57:46.230: INFO: Service test-service-cfxbz patched
STEP: deleting the service 01/05/23 08:57:46.23
STEP: watching for the Service to be deleted 01/05/23 08:57:46.242
Jan  5 08:57:46.243: INFO: Observed event: ADDED
Jan  5 08:57:46.243: INFO: Observed event: MODIFIED
Jan  5 08:57:46.243: INFO: Observed event: MODIFIED
Jan  5 08:57:46.243: INFO: Observed event: MODIFIED
Jan  5 08:57:46.243: INFO: Found Service test-service-cfxbz in namespace services-1409 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan  5 08:57:46.243: INFO: Service test-service-cfxbz deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 08:57:46.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1409" for this suite. 01/05/23 08:57:46.246
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":337,"skipped":6370,"failed":0}
------------------------------
â€¢ [0.128 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:57:46.122
    Jan  5 08:57:46.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 08:57:46.123
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:46.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:46.148
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/05/23 08:57:46.152
    STEP: watching for the Service to be added 01/05/23 08:57:46.165
    Jan  5 08:57:46.166: INFO: Found Service test-service-cfxbz in namespace services-1409 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan  5 08:57:46.166: INFO: Service test-service-cfxbz created
    STEP: Getting /status 01/05/23 08:57:46.166
    Jan  5 08:57:46.168: INFO: Service test-service-cfxbz has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/05/23 08:57:46.168
    STEP: watching for the Service to be patched 01/05/23 08:57:46.187
    Jan  5 08:57:46.188: INFO: observed Service test-service-cfxbz in namespace services-1409 with annotations: map[] & LoadBalancer: {[]}
    Jan  5 08:57:46.188: INFO: Found Service test-service-cfxbz in namespace services-1409 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan  5 08:57:46.188: INFO: Service test-service-cfxbz has service status patched
    STEP: updating the ServiceStatus 01/05/23 08:57:46.188
    Jan  5 08:57:46.198: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/05/23 08:57:46.198
    Jan  5 08:57:46.200: INFO: Observed Service test-service-cfxbz in namespace services-1409 with annotations: map[] & Conditions: {[]}
    Jan  5 08:57:46.200: INFO: Observed event: &Service{ObjectMeta:{test-service-cfxbz  services-1409  6cd0b222-b3e8-46bf-9354-26eab56d0816 33525 0 2023-01-05 08:57:46 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-05 08:57:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-05 08:57:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.107.12.13,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.107.12.13],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan  5 08:57:46.200: INFO: Found Service test-service-cfxbz in namespace services-1409 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 08:57:46.200: INFO: Service test-service-cfxbz has service status updated
    STEP: patching the service 01/05/23 08:57:46.2
    STEP: watching for the Service to be patched 01/05/23 08:57:46.228
    Jan  5 08:57:46.229: INFO: observed Service test-service-cfxbz in namespace services-1409 with labels: map[test-service-static:true]
    Jan  5 08:57:46.229: INFO: observed Service test-service-cfxbz in namespace services-1409 with labels: map[test-service-static:true]
    Jan  5 08:57:46.229: INFO: observed Service test-service-cfxbz in namespace services-1409 with labels: map[test-service-static:true]
    Jan  5 08:57:46.230: INFO: Found Service test-service-cfxbz in namespace services-1409 with labels: map[test-service:patched test-service-static:true]
    Jan  5 08:57:46.230: INFO: Service test-service-cfxbz patched
    STEP: deleting the service 01/05/23 08:57:46.23
    STEP: watching for the Service to be deleted 01/05/23 08:57:46.242
    Jan  5 08:57:46.243: INFO: Observed event: ADDED
    Jan  5 08:57:46.243: INFO: Observed event: MODIFIED
    Jan  5 08:57:46.243: INFO: Observed event: MODIFIED
    Jan  5 08:57:46.243: INFO: Observed event: MODIFIED
    Jan  5 08:57:46.243: INFO: Found Service test-service-cfxbz in namespace services-1409 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan  5 08:57:46.243: INFO: Service test-service-cfxbz deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 08:57:46.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1409" for this suite. 01/05/23 08:57:46.246
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:57:46.251
Jan  5 08:57:46.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename podtemplate 01/05/23 08:57:46.251
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:46.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:46.279
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/05/23 08:57:46.28
Jan  5 08:57:46.285: INFO: created test-podtemplate-1
Jan  5 08:57:46.297: INFO: created test-podtemplate-2
Jan  5 08:57:46.306: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/05/23 08:57:46.306
STEP: delete collection of pod templates 01/05/23 08:57:46.308
Jan  5 08:57:46.308: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/05/23 08:57:46.326
Jan  5 08:57:46.327: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan  5 08:57:46.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6457" for this suite. 01/05/23 08:57:46.337
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":338,"skipped":6375,"failed":0}
------------------------------
â€¢ [0.090 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:57:46.251
    Jan  5 08:57:46.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename podtemplate 01/05/23 08:57:46.251
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:46.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:46.279
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/05/23 08:57:46.28
    Jan  5 08:57:46.285: INFO: created test-podtemplate-1
    Jan  5 08:57:46.297: INFO: created test-podtemplate-2
    Jan  5 08:57:46.306: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/05/23 08:57:46.306
    STEP: delete collection of pod templates 01/05/23 08:57:46.308
    Jan  5 08:57:46.308: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/05/23 08:57:46.326
    Jan  5 08:57:46.327: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan  5 08:57:46.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-6457" for this suite. 01/05/23 08:57:46.337
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:57:46.341
Jan  5 08:57:46.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 08:57:46.342
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:46.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:46.357
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-9872b088-6b0e-4d24-b1ad-3d478bcdaf69 01/05/23 08:57:46.364
STEP: Creating a pod to test consume configMaps 01/05/23 08:57:46.368
Jan  5 08:57:46.377: INFO: Waiting up to 5m0s for pod "pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6" in namespace "configmap-1298" to be "Succeeded or Failed"
Jan  5 08:57:46.380: INFO: Pod "pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.859014ms
Jan  5 08:57:48.385: INFO: Pod "pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00810495s
Jan  5 08:57:50.383: INFO: Pod "pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005580339s
STEP: Saw pod success 01/05/23 08:57:50.383
Jan  5 08:57:50.383: INFO: Pod "pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6" satisfied condition "Succeeded or Failed"
Jan  5 08:57:50.384: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:57:50.388
Jan  5 08:57:50.398: INFO: Waiting for pod pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6 to disappear
Jan  5 08:57:50.404: INFO: Pod pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 08:57:50.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1298" for this suite. 01/05/23 08:57:50.407
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":339,"skipped":6378,"failed":0}
------------------------------
â€¢ [4.076 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:57:46.341
    Jan  5 08:57:46.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 08:57:46.342
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:46.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:46.357
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-9872b088-6b0e-4d24-b1ad-3d478bcdaf69 01/05/23 08:57:46.364
    STEP: Creating a pod to test consume configMaps 01/05/23 08:57:46.368
    Jan  5 08:57:46.377: INFO: Waiting up to 5m0s for pod "pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6" in namespace "configmap-1298" to be "Succeeded or Failed"
    Jan  5 08:57:46.380: INFO: Pod "pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.859014ms
    Jan  5 08:57:48.385: INFO: Pod "pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00810495s
    Jan  5 08:57:50.383: INFO: Pod "pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005580339s
    STEP: Saw pod success 01/05/23 08:57:50.383
    Jan  5 08:57:50.383: INFO: Pod "pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6" satisfied condition "Succeeded or Failed"
    Jan  5 08:57:50.384: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:57:50.388
    Jan  5 08:57:50.398: INFO: Waiting for pod pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6 to disappear
    Jan  5 08:57:50.404: INFO: Pod pod-configmaps-e58461ab-04fd-4c12-9854-e322f7982ad6 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 08:57:50.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1298" for this suite. 01/05/23 08:57:50.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:57:50.417
Jan  5 08:57:50.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename deployment 01/05/23 08:57:50.418
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:50.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:50.433
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan  5 08:57:50.444: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan  5 08:57:55.447: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 08:57:55.447
Jan  5 08:57:55.447: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan  5 08:57:57.450: INFO: Creating deployment "test-rollover-deployment"
Jan  5 08:57:57.463: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan  5 08:57:59.469: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan  5 08:57:59.472: INFO: Ensure that both replica sets have 1 created replica
Jan  5 08:57:59.475: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan  5 08:57:59.480: INFO: Updating deployment test-rollover-deployment
Jan  5 08:57:59.480: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan  5 08:58:01.484: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan  5 08:58:01.487: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan  5 08:58:01.490: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 08:58:01.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:58:03.496: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 08:58:03.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:58:05.501: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 08:58:05.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:58:07.496: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 08:58:07.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:58:09.497: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 08:58:09.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 08:58:11.496: INFO: 
Jan  5 08:58:11.496: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 08:58:11.501: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2748  a6cf472a-9dfa-4c86-9232-c7b59399bd23 33740 2 2023-01-05 08:57:57 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 08:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038cc1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 08:57:57 +0000 UTC,LastTransitionTime:2023-01-05 08:57:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-05 08:58:11 +0000 UTC,LastTransitionTime:2023-01-05 08:57:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 08:58:11.502: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-2748  495c22fb-9534-42db-9b56-8de2b8548611 33730 2 2023-01-05 08:57:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a6cf472a-9dfa-4c86-9232-c7b59399bd23 0xc005654537 0xc005654538}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6cf472a-9dfa-4c86-9232-c7b59399bd23\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:58:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056545e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 08:58:11.502: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan  5 08:58:11.503: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2748  824b4ed0-a722-4443-8184-9df5a88d6e60 33739 2 2023-01-05 08:57:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a6cf472a-9dfa-4c86-9232-c7b59399bd23 0xc0056542d7 0xc0056542d8}] [] [{e2e.test Update apps/v1 2023-01-05 08:57:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6cf472a-9dfa-4c86-9232-c7b59399bd23\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:58:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005654398 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 08:58:11.503: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-2748  1ab3a52d-9278-4dec-91ea-87e28dd36fe9 33695 2 2023-01-05 08:57:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a6cf472a-9dfa-4c86-9232-c7b59399bd23 0xc005654407 0xc005654408}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6cf472a-9dfa-4c86-9232-c7b59399bd23\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:57:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056544b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 08:58:11.505: INFO: Pod "test-rollover-deployment-6d45fd857b-t9t9c" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-t9t9c test-rollover-deployment-6d45fd857b- deployment-2748  d382dad5-222f-44de-9e34-ce42b18a9c8d 33711 0 2023-01-05 08:57:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:c6e32c78ee384a4ac6a1d37761094a4dbce5c440573c32f967630e4d61b2a63d cni.projectcalico.org/podIP:10.244.1.94/32 cni.projectcalico.org/podIPs:10.244.1.94/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 495c22fb-9534-42db-9b56-8de2b8548611 0xc0038cc587 0xc0038cc588}] [] [{kube-controller-manager Update v1 2023-01-05 08:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"495c22fb-9534-42db-9b56-8de2b8548611\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:58:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:58:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ww56f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ww56f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:58:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:58:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:57:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.94,StartTime:2023-01-05 08:57:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:58:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://71ce515d482a45a0055e9131224008d5d39afd1350fa3582f4a954bf97de2aed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  5 08:58:11.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2748" for this suite. 01/05/23 08:58:11.507
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":340,"skipped":6389,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.095 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:57:50.417
    Jan  5 08:57:50.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename deployment 01/05/23 08:57:50.418
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:57:50.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:57:50.433
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan  5 08:57:50.444: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan  5 08:57:55.447: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 08:57:55.447
    Jan  5 08:57:55.447: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan  5 08:57:57.450: INFO: Creating deployment "test-rollover-deployment"
    Jan  5 08:57:57.463: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan  5 08:57:59.469: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan  5 08:57:59.472: INFO: Ensure that both replica sets have 1 created replica
    Jan  5 08:57:59.475: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan  5 08:57:59.480: INFO: Updating deployment test-rollover-deployment
    Jan  5 08:57:59.480: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan  5 08:58:01.484: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan  5 08:58:01.487: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan  5 08:58:01.490: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 08:58:01.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:58:03.496: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 08:58:03.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:58:05.501: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 08:58:05.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:58:07.496: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 08:58:07.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:58:09.497: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 08:58:09.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 8, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 8, 57, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 08:58:11.496: INFO: 
    Jan  5 08:58:11.496: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 08:58:11.501: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2748  a6cf472a-9dfa-4c86-9232-c7b59399bd23 33740 2 2023-01-05 08:57:57 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 08:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038cc1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 08:57:57 +0000 UTC,LastTransitionTime:2023-01-05 08:57:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-05 08:58:11 +0000 UTC,LastTransitionTime:2023-01-05 08:57:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 08:58:11.502: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-2748  495c22fb-9534-42db-9b56-8de2b8548611 33730 2 2023-01-05 08:57:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a6cf472a-9dfa-4c86-9232-c7b59399bd23 0xc005654537 0xc005654538}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6cf472a-9dfa-4c86-9232-c7b59399bd23\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:58:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056545e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 08:58:11.502: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan  5 08:58:11.503: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2748  824b4ed0-a722-4443-8184-9df5a88d6e60 33739 2 2023-01-05 08:57:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a6cf472a-9dfa-4c86-9232-c7b59399bd23 0xc0056542d7 0xc0056542d8}] [] [{e2e.test Update apps/v1 2023-01-05 08:57:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6cf472a-9dfa-4c86-9232-c7b59399bd23\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:58:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005654398 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 08:58:11.503: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-2748  1ab3a52d-9278-4dec-91ea-87e28dd36fe9 33695 2 2023-01-05 08:57:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a6cf472a-9dfa-4c86-9232-c7b59399bd23 0xc005654407 0xc005654408}] [] [{kube-controller-manager Update apps/v1 2023-01-05 08:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6cf472a-9dfa-4c86-9232-c7b59399bd23\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 08:57:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056544b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 08:58:11.505: INFO: Pod "test-rollover-deployment-6d45fd857b-t9t9c" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-t9t9c test-rollover-deployment-6d45fd857b- deployment-2748  d382dad5-222f-44de-9e34-ce42b18a9c8d 33711 0 2023-01-05 08:57:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:c6e32c78ee384a4ac6a1d37761094a4dbce5c440573c32f967630e4d61b2a63d cni.projectcalico.org/podIP:10.244.1.94/32 cni.projectcalico.org/podIPs:10.244.1.94/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 495c22fb-9534-42db-9b56-8de2b8548611 0xc0038cc587 0xc0038cc588}] [] [{kube-controller-manager Update v1 2023-01-05 08:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"495c22fb-9534-42db-9b56-8de2b8548611\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-05 08:58:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-05 08:58:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ww56f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ww56f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-vm724.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:58:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:58:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 08:57:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.14.214,PodIP:10.244.1.94,StartTime:2023-01-05 08:57:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 08:58:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://71ce515d482a45a0055e9131224008d5d39afd1350fa3582f4a954bf97de2aed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  5 08:58:11.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2748" for this suite. 01/05/23 08:58:11.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:58:11.512
Jan  5 08:58:11.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename resourcequota 01/05/23 08:58:11.513
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:11.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:11.537
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/05/23 08:58:11.539
STEP: Ensuring ResourceQuota status is calculated 01/05/23 08:58:11.549
STEP: Creating a ResourceQuota with not best effort scope 01/05/23 08:58:13.552
STEP: Ensuring ResourceQuota status is calculated 01/05/23 08:58:13.556
STEP: Creating a best-effort pod 01/05/23 08:58:15.559
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/05/23 08:58:15.577
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/05/23 08:58:17.581
STEP: Deleting the pod 01/05/23 08:58:19.584
STEP: Ensuring resource quota status released the pod usage 01/05/23 08:58:19.615
STEP: Creating a not best-effort pod 01/05/23 08:58:21.619
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/05/23 08:58:21.632
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/05/23 08:58:23.636
STEP: Deleting the pod 01/05/23 08:58:25.64
STEP: Ensuring resource quota status released the pod usage 01/05/23 08:58:25.654
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 08:58:27.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7303" for this suite. 01/05/23 08:58:27.66
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":341,"skipped":6409,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.153 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:58:11.512
    Jan  5 08:58:11.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename resourcequota 01/05/23 08:58:11.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:11.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:11.537
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/05/23 08:58:11.539
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 08:58:11.549
    STEP: Creating a ResourceQuota with not best effort scope 01/05/23 08:58:13.552
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 08:58:13.556
    STEP: Creating a best-effort pod 01/05/23 08:58:15.559
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/05/23 08:58:15.577
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/05/23 08:58:17.581
    STEP: Deleting the pod 01/05/23 08:58:19.584
    STEP: Ensuring resource quota status released the pod usage 01/05/23 08:58:19.615
    STEP: Creating a not best-effort pod 01/05/23 08:58:21.619
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/05/23 08:58:21.632
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/05/23 08:58:23.636
    STEP: Deleting the pod 01/05/23 08:58:25.64
    STEP: Ensuring resource quota status released the pod usage 01/05/23 08:58:25.654
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 08:58:27.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7303" for this suite. 01/05/23 08:58:27.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:58:27.666
Jan  5 08:58:27.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 08:58:27.666
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:27.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:27.698
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/05/23 08:58:27.702
Jan  5 08:58:27.707: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9172" to be "running and ready"
Jan  5 08:58:27.710: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.38231ms
Jan  5 08:58:27.710: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:58:29.713: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005998266s
Jan  5 08:58:29.713: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 08:58:29.713: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/05/23 08:58:29.715
Jan  5 08:58:29.719: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9172" to be "running and ready"
Jan  5 08:58:29.720: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.434861ms
Jan  5 08:58:29.720: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:58:31.723: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004215034s
Jan  5 08:58:31.723: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan  5 08:58:31.723: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/05/23 08:58:31.726
Jan  5 08:58:31.734: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  5 08:58:31.736: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  5 08:58:33.737: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  5 08:58:33.739: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  5 08:58:35.737: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  5 08:58:35.740: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/05/23 08:58:35.74
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  5 08:58:35.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9172" for this suite. 01/05/23 08:58:35.75
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":342,"skipped":6420,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.088 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:58:27.666
    Jan  5 08:58:27.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 08:58:27.666
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:27.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:27.698
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 08:58:27.702
    Jan  5 08:58:27.707: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9172" to be "running and ready"
    Jan  5 08:58:27.710: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.38231ms
    Jan  5 08:58:27.710: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:58:29.713: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005998266s
    Jan  5 08:58:29.713: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 08:58:29.713: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/05/23 08:58:29.715
    Jan  5 08:58:29.719: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9172" to be "running and ready"
    Jan  5 08:58:29.720: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.434861ms
    Jan  5 08:58:29.720: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:58:31.723: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004215034s
    Jan  5 08:58:31.723: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan  5 08:58:31.723: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/05/23 08:58:31.726
    Jan  5 08:58:31.734: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  5 08:58:31.736: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  5 08:58:33.737: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  5 08:58:33.739: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  5 08:58:35.737: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  5 08:58:35.740: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/05/23 08:58:35.74
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  5 08:58:35.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9172" for this suite. 01/05/23 08:58:35.75
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:58:35.754
Jan  5 08:58:35.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename replicaset 01/05/23 08:58:35.755
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:35.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:35.777
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan  5 08:58:35.779: INFO: Creating ReplicaSet my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053
Jan  5 08:58:35.787: INFO: Pod name my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053: Found 0 pods out of 1
Jan  5 08:58:40.788: INFO: Pod name my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053: Found 1 pods out of 1
Jan  5 08:58:40.788: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053" is running
Jan  5 08:58:40.788: INFO: Waiting up to 5m0s for pod "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk" in namespace "replicaset-2630" to be "running"
Jan  5 08:58:40.790: INFO: Pod "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk": Phase="Running", Reason="", readiness=true. Elapsed: 1.874999ms
Jan  5 08:58:40.790: INFO: Pod "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk" satisfied condition "running"
Jan  5 08:58:40.790: INFO: Pod "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:58:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:58:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:58:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:58:35 +0000 UTC Reason: Message:}])
Jan  5 08:58:40.790: INFO: Trying to dial the pod
Jan  5 08:58:45.800: INFO: Controller my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053: Got expected result from replica 1 [my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk]: "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  5 08:58:45.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2630" for this suite. 01/05/23 08:58:45.803
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":343,"skipped":6420,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.054 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:58:35.754
    Jan  5 08:58:35.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename replicaset 01/05/23 08:58:35.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:35.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:35.777
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan  5 08:58:35.779: INFO: Creating ReplicaSet my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053
    Jan  5 08:58:35.787: INFO: Pod name my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053: Found 0 pods out of 1
    Jan  5 08:58:40.788: INFO: Pod name my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053: Found 1 pods out of 1
    Jan  5 08:58:40.788: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053" is running
    Jan  5 08:58:40.788: INFO: Waiting up to 5m0s for pod "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk" in namespace "replicaset-2630" to be "running"
    Jan  5 08:58:40.790: INFO: Pod "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk": Phase="Running", Reason="", readiness=true. Elapsed: 1.874999ms
    Jan  5 08:58:40.790: INFO: Pod "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk" satisfied condition "running"
    Jan  5 08:58:40.790: INFO: Pod "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:58:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:58:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:58:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 08:58:35 +0000 UTC Reason: Message:}])
    Jan  5 08:58:40.790: INFO: Trying to dial the pod
    Jan  5 08:58:45.800: INFO: Controller my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053: Got expected result from replica 1 [my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk]: "my-hostname-basic-94ce25a2-c871-4052-8143-cdfdaff34053-x9skk", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  5 08:58:45.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2630" for this suite. 01/05/23 08:58:45.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:58:45.81
Jan  5 08:58:45.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 08:58:45.812
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:45.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:45.832
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan  5 08:58:45.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 08:58:47.992
Jan  5 08:58:47.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-4859 --namespace=crd-publish-openapi-4859 create -f -'
Jan  5 08:58:48.615: INFO: stderr: ""
Jan  5 08:58:48.615: INFO: stdout: "e2e-test-crd-publish-openapi-5149-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  5 08:58:48.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-4859 --namespace=crd-publish-openapi-4859 delete e2e-test-crd-publish-openapi-5149-crds test-cr'
Jan  5 08:58:48.681: INFO: stderr: ""
Jan  5 08:58:48.681: INFO: stdout: "e2e-test-crd-publish-openapi-5149-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan  5 08:58:48.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-4859 --namespace=crd-publish-openapi-4859 apply -f -'
Jan  5 08:58:48.908: INFO: stderr: ""
Jan  5 08:58:48.908: INFO: stdout: "e2e-test-crd-publish-openapi-5149-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  5 08:58:48.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-4859 --namespace=crd-publish-openapi-4859 delete e2e-test-crd-publish-openapi-5149-crds test-cr'
Jan  5 08:58:48.972: INFO: stderr: ""
Jan  5 08:58:48.972: INFO: stdout: "e2e-test-crd-publish-openapi-5149-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/05/23 08:58:48.972
Jan  5 08:58:48.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-4859 explain e2e-test-crd-publish-openapi-5149-crds'
Jan  5 08:58:49.179: INFO: stderr: ""
Jan  5 08:58:49.179: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5149-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:58:52.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4859" for this suite. 01/05/23 08:58:52.236
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":344,"skipped":6450,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.431 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:58:45.81
    Jan  5 08:58:45.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 08:58:45.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:45.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:45.832
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan  5 08:58:45.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 08:58:47.992
    Jan  5 08:58:47.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-4859 --namespace=crd-publish-openapi-4859 create -f -'
    Jan  5 08:58:48.615: INFO: stderr: ""
    Jan  5 08:58:48.615: INFO: stdout: "e2e-test-crd-publish-openapi-5149-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  5 08:58:48.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-4859 --namespace=crd-publish-openapi-4859 delete e2e-test-crd-publish-openapi-5149-crds test-cr'
    Jan  5 08:58:48.681: INFO: stderr: ""
    Jan  5 08:58:48.681: INFO: stdout: "e2e-test-crd-publish-openapi-5149-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan  5 08:58:48.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-4859 --namespace=crd-publish-openapi-4859 apply -f -'
    Jan  5 08:58:48.908: INFO: stderr: ""
    Jan  5 08:58:48.908: INFO: stdout: "e2e-test-crd-publish-openapi-5149-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  5 08:58:48.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-4859 --namespace=crd-publish-openapi-4859 delete e2e-test-crd-publish-openapi-5149-crds test-cr'
    Jan  5 08:58:48.972: INFO: stderr: ""
    Jan  5 08:58:48.972: INFO: stdout: "e2e-test-crd-publish-openapi-5149-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/05/23 08:58:48.972
    Jan  5 08:58:48.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=crd-publish-openapi-4859 explain e2e-test-crd-publish-openapi-5149-crds'
    Jan  5 08:58:49.179: INFO: stderr: ""
    Jan  5 08:58:49.179: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5149-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:58:52.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4859" for this suite. 01/05/23 08:58:52.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:58:52.242
Jan  5 08:58:52.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename configmap 01/05/23 08:58:52.242
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:52.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:52.277
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-f8a83d45-64cb-4db6-91ee-683a424c859d 01/05/23 08:58:52.279
STEP: Creating a pod to test consume configMaps 01/05/23 08:58:52.282
Jan  5 08:58:52.294: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305" in namespace "configmap-5011" to be "Succeeded or Failed"
Jan  5 08:58:52.297: INFO: Pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.850917ms
Jan  5 08:58:54.302: INFO: Pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007592785s
Jan  5 08:58:56.301: INFO: Pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006718445s
Jan  5 08:58:58.300: INFO: Pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006443128s
STEP: Saw pod success 01/05/23 08:58:58.3
Jan  5 08:58:58.300: INFO: Pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305" satisfied condition "Succeeded or Failed"
Jan  5 08:58:58.302: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 08:58:58.307
Jan  5 08:58:58.323: INFO: Waiting for pod pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305 to disappear
Jan  5 08:58:58.325: INFO: Pod pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  5 08:58:58.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5011" for this suite. 01/05/23 08:58:58.327
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":345,"skipped":6455,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.093 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:58:52.242
    Jan  5 08:58:52.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename configmap 01/05/23 08:58:52.242
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:52.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:52.277
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-f8a83d45-64cb-4db6-91ee-683a424c859d 01/05/23 08:58:52.279
    STEP: Creating a pod to test consume configMaps 01/05/23 08:58:52.282
    Jan  5 08:58:52.294: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305" in namespace "configmap-5011" to be "Succeeded or Failed"
    Jan  5 08:58:52.297: INFO: Pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.850917ms
    Jan  5 08:58:54.302: INFO: Pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007592785s
    Jan  5 08:58:56.301: INFO: Pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006718445s
    Jan  5 08:58:58.300: INFO: Pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006443128s
    STEP: Saw pod success 01/05/23 08:58:58.3
    Jan  5 08:58:58.300: INFO: Pod "pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305" satisfied condition "Succeeded or Failed"
    Jan  5 08:58:58.302: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 08:58:58.307
    Jan  5 08:58:58.323: INFO: Waiting for pod pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305 to disappear
    Jan  5 08:58:58.325: INFO: Pod pod-configmaps-9b4197c3-433b-449e-82f3-5be03b369305 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  5 08:58:58.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5011" for this suite. 01/05/23 08:58:58.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:58:58.337
Jan  5 08:58:58.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 08:58:58.338
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:58.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:58.361
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-aa4cadae-580d-49a1-aca8-490f4c566b2b 01/05/23 08:58:58.364
STEP: Creating a pod to test consume secrets 01/05/23 08:58:58.369
Jan  5 08:58:58.381: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf" in namespace "projected-4611" to be "Succeeded or Failed"
Jan  5 08:58:58.382: INFO: Pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.766607ms
Jan  5 08:59:00.386: INFO: Pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005501577s
Jan  5 08:59:02.385: INFO: Pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004902801s
Jan  5 08:59:04.388: INFO: Pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007840458s
STEP: Saw pod success 01/05/23 08:59:04.388
Jan  5 08:59:04.389: INFO: Pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf" satisfied condition "Succeeded or Failed"
Jan  5 08:59:04.391: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 08:59:04.395
Jan  5 08:59:04.405: INFO: Waiting for pod pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf to disappear
Jan  5 08:59:04.407: INFO: Pod pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  5 08:59:04.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4611" for this suite. 01/05/23 08:59:04.409
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":346,"skipped":6484,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.090 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:58:58.337
    Jan  5 08:58:58.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 08:58:58.338
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:58:58.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:58:58.361
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-aa4cadae-580d-49a1-aca8-490f4c566b2b 01/05/23 08:58:58.364
    STEP: Creating a pod to test consume secrets 01/05/23 08:58:58.369
    Jan  5 08:58:58.381: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf" in namespace "projected-4611" to be "Succeeded or Failed"
    Jan  5 08:58:58.382: INFO: Pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.766607ms
    Jan  5 08:59:00.386: INFO: Pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005501577s
    Jan  5 08:59:02.385: INFO: Pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004902801s
    Jan  5 08:59:04.388: INFO: Pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007840458s
    STEP: Saw pod success 01/05/23 08:59:04.388
    Jan  5 08:59:04.389: INFO: Pod "pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf" satisfied condition "Succeeded or Failed"
    Jan  5 08:59:04.391: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 08:59:04.395
    Jan  5 08:59:04.405: INFO: Waiting for pod pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf to disappear
    Jan  5 08:59:04.407: INFO: Pod pod-projected-secrets-6b5bca24-5c88-4fe4-b651-b63b44bbb8bf no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  5 08:59:04.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4611" for this suite. 01/05/23 08:59:04.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:59:04.428
Jan  5 08:59:04.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename webhook 01/05/23 08:59:04.429
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:04.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:04.446
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/05/23 08:59:04.462
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:59:04.859
STEP: Deploying the webhook pod 01/05/23 08:59:04.866
STEP: Wait for the deployment to be ready 01/05/23 08:59:04.885
Jan  5 08:59:04.889: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 08:59:06.897
STEP: Verifying the service has paired with the endpoint 01/05/23 08:59:06.909
Jan  5 08:59:07.909: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 08:59:07.912
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 08:59:07.923
STEP: Creating a dummy validating-webhook-configuration object 01/05/23 08:59:07.937
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/05/23 08:59:08.15
STEP: Creating a dummy mutating-webhook-configuration object 01/05/23 08:59:08.155
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/05/23 08:59:08.164
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  5 08:59:08.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9817" for this suite. 01/05/23 08:59:08.182
STEP: Destroying namespace "webhook-9817-markers" for this suite. 01/05/23 08:59:08.199
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":347,"skipped":6495,"failed":0}
------------------------------
â€¢ [3.812 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:59:04.428
    Jan  5 08:59:04.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename webhook 01/05/23 08:59:04.429
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:04.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:04.446
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/05/23 08:59:04.462
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 08:59:04.859
    STEP: Deploying the webhook pod 01/05/23 08:59:04.866
    STEP: Wait for the deployment to be ready 01/05/23 08:59:04.885
    Jan  5 08:59:04.889: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 08:59:06.897
    STEP: Verifying the service has paired with the endpoint 01/05/23 08:59:06.909
    Jan  5 08:59:07.909: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 08:59:07.912
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 08:59:07.923
    STEP: Creating a dummy validating-webhook-configuration object 01/05/23 08:59:07.937
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/05/23 08:59:08.15
    STEP: Creating a dummy mutating-webhook-configuration object 01/05/23 08:59:08.155
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/05/23 08:59:08.164
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  5 08:59:08.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9817" for this suite. 01/05/23 08:59:08.182
    STEP: Destroying namespace "webhook-9817-markers" for this suite. 01/05/23 08:59:08.199
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:59:08.242
Jan  5 08:59:08.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename disruption 01/05/23 08:59:08.243
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:08.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:08.267
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/05/23 08:59:08.275
STEP: Updating PodDisruptionBudget status 01/05/23 08:59:10.28
STEP: Waiting for all pods to be running 01/05/23 08:59:10.285
Jan  5 08:59:10.287: INFO: running pods: 0 < 1
Jan  5 08:59:12.292: INFO: running pods: 0 < 1
STEP: locating a running pod 01/05/23 08:59:14.291
STEP: Waiting for the pdb to be processed 01/05/23 08:59:14.298
STEP: Patching PodDisruptionBudget status 01/05/23 08:59:14.308
STEP: Waiting for the pdb to be processed 01/05/23 08:59:14.314
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  5 08:59:14.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1618" for this suite. 01/05/23 08:59:14.322
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":348,"skipped":6523,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.084 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:59:08.242
    Jan  5 08:59:08.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename disruption 01/05/23 08:59:08.243
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:08.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:08.267
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/05/23 08:59:08.275
    STEP: Updating PodDisruptionBudget status 01/05/23 08:59:10.28
    STEP: Waiting for all pods to be running 01/05/23 08:59:10.285
    Jan  5 08:59:10.287: INFO: running pods: 0 < 1
    Jan  5 08:59:12.292: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/05/23 08:59:14.291
    STEP: Waiting for the pdb to be processed 01/05/23 08:59:14.298
    STEP: Patching PodDisruptionBudget status 01/05/23 08:59:14.308
    STEP: Waiting for the pdb to be processed 01/05/23 08:59:14.314
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  5 08:59:14.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1618" for this suite. 01/05/23 08:59:14.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:59:14.328
Jan  5 08:59:14.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/05/23 08:59:14.328
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:14.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:14.344
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/05/23 08:59:14.348
STEP: Creating hostNetwork=false pod 01/05/23 08:59:14.348
Jan  5 08:59:14.353: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9298" to be "running and ready"
Jan  5 08:59:14.355: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.722998ms
Jan  5 08:59:14.355: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:59:16.358: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005182421s
Jan  5 08:59:16.358: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:59:18.358: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004786554s
Jan  5 08:59:18.358: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan  5 08:59:18.358: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/05/23 08:59:18.361
Jan  5 08:59:18.365: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9298" to be "running and ready"
Jan  5 08:59:18.367: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.626682ms
Jan  5 08:59:18.367: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:59:20.370: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004655028s
Jan  5 08:59:20.370: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:59:22.372: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006417753s
Jan  5 08:59:22.372: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan  5 08:59:22.372: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/05/23 08:59:22.374
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/05/23 08:59:22.374
Jan  5 08:59:22.374: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:59:22.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:59:22.375: INFO: ExecWithOptions: Clientset creation
Jan  5 08:59:22.375: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 08:59:22.453: INFO: Exec stderr: ""
Jan  5 08:59:22.453: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:59:22.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:59:22.454: INFO: ExecWithOptions: Clientset creation
Jan  5 08:59:22.454: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 08:59:22.518: INFO: Exec stderr: ""
Jan  5 08:59:22.518: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:59:22.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:59:22.518: INFO: ExecWithOptions: Clientset creation
Jan  5 08:59:22.518: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 08:59:22.576: INFO: Exec stderr: ""
Jan  5 08:59:22.576: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:59:22.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:59:22.577: INFO: ExecWithOptions: Clientset creation
Jan  5 08:59:22.577: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 08:59:22.617: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/05/23 08:59:22.618
Jan  5 08:59:22.618: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:59:22.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:59:22.618: INFO: ExecWithOptions: Clientset creation
Jan  5 08:59:22.618: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  5 08:59:22.679: INFO: Exec stderr: ""
Jan  5 08:59:22.679: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:59:22.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:59:22.679: INFO: ExecWithOptions: Clientset creation
Jan  5 08:59:22.679: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  5 08:59:22.721: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/05/23 08:59:22.722
Jan  5 08:59:22.722: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:59:22.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:59:22.722: INFO: ExecWithOptions: Clientset creation
Jan  5 08:59:22.722: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 08:59:22.775: INFO: Exec stderr: ""
Jan  5 08:59:22.775: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:59:22.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:59:22.776: INFO: ExecWithOptions: Clientset creation
Jan  5 08:59:22.776: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 08:59:22.812: INFO: Exec stderr: ""
Jan  5 08:59:22.812: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:59:22.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:59:22.812: INFO: ExecWithOptions: Clientset creation
Jan  5 08:59:22.812: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 08:59:22.861: INFO: Exec stderr: ""
Jan  5 08:59:22.861: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 08:59:22.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
Jan  5 08:59:22.861: INFO: ExecWithOptions: Clientset creation
Jan  5 08:59:22.861: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 08:59:22.893: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan  5 08:59:22.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9298" for this suite. 01/05/23 08:59:22.896
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":349,"skipped":6583,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.573 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:59:14.328
    Jan  5 08:59:14.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/05/23 08:59:14.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:14.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:14.344
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/05/23 08:59:14.348
    STEP: Creating hostNetwork=false pod 01/05/23 08:59:14.348
    Jan  5 08:59:14.353: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9298" to be "running and ready"
    Jan  5 08:59:14.355: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.722998ms
    Jan  5 08:59:14.355: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:59:16.358: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005182421s
    Jan  5 08:59:16.358: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:59:18.358: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004786554s
    Jan  5 08:59:18.358: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan  5 08:59:18.358: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/05/23 08:59:18.361
    Jan  5 08:59:18.365: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9298" to be "running and ready"
    Jan  5 08:59:18.367: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.626682ms
    Jan  5 08:59:18.367: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:59:20.370: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004655028s
    Jan  5 08:59:20.370: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:59:22.372: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006417753s
    Jan  5 08:59:22.372: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan  5 08:59:22.372: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/05/23 08:59:22.374
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/05/23 08:59:22.374
    Jan  5 08:59:22.374: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:59:22.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:59:22.375: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:59:22.375: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 08:59:22.453: INFO: Exec stderr: ""
    Jan  5 08:59:22.453: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:59:22.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:59:22.454: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:59:22.454: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 08:59:22.518: INFO: Exec stderr: ""
    Jan  5 08:59:22.518: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:59:22.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:59:22.518: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:59:22.518: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 08:59:22.576: INFO: Exec stderr: ""
    Jan  5 08:59:22.576: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:59:22.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:59:22.577: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:59:22.577: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 08:59:22.617: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/05/23 08:59:22.618
    Jan  5 08:59:22.618: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:59:22.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:59:22.618: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:59:22.618: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  5 08:59:22.679: INFO: Exec stderr: ""
    Jan  5 08:59:22.679: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:59:22.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:59:22.679: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:59:22.679: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  5 08:59:22.721: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/05/23 08:59:22.722
    Jan  5 08:59:22.722: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:59:22.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:59:22.722: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:59:22.722: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 08:59:22.775: INFO: Exec stderr: ""
    Jan  5 08:59:22.775: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:59:22.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:59:22.776: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:59:22.776: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 08:59:22.812: INFO: Exec stderr: ""
    Jan  5 08:59:22.812: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:59:22.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:59:22.812: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:59:22.812: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 08:59:22.861: INFO: Exec stderr: ""
    Jan  5 08:59:22.861: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9298 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 08:59:22.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    Jan  5 08:59:22.861: INFO: ExecWithOptions: Clientset creation
    Jan  5 08:59:22.861: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9298/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 08:59:22.893: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan  5 08:59:22.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-9298" for this suite. 01/05/23 08:59:22.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:59:22.901
Jan  5 08:59:22.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename dns 01/05/23 08:59:22.902
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:22.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:22.918
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/05/23 08:59:22.92
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/05/23 08:59:22.924
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/05/23 08:59:22.924
STEP: creating a pod to probe DNS 01/05/23 08:59:22.924
STEP: submitting the pod to kubernetes 01/05/23 08:59:22.924
Jan  5 08:59:22.942: INFO: Waiting up to 15m0s for pod "dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25" in namespace "dns-737" to be "running"
Jan  5 08:59:22.944: INFO: Pod "dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1.684182ms
Jan  5 08:59:24.947: INFO: Pod "dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004744329s
Jan  5 08:59:26.946: INFO: Pod "dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25": Phase="Running", Reason="", readiness=true. Elapsed: 4.003972809s
Jan  5 08:59:26.946: INFO: Pod "dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25" satisfied condition "running"
STEP: retrieving the pod 01/05/23 08:59:26.946
STEP: looking for the results for each expected name from probers 01/05/23 08:59:26.948
Jan  5 08:59:26.956: INFO: DNS probes using dns-737/dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25 succeeded

STEP: deleting the pod 01/05/23 08:59:26.956
STEP: deleting the test headless service 01/05/23 08:59:26.978
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  5 08:59:26.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-737" for this suite. 01/05/23 08:59:26.992
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":350,"skipped":6599,"failed":0}
------------------------------
â€¢ [4.100 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:59:22.901
    Jan  5 08:59:22.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename dns 01/05/23 08:59:22.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:22.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:22.918
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/05/23 08:59:22.92
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/05/23 08:59:22.924
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/05/23 08:59:22.924
    STEP: creating a pod to probe DNS 01/05/23 08:59:22.924
    STEP: submitting the pod to kubernetes 01/05/23 08:59:22.924
    Jan  5 08:59:22.942: INFO: Waiting up to 15m0s for pod "dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25" in namespace "dns-737" to be "running"
    Jan  5 08:59:22.944: INFO: Pod "dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1.684182ms
    Jan  5 08:59:24.947: INFO: Pod "dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004744329s
    Jan  5 08:59:26.946: INFO: Pod "dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25": Phase="Running", Reason="", readiness=true. Elapsed: 4.003972809s
    Jan  5 08:59:26.946: INFO: Pod "dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 08:59:26.946
    STEP: looking for the results for each expected name from probers 01/05/23 08:59:26.948
    Jan  5 08:59:26.956: INFO: DNS probes using dns-737/dns-test-141d2e13-204d-4df9-8f77-ddbbb7b93c25 succeeded

    STEP: deleting the pod 01/05/23 08:59:26.956
    STEP: deleting the test headless service 01/05/23 08:59:26.978
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  5 08:59:26.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-737" for this suite. 01/05/23 08:59:26.992
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:59:27.002
Jan  5 08:59:27.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename resourcequota 01/05/23 08:59:27.003
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:27.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:27.016
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/05/23 08:59:44.028
STEP: Creating a ResourceQuota 01/05/23 08:59:49.031
STEP: Ensuring resource quota status is calculated 01/05/23 08:59:49.036
STEP: Creating a ConfigMap 01/05/23 08:59:51.04
STEP: Ensuring resource quota status captures configMap creation 01/05/23 08:59:51.057
STEP: Deleting a ConfigMap 01/05/23 08:59:53.061
STEP: Ensuring resource quota status released usage 01/05/23 08:59:53.065
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  5 08:59:55.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4870" for this suite. 01/05/23 08:59:55.071
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":351,"skipped":6601,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.073 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:59:27.002
    Jan  5 08:59:27.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename resourcequota 01/05/23 08:59:27.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:27.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:27.016
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/05/23 08:59:44.028
    STEP: Creating a ResourceQuota 01/05/23 08:59:49.031
    STEP: Ensuring resource quota status is calculated 01/05/23 08:59:49.036
    STEP: Creating a ConfigMap 01/05/23 08:59:51.04
    STEP: Ensuring resource quota status captures configMap creation 01/05/23 08:59:51.057
    STEP: Deleting a ConfigMap 01/05/23 08:59:53.061
    STEP: Ensuring resource quota status released usage 01/05/23 08:59:53.065
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  5 08:59:55.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4870" for this suite. 01/05/23 08:59:55.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:59:55.076
Jan  5 08:59:55.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename daemonsets 01/05/23 08:59:55.077
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:55.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:55.092
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/05/23 08:59:55.104
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 08:59:55.116
Jan  5 08:59:55.118: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:59:55.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:59:55.119: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:59:56.124: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:59:56.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 08:59:56.126: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 08:59:57.124: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 08:59:57.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 08:59:57.127: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/05/23 08:59:57.129
STEP: DeleteCollection of the DaemonSets 01/05/23 08:59:57.132
STEP: Verify that ReplicaSets have been deleted 01/05/23 08:59:57.139
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan  5 08:59:57.144: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34433"},"items":null}

Jan  5 08:59:57.145: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34433"},"items":[{"metadata":{"name":"daemon-set-pbgt4","generateName":"daemon-set-","namespace":"daemonsets-6251","uid":"c4a56680-4174-429f-844d-4af4ad028144","resourceVersion":"34430","creationTimestamp":"2023-01-05T08:59:55Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"cad478bd2c4b62da080fe9b5c9334f76f72bf6ccb767a8ba73a66b155bb7a20f","cni.projectcalico.org/podIP":"10.244.1.104/32","cni.projectcalico.org/podIPs":"10.244.1.104/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"00802f53-2aa1-445d-9ba6-2ccc88c6ed1e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00802f53-2aa1-445d-9ba6-2ccc88c6ed1e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gj5xr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gj5xr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"mip-bd-vm724.mip.storage.hpecorp.net","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["mip-bd-vm724.mip.storage.hpecorp.net"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:55Z"}],"hostIP":"16.0.14.214","podIP":"10.244.1.104","podIPs":[{"ip":"10.244.1.104"}],"startTime":"2023-01-05T08:59:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T08:59:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://9569860e39ce91babc0d404ea75739f2cf501b5538d65b8c5ada4a9f369f8f2b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-zjjjd","generateName":"daemon-set-","namespace":"daemonsets-6251","uid":"bf537b58-8f35-4f01-afd0-562b82fd4801","resourceVersion":"34427","creationTimestamp":"2023-01-05T08:59:55Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"bd6eb231c84c3cbe438f00d09de2ba12571eb833bcf115a9e9480fb3355eb3ec","cni.projectcalico.org/podIP":"10.244.0.192/32","cni.projectcalico.org/podIPs":"10.244.0.192/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"00802f53-2aa1-445d-9ba6-2ccc88c6ed1e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00802f53-2aa1-445d-9ba6-2ccc88c6ed1e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gt9jz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gt9jz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"mip-bd-vm722.mip.storage.hpecorp.net","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["mip-bd-vm722.mip.storage.hpecorp.net"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:55Z"}],"hostIP":"16.0.14.212","podIP":"10.244.0.192","podIPs":[{"ip":"10.244.0.192"}],"startTime":"2023-01-05T08:59:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T08:59:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://4da16a26d95a0c11951ed2828dab8c007249145d85ab165df2252987a341254f","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  5 08:59:57.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6251" for this suite. 01/05/23 08:59:57.169
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":352,"skipped":6608,"failed":0}
------------------------------
â€¢ [2.105 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:59:55.076
    Jan  5 08:59:55.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename daemonsets 01/05/23 08:59:55.077
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:55.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:55.092
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/05/23 08:59:55.104
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 08:59:55.116
    Jan  5 08:59:55.118: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:59:55.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:59:55.119: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:59:56.124: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:59:56.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 08:59:56.126: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 08:59:57.124: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 08:59:57.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 08:59:57.127: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/05/23 08:59:57.129
    STEP: DeleteCollection of the DaemonSets 01/05/23 08:59:57.132
    STEP: Verify that ReplicaSets have been deleted 01/05/23 08:59:57.139
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan  5 08:59:57.144: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34433"},"items":null}

    Jan  5 08:59:57.145: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34433"},"items":[{"metadata":{"name":"daemon-set-pbgt4","generateName":"daemon-set-","namespace":"daemonsets-6251","uid":"c4a56680-4174-429f-844d-4af4ad028144","resourceVersion":"34430","creationTimestamp":"2023-01-05T08:59:55Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"cad478bd2c4b62da080fe9b5c9334f76f72bf6ccb767a8ba73a66b155bb7a20f","cni.projectcalico.org/podIP":"10.244.1.104/32","cni.projectcalico.org/podIPs":"10.244.1.104/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"00802f53-2aa1-445d-9ba6-2ccc88c6ed1e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00802f53-2aa1-445d-9ba6-2ccc88c6ed1e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gj5xr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gj5xr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"mip-bd-vm724.mip.storage.hpecorp.net","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["mip-bd-vm724.mip.storage.hpecorp.net"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:55Z"}],"hostIP":"16.0.14.214","podIP":"10.244.1.104","podIPs":[{"ip":"10.244.1.104"}],"startTime":"2023-01-05T08:59:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T08:59:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://9569860e39ce91babc0d404ea75739f2cf501b5538d65b8c5ada4a9f369f8f2b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-zjjjd","generateName":"daemon-set-","namespace":"daemonsets-6251","uid":"bf537b58-8f35-4f01-afd0-562b82fd4801","resourceVersion":"34427","creationTimestamp":"2023-01-05T08:59:55Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"bd6eb231c84c3cbe438f00d09de2ba12571eb833bcf115a9e9480fb3355eb3ec","cni.projectcalico.org/podIP":"10.244.0.192/32","cni.projectcalico.org/podIPs":"10.244.0.192/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"00802f53-2aa1-445d-9ba6-2ccc88c6ed1e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00802f53-2aa1-445d-9ba6-2ccc88c6ed1e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T08:59:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gt9jz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gt9jz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"mip-bd-vm722.mip.storage.hpecorp.net","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["mip-bd-vm722.mip.storage.hpecorp.net"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T08:59:55Z"}],"hostIP":"16.0.14.212","podIP":"10.244.0.192","podIPs":[{"ip":"10.244.0.192"}],"startTime":"2023-01-05T08:59:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T08:59:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://4da16a26d95a0c11951ed2828dab8c007249145d85ab165df2252987a341254f","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 08:59:57.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6251" for this suite. 01/05/23 08:59:57.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:59:57.181
Jan  5 08:59:57.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename pods 01/05/23 08:59:57.182
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:57.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:57.203
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/05/23 08:59:57.205
Jan  5 08:59:57.215: INFO: Waiting up to 5m0s for pod "pod-hostip-980a335d-f675-490b-a201-4ad415603d2f" in namespace "pods-4486" to be "running and ready"
Jan  5 08:59:57.217: INFO: Pod "pod-hostip-980a335d-f675-490b-a201-4ad415603d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.842875ms
Jan  5 08:59:57.217: INFO: The phase of Pod pod-hostip-980a335d-f675-490b-a201-4ad415603d2f is Pending, waiting for it to be Running (with Ready = true)
Jan  5 08:59:59.220: INFO: Pod "pod-hostip-980a335d-f675-490b-a201-4ad415603d2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004965881s
Jan  5 08:59:59.220: INFO: The phase of Pod pod-hostip-980a335d-f675-490b-a201-4ad415603d2f is Running (Ready = true)
Jan  5 08:59:59.220: INFO: Pod "pod-hostip-980a335d-f675-490b-a201-4ad415603d2f" satisfied condition "running and ready"
Jan  5 08:59:59.224: INFO: Pod pod-hostip-980a335d-f675-490b-a201-4ad415603d2f has hostIP: 16.0.14.214
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  5 08:59:59.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4486" for this suite. 01/05/23 08:59:59.227
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":353,"skipped":6624,"failed":0}
------------------------------
â€¢ [2.050 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:59:57.181
    Jan  5 08:59:57.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename pods 01/05/23 08:59:57.182
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:57.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:57.203
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/05/23 08:59:57.205
    Jan  5 08:59:57.215: INFO: Waiting up to 5m0s for pod "pod-hostip-980a335d-f675-490b-a201-4ad415603d2f" in namespace "pods-4486" to be "running and ready"
    Jan  5 08:59:57.217: INFO: Pod "pod-hostip-980a335d-f675-490b-a201-4ad415603d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.842875ms
    Jan  5 08:59:57.217: INFO: The phase of Pod pod-hostip-980a335d-f675-490b-a201-4ad415603d2f is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 08:59:59.220: INFO: Pod "pod-hostip-980a335d-f675-490b-a201-4ad415603d2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004965881s
    Jan  5 08:59:59.220: INFO: The phase of Pod pod-hostip-980a335d-f675-490b-a201-4ad415603d2f is Running (Ready = true)
    Jan  5 08:59:59.220: INFO: Pod "pod-hostip-980a335d-f675-490b-a201-4ad415603d2f" satisfied condition "running and ready"
    Jan  5 08:59:59.224: INFO: Pod pod-hostip-980a335d-f675-490b-a201-4ad415603d2f has hostIP: 16.0.14.214
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  5 08:59:59.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4486" for this suite. 01/05/23 08:59:59.227
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 08:59:59.231
Jan  5 08:59:59.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename subpath 01/05/23 08:59:59.232
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:59.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:59.256
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 08:59:59.258
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-fpnm 01/05/23 08:59:59.283
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 08:59:59.283
Jan  5 08:59:59.290: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fpnm" in namespace "subpath-142" to be "Succeeded or Failed"
Jan  5 08:59:59.292: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277377ms
Jan  5 09:00:01.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006208464s
Jan  5 09:00:03.297: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 4.006914913s
Jan  5 09:00:05.295: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 6.005601319s
Jan  5 09:00:07.295: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 8.005126313s
Jan  5 09:00:09.295: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 10.005525603s
Jan  5 09:00:11.295: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 12.005764601s
Jan  5 09:00:13.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 14.006354881s
Jan  5 09:00:15.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 16.006473415s
Jan  5 09:00:17.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 18.006772332s
Jan  5 09:00:19.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 20.006271167s
Jan  5 09:00:21.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 22.006405825s
Jan  5 09:00:23.297: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=false. Elapsed: 24.007668092s
Jan  5 09:00:25.295: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.005680919s
STEP: Saw pod success 01/05/23 09:00:25.295
Jan  5 09:00:25.295: INFO: Pod "pod-subpath-test-secret-fpnm" satisfied condition "Succeeded or Failed"
Jan  5 09:00:25.297: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-subpath-test-secret-fpnm container test-container-subpath-secret-fpnm: <nil>
STEP: delete the pod 01/05/23 09:00:25.301
Jan  5 09:00:25.318: INFO: Waiting for pod pod-subpath-test-secret-fpnm to disappear
Jan  5 09:00:25.320: INFO: Pod pod-subpath-test-secret-fpnm no longer exists
STEP: Deleting pod pod-subpath-test-secret-fpnm 01/05/23 09:00:25.32
Jan  5 09:00:25.320: INFO: Deleting pod "pod-subpath-test-secret-fpnm" in namespace "subpath-142"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  5 09:00:25.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-142" for this suite. 01/05/23 09:00:25.325
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":354,"skipped":6624,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.102 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 08:59:59.231
    Jan  5 08:59:59.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename subpath 01/05/23 08:59:59.232
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 08:59:59.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 08:59:59.256
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 08:59:59.258
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-fpnm 01/05/23 08:59:59.283
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 08:59:59.283
    Jan  5 08:59:59.290: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fpnm" in namespace "subpath-142" to be "Succeeded or Failed"
    Jan  5 08:59:59.292: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277377ms
    Jan  5 09:00:01.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006208464s
    Jan  5 09:00:03.297: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 4.006914913s
    Jan  5 09:00:05.295: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 6.005601319s
    Jan  5 09:00:07.295: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 8.005126313s
    Jan  5 09:00:09.295: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 10.005525603s
    Jan  5 09:00:11.295: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 12.005764601s
    Jan  5 09:00:13.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 14.006354881s
    Jan  5 09:00:15.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 16.006473415s
    Jan  5 09:00:17.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 18.006772332s
    Jan  5 09:00:19.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 20.006271167s
    Jan  5 09:00:21.296: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=true. Elapsed: 22.006405825s
    Jan  5 09:00:23.297: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Running", Reason="", readiness=false. Elapsed: 24.007668092s
    Jan  5 09:00:25.295: INFO: Pod "pod-subpath-test-secret-fpnm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.005680919s
    STEP: Saw pod success 01/05/23 09:00:25.295
    Jan  5 09:00:25.295: INFO: Pod "pod-subpath-test-secret-fpnm" satisfied condition "Succeeded or Failed"
    Jan  5 09:00:25.297: INFO: Trying to get logs from node mip-bd-vm724.mip.storage.hpecorp.net pod pod-subpath-test-secret-fpnm container test-container-subpath-secret-fpnm: <nil>
    STEP: delete the pod 01/05/23 09:00:25.301
    Jan  5 09:00:25.318: INFO: Waiting for pod pod-subpath-test-secret-fpnm to disappear
    Jan  5 09:00:25.320: INFO: Pod pod-subpath-test-secret-fpnm no longer exists
    STEP: Deleting pod pod-subpath-test-secret-fpnm 01/05/23 09:00:25.32
    Jan  5 09:00:25.320: INFO: Deleting pod "pod-subpath-test-secret-fpnm" in namespace "subpath-142"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  5 09:00:25.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-142" for this suite. 01/05/23 09:00:25.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:00:25.334
Jan  5 09:00:25.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename watch 01/05/23 09:00:25.335
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:00:25.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:00:25.351
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/05/23 09:00:25.352
STEP: starting a background goroutine to produce watch events 01/05/23 09:00:25.354
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/05/23 09:00:25.354
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  5 09:00:28.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8930" for this suite. 01/05/23 09:00:28.948
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":355,"skipped":6634,"failed":0}
------------------------------
â€¢ [3.624 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:00:25.334
    Jan  5 09:00:25.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename watch 01/05/23 09:00:25.335
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:00:25.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:00:25.351
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/05/23 09:00:25.352
    STEP: starting a background goroutine to produce watch events 01/05/23 09:00:25.354
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/05/23 09:00:25.354
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  5 09:00:28.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8930" for this suite. 01/05/23 09:00:28.948
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:00:28.959
Jan  5 09:00:28.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename watch 01/05/23 09:00:28.96
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:00:28.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:00:28.993
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/05/23 09:00:28.995
STEP: creating a new configmap 01/05/23 09:00:28.997
STEP: modifying the configmap once 01/05/23 09:00:29.018
STEP: changing the label value of the configmap 01/05/23 09:00:29.024
STEP: Expecting to observe a delete notification for the watched object 01/05/23 09:00:29.664
Jan  5 09:00:29.665: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34678 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:00:29.665: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34679 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:00:29.665: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34681 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/05/23 09:00:29.665
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/05/23 09:00:29.673
STEP: changing the label value of the configmap back 01/05/23 09:00:39.674
STEP: modifying the configmap a third time 01/05/23 09:00:39.774
STEP: deleting the configmap 01/05/23 09:00:39.78
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/05/23 09:00:39.787
Jan  5 09:00:39.787: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34724 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:00:39.787: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34725 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 09:00:39.787: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34726 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  5 09:00:39.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-572" for this suite. 01/05/23 09:00:39.79
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":356,"skipped":6635,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.837 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:00:28.959
    Jan  5 09:00:28.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename watch 01/05/23 09:00:28.96
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:00:28.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:00:28.993
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/05/23 09:00:28.995
    STEP: creating a new configmap 01/05/23 09:00:28.997
    STEP: modifying the configmap once 01/05/23 09:00:29.018
    STEP: changing the label value of the configmap 01/05/23 09:00:29.024
    STEP: Expecting to observe a delete notification for the watched object 01/05/23 09:00:29.664
    Jan  5 09:00:29.665: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34678 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:00:29.665: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34679 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:00:29.665: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34681 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/05/23 09:00:29.665
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/05/23 09:00:29.673
    STEP: changing the label value of the configmap back 01/05/23 09:00:39.674
    STEP: modifying the configmap a third time 01/05/23 09:00:39.774
    STEP: deleting the configmap 01/05/23 09:00:39.78
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/05/23 09:00:39.787
    Jan  5 09:00:39.787: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34724 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:00:39.787: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34725 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 09:00:39.787: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-572  0f2b5d77-f5f2-4527-af63-e2b34c5c0ece 34726 0 2023-01-05 09:00:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 09:00:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  5 09:00:39.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-572" for this suite. 01/05/23 09:00:39.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:00:39.796
Jan  5 09:00:39.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename job 01/05/23 09:00:39.797
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:00:39.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:00:39.816
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/05/23 09:00:39.823
STEP: Ensuring job reaches completions 01/05/23 09:00:39.826
STEP: Ensuring pods with index for job exist 01/05/23 09:00:57.831
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  5 09:00:57.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1633" for this suite. 01/05/23 09:00:57.838
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":357,"skipped":6640,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.045 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:00:39.796
    Jan  5 09:00:39.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename job 01/05/23 09:00:39.797
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:00:39.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:00:39.816
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/05/23 09:00:39.823
    STEP: Ensuring job reaches completions 01/05/23 09:00:39.826
    STEP: Ensuring pods with index for job exist 01/05/23 09:00:57.831
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  5 09:00:57.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1633" for this suite. 01/05/23 09:00:57.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:00:57.842
Jan  5 09:00:57.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename projected 01/05/23 09:00:57.843
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:00:57.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:00:57.867
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/05/23 09:00:57.87
Jan  5 09:00:57.875: INFO: Waiting up to 5m0s for pod "annotationupdate86101d26-74aa-408e-a73f-cba22e848b11" in namespace "projected-2520" to be "running and ready"
Jan  5 09:00:57.890: INFO: Pod "annotationupdate86101d26-74aa-408e-a73f-cba22e848b11": Phase="Pending", Reason="", readiness=false. Elapsed: 14.944288ms
Jan  5 09:00:57.890: INFO: The phase of Pod annotationupdate86101d26-74aa-408e-a73f-cba22e848b11 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 09:00:59.893: INFO: Pod "annotationupdate86101d26-74aa-408e-a73f-cba22e848b11": Phase="Running", Reason="", readiness=true. Elapsed: 2.017815464s
Jan  5 09:00:59.893: INFO: The phase of Pod annotationupdate86101d26-74aa-408e-a73f-cba22e848b11 is Running (Ready = true)
Jan  5 09:00:59.893: INFO: Pod "annotationupdate86101d26-74aa-408e-a73f-cba22e848b11" satisfied condition "running and ready"
Jan  5 09:01:00.414: INFO: Successfully updated pod "annotationupdate86101d26-74aa-408e-a73f-cba22e848b11"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  5 09:01:02.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2520" for this suite. 01/05/23 09:01:02.431
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":358,"skipped":6672,"failed":0}
------------------------------
â€¢ [4.593 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:00:57.842
    Jan  5 09:00:57.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename projected 01/05/23 09:00:57.843
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:00:57.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:00:57.867
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/05/23 09:00:57.87
    Jan  5 09:00:57.875: INFO: Waiting up to 5m0s for pod "annotationupdate86101d26-74aa-408e-a73f-cba22e848b11" in namespace "projected-2520" to be "running and ready"
    Jan  5 09:00:57.890: INFO: Pod "annotationupdate86101d26-74aa-408e-a73f-cba22e848b11": Phase="Pending", Reason="", readiness=false. Elapsed: 14.944288ms
    Jan  5 09:00:57.890: INFO: The phase of Pod annotationupdate86101d26-74aa-408e-a73f-cba22e848b11 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 09:00:59.893: INFO: Pod "annotationupdate86101d26-74aa-408e-a73f-cba22e848b11": Phase="Running", Reason="", readiness=true. Elapsed: 2.017815464s
    Jan  5 09:00:59.893: INFO: The phase of Pod annotationupdate86101d26-74aa-408e-a73f-cba22e848b11 is Running (Ready = true)
    Jan  5 09:00:59.893: INFO: Pod "annotationupdate86101d26-74aa-408e-a73f-cba22e848b11" satisfied condition "running and ready"
    Jan  5 09:01:00.414: INFO: Successfully updated pod "annotationupdate86101d26-74aa-408e-a73f-cba22e848b11"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  5 09:01:02.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2520" for this suite. 01/05/23 09:01:02.431
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:01:02.437
Jan  5 09:01:02.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename endpointslice 01/05/23 09:01:02.438
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:01:02.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:01:02.455
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/05/23 09:01:02.461
STEP: getting /apis/discovery.k8s.io 01/05/23 09:01:02.462
STEP: getting /apis/discovery.k8s.iov1 01/05/23 09:01:02.462
STEP: creating 01/05/23 09:01:02.463
STEP: getting 01/05/23 09:01:02.476
STEP: listing 01/05/23 09:01:02.477
STEP: watching 01/05/23 09:01:02.479
Jan  5 09:01:02.479: INFO: starting watch
STEP: cluster-wide listing 01/05/23 09:01:02.479
STEP: cluster-wide watching 01/05/23 09:01:02.487
Jan  5 09:01:02.488: INFO: starting watch
STEP: patching 01/05/23 09:01:02.488
STEP: updating 01/05/23 09:01:02.492
Jan  5 09:01:02.503: INFO: waiting for watch events with expected annotations
Jan  5 09:01:02.503: INFO: saw patched and updated annotations
STEP: deleting 01/05/23 09:01:02.503
STEP: deleting a collection 01/05/23 09:01:02.51
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  5 09:01:02.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7008" for this suite. 01/05/23 09:01:02.524
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":359,"skipped":6676,"failed":0}
------------------------------
â€¢ [0.099 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:01:02.437
    Jan  5 09:01:02.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename endpointslice 01/05/23 09:01:02.438
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:01:02.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:01:02.455
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/05/23 09:01:02.461
    STEP: getting /apis/discovery.k8s.io 01/05/23 09:01:02.462
    STEP: getting /apis/discovery.k8s.iov1 01/05/23 09:01:02.462
    STEP: creating 01/05/23 09:01:02.463
    STEP: getting 01/05/23 09:01:02.476
    STEP: listing 01/05/23 09:01:02.477
    STEP: watching 01/05/23 09:01:02.479
    Jan  5 09:01:02.479: INFO: starting watch
    STEP: cluster-wide listing 01/05/23 09:01:02.479
    STEP: cluster-wide watching 01/05/23 09:01:02.487
    Jan  5 09:01:02.488: INFO: starting watch
    STEP: patching 01/05/23 09:01:02.488
    STEP: updating 01/05/23 09:01:02.492
    Jan  5 09:01:02.503: INFO: waiting for watch events with expected annotations
    Jan  5 09:01:02.503: INFO: saw patched and updated annotations
    STEP: deleting 01/05/23 09:01:02.503
    STEP: deleting a collection 01/05/23 09:01:02.51
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  5 09:01:02.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7008" for this suite. 01/05/23 09:01:02.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:01:02.536
Jan  5 09:01:02.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename gc 01/05/23 09:01:02.537
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:01:02.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:01:02.557
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/05/23 09:01:02.561
STEP: delete the rc 01/05/23 09:01:07.57
STEP: wait for the rc to be deleted 01/05/23 09:01:07.578
Jan  5 09:01:08.585: INFO: 80 pods remaining
Jan  5 09:01:08.585: INFO: 80 pods has nil DeletionTimestamp
Jan  5 09:01:08.585: INFO: 
Jan  5 09:01:09.639: INFO: 75 pods remaining
Jan  5 09:01:09.639: INFO: 69 pods has nil DeletionTimestamp
Jan  5 09:01:09.639: INFO: 
Jan  5 09:01:10.684: INFO: 60 pods remaining
Jan  5 09:01:10.684: INFO: 59 pods has nil DeletionTimestamp
Jan  5 09:01:10.684: INFO: 
Jan  5 09:01:11.707: INFO: 41 pods remaining
Jan  5 09:01:11.707: INFO: 41 pods has nil DeletionTimestamp
Jan  5 09:01:11.707: INFO: 
Jan  5 09:01:13.251: INFO: 33 pods remaining
Jan  5 09:01:13.251: INFO: 28 pods has nil DeletionTimestamp
Jan  5 09:01:13.251: INFO: 
Jan  5 09:01:13.583: INFO: 14 pods remaining
Jan  5 09:01:13.583: INFO: 14 pods has nil DeletionTimestamp
Jan  5 09:01:13.583: INFO: 
STEP: Gathering metrics 01/05/23 09:01:14.581
W0105 09:01:14.583979      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 09:01:14.584: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  5 09:01:14.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5734" for this suite. 01/05/23 09:01:14.586
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":360,"skipped":6683,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.053 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:01:02.536
    Jan  5 09:01:02.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename gc 01/05/23 09:01:02.537
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:01:02.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:01:02.557
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/05/23 09:01:02.561
    STEP: delete the rc 01/05/23 09:01:07.57
    STEP: wait for the rc to be deleted 01/05/23 09:01:07.578
    Jan  5 09:01:08.585: INFO: 80 pods remaining
    Jan  5 09:01:08.585: INFO: 80 pods has nil DeletionTimestamp
    Jan  5 09:01:08.585: INFO: 
    Jan  5 09:01:09.639: INFO: 75 pods remaining
    Jan  5 09:01:09.639: INFO: 69 pods has nil DeletionTimestamp
    Jan  5 09:01:09.639: INFO: 
    Jan  5 09:01:10.684: INFO: 60 pods remaining
    Jan  5 09:01:10.684: INFO: 59 pods has nil DeletionTimestamp
    Jan  5 09:01:10.684: INFO: 
    Jan  5 09:01:11.707: INFO: 41 pods remaining
    Jan  5 09:01:11.707: INFO: 41 pods has nil DeletionTimestamp
    Jan  5 09:01:11.707: INFO: 
    Jan  5 09:01:13.251: INFO: 33 pods remaining
    Jan  5 09:01:13.251: INFO: 28 pods has nil DeletionTimestamp
    Jan  5 09:01:13.251: INFO: 
    Jan  5 09:01:13.583: INFO: 14 pods remaining
    Jan  5 09:01:13.583: INFO: 14 pods has nil DeletionTimestamp
    Jan  5 09:01:13.583: INFO: 
    STEP: Gathering metrics 01/05/23 09:01:14.581
    W0105 09:01:14.583979      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 09:01:14.584: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  5 09:01:14.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5734" for this suite. 01/05/23 09:01:14.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:01:14.589
Jan  5 09:01:14.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename controllerrevisions 01/05/23 09:01:14.59
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:01:14.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:01:14.611
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-ljmtb-daemon-set" 01/05/23 09:01:14.621
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:01:14.628
Jan  5 09:01:14.630: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:14.633: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:14.633: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:15.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:15.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:15.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:16.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:16.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:16.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:17.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:17.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:17.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:18.822: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:18.824: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:18.824: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:19.639: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:19.642: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:19.642: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:20.643: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:20.650: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:20.650: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:21.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:21.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:21.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:22.635: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:22.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:22.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:23.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:23.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:23.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:24.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:24.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:24.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:25.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:25.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:25.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:26.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:26.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:26.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:27.635: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:27.637: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:27.637: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:28.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:28.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:28.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:29.818: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:29.820: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:29.820: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:30.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:30.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:30.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:31.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:31.637: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:31.637: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:32.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:32.639: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:32.639: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:33.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:33.640: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:33.640: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:34.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:34.639: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:34.639: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:35.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:35.639: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:35.639: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:36.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:36.641: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:36.641: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:37.866: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:37.869: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:37.869: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:38.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:38.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:38.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:39.638: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:39.641: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:39.641: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:40.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:40.640: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
Jan  5 09:01:40.640: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:41.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:41.639: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
Jan  5 09:01:41.639: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:42.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:42.639: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
Jan  5 09:01:42.639: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:43.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:43.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
Jan  5 09:01:43.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:44.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:44.640: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
Jan  5 09:01:44.640: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:45.673: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:45.676: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
Jan  5 09:01:45.676: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:46.638: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:46.643: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
Jan  5 09:01:46.643: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:47.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:47.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
Jan  5 09:01:47.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:48.676: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:48.680: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
Jan  5 09:01:48.680: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:49.880: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:49.882: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
Jan  5 09:01:49.882: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:50.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:50.640: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
Jan  5 09:01:50.640: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
Jan  5 09:01:51.638: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan  5 09:01:51.641: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 2
Jan  5 09:01:51.641: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-ljmtb-daemon-set
STEP: Confirm DaemonSet "e2e-ljmtb-daemon-set" successfully created with "daemonset-name=e2e-ljmtb-daemon-set" label 01/05/23 09:01:51.643
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-ljmtb-daemon-set" 01/05/23 09:01:51.649
Jan  5 09:01:51.650: INFO: Located ControllerRevision: "e2e-ljmtb-daemon-set-c776cbfd4"
STEP: Patching ControllerRevision "e2e-ljmtb-daemon-set-c776cbfd4" 01/05/23 09:01:51.652
Jan  5 09:01:51.667: INFO: e2e-ljmtb-daemon-set-c776cbfd4 has been patched
STEP: Create a new ControllerRevision 01/05/23 09:01:51.667
Jan  5 09:01:51.693: INFO: Created ControllerRevision: e2e-ljmtb-daemon-set-86979b99f8
STEP: Confirm that there are two ControllerRevisions 01/05/23 09:01:51.693
Jan  5 09:01:51.693: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 09:01:51.695: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-ljmtb-daemon-set-c776cbfd4" 01/05/23 09:01:51.695
STEP: Confirm that there is only one ControllerRevision 01/05/23 09:01:51.699
Jan  5 09:01:51.699: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 09:01:51.700: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-ljmtb-daemon-set-86979b99f8" 01/05/23 09:01:51.702
Jan  5 09:01:51.726: INFO: e2e-ljmtb-daemon-set-86979b99f8 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/05/23 09:01:51.726
W0105 09:01:51.733026      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/05/23 09:01:51.733
Jan  5 09:01:51.733: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 09:01:52.735: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 09:01:52.738: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-ljmtb-daemon-set-86979b99f8=updated" 01/05/23 09:01:52.738
STEP: Confirm that there is only one ControllerRevision 01/05/23 09:01:52.742
Jan  5 09:01:52.742: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 09:01:52.744: INFO: Found 1 ControllerRevisions
Jan  5 09:01:52.746: INFO: ControllerRevision "e2e-ljmtb-daemon-set-7849fcd7b7" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-ljmtb-daemon-set" 01/05/23 09:01:52.747
STEP: deleting DaemonSet.extensions e2e-ljmtb-daemon-set in namespace controllerrevisions-3827, will wait for the garbage collector to delete the pods 01/05/23 09:01:52.748
Jan  5 09:01:52.806: INFO: Deleting DaemonSet.extensions e2e-ljmtb-daemon-set took: 5.420254ms
Jan  5 09:01:52.907: INFO: Terminating DaemonSet.extensions e2e-ljmtb-daemon-set pods took: 101.064076ms
Jan  5 09:01:53.809: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
Jan  5 09:01:53.809: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-ljmtb-daemon-set
Jan  5 09:01:53.811: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35929"},"items":null}

Jan  5 09:01:53.812: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35929"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan  5 09:01:53.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-3827" for this suite. 01/05/23 09:01:53.824
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":361,"skipped":6701,"failed":0}
------------------------------
â€¢ [SLOW TEST] [39.238 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:01:14.589
    Jan  5 09:01:14.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename controllerrevisions 01/05/23 09:01:14.59
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:01:14.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:01:14.611
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-ljmtb-daemon-set" 01/05/23 09:01:14.621
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 09:01:14.628
    Jan  5 09:01:14.630: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:14.633: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:14.633: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:15.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:15.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:15.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:16.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:16.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:16.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:17.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:17.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:17.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:18.822: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:18.824: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:18.824: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:19.639: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:19.642: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:19.642: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:20.643: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:20.650: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:20.650: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:21.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:21.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:21.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:22.635: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:22.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:22.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:23.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:23.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:23.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:24.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:24.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:24.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:25.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:25.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:25.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:26.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:26.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:26.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:27.635: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:27.637: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:27.637: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:28.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:28.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:28.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:29.818: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:29.820: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:29.820: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:30.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:30.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:30.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:31.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:31.637: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:31.637: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:32.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:32.639: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:32.639: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:33.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:33.640: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:33.640: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:34.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:34.639: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:34.639: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:35.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:35.639: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:35.639: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:36.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:36.641: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:36.641: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:37.866: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:37.869: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:37.869: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:38.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:38.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:38.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:39.638: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:39.641: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:39.641: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:40.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:40.640: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
    Jan  5 09:01:40.640: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:41.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:41.639: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
    Jan  5 09:01:41.639: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:42.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:42.639: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
    Jan  5 09:01:42.639: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:43.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:43.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
    Jan  5 09:01:43.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:44.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:44.640: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
    Jan  5 09:01:44.640: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:45.673: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:45.676: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
    Jan  5 09:01:45.676: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:46.638: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:46.643: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
    Jan  5 09:01:46.643: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:47.636: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:47.638: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
    Jan  5 09:01:47.638: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:48.676: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:48.680: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
    Jan  5 09:01:48.680: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:49.880: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:49.882: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
    Jan  5 09:01:49.882: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:50.637: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:50.640: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 1
    Jan  5 09:01:50.640: INFO: Node mip-bd-vm722.mip.storage.hpecorp.net is running 0 daemon pod, expected 1
    Jan  5 09:01:51.638: INFO: DaemonSet pods can't tolerate node mip-bd-vm721.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan  5 09:01:51.641: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 2
    Jan  5 09:01:51.641: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-ljmtb-daemon-set
    STEP: Confirm DaemonSet "e2e-ljmtb-daemon-set" successfully created with "daemonset-name=e2e-ljmtb-daemon-set" label 01/05/23 09:01:51.643
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-ljmtb-daemon-set" 01/05/23 09:01:51.649
    Jan  5 09:01:51.650: INFO: Located ControllerRevision: "e2e-ljmtb-daemon-set-c776cbfd4"
    STEP: Patching ControllerRevision "e2e-ljmtb-daemon-set-c776cbfd4" 01/05/23 09:01:51.652
    Jan  5 09:01:51.667: INFO: e2e-ljmtb-daemon-set-c776cbfd4 has been patched
    STEP: Create a new ControllerRevision 01/05/23 09:01:51.667
    Jan  5 09:01:51.693: INFO: Created ControllerRevision: e2e-ljmtb-daemon-set-86979b99f8
    STEP: Confirm that there are two ControllerRevisions 01/05/23 09:01:51.693
    Jan  5 09:01:51.693: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 09:01:51.695: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-ljmtb-daemon-set-c776cbfd4" 01/05/23 09:01:51.695
    STEP: Confirm that there is only one ControllerRevision 01/05/23 09:01:51.699
    Jan  5 09:01:51.699: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 09:01:51.700: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-ljmtb-daemon-set-86979b99f8" 01/05/23 09:01:51.702
    Jan  5 09:01:51.726: INFO: e2e-ljmtb-daemon-set-86979b99f8 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/05/23 09:01:51.726
    W0105 09:01:51.733026      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/05/23 09:01:51.733
    Jan  5 09:01:51.733: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 09:01:52.735: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 09:01:52.738: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-ljmtb-daemon-set-86979b99f8=updated" 01/05/23 09:01:52.738
    STEP: Confirm that there is only one ControllerRevision 01/05/23 09:01:52.742
    Jan  5 09:01:52.742: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 09:01:52.744: INFO: Found 1 ControllerRevisions
    Jan  5 09:01:52.746: INFO: ControllerRevision "e2e-ljmtb-daemon-set-7849fcd7b7" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-ljmtb-daemon-set" 01/05/23 09:01:52.747
    STEP: deleting DaemonSet.extensions e2e-ljmtb-daemon-set in namespace controllerrevisions-3827, will wait for the garbage collector to delete the pods 01/05/23 09:01:52.748
    Jan  5 09:01:52.806: INFO: Deleting DaemonSet.extensions e2e-ljmtb-daemon-set took: 5.420254ms
    Jan  5 09:01:52.907: INFO: Terminating DaemonSet.extensions e2e-ljmtb-daemon-set pods took: 101.064076ms
    Jan  5 09:01:53.809: INFO: Number of nodes with available pods controlled by daemonset e2e-ljmtb-daemon-set: 0
    Jan  5 09:01:53.809: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-ljmtb-daemon-set
    Jan  5 09:01:53.811: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35929"},"items":null}

    Jan  5 09:01:53.812: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35929"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan  5 09:01:53.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-3827" for this suite. 01/05/23 09:01:53.824
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/05/23 09:01:53.828
Jan  5 09:01:53.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
STEP: Building a namespace api object, basename services 01/05/23 09:01:53.828
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:01:53.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:01:53.851
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7059 01/05/23 09:01:53.853
STEP: changing the ExternalName service to type=ClusterIP 01/05/23 09:01:53.855
STEP: creating replication controller externalname-service in namespace services-7059 01/05/23 09:01:53.876
I0105 09:01:53.885855      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7059, replica count: 2
I0105 09:01:56.937357      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 09:01:56.937: INFO: Creating new exec pod
Jan  5 09:01:56.942: INFO: Waiting up to 5m0s for pod "execpod6pjmk" in namespace "services-7059" to be "running"
Jan  5 09:01:56.943: INFO: Pod "execpod6pjmk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.833083ms
Jan  5 09:01:58.949: INFO: Pod "execpod6pjmk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007497481s
Jan  5 09:02:00.948: INFO: Pod "execpod6pjmk": Phase="Running", Reason="", readiness=true. Elapsed: 4.006388862s
Jan  5 09:02:00.948: INFO: Pod "execpod6pjmk" satisfied condition "running"
Jan  5 09:02:01.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7059 exec execpod6pjmk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan  5 09:02:02.079: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  5 09:02:02.079: INFO: stdout: "externalname-service-7694w"
Jan  5 09:02:02.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7059 exec execpod6pjmk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.12.23 80'
Jan  5 09:02:02.206: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.12.23 80\nConnection to 10.98.12.23 80 port [tcp/http] succeeded!\n"
Jan  5 09:02:02.206: INFO: stdout: "externalname-service-q45bc"
Jan  5 09:02:02.206: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  5 09:02:02.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7059" for this suite. 01/05/23 09:02:02.231
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":362,"skipped":6702,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.411 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/05/23 09:01:53.828
    Jan  5 09:01:53.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1138513488
    STEP: Building a namespace api object, basename services 01/05/23 09:01:53.828
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 09:01:53.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 09:01:53.851
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7059 01/05/23 09:01:53.853
    STEP: changing the ExternalName service to type=ClusterIP 01/05/23 09:01:53.855
    STEP: creating replication controller externalname-service in namespace services-7059 01/05/23 09:01:53.876
    I0105 09:01:53.885855      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7059, replica count: 2
    I0105 09:01:56.937357      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 09:01:56.937: INFO: Creating new exec pod
    Jan  5 09:01:56.942: INFO: Waiting up to 5m0s for pod "execpod6pjmk" in namespace "services-7059" to be "running"
    Jan  5 09:01:56.943: INFO: Pod "execpod6pjmk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.833083ms
    Jan  5 09:01:58.949: INFO: Pod "execpod6pjmk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007497481s
    Jan  5 09:02:00.948: INFO: Pod "execpod6pjmk": Phase="Running", Reason="", readiness=true. Elapsed: 4.006388862s
    Jan  5 09:02:00.948: INFO: Pod "execpod6pjmk" satisfied condition "running"
    Jan  5 09:02:01.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7059 exec execpod6pjmk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan  5 09:02:02.079: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  5 09:02:02.079: INFO: stdout: "externalname-service-7694w"
    Jan  5 09:02:02.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1138513488 --namespace=services-7059 exec execpod6pjmk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.12.23 80'
    Jan  5 09:02:02.206: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.12.23 80\nConnection to 10.98.12.23 80 port [tcp/http] succeeded!\n"
    Jan  5 09:02:02.206: INFO: stdout: "externalname-service-q45bc"
    Jan  5 09:02:02.206: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  5 09:02:02.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7059" for this suite. 01/05/23 09:02:02.231
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Jan  5 09:02:02.240: INFO: Running AfterSuite actions on all nodes
Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan  5 09:02:02.240: INFO: Running AfterSuite actions on node 1
Jan  5 09:02:02.240: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan  5 09:02:02.240: INFO: Running AfterSuite actions on all nodes
    Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan  5 09:02:02.240: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan  5 09:02:02.240: INFO: Running AfterSuite actions on node 1
    Jan  5 09:02:02.240: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.054 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 5952.577 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h39m12.772923493s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

