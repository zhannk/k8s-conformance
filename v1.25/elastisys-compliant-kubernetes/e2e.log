I0302 12:35:47.135094      20 e2e.go:116] Starting e2e run "21912a32-1801-4cee-be61-4faef0396552" on Ginkgo node 1
Mar  2 12:35:47.150: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1677760546 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Mar  2 12:35:47.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 12:35:47.311: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  2 12:35:47.338: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  2 12:35:47.387: INFO: 40 / 40 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  2 12:35:47.387: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Mar  2 12:35:47.387: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  2 12:35:47.400: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-accountant' (0 seconds elapsed)
Mar  2 12:35:47.400: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar  2 12:35:47.400: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Mar  2 12:35:47.400: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  2 12:35:47.400: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Mar  2 12:35:47.400: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Mar  2 12:35:47.400: INFO: e2e test version: v1.25.6
Mar  2 12:35:47.401: INFO: kube-apiserver version: v1.25.6
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Mar  2 12:35:47.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 12:35:47.430: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.130 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar  2 12:35:47.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 12:35:47.311: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Mar  2 12:35:47.338: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Mar  2 12:35:47.387: INFO: 40 / 40 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar  2 12:35:47.387: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
    Mar  2 12:35:47.387: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar  2 12:35:47.400: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-accountant' (0 seconds elapsed)
    Mar  2 12:35:47.400: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Mar  2 12:35:47.400: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
    Mar  2 12:35:47.400: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Mar  2 12:35:47.400: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Mar  2 12:35:47.400: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
    Mar  2 12:35:47.400: INFO: e2e test version: v1.25.6
    Mar  2 12:35:47.401: INFO: kube-apiserver version: v1.25.6
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar  2 12:35:47.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 12:35:47.430: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:35:47.528
Mar  2 12:35:47.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename resourcequota 03/02/23 12:35:47.537
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:35:47.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:35:47.58
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 03/02/23 12:35:47.602
STEP: Ensuring ResourceQuota status is calculated 03/02/23 12:35:47.62
STEP: Creating a ResourceQuota with not best effort scope 03/02/23 12:35:49.63
STEP: Ensuring ResourceQuota status is calculated 03/02/23 12:35:49.639
STEP: Creating a best-effort pod 03/02/23 12:35:51.648
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/02/23 12:35:51.697
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/02/23 12:35:53.701
STEP: Deleting the pod 03/02/23 12:35:55.707
STEP: Ensuring resource quota status released the pod usage 03/02/23 12:35:55.734
STEP: Creating a not best-effort pod 03/02/23 12:35:57.743
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/02/23 12:35:57.763
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/02/23 12:35:59.771
STEP: Deleting the pod 03/02/23 12:36:01.781
STEP: Ensuring resource quota status released the pod usage 03/02/23 12:36:01.804
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 12:36:03.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4403" for this suite. 03/02/23 12:36:03.826
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":1,"skipped":7,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.304 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:35:47.528
    Mar  2 12:35:47.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename resourcequota 03/02/23 12:35:47.537
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:35:47.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:35:47.58
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 03/02/23 12:35:47.602
    STEP: Ensuring ResourceQuota status is calculated 03/02/23 12:35:47.62
    STEP: Creating a ResourceQuota with not best effort scope 03/02/23 12:35:49.63
    STEP: Ensuring ResourceQuota status is calculated 03/02/23 12:35:49.639
    STEP: Creating a best-effort pod 03/02/23 12:35:51.648
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/02/23 12:35:51.697
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/02/23 12:35:53.701
    STEP: Deleting the pod 03/02/23 12:35:55.707
    STEP: Ensuring resource quota status released the pod usage 03/02/23 12:35:55.734
    STEP: Creating a not best-effort pod 03/02/23 12:35:57.743
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/02/23 12:35:57.763
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/02/23 12:35:59.771
    STEP: Deleting the pod 03/02/23 12:36:01.781
    STEP: Ensuring resource quota status released the pod usage 03/02/23 12:36:01.804
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 12:36:03.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4403" for this suite. 03/02/23 12:36:03.826
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:36:03.838
Mar  2 12:36:03.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pods 03/02/23 12:36:03.844
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:03.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:03.879
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 03/02/23 12:36:03.882
STEP: setting up watch 03/02/23 12:36:03.883
STEP: submitting the pod to kubernetes 03/02/23 12:36:03.985
STEP: verifying the pod is in kubernetes 03/02/23 12:36:04.012
STEP: verifying pod creation was observed 03/02/23 12:36:04.027
Mar  2 12:36:04.031: INFO: Waiting up to 5m0s for pod "pod-submit-remove-58921329-8986-4c5d-aeda-23c56d82ed3b" in namespace "pods-6495" to be "running"
Mar  2 12:36:04.039: INFO: Pod "pod-submit-remove-58921329-8986-4c5d-aeda-23c56d82ed3b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058034ms
Mar  2 12:36:06.049: INFO: Pod "pod-submit-remove-58921329-8986-4c5d-aeda-23c56d82ed3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.018485374s
Mar  2 12:36:06.049: INFO: Pod "pod-submit-remove-58921329-8986-4c5d-aeda-23c56d82ed3b" satisfied condition "running"
STEP: deleting the pod gracefully 03/02/23 12:36:06.054
STEP: verifying pod deletion was observed 03/02/23 12:36:06.061
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 12:36:08.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6495" for this suite. 03/02/23 12:36:08.425
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":2,"skipped":11,"failed":0}
------------------------------
â€¢ [4.593 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:36:03.838
    Mar  2 12:36:03.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pods 03/02/23 12:36:03.844
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:03.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:03.879
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 03/02/23 12:36:03.882
    STEP: setting up watch 03/02/23 12:36:03.883
    STEP: submitting the pod to kubernetes 03/02/23 12:36:03.985
    STEP: verifying the pod is in kubernetes 03/02/23 12:36:04.012
    STEP: verifying pod creation was observed 03/02/23 12:36:04.027
    Mar  2 12:36:04.031: INFO: Waiting up to 5m0s for pod "pod-submit-remove-58921329-8986-4c5d-aeda-23c56d82ed3b" in namespace "pods-6495" to be "running"
    Mar  2 12:36:04.039: INFO: Pod "pod-submit-remove-58921329-8986-4c5d-aeda-23c56d82ed3b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058034ms
    Mar  2 12:36:06.049: INFO: Pod "pod-submit-remove-58921329-8986-4c5d-aeda-23c56d82ed3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.018485374s
    Mar  2 12:36:06.049: INFO: Pod "pod-submit-remove-58921329-8986-4c5d-aeda-23c56d82ed3b" satisfied condition "running"
    STEP: deleting the pod gracefully 03/02/23 12:36:06.054
    STEP: verifying pod deletion was observed 03/02/23 12:36:06.061
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 12:36:08.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6495" for this suite. 03/02/23 12:36:08.425
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:36:08.432
Mar  2 12:36:08.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 12:36:08.434
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:08.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:08.479
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-ba134caa-7281-4944-a3b6-86d3eb296205 03/02/23 12:36:08.485
STEP: Creating a pod to test consume configMaps 03/02/23 12:36:08.493
Mar  2 12:36:08.503: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c" in namespace "projected-7862" to be "Succeeded or Failed"
Mar  2 12:36:08.509: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.185951ms
Mar  2 12:36:10.515: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010971131s
Mar  2 12:36:12.516: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012294863s
Mar  2 12:36:14.514: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010508125s
Mar  2 12:36:16.525: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021141655s
STEP: Saw pod success 03/02/23 12:36:16.525
Mar  2 12:36:16.525: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c" satisfied condition "Succeeded or Failed"
Mar  2 12:36:16.539: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c container agnhost-container: <nil>
STEP: delete the pod 03/02/23 12:36:16.545
Mar  2 12:36:16.565: INFO: Waiting for pod pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c to disappear
Mar  2 12:36:16.571: INFO: Pod pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 12:36:16.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7862" for this suite. 03/02/23 12:36:16.587
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":3,"skipped":13,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.166 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:36:08.432
    Mar  2 12:36:08.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 12:36:08.434
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:08.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:08.479
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-ba134caa-7281-4944-a3b6-86d3eb296205 03/02/23 12:36:08.485
    STEP: Creating a pod to test consume configMaps 03/02/23 12:36:08.493
    Mar  2 12:36:08.503: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c" in namespace "projected-7862" to be "Succeeded or Failed"
    Mar  2 12:36:08.509: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.185951ms
    Mar  2 12:36:10.515: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010971131s
    Mar  2 12:36:12.516: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012294863s
    Mar  2 12:36:14.514: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010508125s
    Mar  2 12:36:16.525: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021141655s
    STEP: Saw pod success 03/02/23 12:36:16.525
    Mar  2 12:36:16.525: INFO: Pod "pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c" satisfied condition "Succeeded or Failed"
    Mar  2 12:36:16.539: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 12:36:16.545
    Mar  2 12:36:16.565: INFO: Waiting for pod pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c to disappear
    Mar  2 12:36:16.571: INFO: Pod pod-projected-configmaps-92145c82-a2fe-4207-b36d-21c6b8cfbd5c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 12:36:16.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7862" for this suite. 03/02/23 12:36:16.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:36:16.605
Mar  2 12:36:16.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename init-container 03/02/23 12:36:16.606
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:16.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:16.639
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 03/02/23 12:36:16.643
Mar  2 12:36:16.643: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 12:36:24.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9723" for this suite. 03/02/23 12:36:24.529
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":4,"skipped":24,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.936 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:36:16.605
    Mar  2 12:36:16.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename init-container 03/02/23 12:36:16.606
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:16.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:16.639
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 03/02/23 12:36:16.643
    Mar  2 12:36:16.643: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 12:36:24.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9723" for this suite. 03/02/23 12:36:24.529
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:36:24.542
Mar  2 12:36:24.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sched-pred 03/02/23 12:36:24.544
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:24.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:24.571
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 12:36:24.585: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 12:36:24.599: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 12:36:24.609: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv0 before test
Mar  2 12:36:24.661: INFO: falco-exporter-r2zfx from falco started at 2023-02-27 15:51:02 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.663: INFO: 	Container falco-exporter ready: true, restart count 3
Mar  2 12:36:24.663: INFO: falco-falcosidekick-5d5c7d4db-r6952 from falco started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.663: INFO: 	Container falcosidekick ready: true, restart count 0
Mar  2 12:36:24.663: INFO: falco-trjnh from falco started at 2023-02-27 15:53:31 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.663: INFO: 	Container falco ready: true, restart count 1
Mar  2 12:36:24.663: INFO: fluentd-forwarder-qxbtj from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.663: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 12:36:24.663: INFO: ingress-nginx-controller-qprg2 from ingress-nginx started at 2023-02-27 13:57:01 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.663: INFO: 	Container controller ready: true, restart count 2
Mar  2 12:36:24.663: INFO: calico-accountant-k6t4p from kube-system started at 2023-02-27 13:50:57 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.663: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 12:36:24.663: INFO: calico-node-dz84l from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.663: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 12:36:24.663: INFO: coredns-588bb58b94-k6j76 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.663: INFO: 	Container coredns ready: true, restart count 0
Mar  2 12:36:24.663: INFO: csi-cinder-nodeplugin-9nct9 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
Mar  2 12:36:24.663: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
Mar  2 12:36:24.663: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 12:36:24.663: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 12:36:24.663: INFO: kube-proxy-7bm9z from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.663: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 12:36:24.664: INFO: nginx-proxy-aarnq-sc-k8s-node-srv0 from kube-system started at 2023-02-27 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 12:36:24.664: INFO: node-local-dns-dk8hd from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 12:36:24.664: INFO: snapshot-controller-7d445c66c9-6w4w5 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 12:36:24.664: INFO: kured-hdkrw from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container kured ready: true, restart count 3
Mar  2 12:36:24.664: INFO: alertmanager-kube-prometheus-stack-alertmanager-0 from monitoring started at 2023-03-01 07:32:10 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 12:36:24.664: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 12:36:24.664: INFO: kube-prometheus-stack-grafana-84f79f467b-sr7kl from monitoring started at 2023-02-28 08:03:49 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container grafana ready: true, restart count 0
Mar  2 12:36:24.664: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar  2 12:36:24.664: INFO: kube-prometheus-stack-prometheus-node-exporter-nl9pw from monitoring started at 2023-02-27 13:49:55 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 12:36:24.664: INFO: prometheus-blackbox-exporter-677b579798-7xm9d from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar  2 12:36:24.664: INFO: s3-exporter-867c5b9457-lfcsf from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container s3-exporter ready: true, restart count 0
Mar  2 12:36:24.664: INFO: opensearch-dashboards-58c8d95f7b-9spcv from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container dashboards ready: true, restart count 0
Mar  2 12:36:24.664: INFO: opensearch-master-2 from opensearch-system started at 2023-02-28 08:03:54 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 12:36:24.664: INFO: prometheus-opensearch-exporter-5688c84dcd-95vjh from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container exporter ready: true, restart count 0
Mar  2 12:36:24.664: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-qv9vz from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 12:36:24.664: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 12:36:24.664: INFO: thanos-query-query-69fc6f554b-db7w4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container query ready: true, restart count 0
Mar  2 12:36:24.664: INFO: thanos-receiver-bucketweb-b4955fcf8-8w2xg from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container bucketweb ready: true, restart count 0
Mar  2 12:36:24.664: INFO: thanos-receiver-compactor-848df7b5d7-z2jh4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container compactor ready: true, restart count 0
Mar  2 12:36:24.664: INFO: thanos-receiver-receive-0 from thanos started at 2023-02-28 08:04:00 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container receive ready: true, restart count 0
Mar  2 12:36:24.664: INFO: thanos-receiver-receive-distributor-779c5d74d8-7hhmb from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container receive ready: true, restart count 0
Mar  2 12:36:24.664: INFO: thanos-receiver-ruler-0 from thanos started at 2023-02-28 08:03:52 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 12:36:24.664: INFO: 	Container ruler ready: true, restart count 0
Mar  2 12:36:24.664: INFO: restic-k4z4s from velero started at 2023-02-27 13:13:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container restic ready: true, restart count 2
Mar  2 12:36:24.664: INFO: velero-7bbd458dfc-s8n2h from velero started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.664: INFO: 	Container velero ready: true, restart count 0
Mar  2 12:36:24.664: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv1 before test
Mar  2 12:36:24.711: INFO: cert-manager-754b766f8b-fvh5z from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.712: INFO: 	Container cert-manager-controller ready: true, restart count 0
Mar  2 12:36:24.712: INFO: cert-manager-webhook-875cdf98f-lfgn7 from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.712: INFO: 	Container cert-manager-webhook ready: true, restart count 0
Mar  2 12:36:24.712: INFO: dex-58d8c68494-flfvv from dex started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.712: INFO: 	Container dex ready: true, restart count 0
Mar  2 12:36:24.713: INFO: falco-8cz2x from falco started at 2023-02-27 15:54:18 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.713: INFO: 	Container falco ready: true, restart count 1
Mar  2 12:36:24.713: INFO: falco-exporter-srbrb from falco started at 2023-02-27 15:51:10 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.713: INFO: 	Container falco-exporter ready: true, restart count 3
Mar  2 12:36:24.713: INFO: fluentd-aggregator-0 from fluentd-system started at 2023-02-28 08:16:03 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.713: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 12:36:24.713: INFO: fluentd-forwarder-zgcds from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.713: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 12:36:24.714: INFO: harbor-chartmuseum-5c9477455d-hp9zb from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.714: INFO: 	Container chartmuseum ready: true, restart count 0
Mar  2 12:36:24.714: INFO: harbor-core-58dc955656-2vz5k from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.714: INFO: 	Container core ready: true, restart count 1
Mar  2 12:36:24.714: INFO: harbor-database-0 from harbor started at 2023-02-28 08:16:02 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.714: INFO: 	Container database ready: true, restart count 0
Mar  2 12:36:24.714: INFO: harbor-jobservice-69c4c778fb-8qt7s from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.714: INFO: 	Container jobservice ready: true, restart count 2
Mar  2 12:36:24.714: INFO: harbor-notary-server-6cfdf66b5-sxpqw from harbor started at 2023-02-28 08:15:44 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.715: INFO: 	Container notary-server ready: true, restart count 2
Mar  2 12:36:24.715: INFO: harbor-notary-signer-5d6d45f584-rfqm7 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.715: INFO: 	Container notary-signer ready: true, restart count 2
Mar  2 12:36:24.715: INFO: harbor-portal-77d6c78fd9-p7t57 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.715: INFO: 	Container portal ready: true, restart count 0
Mar  2 12:36:24.715: INFO: harbor-redis-0 from harbor started at 2023-02-28 08:16:04 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.715: INFO: 	Container redis ready: true, restart count 0
Mar  2 12:36:24.715: INFO: harbor-registry-787bfb74d7-9vbht from harbor started at 2023-02-28 08:15:42 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.716: INFO: 	Container registry ready: true, restart count 0
Mar  2 12:36:24.716: INFO: 	Container registryctl ready: true, restart count 0
Mar  2 12:36:24.716: INFO: harbor-trivy-0 from harbor started at 2023-02-28 08:15:58 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.716: INFO: 	Container trivy ready: true, restart count 0
Mar  2 12:36:24.716: INFO: ingress-nginx-controller-8jd6t from ingress-nginx started at 2023-02-27 13:52:07 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.716: INFO: 	Container controller ready: true, restart count 2
Mar  2 12:36:24.716: INFO: calico-accountant-sfvmv from kube-system started at 2023-02-27 13:50:54 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.716: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 12:36:24.716: INFO: calico-node-vj6gp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.716: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 12:36:24.716: INFO: csi-cinder-nodeplugin-lvpvh from kube-system started at 2023-02-27 13:28:22 +0000 UTC (3 container statuses recorded)
Mar  2 12:36:24.716: INFO: 	Container cinder-csi-plugin ready: true, restart count 2
Mar  2 12:36:24.716: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 12:36:24.716: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 12:36:24.716: INFO: kube-proxy-nrgbs from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.716: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 12:36:24.716: INFO: metrics-server-d9dcc77d6-z4sx7 from kube-system started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.716: INFO: 	Container metrics-server ready: true, restart count 0
Mar  2 12:36:24.716: INFO: nginx-proxy-aarnq-sc-k8s-node-srv1 from kube-system started at 2023-02-27 14:17:16 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.716: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 12:36:24.716: INFO: node-local-dns-b8kzp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.716: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 12:36:24.717: INFO: kured-kbmf8 from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.717: INFO: 	Container kured ready: true, restart count 3
Mar  2 12:36:24.717: INFO: alertmanager-kube-prometheus-stack-alertmanager-1 from monitoring started at 2023-02-28 08:16:00 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.717: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 12:36:24.717: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 12:36:24.717: INFO: kube-prometheus-stack-prometheus-node-exporter-jmbsj from monitoring started at 2023-02-27 13:49:59 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.717: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 12:36:24.717: INFO: user-grafana-6f7c7d589-q6clc from monitoring started at 2023-02-28 08:15:43 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.717: INFO: 	Container grafana ready: true, restart count 0
Mar  2 12:36:24.717: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar  2 12:36:24.717: INFO: opensearch-master-0 from opensearch-system started at 2023-02-28 08:16:01 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.717: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 12:36:24.717: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-j5shm from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 12:36:24.717: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 12:36:24.717: INFO: thanos-query-query-69fc6f554b-d2q5v from thanos started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.717: INFO: 	Container query ready: true, restart count 0
Mar  2 12:36:24.717: INFO: thanos-receiver-receive-2 from thanos started at 2023-02-28 08:16:05 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.717: INFO: 	Container receive ready: true, restart count 0
Mar  2 12:36:24.717: INFO: thanos-receiver-ruler-1 from thanos started at 2023-02-28 08:15:57 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.717: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 12:36:24.717: INFO: 	Container ruler ready: true, restart count 0
Mar  2 12:36:24.718: INFO: restic-zvhnj from velero started at 2023-02-27 13:13:38 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.718: INFO: 	Container restic ready: true, restart count 2
Mar  2 12:36:24.718: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv2 before test
Mar  2 12:36:24.754: INFO: falco-4c5wt from falco started at 2023-02-27 15:51:55 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container falco ready: true, restart count 3
Mar  2 12:36:24.754: INFO: falco-exporter-cvpnp from falco started at 2023-02-27 15:51:06 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container falco-exporter ready: true, restart count 5
Mar  2 12:36:24.754: INFO: aarnq-sc-logs-logs-compaction-27961830-qfpkw from fluentd-system started at 2023-03-01 22:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container compaction ready: false, restart count 0
Mar  2 12:36:24.754: INFO: aarnq-sc-logs-logs-retention-27961890-98gmj from fluentd-system started at 2023-03-01 23:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container retention ready: false, restart count 0
Mar  2 12:36:24.754: INFO: fluentd-forwarder-b54pr from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container fluentd-forwarder ready: true, restart count 3
Mar  2 12:36:24.754: INFO: harbor-backup-cronjob-27961920-n2gpm from harbor started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container run ready: false, restart count 0
Mar  2 12:36:24.754: INFO: ingress-nginx-controller-lbvdv from ingress-nginx started at 2023-02-27 13:51:24 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container controller ready: true, restart count 3
Mar  2 12:36:24.754: INFO: pod-init-3aad4ffd-ae74-4aa3-98a6-faea2957e3b8 from init-container-9723 started at 2023-03-02 12:36:16 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container run1 ready: true, restart count 0
Mar  2 12:36:24.754: INFO: calico-accountant-sb26b from kube-system started at 2023-02-27 13:50:50 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container calico-accountant ready: true, restart count 3
Mar  2 12:36:24.754: INFO: calico-node-9ps2k from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container calico-node ready: true, restart count 3
Mar  2 12:36:24.754: INFO: csi-cinder-nodeplugin-8qsk5 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container cinder-csi-plugin ready: true, restart count 9
Mar  2 12:36:24.754: INFO: 	Container liveness-probe ready: true, restart count 3
Mar  2 12:36:24.754: INFO: 	Container node-driver-registrar ready: true, restart count 3
Mar  2 12:36:24.754: INFO: kube-proxy-nrj68 from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container kube-proxy ready: true, restart count 3
Mar  2 12:36:24.754: INFO: nginx-proxy-aarnq-sc-k8s-node-srv2 from kube-system started at 2023-02-28 07:06:42 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container nginx-proxy ready: true, restart count 3
Mar  2 12:36:24.754: INFO: node-local-dns-pwwsn from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container node-cache ready: true, restart count 3
Mar  2 12:36:24.754: INFO: kured-fmhzj from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container kured ready: true, restart count 4
Mar  2 12:36:24.754: INFO: ciskubebench-exporter-68bcb66c46-rjnj9 from monitoring started at 2023-03-02 11:51:13 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container metrics-collector ready: true, restart count 0
Mar  2 12:36:24.754: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 12:36:24.754: INFO: kube-prometheus-stack-prometheus-node-exporter-rk7sg from monitoring started at 2023-02-27 13:49:53 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container node-exporter ready: true, restart count 3
Mar  2 12:36:24.754: INFO: scan-vulnerabilityreport-6f57675ddd-td6bx from monitoring started at 2023-03-02 12:36:12 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 12:36:24.754: INFO: starboard-operator-7f84bbf756-grncj from monitoring started at 2023-03-02 11:50:59 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container starboard-operator ready: true, restart count 0
Mar  2 12:36:24.754: INFO: vulnerability-exporter-8485469578-jvppp from monitoring started at 2023-03-02 11:51:29 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container metrics-collector ready: true, restart count 0
Mar  2 12:36:24.754: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 12:36:24.754: INFO: opensearch-curator-27962675-rqczr from opensearch-system started at 2023-03-02 12:35:00 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container opensearch-curator ready: false, restart count 0
Mar  2 12:36:24.754: INFO: sonobuoy from sonobuoy started at 2023-03-02 12:35:12 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 12:36:24.754: INFO: sonobuoy-e2e-job-eae18696d9844ddc from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container e2e ready: true, restart count 0
Mar  2 12:36:24.754: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 12:36:24.754: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-zf5bk from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 12:36:24.754: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 12:36:24.754: INFO: restic-wcgdp from velero started at 2023-02-27 13:41:53 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.754: INFO: 	Container restic ready: true, restart count 3
Mar  2 12:36:24.754: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv3 before test
Mar  2 12:36:24.793: INFO: cert-manager-cainjector-655cfbc4d-vc586 from cert-manager started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.793: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
Mar  2 12:36:24.793: INFO: dex-58d8c68494-gnrr5 from dex started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.793: INFO: 	Container dex ready: true, restart count 0
Mar  2 12:36:24.793: INFO: falco-9v9b5 from falco started at 2023-02-27 15:52:43 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.793: INFO: 	Container falco ready: true, restart count 1
Mar  2 12:36:24.793: INFO: falco-exporter-457cd from falco started at 2023-02-27 15:51:13 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.793: INFO: 	Container falco-exporter ready: true, restart count 3
Mar  2 12:36:24.793: INFO: falco-falcosidekick-5d5c7d4db-hddwc from falco started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.793: INFO: 	Container falcosidekick ready: true, restart count 0
Mar  2 12:36:24.793: INFO: aarnq-sc-logs-logs-compaction-27960390-dkhl6 from fluentd-system started at 2023-02-28 22:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.793: INFO: 	Container compaction ready: false, restart count 0
Mar  2 12:36:24.793: INFO: aarnq-sc-logs-logs-retention-27960450-xntms from fluentd-system started at 2023-02-28 23:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.793: INFO: 	Container retention ready: false, restart count 0
Mar  2 12:36:24.793: INFO: fluentd-forwarder-9smtw from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.793: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 12:36:24.793: INFO: harbor-backup-cronjob-27960480-trvs5 from harbor started at 2023-03-01 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.793: INFO: 	Container run ready: false, restart count 0
Mar  2 12:36:24.793: INFO: ingress-nginx-controller-4bgc8 from ingress-nginx started at 2023-02-27 13:54:29 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.793: INFO: 	Container controller ready: true, restart count 2
Mar  2 12:36:24.793: INFO: ingress-nginx-default-backend-64599cb78d-t9m7m from ingress-nginx started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Mar  2 12:36:24.794: INFO: calico-accountant-wgpwj from kube-system started at 2023-02-27 13:50:52 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 12:36:24.794: INFO: calico-node-7vgvf from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 12:36:24.794: INFO: csi-cinder-controllerplugin-6fdb685467-qppqd from kube-system started at 2023-03-01 07:31:56 +0000 UTC (6 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  2 12:36:24.794: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  2 12:36:24.794: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  2 12:36:24.794: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  2 12:36:24.794: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar  2 12:36:24.794: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 12:36:24.794: INFO: csi-cinder-nodeplugin-wn26q from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
Mar  2 12:36:24.794: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 12:36:24.794: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 12:36:24.794: INFO: kube-proxy-t9sqm from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 12:36:24.794: INFO: nginx-proxy-aarnq-sc-k8s-node-srv3 from kube-system started at 2023-02-27 13:14:14 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 12:36:24.794: INFO: node-local-dns-jf9nv from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 12:36:24.794: INFO: kured-g9qpk from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container kured ready: true, restart count 3
Mar  2 12:36:24.794: INFO: grafana-label-enforcer-ff6966584-d9872 from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container prom-label-enforcer ready: true, restart count 0
Mar  2 12:36:24.794: INFO: kube-prometheus-stack-kube-state-metrics-5584579f7d-jmrqj from monitoring started at 2023-03-01 07:31:57 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 12:36:24.794: INFO: kube-prometheus-stack-operator-6bd84664f-wxlxc from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Mar  2 12:36:24.794: INFO: kube-prometheus-stack-prometheus-node-exporter-9v6v5 from monitoring started at 2023-02-27 13:50:02 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 12:36:24.794: INFO: prometheus-kube-prometheus-stack-prometheus-0 from monitoring started at 2023-03-01 07:32:02 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 12:36:24.794: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 12:36:24.794: INFO: opensearch-master-1 from opensearch-system started at 2023-03-01 07:32:10 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 12:36:24.794: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-m5t49 from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 12:36:24.794: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 12:36:24.794: INFO: thanos-query-query-frontend-6c58dbdc6-6g7jx from thanos started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container query-frontend ready: true, restart count 0
Mar  2 12:36:24.794: INFO: thanos-receiver-receive-1 from thanos started at 2023-03-01 07:32:11 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container receive ready: true, restart count 0
Mar  2 12:36:24.794: INFO: thanos-receiver-storegateway-0 from thanos started at 2023-03-01 07:32:06 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container storegateway ready: true, restart count 0
Mar  2 12:36:24.794: INFO: restic-hwpfm from velero started at 2023-02-27 13:13:47 +0000 UTC (1 container statuses recorded)
Mar  2 12:36:24.794: INFO: 	Container restic ready: true, restart count 3
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node aarnq-sc-k8s-node-srv0 03/02/23 12:36:24.898
STEP: verifying the node has the label node aarnq-sc-k8s-node-srv1 03/02/23 12:36:24.918
STEP: verifying the node has the label node aarnq-sc-k8s-node-srv2 03/02/23 12:36:24.942
STEP: verifying the node has the label node aarnq-sc-k8s-node-srv3 03/02/23 12:36:24.973
Mar  2 12:36:25.074: INFO: Pod cert-manager-754b766f8b-fvh5z requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod cert-manager-cainjector-655cfbc4d-vc586 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod cert-manager-webhook-875cdf98f-lfgn7 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod dex-58d8c68494-flfvv requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod dex-58d8c68494-gnrr5 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod falco-4c5wt requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod falco-8cz2x requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod falco-9v9b5 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod falco-exporter-457cd requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod falco-exporter-cvpnp requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod falco-exporter-r2zfx requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod falco-exporter-srbrb requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod falco-falcosidekick-5d5c7d4db-hddwc requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod falco-falcosidekick-5d5c7d4db-r6952 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod falco-trjnh requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod fluentd-aggregator-0 requesting resource cpu=300m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod fluentd-forwarder-9smtw requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod fluentd-forwarder-b54pr requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod fluentd-forwarder-qxbtj requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod fluentd-forwarder-zgcds requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod harbor-chartmuseum-5c9477455d-hp9zb requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod harbor-core-58dc955656-2vz5k requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod harbor-database-0 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod harbor-jobservice-69c4c778fb-8qt7s requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod harbor-notary-server-6cfdf66b5-sxpqw requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod harbor-notary-signer-5d6d45f584-rfqm7 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod harbor-portal-77d6c78fd9-p7t57 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod harbor-redis-0 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod harbor-registry-787bfb74d7-9vbht requesting resource cpu=20m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod harbor-trivy-0 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod ingress-nginx-controller-4bgc8 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod ingress-nginx-controller-8jd6t requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod ingress-nginx-controller-lbvdv requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod ingress-nginx-controller-qprg2 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod ingress-nginx-default-backend-64599cb78d-t9m7m requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod pod-init-3aad4ffd-ae74-4aa3-98a6-faea2957e3b8 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod calico-accountant-k6t4p requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod calico-accountant-sb26b requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod calico-accountant-sfvmv requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod calico-accountant-wgpwj requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod calico-node-7vgvf requesting resource cpu=150m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod calico-node-9ps2k requesting resource cpu=150m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod calico-node-dz84l requesting resource cpu=150m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod calico-node-vj6gp requesting resource cpu=150m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod coredns-588bb58b94-k6j76 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod csi-cinder-controllerplugin-6fdb685467-qppqd requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod csi-cinder-nodeplugin-8qsk5 requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod csi-cinder-nodeplugin-9nct9 requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod csi-cinder-nodeplugin-lvpvh requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod csi-cinder-nodeplugin-wn26q requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod kube-proxy-7bm9z requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod kube-proxy-nrgbs requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod kube-proxy-nrj68 requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod kube-proxy-t9sqm requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod metrics-server-d9dcc77d6-z4sx7 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod nginx-proxy-aarnq-sc-k8s-node-srv0 requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod nginx-proxy-aarnq-sc-k8s-node-srv1 requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod nginx-proxy-aarnq-sc-k8s-node-srv2 requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod nginx-proxy-aarnq-sc-k8s-node-srv3 requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod node-local-dns-b8kzp requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod node-local-dns-dk8hd requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod node-local-dns-jf9nv requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod node-local-dns-pwwsn requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod snapshot-controller-7d445c66c9-6w4w5 requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod kured-fmhzj requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod kured-g9qpk requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod kured-hdkrw requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod kured-kbmf8 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod alertmanager-kube-prometheus-stack-alertmanager-0 requesting resource cpu=60m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod alertmanager-kube-prometheus-stack-alertmanager-1 requesting resource cpu=60m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod ciskubebench-exporter-68bcb66c46-rjnj9 requesting resource cpu=30m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod grafana-label-enforcer-ff6966584-d9872 requesting resource cpu=20m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-grafana-84f79f467b-sr7kl requesting resource cpu=110m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-kube-state-metrics-5584579f7d-jmrqj requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-operator-6bd84664f-wxlxc requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-prometheus-node-exporter-9v6v5 requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-prometheus-node-exporter-jmbsj requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-prometheus-node-exporter-nl9pw requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-prometheus-node-exporter-rk7sg requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod prometheus-blackbox-exporter-677b579798-7xm9d requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod prometheus-kube-prometheus-stack-prometheus-0 requesting resource cpu=250m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.074: INFO: Pod s3-exporter-867c5b9457-lfcsf requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.074: INFO: Pod scan-vulnerabilityreport-6f57675ddd-td6bx requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod starboard-operator-7f84bbf756-grncj requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.074: INFO: Pod user-grafana-6f7c7d589-q6clc requesting resource cpu=110m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.074: INFO: Pod vulnerability-exporter-8485469578-jvppp requesting resource cpu=30m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.075: INFO: Pod opensearch-dashboards-58c8d95f7b-9spcv requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.075: INFO: Pod opensearch-master-0 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.075: INFO: Pod opensearch-master-1 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.075: INFO: Pod opensearch-master-2 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.075: INFO: Pod prometheus-opensearch-exporter-5688c84dcd-95vjh requesting resource cpu=15m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.075: INFO: Pod sonobuoy requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.075: INFO: Pod sonobuoy-e2e-job-eae18696d9844ddc requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.075: INFO: Pod sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-j5shm requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.075: INFO: Pod sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-m5t49 requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.075: INFO: Pod sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-qv9vz requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.075: INFO: Pod sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-zf5bk requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.075: INFO: Pod thanos-query-query-69fc6f554b-d2q5v requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.075: INFO: Pod thanos-query-query-69fc6f554b-db7w4 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.075: INFO: Pod thanos-query-query-frontend-6c58dbdc6-6g7jx requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.075: INFO: Pod thanos-receiver-bucketweb-b4955fcf8-8w2xg requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.075: INFO: Pod thanos-receiver-compactor-848df7b5d7-z2jh4 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.075: INFO: Pod thanos-receiver-receive-0 requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.075: INFO: Pod thanos-receiver-receive-1 requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.075: INFO: Pod thanos-receiver-receive-2 requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.075: INFO: Pod thanos-receiver-receive-distributor-779c5d74d8-7hhmb requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.075: INFO: Pod thanos-receiver-ruler-0 requesting resource cpu=110m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.075: INFO: Pod thanos-receiver-ruler-1 requesting resource cpu=110m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.075: INFO: Pod thanos-receiver-storegateway-0 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.075: INFO: Pod restic-hwpfm requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.075: INFO: Pod restic-k4z4s requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.075: INFO: Pod restic-wcgdp requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.075: INFO: Pod restic-zvhnj requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.075: INFO: Pod velero-7bbd458dfc-s8n2h requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv0
STEP: Starting Pods to consume most of the cluster CPU. 03/02/23 12:36:25.075
Mar  2 12:36:25.075: INFO: Creating a pod which consumes cpu=255m on Node aarnq-sc-k8s-node-srv0
Mar  2 12:36:25.090: INFO: Creating a pod which consumes cpu=196m on Node aarnq-sc-k8s-node-srv1
Mar  2 12:36:25.106: INFO: Creating a pod which consumes cpu=735m on Node aarnq-sc-k8s-node-srv2
Mar  2 12:36:25.116: INFO: Creating a pod which consumes cpu=336m on Node aarnq-sc-k8s-node-srv3
Mar  2 12:36:25.130: INFO: Waiting up to 5m0s for pod "filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a" in namespace "sched-pred-7610" to be "running"
Mar  2 12:36:25.146: INFO: Pod "filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.046056ms
Mar  2 12:36:27.158: INFO: Pod "filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a": Phase="Running", Reason="", readiness=true. Elapsed: 2.027539638s
Mar  2 12:36:27.158: INFO: Pod "filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a" satisfied condition "running"
Mar  2 12:36:27.158: INFO: Waiting up to 5m0s for pod "filler-pod-6fad30cc-254b-4670-9fd9-52e109294257" in namespace "sched-pred-7610" to be "running"
Mar  2 12:36:27.167: INFO: Pod "filler-pod-6fad30cc-254b-4670-9fd9-52e109294257": Phase="Running", Reason="", readiness=true. Elapsed: 8.397753ms
Mar  2 12:36:27.167: INFO: Pod "filler-pod-6fad30cc-254b-4670-9fd9-52e109294257" satisfied condition "running"
Mar  2 12:36:27.167: INFO: Waiting up to 5m0s for pod "filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd" in namespace "sched-pred-7610" to be "running"
Mar  2 12:36:27.171: INFO: Pod "filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd": Phase="Running", Reason="", readiness=true. Elapsed: 3.89467ms
Mar  2 12:36:27.171: INFO: Pod "filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd" satisfied condition "running"
Mar  2 12:36:27.171: INFO: Waiting up to 5m0s for pod "filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe" in namespace "sched-pred-7610" to be "running"
Mar  2 12:36:27.174: INFO: Pod "filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.231521ms
Mar  2 12:36:29.182: INFO: Pod "filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe": Phase="Running", Reason="", readiness=true. Elapsed: 2.010759524s
Mar  2 12:36:29.182: INFO: Pod "filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/02/23 12:36:29.182
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd.17489a5ceef5ecca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7610/filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd to aarnq-sc-k8s-node-srv2] 03/02/23 12:36:29.188
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd.17489a5d231fe7d0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 12:36:29.189
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd.17489a5d2840dd5f], Reason = [Created], Message = [Created container filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd] 03/02/23 12:36:29.189
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd.17489a5d2f4a83ce], Reason = [Started], Message = [Started container filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd] 03/02/23 12:36:29.189
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6fad30cc-254b-4670-9fd9-52e109294257.17489a5cee6f9e01], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7610/filler-pod-6fad30cc-254b-4670-9fd9-52e109294257 to aarnq-sc-k8s-node-srv1] 03/02/23 12:36:29.189
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6fad30cc-254b-4670-9fd9-52e109294257.17489a5d20da3877], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 12:36:29.189
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6fad30cc-254b-4670-9fd9-52e109294257.17489a5d2458b636], Reason = [Created], Message = [Created container filler-pod-6fad30cc-254b-4670-9fd9-52e109294257] 03/02/23 12:36:29.189
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6fad30cc-254b-4670-9fd9-52e109294257.17489a5d33465b35], Reason = [Started], Message = [Started container filler-pod-6fad30cc-254b-4670-9fd9-52e109294257] 03/02/23 12:36:29.19
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a.17489a5ced6625a8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7610/filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a to aarnq-sc-k8s-node-srv0] 03/02/23 12:36:29.19
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a.17489a5d230be8c6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 12:36:29.19
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a.17489a5d25879c89], Reason = [Created], Message = [Created container filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a] 03/02/23 12:36:29.19
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a.17489a5d310c5f73], Reason = [Started], Message = [Started container filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a] 03/02/23 12:36:29.191
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe.17489a5cef9ab054], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7610/filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe to aarnq-sc-k8s-node-srv3] 03/02/23 12:36:29.191
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe.17489a5d2be5ebec], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 12:36:29.191
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe.17489a5d2f244822], Reason = [Created], Message = [Created container filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe] 03/02/23 12:36:29.191
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe.17489a5d3c8f37e1], Reason = [Started], Message = [Started container filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe] 03/02/23 12:36:29.192
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17489a5de19648e3], Reason = [FailedScheduling], Message = [0/5 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 4 Insufficient cpu. preemption: 0/5 nodes are available: 1 Preemption is not helpful for scheduling, 4 No preemption victims found for incoming pod.] 03/02/23 12:36:29.211
STEP: removing the label node off the node aarnq-sc-k8s-node-srv0 03/02/23 12:36:30.223
STEP: verifying the node doesn't have the label node 03/02/23 12:36:30.238
STEP: removing the label node off the node aarnq-sc-k8s-node-srv1 03/02/23 12:36:30.248
STEP: verifying the node doesn't have the label node 03/02/23 12:36:30.275
STEP: removing the label node off the node aarnq-sc-k8s-node-srv2 03/02/23 12:36:30.282
STEP: verifying the node doesn't have the label node 03/02/23 12:36:30.299
STEP: removing the label node off the node aarnq-sc-k8s-node-srv3 03/02/23 12:36:30.303
STEP: verifying the node doesn't have the label node 03/02/23 12:36:30.318
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  2 12:36:30.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7610" for this suite. 03/02/23 12:36:30.362
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":5,"skipped":25,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.829 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:36:24.542
    Mar  2 12:36:24.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sched-pred 03/02/23 12:36:24.544
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:24.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:24.571
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  2 12:36:24.585: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  2 12:36:24.599: INFO: Waiting for terminating namespaces to be deleted...
    Mar  2 12:36:24.609: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv0 before test
    Mar  2 12:36:24.661: INFO: falco-exporter-r2zfx from falco started at 2023-02-27 15:51:02 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.663: INFO: 	Container falco-exporter ready: true, restart count 3
    Mar  2 12:36:24.663: INFO: falco-falcosidekick-5d5c7d4db-r6952 from falco started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.663: INFO: 	Container falcosidekick ready: true, restart count 0
    Mar  2 12:36:24.663: INFO: falco-trjnh from falco started at 2023-02-27 15:53:31 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.663: INFO: 	Container falco ready: true, restart count 1
    Mar  2 12:36:24.663: INFO: fluentd-forwarder-qxbtj from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.663: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 12:36:24.663: INFO: ingress-nginx-controller-qprg2 from ingress-nginx started at 2023-02-27 13:57:01 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.663: INFO: 	Container controller ready: true, restart count 2
    Mar  2 12:36:24.663: INFO: calico-accountant-k6t4p from kube-system started at 2023-02-27 13:50:57 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.663: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 12:36:24.663: INFO: calico-node-dz84l from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.663: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 12:36:24.663: INFO: coredns-588bb58b94-k6j76 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.663: INFO: 	Container coredns ready: true, restart count 0
    Mar  2 12:36:24.663: INFO: csi-cinder-nodeplugin-9nct9 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
    Mar  2 12:36:24.663: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
    Mar  2 12:36:24.663: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 12:36:24.663: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 12:36:24.663: INFO: kube-proxy-7bm9z from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.663: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 12:36:24.664: INFO: nginx-proxy-aarnq-sc-k8s-node-srv0 from kube-system started at 2023-02-27 14:10:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 12:36:24.664: INFO: node-local-dns-dk8hd from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 12:36:24.664: INFO: snapshot-controller-7d445c66c9-6w4w5 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: kured-hdkrw from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container kured ready: true, restart count 3
    Mar  2 12:36:24.664: INFO: alertmanager-kube-prometheus-stack-alertmanager-0 from monitoring started at 2023-03-01 07:32:10 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container alertmanager ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: kube-prometheus-stack-grafana-84f79f467b-sr7kl from monitoring started at 2023-02-28 08:03:49 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container grafana ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: kube-prometheus-stack-prometheus-node-exporter-nl9pw from monitoring started at 2023-02-27 13:49:55 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 12:36:24.664: INFO: prometheus-blackbox-exporter-677b579798-7xm9d from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: s3-exporter-867c5b9457-lfcsf from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container s3-exporter ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: opensearch-dashboards-58c8d95f7b-9spcv from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container dashboards ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: opensearch-master-2 from opensearch-system started at 2023-02-28 08:03:54 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: prometheus-opensearch-exporter-5688c84dcd-95vjh from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container exporter ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-qv9vz from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: thanos-query-query-69fc6f554b-db7w4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container query ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: thanos-receiver-bucketweb-b4955fcf8-8w2xg from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container bucketweb ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: thanos-receiver-compactor-848df7b5d7-z2jh4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container compactor ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: thanos-receiver-receive-0 from thanos started at 2023-02-28 08:04:00 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container receive ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: thanos-receiver-receive-distributor-779c5d74d8-7hhmb from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container receive ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: thanos-receiver-ruler-0 from thanos started at 2023-02-28 08:03:52 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: 	Container ruler ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: restic-k4z4s from velero started at 2023-02-27 13:13:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container restic ready: true, restart count 2
    Mar  2 12:36:24.664: INFO: velero-7bbd458dfc-s8n2h from velero started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.664: INFO: 	Container velero ready: true, restart count 0
    Mar  2 12:36:24.664: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv1 before test
    Mar  2 12:36:24.711: INFO: cert-manager-754b766f8b-fvh5z from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.712: INFO: 	Container cert-manager-controller ready: true, restart count 0
    Mar  2 12:36:24.712: INFO: cert-manager-webhook-875cdf98f-lfgn7 from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.712: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    Mar  2 12:36:24.712: INFO: dex-58d8c68494-flfvv from dex started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.712: INFO: 	Container dex ready: true, restart count 0
    Mar  2 12:36:24.713: INFO: falco-8cz2x from falco started at 2023-02-27 15:54:18 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.713: INFO: 	Container falco ready: true, restart count 1
    Mar  2 12:36:24.713: INFO: falco-exporter-srbrb from falco started at 2023-02-27 15:51:10 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.713: INFO: 	Container falco-exporter ready: true, restart count 3
    Mar  2 12:36:24.713: INFO: fluentd-aggregator-0 from fluentd-system started at 2023-02-28 08:16:03 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.713: INFO: 	Container fluentd ready: true, restart count 0
    Mar  2 12:36:24.713: INFO: fluentd-forwarder-zgcds from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.713: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 12:36:24.714: INFO: harbor-chartmuseum-5c9477455d-hp9zb from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.714: INFO: 	Container chartmuseum ready: true, restart count 0
    Mar  2 12:36:24.714: INFO: harbor-core-58dc955656-2vz5k from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.714: INFO: 	Container core ready: true, restart count 1
    Mar  2 12:36:24.714: INFO: harbor-database-0 from harbor started at 2023-02-28 08:16:02 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.714: INFO: 	Container database ready: true, restart count 0
    Mar  2 12:36:24.714: INFO: harbor-jobservice-69c4c778fb-8qt7s from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.714: INFO: 	Container jobservice ready: true, restart count 2
    Mar  2 12:36:24.714: INFO: harbor-notary-server-6cfdf66b5-sxpqw from harbor started at 2023-02-28 08:15:44 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.715: INFO: 	Container notary-server ready: true, restart count 2
    Mar  2 12:36:24.715: INFO: harbor-notary-signer-5d6d45f584-rfqm7 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.715: INFO: 	Container notary-signer ready: true, restart count 2
    Mar  2 12:36:24.715: INFO: harbor-portal-77d6c78fd9-p7t57 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.715: INFO: 	Container portal ready: true, restart count 0
    Mar  2 12:36:24.715: INFO: harbor-redis-0 from harbor started at 2023-02-28 08:16:04 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.715: INFO: 	Container redis ready: true, restart count 0
    Mar  2 12:36:24.715: INFO: harbor-registry-787bfb74d7-9vbht from harbor started at 2023-02-28 08:15:42 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.716: INFO: 	Container registry ready: true, restart count 0
    Mar  2 12:36:24.716: INFO: 	Container registryctl ready: true, restart count 0
    Mar  2 12:36:24.716: INFO: harbor-trivy-0 from harbor started at 2023-02-28 08:15:58 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.716: INFO: 	Container trivy ready: true, restart count 0
    Mar  2 12:36:24.716: INFO: ingress-nginx-controller-8jd6t from ingress-nginx started at 2023-02-27 13:52:07 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.716: INFO: 	Container controller ready: true, restart count 2
    Mar  2 12:36:24.716: INFO: calico-accountant-sfvmv from kube-system started at 2023-02-27 13:50:54 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.716: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 12:36:24.716: INFO: calico-node-vj6gp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.716: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 12:36:24.716: INFO: csi-cinder-nodeplugin-lvpvh from kube-system started at 2023-02-27 13:28:22 +0000 UTC (3 container statuses recorded)
    Mar  2 12:36:24.716: INFO: 	Container cinder-csi-plugin ready: true, restart count 2
    Mar  2 12:36:24.716: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 12:36:24.716: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 12:36:24.716: INFO: kube-proxy-nrgbs from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.716: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 12:36:24.716: INFO: metrics-server-d9dcc77d6-z4sx7 from kube-system started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.716: INFO: 	Container metrics-server ready: true, restart count 0
    Mar  2 12:36:24.716: INFO: nginx-proxy-aarnq-sc-k8s-node-srv1 from kube-system started at 2023-02-27 14:17:16 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.716: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 12:36:24.716: INFO: node-local-dns-b8kzp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.716: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 12:36:24.717: INFO: kured-kbmf8 from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.717: INFO: 	Container kured ready: true, restart count 3
    Mar  2 12:36:24.717: INFO: alertmanager-kube-prometheus-stack-alertmanager-1 from monitoring started at 2023-02-28 08:16:00 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.717: INFO: 	Container alertmanager ready: true, restart count 0
    Mar  2 12:36:24.717: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 12:36:24.717: INFO: kube-prometheus-stack-prometheus-node-exporter-jmbsj from monitoring started at 2023-02-27 13:49:59 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.717: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 12:36:24.717: INFO: user-grafana-6f7c7d589-q6clc from monitoring started at 2023-02-28 08:15:43 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.717: INFO: 	Container grafana ready: true, restart count 0
    Mar  2 12:36:24.717: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Mar  2 12:36:24.717: INFO: opensearch-master-0 from opensearch-system started at 2023-02-28 08:16:01 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.717: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 12:36:24.717: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-j5shm from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 12:36:24.717: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 12:36:24.717: INFO: thanos-query-query-69fc6f554b-d2q5v from thanos started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.717: INFO: 	Container query ready: true, restart count 0
    Mar  2 12:36:24.717: INFO: thanos-receiver-receive-2 from thanos started at 2023-02-28 08:16:05 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.717: INFO: 	Container receive ready: true, restart count 0
    Mar  2 12:36:24.717: INFO: thanos-receiver-ruler-1 from thanos started at 2023-02-28 08:15:57 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.717: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 12:36:24.717: INFO: 	Container ruler ready: true, restart count 0
    Mar  2 12:36:24.718: INFO: restic-zvhnj from velero started at 2023-02-27 13:13:38 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.718: INFO: 	Container restic ready: true, restart count 2
    Mar  2 12:36:24.718: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv2 before test
    Mar  2 12:36:24.754: INFO: falco-4c5wt from falco started at 2023-02-27 15:51:55 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container falco ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: falco-exporter-cvpnp from falco started at 2023-02-27 15:51:06 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container falco-exporter ready: true, restart count 5
    Mar  2 12:36:24.754: INFO: aarnq-sc-logs-logs-compaction-27961830-qfpkw from fluentd-system started at 2023-03-01 22:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container compaction ready: false, restart count 0
    Mar  2 12:36:24.754: INFO: aarnq-sc-logs-logs-retention-27961890-98gmj from fluentd-system started at 2023-03-01 23:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container retention ready: false, restart count 0
    Mar  2 12:36:24.754: INFO: fluentd-forwarder-b54pr from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container fluentd-forwarder ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: harbor-backup-cronjob-27961920-n2gpm from harbor started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container run ready: false, restart count 0
    Mar  2 12:36:24.754: INFO: ingress-nginx-controller-lbvdv from ingress-nginx started at 2023-02-27 13:51:24 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container controller ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: pod-init-3aad4ffd-ae74-4aa3-98a6-faea2957e3b8 from init-container-9723 started at 2023-03-02 12:36:16 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container run1 ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: calico-accountant-sb26b from kube-system started at 2023-02-27 13:50:50 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container calico-accountant ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: calico-node-9ps2k from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container calico-node ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: csi-cinder-nodeplugin-8qsk5 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container cinder-csi-plugin ready: true, restart count 9
    Mar  2 12:36:24.754: INFO: 	Container liveness-probe ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: 	Container node-driver-registrar ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: kube-proxy-nrj68 from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container kube-proxy ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: nginx-proxy-aarnq-sc-k8s-node-srv2 from kube-system started at 2023-02-28 07:06:42 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container nginx-proxy ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: node-local-dns-pwwsn from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container node-cache ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: kured-fmhzj from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container kured ready: true, restart count 4
    Mar  2 12:36:24.754: INFO: ciskubebench-exporter-68bcb66c46-rjnj9 from monitoring started at 2023-03-02 11:51:13 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container metrics-collector ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: kube-prometheus-stack-prometheus-node-exporter-rk7sg from monitoring started at 2023-02-27 13:49:53 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container node-exporter ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: scan-vulnerabilityreport-6f57675ddd-td6bx from monitoring started at 2023-03-02 12:36:12 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: starboard-operator-7f84bbf756-grncj from monitoring started at 2023-03-02 11:50:59 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container starboard-operator ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: vulnerability-exporter-8485469578-jvppp from monitoring started at 2023-03-02 11:51:29 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container metrics-collector ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: opensearch-curator-27962675-rqczr from opensearch-system started at 2023-03-02 12:35:00 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container opensearch-curator ready: false, restart count 0
    Mar  2 12:36:24.754: INFO: sonobuoy from sonobuoy started at 2023-03-02 12:35:12 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: sonobuoy-e2e-job-eae18696d9844ddc from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container e2e ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-zf5bk from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 12:36:24.754: INFO: restic-wcgdp from velero started at 2023-02-27 13:41:53 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.754: INFO: 	Container restic ready: true, restart count 3
    Mar  2 12:36:24.754: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv3 before test
    Mar  2 12:36:24.793: INFO: cert-manager-cainjector-655cfbc4d-vc586 from cert-manager started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.793: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    Mar  2 12:36:24.793: INFO: dex-58d8c68494-gnrr5 from dex started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.793: INFO: 	Container dex ready: true, restart count 0
    Mar  2 12:36:24.793: INFO: falco-9v9b5 from falco started at 2023-02-27 15:52:43 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.793: INFO: 	Container falco ready: true, restart count 1
    Mar  2 12:36:24.793: INFO: falco-exporter-457cd from falco started at 2023-02-27 15:51:13 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.793: INFO: 	Container falco-exporter ready: true, restart count 3
    Mar  2 12:36:24.793: INFO: falco-falcosidekick-5d5c7d4db-hddwc from falco started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.793: INFO: 	Container falcosidekick ready: true, restart count 0
    Mar  2 12:36:24.793: INFO: aarnq-sc-logs-logs-compaction-27960390-dkhl6 from fluentd-system started at 2023-02-28 22:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.793: INFO: 	Container compaction ready: false, restart count 0
    Mar  2 12:36:24.793: INFO: aarnq-sc-logs-logs-retention-27960450-xntms from fluentd-system started at 2023-02-28 23:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.793: INFO: 	Container retention ready: false, restart count 0
    Mar  2 12:36:24.793: INFO: fluentd-forwarder-9smtw from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.793: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 12:36:24.793: INFO: harbor-backup-cronjob-27960480-trvs5 from harbor started at 2023-03-01 00:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.793: INFO: 	Container run ready: false, restart count 0
    Mar  2 12:36:24.793: INFO: ingress-nginx-controller-4bgc8 from ingress-nginx started at 2023-02-27 13:54:29 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.793: INFO: 	Container controller ready: true, restart count 2
    Mar  2 12:36:24.793: INFO: ingress-nginx-default-backend-64599cb78d-t9m7m from ingress-nginx started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: calico-accountant-wgpwj from kube-system started at 2023-02-27 13:50:52 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 12:36:24.794: INFO: calico-node-7vgvf from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 12:36:24.794: INFO: csi-cinder-controllerplugin-6fdb685467-qppqd from kube-system started at 2023-03-01 07:31:56 +0000 UTC (6 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: csi-cinder-nodeplugin-wn26q from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
    Mar  2 12:36:24.794: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 12:36:24.794: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 12:36:24.794: INFO: kube-proxy-t9sqm from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 12:36:24.794: INFO: nginx-proxy-aarnq-sc-k8s-node-srv3 from kube-system started at 2023-02-27 13:14:14 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 12:36:24.794: INFO: node-local-dns-jf9nv from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 12:36:24.794: INFO: kured-g9qpk from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container kured ready: true, restart count 3
    Mar  2 12:36:24.794: INFO: grafana-label-enforcer-ff6966584-d9872 from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container prom-label-enforcer ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: kube-prometheus-stack-kube-state-metrics-5584579f7d-jmrqj from monitoring started at 2023-03-01 07:31:57 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: kube-prometheus-stack-operator-6bd84664f-wxlxc from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: kube-prometheus-stack-prometheus-node-exporter-9v6v5 from monitoring started at 2023-02-27 13:50:02 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 12:36:24.794: INFO: prometheus-kube-prometheus-stack-prometheus-0 from monitoring started at 2023-03-01 07:32:02 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: opensearch-master-1 from opensearch-system started at 2023-03-01 07:32:10 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-m5t49 from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: thanos-query-query-frontend-6c58dbdc6-6g7jx from thanos started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container query-frontend ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: thanos-receiver-receive-1 from thanos started at 2023-03-01 07:32:11 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container receive ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: thanos-receiver-storegateway-0 from thanos started at 2023-03-01 07:32:06 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container storegateway ready: true, restart count 0
    Mar  2 12:36:24.794: INFO: restic-hwpfm from velero started at 2023-02-27 13:13:47 +0000 UTC (1 container statuses recorded)
    Mar  2 12:36:24.794: INFO: 	Container restic ready: true, restart count 3
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node aarnq-sc-k8s-node-srv0 03/02/23 12:36:24.898
    STEP: verifying the node has the label node aarnq-sc-k8s-node-srv1 03/02/23 12:36:24.918
    STEP: verifying the node has the label node aarnq-sc-k8s-node-srv2 03/02/23 12:36:24.942
    STEP: verifying the node has the label node aarnq-sc-k8s-node-srv3 03/02/23 12:36:24.973
    Mar  2 12:36:25.074: INFO: Pod cert-manager-754b766f8b-fvh5z requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod cert-manager-cainjector-655cfbc4d-vc586 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod cert-manager-webhook-875cdf98f-lfgn7 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod dex-58d8c68494-flfvv requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod dex-58d8c68494-gnrr5 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod falco-4c5wt requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod falco-8cz2x requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod falco-9v9b5 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod falco-exporter-457cd requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod falco-exporter-cvpnp requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod falco-exporter-r2zfx requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod falco-exporter-srbrb requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod falco-falcosidekick-5d5c7d4db-hddwc requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod falco-falcosidekick-5d5c7d4db-r6952 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod falco-trjnh requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod fluentd-aggregator-0 requesting resource cpu=300m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod fluentd-forwarder-9smtw requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod fluentd-forwarder-b54pr requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod fluentd-forwarder-qxbtj requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod fluentd-forwarder-zgcds requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod harbor-chartmuseum-5c9477455d-hp9zb requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod harbor-core-58dc955656-2vz5k requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod harbor-database-0 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod harbor-jobservice-69c4c778fb-8qt7s requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod harbor-notary-server-6cfdf66b5-sxpqw requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod harbor-notary-signer-5d6d45f584-rfqm7 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod harbor-portal-77d6c78fd9-p7t57 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod harbor-redis-0 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod harbor-registry-787bfb74d7-9vbht requesting resource cpu=20m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod harbor-trivy-0 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod ingress-nginx-controller-4bgc8 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod ingress-nginx-controller-8jd6t requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod ingress-nginx-controller-lbvdv requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod ingress-nginx-controller-qprg2 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod ingress-nginx-default-backend-64599cb78d-t9m7m requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod pod-init-3aad4ffd-ae74-4aa3-98a6-faea2957e3b8 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod calico-accountant-k6t4p requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod calico-accountant-sb26b requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod calico-accountant-sfvmv requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod calico-accountant-wgpwj requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod calico-node-7vgvf requesting resource cpu=150m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod calico-node-9ps2k requesting resource cpu=150m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod calico-node-dz84l requesting resource cpu=150m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod calico-node-vj6gp requesting resource cpu=150m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod coredns-588bb58b94-k6j76 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod csi-cinder-controllerplugin-6fdb685467-qppqd requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod csi-cinder-nodeplugin-8qsk5 requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod csi-cinder-nodeplugin-9nct9 requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod csi-cinder-nodeplugin-lvpvh requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod csi-cinder-nodeplugin-wn26q requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod kube-proxy-7bm9z requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod kube-proxy-nrgbs requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod kube-proxy-nrj68 requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod kube-proxy-t9sqm requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod metrics-server-d9dcc77d6-z4sx7 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod nginx-proxy-aarnq-sc-k8s-node-srv0 requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod nginx-proxy-aarnq-sc-k8s-node-srv1 requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod nginx-proxy-aarnq-sc-k8s-node-srv2 requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod nginx-proxy-aarnq-sc-k8s-node-srv3 requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod node-local-dns-b8kzp requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod node-local-dns-dk8hd requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod node-local-dns-jf9nv requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod node-local-dns-pwwsn requesting resource cpu=25m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod snapshot-controller-7d445c66c9-6w4w5 requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod kured-fmhzj requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod kured-g9qpk requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod kured-hdkrw requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod kured-kbmf8 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod alertmanager-kube-prometheus-stack-alertmanager-0 requesting resource cpu=60m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod alertmanager-kube-prometheus-stack-alertmanager-1 requesting resource cpu=60m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod ciskubebench-exporter-68bcb66c46-rjnj9 requesting resource cpu=30m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod grafana-label-enforcer-ff6966584-d9872 requesting resource cpu=20m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-grafana-84f79f467b-sr7kl requesting resource cpu=110m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-kube-state-metrics-5584579f7d-jmrqj requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-operator-6bd84664f-wxlxc requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-prometheus-node-exporter-9v6v5 requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-prometheus-node-exporter-jmbsj requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-prometheus-node-exporter-nl9pw requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod kube-prometheus-stack-prometheus-node-exporter-rk7sg requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod prometheus-blackbox-exporter-677b579798-7xm9d requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod prometheus-kube-prometheus-stack-prometheus-0 requesting resource cpu=250m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.074: INFO: Pod s3-exporter-867c5b9457-lfcsf requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.074: INFO: Pod scan-vulnerabilityreport-6f57675ddd-td6bx requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod starboard-operator-7f84bbf756-grncj requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.074: INFO: Pod user-grafana-6f7c7d589-q6clc requesting resource cpu=110m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.074: INFO: Pod vulnerability-exporter-8485469578-jvppp requesting resource cpu=30m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.075: INFO: Pod opensearch-dashboards-58c8d95f7b-9spcv requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.075: INFO: Pod opensearch-master-0 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.075: INFO: Pod opensearch-master-1 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.075: INFO: Pod opensearch-master-2 requesting resource cpu=100m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.075: INFO: Pod prometheus-opensearch-exporter-5688c84dcd-95vjh requesting resource cpu=15m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.075: INFO: Pod sonobuoy requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.075: INFO: Pod sonobuoy-e2e-job-eae18696d9844ddc requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.075: INFO: Pod sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-j5shm requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.075: INFO: Pod sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-m5t49 requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.075: INFO: Pod sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-qv9vz requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.075: INFO: Pod sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-zf5bk requesting resource cpu=0m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.075: INFO: Pod thanos-query-query-69fc6f554b-d2q5v requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.075: INFO: Pod thanos-query-query-69fc6f554b-db7w4 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.075: INFO: Pod thanos-query-query-frontend-6c58dbdc6-6g7jx requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.075: INFO: Pod thanos-receiver-bucketweb-b4955fcf8-8w2xg requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.075: INFO: Pod thanos-receiver-compactor-848df7b5d7-z2jh4 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.075: INFO: Pod thanos-receiver-receive-0 requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.075: INFO: Pod thanos-receiver-receive-1 requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.075: INFO: Pod thanos-receiver-receive-2 requesting resource cpu=200m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.075: INFO: Pod thanos-receiver-receive-distributor-779c5d74d8-7hhmb requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.075: INFO: Pod thanos-receiver-ruler-0 requesting resource cpu=110m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.075: INFO: Pod thanos-receiver-ruler-1 requesting resource cpu=110m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.075: INFO: Pod thanos-receiver-storegateway-0 requesting resource cpu=10m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.075: INFO: Pod restic-hwpfm requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.075: INFO: Pod restic-k4z4s requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.075: INFO: Pod restic-wcgdp requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.075: INFO: Pod restic-zvhnj requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.075: INFO: Pod velero-7bbd458dfc-s8n2h requesting resource cpu=50m on Node aarnq-sc-k8s-node-srv0
    STEP: Starting Pods to consume most of the cluster CPU. 03/02/23 12:36:25.075
    Mar  2 12:36:25.075: INFO: Creating a pod which consumes cpu=255m on Node aarnq-sc-k8s-node-srv0
    Mar  2 12:36:25.090: INFO: Creating a pod which consumes cpu=196m on Node aarnq-sc-k8s-node-srv1
    Mar  2 12:36:25.106: INFO: Creating a pod which consumes cpu=735m on Node aarnq-sc-k8s-node-srv2
    Mar  2 12:36:25.116: INFO: Creating a pod which consumes cpu=336m on Node aarnq-sc-k8s-node-srv3
    Mar  2 12:36:25.130: INFO: Waiting up to 5m0s for pod "filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a" in namespace "sched-pred-7610" to be "running"
    Mar  2 12:36:25.146: INFO: Pod "filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.046056ms
    Mar  2 12:36:27.158: INFO: Pod "filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a": Phase="Running", Reason="", readiness=true. Elapsed: 2.027539638s
    Mar  2 12:36:27.158: INFO: Pod "filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a" satisfied condition "running"
    Mar  2 12:36:27.158: INFO: Waiting up to 5m0s for pod "filler-pod-6fad30cc-254b-4670-9fd9-52e109294257" in namespace "sched-pred-7610" to be "running"
    Mar  2 12:36:27.167: INFO: Pod "filler-pod-6fad30cc-254b-4670-9fd9-52e109294257": Phase="Running", Reason="", readiness=true. Elapsed: 8.397753ms
    Mar  2 12:36:27.167: INFO: Pod "filler-pod-6fad30cc-254b-4670-9fd9-52e109294257" satisfied condition "running"
    Mar  2 12:36:27.167: INFO: Waiting up to 5m0s for pod "filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd" in namespace "sched-pred-7610" to be "running"
    Mar  2 12:36:27.171: INFO: Pod "filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd": Phase="Running", Reason="", readiness=true. Elapsed: 3.89467ms
    Mar  2 12:36:27.171: INFO: Pod "filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd" satisfied condition "running"
    Mar  2 12:36:27.171: INFO: Waiting up to 5m0s for pod "filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe" in namespace "sched-pred-7610" to be "running"
    Mar  2 12:36:27.174: INFO: Pod "filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.231521ms
    Mar  2 12:36:29.182: INFO: Pod "filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe": Phase="Running", Reason="", readiness=true. Elapsed: 2.010759524s
    Mar  2 12:36:29.182: INFO: Pod "filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/02/23 12:36:29.182
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd.17489a5ceef5ecca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7610/filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd to aarnq-sc-k8s-node-srv2] 03/02/23 12:36:29.188
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd.17489a5d231fe7d0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 12:36:29.189
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd.17489a5d2840dd5f], Reason = [Created], Message = [Created container filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd] 03/02/23 12:36:29.189
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd.17489a5d2f4a83ce], Reason = [Started], Message = [Started container filler-pod-6f876a37-fc74-4e65-9335-c19613d2ffdd] 03/02/23 12:36:29.189
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6fad30cc-254b-4670-9fd9-52e109294257.17489a5cee6f9e01], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7610/filler-pod-6fad30cc-254b-4670-9fd9-52e109294257 to aarnq-sc-k8s-node-srv1] 03/02/23 12:36:29.189
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6fad30cc-254b-4670-9fd9-52e109294257.17489a5d20da3877], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 12:36:29.189
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6fad30cc-254b-4670-9fd9-52e109294257.17489a5d2458b636], Reason = [Created], Message = [Created container filler-pod-6fad30cc-254b-4670-9fd9-52e109294257] 03/02/23 12:36:29.189
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6fad30cc-254b-4670-9fd9-52e109294257.17489a5d33465b35], Reason = [Started], Message = [Started container filler-pod-6fad30cc-254b-4670-9fd9-52e109294257] 03/02/23 12:36:29.19
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a.17489a5ced6625a8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7610/filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a to aarnq-sc-k8s-node-srv0] 03/02/23 12:36:29.19
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a.17489a5d230be8c6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 12:36:29.19
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a.17489a5d25879c89], Reason = [Created], Message = [Created container filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a] 03/02/23 12:36:29.19
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a.17489a5d310c5f73], Reason = [Started], Message = [Started container filler-pod-ed6d0d6e-0a76-4a99-a049-879d011a252a] 03/02/23 12:36:29.191
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe.17489a5cef9ab054], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7610/filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe to aarnq-sc-k8s-node-srv3] 03/02/23 12:36:29.191
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe.17489a5d2be5ebec], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 12:36:29.191
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe.17489a5d2f244822], Reason = [Created], Message = [Created container filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe] 03/02/23 12:36:29.191
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe.17489a5d3c8f37e1], Reason = [Started], Message = [Started container filler-pod-fa856b95-a568-4b28-b6db-181ff90fdafe] 03/02/23 12:36:29.192
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17489a5de19648e3], Reason = [FailedScheduling], Message = [0/5 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 4 Insufficient cpu. preemption: 0/5 nodes are available: 1 Preemption is not helpful for scheduling, 4 No preemption victims found for incoming pod.] 03/02/23 12:36:29.211
    STEP: removing the label node off the node aarnq-sc-k8s-node-srv0 03/02/23 12:36:30.223
    STEP: verifying the node doesn't have the label node 03/02/23 12:36:30.238
    STEP: removing the label node off the node aarnq-sc-k8s-node-srv1 03/02/23 12:36:30.248
    STEP: verifying the node doesn't have the label node 03/02/23 12:36:30.275
    STEP: removing the label node off the node aarnq-sc-k8s-node-srv2 03/02/23 12:36:30.282
    STEP: verifying the node doesn't have the label node 03/02/23 12:36:30.299
    STEP: removing the label node off the node aarnq-sc-k8s-node-srv3 03/02/23 12:36:30.303
    STEP: verifying the node doesn't have the label node 03/02/23 12:36:30.318
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 12:36:30.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7610" for this suite. 03/02/23 12:36:30.362
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:36:30.376
Mar  2 12:36:30.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 12:36:30.379
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:30.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:30.435
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 03/02/23 12:36:30.438
Mar  2 12:36:30.447: INFO: Waiting up to 5m0s for pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091" in namespace "projected-9285" to be "running and ready"
Mar  2 12:36:30.467: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Pending", Reason="", readiness=false. Elapsed: 20.362482ms
Mar  2 12:36:30.467: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:36:32.476: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028950639s
Mar  2 12:36:32.476: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:36:34.491: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043457215s
Mar  2 12:36:34.491: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:36:36.477: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03002576s
Mar  2 12:36:36.477: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:36:38.517: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069529486s
Mar  2 12:36:38.517: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:36:40.478: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Running", Reason="", readiness=true. Elapsed: 10.030718638s
Mar  2 12:36:40.478: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Running (Ready = true)
Mar  2 12:36:40.478: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091" satisfied condition "running and ready"
Mar  2 12:36:41.028: INFO: Successfully updated pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 12:36:43.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9285" for this suite. 03/02/23 12:36:43.054
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":6,"skipped":28,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.684 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:36:30.376
    Mar  2 12:36:30.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 12:36:30.379
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:30.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:30.435
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 03/02/23 12:36:30.438
    Mar  2 12:36:30.447: INFO: Waiting up to 5m0s for pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091" in namespace "projected-9285" to be "running and ready"
    Mar  2 12:36:30.467: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Pending", Reason="", readiness=false. Elapsed: 20.362482ms
    Mar  2 12:36:30.467: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:36:32.476: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028950639s
    Mar  2 12:36:32.476: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:36:34.491: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043457215s
    Mar  2 12:36:34.491: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:36:36.477: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03002576s
    Mar  2 12:36:36.477: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:36:38.517: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069529486s
    Mar  2 12:36:38.517: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:36:40.478: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091": Phase="Running", Reason="", readiness=true. Elapsed: 10.030718638s
    Mar  2 12:36:40.478: INFO: The phase of Pod labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091 is Running (Ready = true)
    Mar  2 12:36:40.478: INFO: Pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091" satisfied condition "running and ready"
    Mar  2 12:36:41.028: INFO: Successfully updated pod "labelsupdatec7b67cd8-02b1-4a65-a89d-679228186091"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 12:36:43.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9285" for this suite. 03/02/23 12:36:43.054
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:36:43.066
Mar  2 12:36:43.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename subpath 03/02/23 12:36:43.067
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:43.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:43.096
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/02/23 12:36:43.101
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-vm8j 03/02/23 12:36:43.118
STEP: Creating a pod to test atomic-volume-subpath 03/02/23 12:36:43.118
Mar  2 12:36:43.132: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vm8j" in namespace "subpath-3128" to be "Succeeded or Failed"
Mar  2 12:36:43.143: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Pending", Reason="", readiness=false. Elapsed: 9.364077ms
Mar  2 12:36:45.155: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 2.021401408s
Mar  2 12:36:47.155: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 4.021354771s
Mar  2 12:36:49.149: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 6.015773961s
Mar  2 12:36:51.149: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 8.015694037s
Mar  2 12:36:53.153: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 10.018959957s
Mar  2 12:36:55.150: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 12.016445642s
Mar  2 12:36:57.150: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 14.016443517s
Mar  2 12:36:59.149: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 16.015019698s
Mar  2 12:37:01.151: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 18.017301016s
Mar  2 12:37:03.152: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 20.018420445s
Mar  2 12:37:05.151: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=false. Elapsed: 22.017867102s
Mar  2 12:37:07.151: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016942469s
STEP: Saw pod success 03/02/23 12:37:07.151
Mar  2 12:37:07.151: INFO: Pod "pod-subpath-test-configmap-vm8j" satisfied condition "Succeeded or Failed"
Mar  2 12:37:07.156: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-subpath-test-configmap-vm8j container test-container-subpath-configmap-vm8j: <nil>
STEP: delete the pod 03/02/23 12:37:07.163
Mar  2 12:37:07.189: INFO: Waiting for pod pod-subpath-test-configmap-vm8j to disappear
Mar  2 12:37:07.205: INFO: Pod pod-subpath-test-configmap-vm8j no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vm8j 03/02/23 12:37:07.205
Mar  2 12:37:07.206: INFO: Deleting pod "pod-subpath-test-configmap-vm8j" in namespace "subpath-3128"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  2 12:37:07.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3128" for this suite. 03/02/23 12:37:07.214
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":7,"skipped":29,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.157 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:36:43.066
    Mar  2 12:36:43.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename subpath 03/02/23 12:36:43.067
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:36:43.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:36:43.096
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/02/23 12:36:43.101
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-vm8j 03/02/23 12:36:43.118
    STEP: Creating a pod to test atomic-volume-subpath 03/02/23 12:36:43.118
    Mar  2 12:36:43.132: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vm8j" in namespace "subpath-3128" to be "Succeeded or Failed"
    Mar  2 12:36:43.143: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Pending", Reason="", readiness=false. Elapsed: 9.364077ms
    Mar  2 12:36:45.155: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 2.021401408s
    Mar  2 12:36:47.155: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 4.021354771s
    Mar  2 12:36:49.149: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 6.015773961s
    Mar  2 12:36:51.149: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 8.015694037s
    Mar  2 12:36:53.153: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 10.018959957s
    Mar  2 12:36:55.150: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 12.016445642s
    Mar  2 12:36:57.150: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 14.016443517s
    Mar  2 12:36:59.149: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 16.015019698s
    Mar  2 12:37:01.151: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 18.017301016s
    Mar  2 12:37:03.152: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=true. Elapsed: 20.018420445s
    Mar  2 12:37:05.151: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Running", Reason="", readiness=false. Elapsed: 22.017867102s
    Mar  2 12:37:07.151: INFO: Pod "pod-subpath-test-configmap-vm8j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016942469s
    STEP: Saw pod success 03/02/23 12:37:07.151
    Mar  2 12:37:07.151: INFO: Pod "pod-subpath-test-configmap-vm8j" satisfied condition "Succeeded or Failed"
    Mar  2 12:37:07.156: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-subpath-test-configmap-vm8j container test-container-subpath-configmap-vm8j: <nil>
    STEP: delete the pod 03/02/23 12:37:07.163
    Mar  2 12:37:07.189: INFO: Waiting for pod pod-subpath-test-configmap-vm8j to disappear
    Mar  2 12:37:07.205: INFO: Pod pod-subpath-test-configmap-vm8j no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-vm8j 03/02/23 12:37:07.205
    Mar  2 12:37:07.206: INFO: Deleting pod "pod-subpath-test-configmap-vm8j" in namespace "subpath-3128"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  2 12:37:07.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3128" for this suite. 03/02/23 12:37:07.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:37:07.257
Mar  2 12:37:07.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename deployment 03/02/23 12:37:07.26
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:37:07.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:37:07.302
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/02/23 12:37:07.308
Mar  2 12:37:07.308: INFO: Creating simple deployment test-deployment-b2qmx
Mar  2 12:37:07.329: INFO: deployment "test-deployment-b2qmx" doesn't have the required revision set
Mar  2 12:37:09.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-b2qmx-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 12:37:11.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-b2qmx-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 12:37:13.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-b2qmx-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 03/02/23 12:37:15.36
Mar  2 12:37:15.368: INFO: Deployment test-deployment-b2qmx has Conditions: [{Available True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b2qmx-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 03/02/23 12:37:15.368
Mar  2 12:37:15.390: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-b2qmx-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/02/23 12:37:15.39
Mar  2 12:37:15.400: INFO: Observed &Deployment event: ADDED
Mar  2 12:37:15.400: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b2qmx-777898ffcc"}
Mar  2 12:37:15.401: INFO: Observed &Deployment event: MODIFIED
Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b2qmx-777898ffcc"}
Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 12:37:15.401: INFO: Observed &Deployment event: MODIFIED
Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b2qmx-777898ffcc" is progressing.}
Mar  2 12:37:15.401: INFO: Observed &Deployment event: MODIFIED
Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b2qmx-777898ffcc" has successfully progressed.}
Mar  2 12:37:15.401: INFO: Observed &Deployment event: MODIFIED
Mar  2 12:37:15.402: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 12:37:15.402: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b2qmx-777898ffcc" has successfully progressed.}
Mar  2 12:37:15.402: INFO: Found Deployment test-deployment-b2qmx in namespace deployment-6880 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 12:37:15.402: INFO: Deployment test-deployment-b2qmx has an updated status
STEP: patching the Statefulset Status 03/02/23 12:37:15.402
Mar  2 12:37:15.402: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 12:37:15.411: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/02/23 12:37:15.411
Mar  2 12:37:15.417: INFO: Observed &Deployment event: ADDED
Mar  2 12:37:15.417: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b2qmx-777898ffcc"}
Mar  2 12:37:15.418: INFO: Observed &Deployment event: MODIFIED
Mar  2 12:37:15.418: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b2qmx-777898ffcc"}
Mar  2 12:37:15.418: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 12:37:15.418: INFO: Observed &Deployment event: MODIFIED
Mar  2 12:37:15.419: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 12:37:15.419: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b2qmx-777898ffcc" is progressing.}
Mar  2 12:37:15.419: INFO: Observed &Deployment event: MODIFIED
Mar  2 12:37:15.420: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 12:37:15.420: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b2qmx-777898ffcc" has successfully progressed.}
Mar  2 12:37:15.420: INFO: Observed &Deployment event: MODIFIED
Mar  2 12:37:15.420: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 12:37:15.420: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b2qmx-777898ffcc" has successfully progressed.}
Mar  2 12:37:15.420: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 12:37:15.421: INFO: Observed &Deployment event: MODIFIED
Mar  2 12:37:15.421: INFO: Found deployment test-deployment-b2qmx in namespace deployment-6880 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar  2 12:37:15.421: INFO: Deployment test-deployment-b2qmx has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 12:37:15.427: INFO: Deployment "test-deployment-b2qmx":
&Deployment{ObjectMeta:{test-deployment-b2qmx  deployment-6880  9eedb2e9-ddc9-4fbe-8624-e49dc21fa477 1917804 1 2023-03-02 12:37:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-02 12:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-02 12:37:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-02 12:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003871618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-b2qmx-777898ffcc",LastUpdateTime:2023-03-02 12:37:15 +0000 UTC,LastTransitionTime:2023-03-02 12:37:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 12:37:15.431: INFO: New ReplicaSet "test-deployment-b2qmx-777898ffcc" of Deployment "test-deployment-b2qmx":
&ReplicaSet{ObjectMeta:{test-deployment-b2qmx-777898ffcc  deployment-6880  80259fc4-3883-47d3-96fb-c162d3dbd58f 1917799 1 2023-03-02 12:37:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-b2qmx 9eedb2e9-ddc9-4fbe-8624-e49dc21fa477 0xc003871a07 0xc003871a08}] [] [{kube-controller-manager Update apps/v1 2023-03-02 12:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9eedb2e9-ddc9-4fbe-8624-e49dc21fa477\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 12:37:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003871ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 12:37:15.439: INFO: Pod "test-deployment-b2qmx-777898ffcc-f6xc8" is available:
&Pod{ObjectMeta:{test-deployment-b2qmx-777898ffcc-f6xc8 test-deployment-b2qmx-777898ffcc- deployment-6880  bac5f69b-4500-4401-9afe-f34eaa58c935 1917797 0 2023-03-02 12:37:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:63d92cdadffd18cc44946a3e5744773b9b770e3b28f4040b25adeb4577d1faf2 cni.projectcalico.org/podIP:10.233.123.96/32 cni.projectcalico.org/podIPs:10.233.123.96/32] [{apps/v1 ReplicaSet test-deployment-b2qmx-777898ffcc 80259fc4-3883-47d3-96fb-c162d3dbd58f 0xc003871e87 0xc003871e88}] [] [{kube-controller-manager Update v1 2023-03-02 12:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"80259fc4-3883-47d3-96fb-c162d3dbd58f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 12:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 12:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pz6bf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pz6bf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:37:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:37:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.96,StartTime:2023-03-02 12:37:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 12:37:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://621a9a6838281b88d542165888b0926f67b77bcbae2ccb91ac7b3d21b0b0f987,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 12:37:15.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6880" for this suite. 03/02/23 12:37:15.523
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":8,"skipped":88,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.284 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:37:07.257
    Mar  2 12:37:07.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename deployment 03/02/23 12:37:07.26
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:37:07.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:37:07.302
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/02/23 12:37:07.308
    Mar  2 12:37:07.308: INFO: Creating simple deployment test-deployment-b2qmx
    Mar  2 12:37:07.329: INFO: deployment "test-deployment-b2qmx" doesn't have the required revision set
    Mar  2 12:37:09.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-b2qmx-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 12:37:11.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-b2qmx-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 12:37:13.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-b2qmx-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 03/02/23 12:37:15.36
    Mar  2 12:37:15.368: INFO: Deployment test-deployment-b2qmx has Conditions: [{Available True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b2qmx-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 03/02/23 12:37:15.368
    Mar  2 12:37:15.390: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 7, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-b2qmx-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/02/23 12:37:15.39
    Mar  2 12:37:15.400: INFO: Observed &Deployment event: ADDED
    Mar  2 12:37:15.400: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b2qmx-777898ffcc"}
    Mar  2 12:37:15.401: INFO: Observed &Deployment event: MODIFIED
    Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b2qmx-777898ffcc"}
    Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  2 12:37:15.401: INFO: Observed &Deployment event: MODIFIED
    Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b2qmx-777898ffcc" is progressing.}
    Mar  2 12:37:15.401: INFO: Observed &Deployment event: MODIFIED
    Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  2 12:37:15.401: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b2qmx-777898ffcc" has successfully progressed.}
    Mar  2 12:37:15.401: INFO: Observed &Deployment event: MODIFIED
    Mar  2 12:37:15.402: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  2 12:37:15.402: INFO: Observed Deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b2qmx-777898ffcc" has successfully progressed.}
    Mar  2 12:37:15.402: INFO: Found Deployment test-deployment-b2qmx in namespace deployment-6880 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  2 12:37:15.402: INFO: Deployment test-deployment-b2qmx has an updated status
    STEP: patching the Statefulset Status 03/02/23 12:37:15.402
    Mar  2 12:37:15.402: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  2 12:37:15.411: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/02/23 12:37:15.411
    Mar  2 12:37:15.417: INFO: Observed &Deployment event: ADDED
    Mar  2 12:37:15.417: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b2qmx-777898ffcc"}
    Mar  2 12:37:15.418: INFO: Observed &Deployment event: MODIFIED
    Mar  2 12:37:15.418: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b2qmx-777898ffcc"}
    Mar  2 12:37:15.418: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  2 12:37:15.418: INFO: Observed &Deployment event: MODIFIED
    Mar  2 12:37:15.419: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  2 12:37:15.419: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:07 +0000 UTC 2023-03-02 12:37:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b2qmx-777898ffcc" is progressing.}
    Mar  2 12:37:15.419: INFO: Observed &Deployment event: MODIFIED
    Mar  2 12:37:15.420: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  2 12:37:15.420: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b2qmx-777898ffcc" has successfully progressed.}
    Mar  2 12:37:15.420: INFO: Observed &Deployment event: MODIFIED
    Mar  2 12:37:15.420: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  2 12:37:15.420: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 12:37:14 +0000 UTC 2023-03-02 12:37:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b2qmx-777898ffcc" has successfully progressed.}
    Mar  2 12:37:15.420: INFO: Observed deployment test-deployment-b2qmx in namespace deployment-6880 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  2 12:37:15.421: INFO: Observed &Deployment event: MODIFIED
    Mar  2 12:37:15.421: INFO: Found deployment test-deployment-b2qmx in namespace deployment-6880 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar  2 12:37:15.421: INFO: Deployment test-deployment-b2qmx has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 12:37:15.427: INFO: Deployment "test-deployment-b2qmx":
    &Deployment{ObjectMeta:{test-deployment-b2qmx  deployment-6880  9eedb2e9-ddc9-4fbe-8624-e49dc21fa477 1917804 1 2023-03-02 12:37:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-02 12:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-02 12:37:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-02 12:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003871618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-b2qmx-777898ffcc",LastUpdateTime:2023-03-02 12:37:15 +0000 UTC,LastTransitionTime:2023-03-02 12:37:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  2 12:37:15.431: INFO: New ReplicaSet "test-deployment-b2qmx-777898ffcc" of Deployment "test-deployment-b2qmx":
    &ReplicaSet{ObjectMeta:{test-deployment-b2qmx-777898ffcc  deployment-6880  80259fc4-3883-47d3-96fb-c162d3dbd58f 1917799 1 2023-03-02 12:37:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-b2qmx 9eedb2e9-ddc9-4fbe-8624-e49dc21fa477 0xc003871a07 0xc003871a08}] [] [{kube-controller-manager Update apps/v1 2023-03-02 12:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9eedb2e9-ddc9-4fbe-8624-e49dc21fa477\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 12:37:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003871ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 12:37:15.439: INFO: Pod "test-deployment-b2qmx-777898ffcc-f6xc8" is available:
    &Pod{ObjectMeta:{test-deployment-b2qmx-777898ffcc-f6xc8 test-deployment-b2qmx-777898ffcc- deployment-6880  bac5f69b-4500-4401-9afe-f34eaa58c935 1917797 0 2023-03-02 12:37:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:63d92cdadffd18cc44946a3e5744773b9b770e3b28f4040b25adeb4577d1faf2 cni.projectcalico.org/podIP:10.233.123.96/32 cni.projectcalico.org/podIPs:10.233.123.96/32] [{apps/v1 ReplicaSet test-deployment-b2qmx-777898ffcc 80259fc4-3883-47d3-96fb-c162d3dbd58f 0xc003871e87 0xc003871e88}] [] [{kube-controller-manager Update v1 2023-03-02 12:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"80259fc4-3883-47d3-96fb-c162d3dbd58f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 12:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 12:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pz6bf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pz6bf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:37:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:37:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.96,StartTime:2023-03-02 12:37:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 12:37:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://621a9a6838281b88d542165888b0926f67b77bcbae2ccb91ac7b3d21b0b0f987,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 12:37:15.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6880" for this suite. 03/02/23 12:37:15.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:37:15.56
Mar  2 12:37:15.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename events 03/02/23 12:37:15.561
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:37:15.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:37:15.611
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/02/23 12:37:15.616
STEP: get a list of Events with a label in the current namespace 03/02/23 12:37:15.638
STEP: delete a list of events 03/02/23 12:37:15.642
Mar  2 12:37:15.642: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/02/23 12:37:15.653
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar  2 12:37:15.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3657" for this suite. 03/02/23 12:37:15.661
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":9,"skipped":127,"failed":0}
------------------------------
â€¢ [0.109 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:37:15.56
    Mar  2 12:37:15.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename events 03/02/23 12:37:15.561
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:37:15.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:37:15.611
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/02/23 12:37:15.616
    STEP: get a list of Events with a label in the current namespace 03/02/23 12:37:15.638
    STEP: delete a list of events 03/02/23 12:37:15.642
    Mar  2 12:37:15.642: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/02/23 12:37:15.653
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar  2 12:37:15.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3657" for this suite. 03/02/23 12:37:15.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:37:15.698
Mar  2 12:37:15.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 12:37:15.699
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:37:15.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:37:15.732
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 12:37:15.744
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:37:16.655
STEP: Deploying the webhook pod 03/02/23 12:37:16.678
STEP: Wait for the deployment to be ready 03/02/23 12:37:16.722
Mar  2 12:37:16.735: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 12:37:18.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 12:37:20.752
STEP: Verifying the service has paired with the endpoint 03/02/23 12:37:20.777
Mar  2 12:37:21.778: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 03/02/23 12:37:21.785
STEP: Creating a custom resource definition that should be denied by the webhook 03/02/23 12:37:21.819
Mar  2 12:37:21.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 12:37:21.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8453" for this suite. 03/02/23 12:37:21.848
STEP: Destroying namespace "webhook-8453-markers" for this suite. 03/02/23 12:37:21.859
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":10,"skipped":156,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.248 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:37:15.698
    Mar  2 12:37:15.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 12:37:15.699
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:37:15.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:37:15.732
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 12:37:15.744
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:37:16.655
    STEP: Deploying the webhook pod 03/02/23 12:37:16.678
    STEP: Wait for the deployment to be ready 03/02/23 12:37:16.722
    Mar  2 12:37:16.735: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 12:37:18.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 37, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 37, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 12:37:20.752
    STEP: Verifying the service has paired with the endpoint 03/02/23 12:37:20.777
    Mar  2 12:37:21.778: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/02/23 12:37:21.785
    STEP: Creating a custom resource definition that should be denied by the webhook 03/02/23 12:37:21.819
    Mar  2 12:37:21.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 12:37:21.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8453" for this suite. 03/02/23 12:37:21.848
    STEP: Destroying namespace "webhook-8453-markers" for this suite. 03/02/23 12:37:21.859
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:37:22.018
Mar  2 12:37:22.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename dns 03/02/23 12:37:22.02
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:37:22.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:37:22.051
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2189.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2189.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/02/23 12:37:22.056
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2189.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2189.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/02/23 12:37:22.056
STEP: creating a pod to probe /etc/hosts 03/02/23 12:37:22.056
STEP: submitting the pod to kubernetes 03/02/23 12:37:22.056
Mar  2 12:37:22.067: INFO: Waiting up to 15m0s for pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085" in namespace "dns-2189" to be "running"
Mar  2 12:37:22.078: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 10.879602ms
Mar  2 12:37:24.087: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019451091s
Mar  2 12:37:26.137: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070002496s
Mar  2 12:37:28.266: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 6.198916815s
Mar  2 12:37:30.087: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019398229s
Mar  2 12:37:32.131: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 10.063861345s
Mar  2 12:37:34.110: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Running", Reason="", readiness=true. Elapsed: 12.042757582s
Mar  2 12:37:34.110: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085" satisfied condition "running"
STEP: retrieving the pod 03/02/23 12:37:34.11
STEP: looking for the results for each expected name from probers 03/02/23 12:37:34.117
Mar  2 12:37:34.141: INFO: DNS probes using dns-2189/dns-test-e949cda2-cba5-477e-8a53-24e257e8e085 succeeded

STEP: deleting the pod 03/02/23 12:37:34.142
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 12:37:34.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2189" for this suite. 03/02/23 12:37:34.213
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":11,"skipped":158,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.206 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:37:22.018
    Mar  2 12:37:22.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename dns 03/02/23 12:37:22.02
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:37:22.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:37:22.051
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2189.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2189.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/02/23 12:37:22.056
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2189.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2189.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/02/23 12:37:22.056
    STEP: creating a pod to probe /etc/hosts 03/02/23 12:37:22.056
    STEP: submitting the pod to kubernetes 03/02/23 12:37:22.056
    Mar  2 12:37:22.067: INFO: Waiting up to 15m0s for pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085" in namespace "dns-2189" to be "running"
    Mar  2 12:37:22.078: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 10.879602ms
    Mar  2 12:37:24.087: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019451091s
    Mar  2 12:37:26.137: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070002496s
    Mar  2 12:37:28.266: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 6.198916815s
    Mar  2 12:37:30.087: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019398229s
    Mar  2 12:37:32.131: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Pending", Reason="", readiness=false. Elapsed: 10.063861345s
    Mar  2 12:37:34.110: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085": Phase="Running", Reason="", readiness=true. Elapsed: 12.042757582s
    Mar  2 12:37:34.110: INFO: Pod "dns-test-e949cda2-cba5-477e-8a53-24e257e8e085" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 12:37:34.11
    STEP: looking for the results for each expected name from probers 03/02/23 12:37:34.117
    Mar  2 12:37:34.141: INFO: DNS probes using dns-2189/dns-test-e949cda2-cba5-477e-8a53-24e257e8e085 succeeded

    STEP: deleting the pod 03/02/23 12:37:34.142
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 12:37:34.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2189" for this suite. 03/02/23 12:37:34.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:37:34.238
Mar  2 12:37:34.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename gc 03/02/23 12:37:34.24
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:37:34.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:37:34.269
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/02/23 12:37:34.317
STEP: delete the rc 03/02/23 12:37:39.36
STEP: wait for the rc to be deleted 03/02/23 12:37:39.37
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/02/23 12:37:44.582
STEP: Gathering metrics 03/02/23 12:38:14.615
Mar  2 12:38:14.659: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
Mar  2 12:38:14.664: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 4.243595ms
Mar  2 12:38:14.664: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
Mar  2 12:38:14.664: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
Mar  2 12:38:14.847: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar  2 12:38:14.849: INFO: Deleting pod "simpletest.rc-24d5n" in namespace "gc-6410"
Mar  2 12:38:14.885: INFO: Deleting pod "simpletest.rc-29bvd" in namespace "gc-6410"
Mar  2 12:38:14.910: INFO: Deleting pod "simpletest.rc-2g4q9" in namespace "gc-6410"
Mar  2 12:38:14.936: INFO: Deleting pod "simpletest.rc-2l6wh" in namespace "gc-6410"
Mar  2 12:38:14.967: INFO: Deleting pod "simpletest.rc-2vfkb" in namespace "gc-6410"
Mar  2 12:38:14.994: INFO: Deleting pod "simpletest.rc-2wfct" in namespace "gc-6410"
Mar  2 12:38:15.034: INFO: Deleting pod "simpletest.rc-2wq4v" in namespace "gc-6410"
Mar  2 12:38:15.063: INFO: Deleting pod "simpletest.rc-46mkm" in namespace "gc-6410"
Mar  2 12:38:15.142: INFO: Deleting pod "simpletest.rc-4b2q7" in namespace "gc-6410"
Mar  2 12:38:15.158: INFO: Deleting pod "simpletest.rc-4sbzs" in namespace "gc-6410"
Mar  2 12:38:15.186: INFO: Deleting pod "simpletest.rc-55mzt" in namespace "gc-6410"
Mar  2 12:38:15.218: INFO: Deleting pod "simpletest.rc-5j749" in namespace "gc-6410"
Mar  2 12:38:15.250: INFO: Deleting pod "simpletest.rc-62znv" in namespace "gc-6410"
Mar  2 12:38:15.266: INFO: Deleting pod "simpletest.rc-67z6s" in namespace "gc-6410"
Mar  2 12:38:15.347: INFO: Deleting pod "simpletest.rc-6827s" in namespace "gc-6410"
Mar  2 12:38:15.369: INFO: Deleting pod "simpletest.rc-6bjgv" in namespace "gc-6410"
Mar  2 12:38:15.396: INFO: Deleting pod "simpletest.rc-6glc5" in namespace "gc-6410"
Mar  2 12:38:15.406: INFO: Deleting pod "simpletest.rc-6ntwc" in namespace "gc-6410"
Mar  2 12:38:15.658: INFO: Deleting pod "simpletest.rc-6qkxr" in namespace "gc-6410"
Mar  2 12:38:15.792: INFO: Deleting pod "simpletest.rc-7bs7g" in namespace "gc-6410"
Mar  2 12:38:15.815: INFO: Deleting pod "simpletest.rc-7wrf5" in namespace "gc-6410"
Mar  2 12:38:15.896: INFO: Deleting pod "simpletest.rc-85cdx" in namespace "gc-6410"
Mar  2 12:38:16.059: INFO: Deleting pod "simpletest.rc-8hck7" in namespace "gc-6410"
Mar  2 12:38:16.124: INFO: Deleting pod "simpletest.rc-92gl9" in namespace "gc-6410"
Mar  2 12:38:16.162: INFO: Deleting pod "simpletest.rc-959p8" in namespace "gc-6410"
Mar  2 12:38:16.181: INFO: Deleting pod "simpletest.rc-9bxxb" in namespace "gc-6410"
Mar  2 12:38:16.250: INFO: Deleting pod "simpletest.rc-9qddg" in namespace "gc-6410"
Mar  2 12:38:16.269: INFO: Deleting pod "simpletest.rc-bdd54" in namespace "gc-6410"
Mar  2 12:38:16.299: INFO: Deleting pod "simpletest.rc-bphpq" in namespace "gc-6410"
Mar  2 12:38:16.316: INFO: Deleting pod "simpletest.rc-bq9v4" in namespace "gc-6410"
Mar  2 12:38:16.342: INFO: Deleting pod "simpletest.rc-c7d6c" in namespace "gc-6410"
Mar  2 12:38:16.456: INFO: Deleting pod "simpletest.rc-cnt24" in namespace "gc-6410"
Mar  2 12:38:16.516: INFO: Deleting pod "simpletest.rc-cqp7s" in namespace "gc-6410"
Mar  2 12:38:16.536: INFO: Deleting pod "simpletest.rc-d7b9q" in namespace "gc-6410"
Mar  2 12:38:16.564: INFO: Deleting pod "simpletest.rc-dfn4k" in namespace "gc-6410"
Mar  2 12:38:16.597: INFO: Deleting pod "simpletest.rc-dk7js" in namespace "gc-6410"
Mar  2 12:38:16.642: INFO: Deleting pod "simpletest.rc-dxccb" in namespace "gc-6410"
Mar  2 12:38:16.667: INFO: Deleting pod "simpletest.rc-frvgk" in namespace "gc-6410"
Mar  2 12:38:16.687: INFO: Deleting pod "simpletest.rc-gjght" in namespace "gc-6410"
Mar  2 12:38:16.723: INFO: Deleting pod "simpletest.rc-h5wr5" in namespace "gc-6410"
Mar  2 12:38:16.758: INFO: Deleting pod "simpletest.rc-h68rc" in namespace "gc-6410"
Mar  2 12:38:16.768: INFO: Deleting pod "simpletest.rc-h6bzs" in namespace "gc-6410"
Mar  2 12:38:16.802: INFO: Deleting pod "simpletest.rc-hgnjl" in namespace "gc-6410"
Mar  2 12:38:16.841: INFO: Deleting pod "simpletest.rc-hxd7r" in namespace "gc-6410"
Mar  2 12:38:16.855: INFO: Deleting pod "simpletest.rc-j6cg9" in namespace "gc-6410"
Mar  2 12:38:16.918: INFO: Deleting pod "simpletest.rc-jbpm7" in namespace "gc-6410"
Mar  2 12:38:16.954: INFO: Deleting pod "simpletest.rc-jnwhs" in namespace "gc-6410"
Mar  2 12:38:16.967: INFO: Deleting pod "simpletest.rc-jrz65" in namespace "gc-6410"
Mar  2 12:38:17.098: INFO: Deleting pod "simpletest.rc-jstx4" in namespace "gc-6410"
Mar  2 12:38:17.158: INFO: Deleting pod "simpletest.rc-jvwks" in namespace "gc-6410"
Mar  2 12:38:17.202: INFO: Deleting pod "simpletest.rc-jx86v" in namespace "gc-6410"
Mar  2 12:38:17.259: INFO: Deleting pod "simpletest.rc-jxgm4" in namespace "gc-6410"
Mar  2 12:38:17.321: INFO: Deleting pod "simpletest.rc-kcz9c" in namespace "gc-6410"
Mar  2 12:38:17.349: INFO: Deleting pod "simpletest.rc-kf5km" in namespace "gc-6410"
Mar  2 12:38:17.366: INFO: Deleting pod "simpletest.rc-kf8zw" in namespace "gc-6410"
Mar  2 12:38:17.412: INFO: Deleting pod "simpletest.rc-ks27m" in namespace "gc-6410"
Mar  2 12:38:17.452: INFO: Deleting pod "simpletest.rc-l9rqj" in namespace "gc-6410"
Mar  2 12:38:17.518: INFO: Deleting pod "simpletest.rc-lkmgq" in namespace "gc-6410"
Mar  2 12:38:17.544: INFO: Deleting pod "simpletest.rc-lldmm" in namespace "gc-6410"
Mar  2 12:38:17.856: INFO: Deleting pod "simpletest.rc-m5cmr" in namespace "gc-6410"
Mar  2 12:38:17.946: INFO: Deleting pod "simpletest.rc-mkqz9" in namespace "gc-6410"
Mar  2 12:38:17.963: INFO: Deleting pod "simpletest.rc-mqr7k" in namespace "gc-6410"
Mar  2 12:38:18.068: INFO: Deleting pod "simpletest.rc-mvjff" in namespace "gc-6410"
Mar  2 12:38:18.127: INFO: Deleting pod "simpletest.rc-nkhkf" in namespace "gc-6410"
Mar  2 12:38:18.221: INFO: Deleting pod "simpletest.rc-np6pp" in namespace "gc-6410"
Mar  2 12:38:18.284: INFO: Deleting pod "simpletest.rc-p7s4q" in namespace "gc-6410"
Mar  2 12:38:18.462: INFO: Deleting pod "simpletest.rc-pfksj" in namespace "gc-6410"
Mar  2 12:38:18.544: INFO: Deleting pod "simpletest.rc-px9kq" in namespace "gc-6410"
Mar  2 12:38:18.560: INFO: Deleting pod "simpletest.rc-qjdrm" in namespace "gc-6410"
Mar  2 12:38:18.591: INFO: Deleting pod "simpletest.rc-qvnf8" in namespace "gc-6410"
Mar  2 12:38:18.606: INFO: Deleting pod "simpletest.rc-rb7v4" in namespace "gc-6410"
Mar  2 12:38:18.622: INFO: Deleting pod "simpletest.rc-s6kzm" in namespace "gc-6410"
Mar  2 12:38:18.658: INFO: Deleting pod "simpletest.rc-s8d46" in namespace "gc-6410"
Mar  2 12:38:18.855: INFO: Deleting pod "simpletest.rc-snnhr" in namespace "gc-6410"
Mar  2 12:38:19.055: INFO: Deleting pod "simpletest.rc-swv4z" in namespace "gc-6410"
Mar  2 12:38:19.071: INFO: Deleting pod "simpletest.rc-t2pvr" in namespace "gc-6410"
Mar  2 12:38:19.159: INFO: Deleting pod "simpletest.rc-t8vfr" in namespace "gc-6410"
Mar  2 12:38:19.266: INFO: Deleting pod "simpletest.rc-tcwgg" in namespace "gc-6410"
Mar  2 12:38:19.327: INFO: Deleting pod "simpletest.rc-tthkn" in namespace "gc-6410"
Mar  2 12:38:19.353: INFO: Deleting pod "simpletest.rc-tvpqf" in namespace "gc-6410"
Mar  2 12:38:19.406: INFO: Deleting pod "simpletest.rc-v5ht8" in namespace "gc-6410"
Mar  2 12:38:19.669: INFO: Deleting pod "simpletest.rc-v8rhf" in namespace "gc-6410"
Mar  2 12:38:19.751: INFO: Deleting pod "simpletest.rc-v8xpq" in namespace "gc-6410"
Mar  2 12:38:19.766: INFO: Deleting pod "simpletest.rc-vcjbz" in namespace "gc-6410"
Mar  2 12:38:19.794: INFO: Deleting pod "simpletest.rc-vhjft" in namespace "gc-6410"
Mar  2 12:38:19.850: INFO: Deleting pod "simpletest.rc-vt9nj" in namespace "gc-6410"
Mar  2 12:38:19.869: INFO: Deleting pod "simpletest.rc-vxnbb" in namespace "gc-6410"
Mar  2 12:38:19.890: INFO: Deleting pod "simpletest.rc-wq2vm" in namespace "gc-6410"
Mar  2 12:38:19.943: INFO: Deleting pod "simpletest.rc-wqb7k" in namespace "gc-6410"
Mar  2 12:38:19.954: INFO: Deleting pod "simpletest.rc-wshcs" in namespace "gc-6410"
Mar  2 12:38:20.030: INFO: Deleting pod "simpletest.rc-xc4tq" in namespace "gc-6410"
Mar  2 12:38:20.042: INFO: Deleting pod "simpletest.rc-xhqjz" in namespace "gc-6410"
Mar  2 12:38:20.053: INFO: Deleting pod "simpletest.rc-xjs62" in namespace "gc-6410"
Mar  2 12:38:20.070: INFO: Deleting pod "simpletest.rc-xjx5j" in namespace "gc-6410"
Mar  2 12:38:20.137: INFO: Deleting pod "simpletest.rc-xpp2t" in namespace "gc-6410"
Mar  2 12:38:20.167: INFO: Deleting pod "simpletest.rc-xvm6c" in namespace "gc-6410"
Mar  2 12:38:20.196: INFO: Deleting pod "simpletest.rc-z29ch" in namespace "gc-6410"
Mar  2 12:38:20.285: INFO: Deleting pod "simpletest.rc-zjlz6" in namespace "gc-6410"
Mar  2 12:38:20.307: INFO: Deleting pod "simpletest.rc-zl98z" in namespace "gc-6410"
Mar  2 12:38:20.320: INFO: Deleting pod "simpletest.rc-znhx2" in namespace "gc-6410"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 12:38:20.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6410" for this suite. 03/02/23 12:38:20.364
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":12,"skipped":213,"failed":0}
------------------------------
â€¢ [SLOW TEST] [46.130 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:37:34.238
    Mar  2 12:37:34.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename gc 03/02/23 12:37:34.24
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:37:34.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:37:34.269
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/02/23 12:37:34.317
    STEP: delete the rc 03/02/23 12:37:39.36
    STEP: wait for the rc to be deleted 03/02/23 12:37:39.37
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/02/23 12:37:44.582
    STEP: Gathering metrics 03/02/23 12:38:14.615
    Mar  2 12:38:14.659: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
    Mar  2 12:38:14.664: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 4.243595ms
    Mar  2 12:38:14.664: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
    Mar  2 12:38:14.664: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
    Mar  2 12:38:14.847: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar  2 12:38:14.849: INFO: Deleting pod "simpletest.rc-24d5n" in namespace "gc-6410"
    Mar  2 12:38:14.885: INFO: Deleting pod "simpletest.rc-29bvd" in namespace "gc-6410"
    Mar  2 12:38:14.910: INFO: Deleting pod "simpletest.rc-2g4q9" in namespace "gc-6410"
    Mar  2 12:38:14.936: INFO: Deleting pod "simpletest.rc-2l6wh" in namespace "gc-6410"
    Mar  2 12:38:14.967: INFO: Deleting pod "simpletest.rc-2vfkb" in namespace "gc-6410"
    Mar  2 12:38:14.994: INFO: Deleting pod "simpletest.rc-2wfct" in namespace "gc-6410"
    Mar  2 12:38:15.034: INFO: Deleting pod "simpletest.rc-2wq4v" in namespace "gc-6410"
    Mar  2 12:38:15.063: INFO: Deleting pod "simpletest.rc-46mkm" in namespace "gc-6410"
    Mar  2 12:38:15.142: INFO: Deleting pod "simpletest.rc-4b2q7" in namespace "gc-6410"
    Mar  2 12:38:15.158: INFO: Deleting pod "simpletest.rc-4sbzs" in namespace "gc-6410"
    Mar  2 12:38:15.186: INFO: Deleting pod "simpletest.rc-55mzt" in namespace "gc-6410"
    Mar  2 12:38:15.218: INFO: Deleting pod "simpletest.rc-5j749" in namespace "gc-6410"
    Mar  2 12:38:15.250: INFO: Deleting pod "simpletest.rc-62znv" in namespace "gc-6410"
    Mar  2 12:38:15.266: INFO: Deleting pod "simpletest.rc-67z6s" in namespace "gc-6410"
    Mar  2 12:38:15.347: INFO: Deleting pod "simpletest.rc-6827s" in namespace "gc-6410"
    Mar  2 12:38:15.369: INFO: Deleting pod "simpletest.rc-6bjgv" in namespace "gc-6410"
    Mar  2 12:38:15.396: INFO: Deleting pod "simpletest.rc-6glc5" in namespace "gc-6410"
    Mar  2 12:38:15.406: INFO: Deleting pod "simpletest.rc-6ntwc" in namespace "gc-6410"
    Mar  2 12:38:15.658: INFO: Deleting pod "simpletest.rc-6qkxr" in namespace "gc-6410"
    Mar  2 12:38:15.792: INFO: Deleting pod "simpletest.rc-7bs7g" in namespace "gc-6410"
    Mar  2 12:38:15.815: INFO: Deleting pod "simpletest.rc-7wrf5" in namespace "gc-6410"
    Mar  2 12:38:15.896: INFO: Deleting pod "simpletest.rc-85cdx" in namespace "gc-6410"
    Mar  2 12:38:16.059: INFO: Deleting pod "simpletest.rc-8hck7" in namespace "gc-6410"
    Mar  2 12:38:16.124: INFO: Deleting pod "simpletest.rc-92gl9" in namespace "gc-6410"
    Mar  2 12:38:16.162: INFO: Deleting pod "simpletest.rc-959p8" in namespace "gc-6410"
    Mar  2 12:38:16.181: INFO: Deleting pod "simpletest.rc-9bxxb" in namespace "gc-6410"
    Mar  2 12:38:16.250: INFO: Deleting pod "simpletest.rc-9qddg" in namespace "gc-6410"
    Mar  2 12:38:16.269: INFO: Deleting pod "simpletest.rc-bdd54" in namespace "gc-6410"
    Mar  2 12:38:16.299: INFO: Deleting pod "simpletest.rc-bphpq" in namespace "gc-6410"
    Mar  2 12:38:16.316: INFO: Deleting pod "simpletest.rc-bq9v4" in namespace "gc-6410"
    Mar  2 12:38:16.342: INFO: Deleting pod "simpletest.rc-c7d6c" in namespace "gc-6410"
    Mar  2 12:38:16.456: INFO: Deleting pod "simpletest.rc-cnt24" in namespace "gc-6410"
    Mar  2 12:38:16.516: INFO: Deleting pod "simpletest.rc-cqp7s" in namespace "gc-6410"
    Mar  2 12:38:16.536: INFO: Deleting pod "simpletest.rc-d7b9q" in namespace "gc-6410"
    Mar  2 12:38:16.564: INFO: Deleting pod "simpletest.rc-dfn4k" in namespace "gc-6410"
    Mar  2 12:38:16.597: INFO: Deleting pod "simpletest.rc-dk7js" in namespace "gc-6410"
    Mar  2 12:38:16.642: INFO: Deleting pod "simpletest.rc-dxccb" in namespace "gc-6410"
    Mar  2 12:38:16.667: INFO: Deleting pod "simpletest.rc-frvgk" in namespace "gc-6410"
    Mar  2 12:38:16.687: INFO: Deleting pod "simpletest.rc-gjght" in namespace "gc-6410"
    Mar  2 12:38:16.723: INFO: Deleting pod "simpletest.rc-h5wr5" in namespace "gc-6410"
    Mar  2 12:38:16.758: INFO: Deleting pod "simpletest.rc-h68rc" in namespace "gc-6410"
    Mar  2 12:38:16.768: INFO: Deleting pod "simpletest.rc-h6bzs" in namespace "gc-6410"
    Mar  2 12:38:16.802: INFO: Deleting pod "simpletest.rc-hgnjl" in namespace "gc-6410"
    Mar  2 12:38:16.841: INFO: Deleting pod "simpletest.rc-hxd7r" in namespace "gc-6410"
    Mar  2 12:38:16.855: INFO: Deleting pod "simpletest.rc-j6cg9" in namespace "gc-6410"
    Mar  2 12:38:16.918: INFO: Deleting pod "simpletest.rc-jbpm7" in namespace "gc-6410"
    Mar  2 12:38:16.954: INFO: Deleting pod "simpletest.rc-jnwhs" in namespace "gc-6410"
    Mar  2 12:38:16.967: INFO: Deleting pod "simpletest.rc-jrz65" in namespace "gc-6410"
    Mar  2 12:38:17.098: INFO: Deleting pod "simpletest.rc-jstx4" in namespace "gc-6410"
    Mar  2 12:38:17.158: INFO: Deleting pod "simpletest.rc-jvwks" in namespace "gc-6410"
    Mar  2 12:38:17.202: INFO: Deleting pod "simpletest.rc-jx86v" in namespace "gc-6410"
    Mar  2 12:38:17.259: INFO: Deleting pod "simpletest.rc-jxgm4" in namespace "gc-6410"
    Mar  2 12:38:17.321: INFO: Deleting pod "simpletest.rc-kcz9c" in namespace "gc-6410"
    Mar  2 12:38:17.349: INFO: Deleting pod "simpletest.rc-kf5km" in namespace "gc-6410"
    Mar  2 12:38:17.366: INFO: Deleting pod "simpletest.rc-kf8zw" in namespace "gc-6410"
    Mar  2 12:38:17.412: INFO: Deleting pod "simpletest.rc-ks27m" in namespace "gc-6410"
    Mar  2 12:38:17.452: INFO: Deleting pod "simpletest.rc-l9rqj" in namespace "gc-6410"
    Mar  2 12:38:17.518: INFO: Deleting pod "simpletest.rc-lkmgq" in namespace "gc-6410"
    Mar  2 12:38:17.544: INFO: Deleting pod "simpletest.rc-lldmm" in namespace "gc-6410"
    Mar  2 12:38:17.856: INFO: Deleting pod "simpletest.rc-m5cmr" in namespace "gc-6410"
    Mar  2 12:38:17.946: INFO: Deleting pod "simpletest.rc-mkqz9" in namespace "gc-6410"
    Mar  2 12:38:17.963: INFO: Deleting pod "simpletest.rc-mqr7k" in namespace "gc-6410"
    Mar  2 12:38:18.068: INFO: Deleting pod "simpletest.rc-mvjff" in namespace "gc-6410"
    Mar  2 12:38:18.127: INFO: Deleting pod "simpletest.rc-nkhkf" in namespace "gc-6410"
    Mar  2 12:38:18.221: INFO: Deleting pod "simpletest.rc-np6pp" in namespace "gc-6410"
    Mar  2 12:38:18.284: INFO: Deleting pod "simpletest.rc-p7s4q" in namespace "gc-6410"
    Mar  2 12:38:18.462: INFO: Deleting pod "simpletest.rc-pfksj" in namespace "gc-6410"
    Mar  2 12:38:18.544: INFO: Deleting pod "simpletest.rc-px9kq" in namespace "gc-6410"
    Mar  2 12:38:18.560: INFO: Deleting pod "simpletest.rc-qjdrm" in namespace "gc-6410"
    Mar  2 12:38:18.591: INFO: Deleting pod "simpletest.rc-qvnf8" in namespace "gc-6410"
    Mar  2 12:38:18.606: INFO: Deleting pod "simpletest.rc-rb7v4" in namespace "gc-6410"
    Mar  2 12:38:18.622: INFO: Deleting pod "simpletest.rc-s6kzm" in namespace "gc-6410"
    Mar  2 12:38:18.658: INFO: Deleting pod "simpletest.rc-s8d46" in namespace "gc-6410"
    Mar  2 12:38:18.855: INFO: Deleting pod "simpletest.rc-snnhr" in namespace "gc-6410"
    Mar  2 12:38:19.055: INFO: Deleting pod "simpletest.rc-swv4z" in namespace "gc-6410"
    Mar  2 12:38:19.071: INFO: Deleting pod "simpletest.rc-t2pvr" in namespace "gc-6410"
    Mar  2 12:38:19.159: INFO: Deleting pod "simpletest.rc-t8vfr" in namespace "gc-6410"
    Mar  2 12:38:19.266: INFO: Deleting pod "simpletest.rc-tcwgg" in namespace "gc-6410"
    Mar  2 12:38:19.327: INFO: Deleting pod "simpletest.rc-tthkn" in namespace "gc-6410"
    Mar  2 12:38:19.353: INFO: Deleting pod "simpletest.rc-tvpqf" in namespace "gc-6410"
    Mar  2 12:38:19.406: INFO: Deleting pod "simpletest.rc-v5ht8" in namespace "gc-6410"
    Mar  2 12:38:19.669: INFO: Deleting pod "simpletest.rc-v8rhf" in namespace "gc-6410"
    Mar  2 12:38:19.751: INFO: Deleting pod "simpletest.rc-v8xpq" in namespace "gc-6410"
    Mar  2 12:38:19.766: INFO: Deleting pod "simpletest.rc-vcjbz" in namespace "gc-6410"
    Mar  2 12:38:19.794: INFO: Deleting pod "simpletest.rc-vhjft" in namespace "gc-6410"
    Mar  2 12:38:19.850: INFO: Deleting pod "simpletest.rc-vt9nj" in namespace "gc-6410"
    Mar  2 12:38:19.869: INFO: Deleting pod "simpletest.rc-vxnbb" in namespace "gc-6410"
    Mar  2 12:38:19.890: INFO: Deleting pod "simpletest.rc-wq2vm" in namespace "gc-6410"
    Mar  2 12:38:19.943: INFO: Deleting pod "simpletest.rc-wqb7k" in namespace "gc-6410"
    Mar  2 12:38:19.954: INFO: Deleting pod "simpletest.rc-wshcs" in namespace "gc-6410"
    Mar  2 12:38:20.030: INFO: Deleting pod "simpletest.rc-xc4tq" in namespace "gc-6410"
    Mar  2 12:38:20.042: INFO: Deleting pod "simpletest.rc-xhqjz" in namespace "gc-6410"
    Mar  2 12:38:20.053: INFO: Deleting pod "simpletest.rc-xjs62" in namespace "gc-6410"
    Mar  2 12:38:20.070: INFO: Deleting pod "simpletest.rc-xjx5j" in namespace "gc-6410"
    Mar  2 12:38:20.137: INFO: Deleting pod "simpletest.rc-xpp2t" in namespace "gc-6410"
    Mar  2 12:38:20.167: INFO: Deleting pod "simpletest.rc-xvm6c" in namespace "gc-6410"
    Mar  2 12:38:20.196: INFO: Deleting pod "simpletest.rc-z29ch" in namespace "gc-6410"
    Mar  2 12:38:20.285: INFO: Deleting pod "simpletest.rc-zjlz6" in namespace "gc-6410"
    Mar  2 12:38:20.307: INFO: Deleting pod "simpletest.rc-zl98z" in namespace "gc-6410"
    Mar  2 12:38:20.320: INFO: Deleting pod "simpletest.rc-znhx2" in namespace "gc-6410"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 12:38:20.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6410" for this suite. 03/02/23 12:38:20.364
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:38:20.369
Mar  2 12:38:20.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename var-expansion 03/02/23 12:38:20.428
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:20.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:20.554
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Mar  2 12:38:20.571: INFO: Waiting up to 2m0s for pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd" in namespace "var-expansion-7176" to be "container 0 failed with reason CreateContainerConfigError"
Mar  2 12:38:20.643: INFO: Pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.053031ms
Mar  2 12:38:22.656: INFO: Pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039881934s
Mar  2 12:38:24.647: INFO: Pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030485144s
Mar  2 12:38:24.647: INFO: Pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar  2 12:38:24.647: INFO: Deleting pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd" in namespace "var-expansion-7176"
Mar  2 12:38:24.656: INFO: Wait up to 5m0s for pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 12:38:28.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7176" for this suite. 03/02/23 12:38:28.676
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":13,"skipped":214,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.316 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:38:20.369
    Mar  2 12:38:20.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename var-expansion 03/02/23 12:38:20.428
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:20.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:20.554
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Mar  2 12:38:20.571: INFO: Waiting up to 2m0s for pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd" in namespace "var-expansion-7176" to be "container 0 failed with reason CreateContainerConfigError"
    Mar  2 12:38:20.643: INFO: Pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.053031ms
    Mar  2 12:38:22.656: INFO: Pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039881934s
    Mar  2 12:38:24.647: INFO: Pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030485144s
    Mar  2 12:38:24.647: INFO: Pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar  2 12:38:24.647: INFO: Deleting pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd" in namespace "var-expansion-7176"
    Mar  2 12:38:24.656: INFO: Wait up to 5m0s for pod "var-expansion-615cdd57-58bf-4e41-a9df-7846c892eecd" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 12:38:28.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7176" for this suite. 03/02/23 12:38:28.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:38:28.706
Mar  2 12:38:28.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 12:38:28.709
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:28.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:28.742
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 03/02/23 12:38:28.746
Mar  2 12:38:28.770: INFO: Waiting up to 5m0s for pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2" in namespace "downward-api-5087" to be "Succeeded or Failed"
Mar  2 12:38:28.778: INFO: Pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.188272ms
Mar  2 12:38:30.782: INFO: Pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012545734s
Mar  2 12:38:32.792: INFO: Pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021999222s
Mar  2 12:38:34.802: INFO: Pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032429067s
STEP: Saw pod success 03/02/23 12:38:34.803
Mar  2 12:38:34.803: INFO: Pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2" satisfied condition "Succeeded or Failed"
Mar  2 12:38:34.810: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2 container dapi-container: <nil>
STEP: delete the pod 03/02/23 12:38:34.83
Mar  2 12:38:34.842: INFO: Waiting for pod downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2 to disappear
Mar  2 12:38:34.845: INFO: Pod downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  2 12:38:34.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5087" for this suite. 03/02/23 12:38:34.854
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":14,"skipped":267,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.155 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:38:28.706
    Mar  2 12:38:28.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 12:38:28.709
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:28.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:28.742
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 03/02/23 12:38:28.746
    Mar  2 12:38:28.770: INFO: Waiting up to 5m0s for pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2" in namespace "downward-api-5087" to be "Succeeded or Failed"
    Mar  2 12:38:28.778: INFO: Pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.188272ms
    Mar  2 12:38:30.782: INFO: Pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012545734s
    Mar  2 12:38:32.792: INFO: Pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021999222s
    Mar  2 12:38:34.802: INFO: Pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032429067s
    STEP: Saw pod success 03/02/23 12:38:34.803
    Mar  2 12:38:34.803: INFO: Pod "downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2" satisfied condition "Succeeded or Failed"
    Mar  2 12:38:34.810: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2 container dapi-container: <nil>
    STEP: delete the pod 03/02/23 12:38:34.83
    Mar  2 12:38:34.842: INFO: Waiting for pod downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2 to disappear
    Mar  2 12:38:34.845: INFO: Pod downward-api-f725c8d8-8282-4c6e-bf63-e18e66c820a2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  2 12:38:34.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5087" for this suite. 03/02/23 12:38:34.854
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:38:34.865
Mar  2 12:38:34.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 12:38:34.867
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:34.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:34.922
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 03/02/23 12:38:34.931
Mar  2 12:38:34.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 create -f -'
Mar  2 12:38:35.525: INFO: stderr: ""
Mar  2 12:38:35.526: INFO: stdout: "pod/pause created\n"
Mar  2 12:38:35.527: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  2 12:38:35.527: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3014" to be "running and ready"
Mar  2 12:38:35.535: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.803377ms
Mar  2 12:38:35.535: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'aarnq-sc-k8s-node-srv2' to be 'Running' but was 'Pending'
Mar  2 12:38:37.540: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012158861s
Mar  2 12:38:37.540: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'aarnq-sc-k8s-node-srv2' to be 'Running' but was 'Pending'
Mar  2 12:38:39.543: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.015099504s
Mar  2 12:38:39.543: INFO: Pod "pause" satisfied condition "running and ready"
Mar  2 12:38:39.544: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 03/02/23 12:38:39.544
Mar  2 12:38:39.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 label pods pause testing-label=testing-label-value'
Mar  2 12:38:39.701: INFO: stderr: ""
Mar  2 12:38:39.701: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/02/23 12:38:39.701
Mar  2 12:38:39.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 get pod pause -L testing-label'
Mar  2 12:38:39.849: INFO: stderr: ""
Mar  2 12:38:39.849: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/02/23 12:38:39.849
Mar  2 12:38:39.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 label pods pause testing-label-'
Mar  2 12:38:40.131: INFO: stderr: ""
Mar  2 12:38:40.131: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/02/23 12:38:40.142
Mar  2 12:38:40.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 get pod pause -L testing-label'
Mar  2 12:38:40.430: INFO: stderr: ""
Mar  2 12:38:40.430: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 03/02/23 12:38:40.43
Mar  2 12:38:40.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 delete --grace-period=0 --force -f -'
Mar  2 12:38:40.664: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 12:38:40.664: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  2 12:38:40.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 get rc,svc -l name=pause --no-headers'
Mar  2 12:38:40.952: INFO: stderr: "No resources found in kubectl-3014 namespace.\n"
Mar  2 12:38:40.952: INFO: stdout: ""
Mar  2 12:38:40.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 12:38:41.151: INFO: stderr: ""
Mar  2 12:38:41.151: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 12:38:41.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3014" for this suite. 03/02/23 12:38:41.159
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":15,"skipped":268,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.303 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:38:34.865
    Mar  2 12:38:34.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 12:38:34.867
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:34.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:34.922
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 03/02/23 12:38:34.931
    Mar  2 12:38:34.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 create -f -'
    Mar  2 12:38:35.525: INFO: stderr: ""
    Mar  2 12:38:35.526: INFO: stdout: "pod/pause created\n"
    Mar  2 12:38:35.527: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar  2 12:38:35.527: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3014" to be "running and ready"
    Mar  2 12:38:35.535: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.803377ms
    Mar  2 12:38:35.535: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'aarnq-sc-k8s-node-srv2' to be 'Running' but was 'Pending'
    Mar  2 12:38:37.540: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012158861s
    Mar  2 12:38:37.540: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'aarnq-sc-k8s-node-srv2' to be 'Running' but was 'Pending'
    Mar  2 12:38:39.543: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.015099504s
    Mar  2 12:38:39.543: INFO: Pod "pause" satisfied condition "running and ready"
    Mar  2 12:38:39.544: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 03/02/23 12:38:39.544
    Mar  2 12:38:39.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 label pods pause testing-label=testing-label-value'
    Mar  2 12:38:39.701: INFO: stderr: ""
    Mar  2 12:38:39.701: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/02/23 12:38:39.701
    Mar  2 12:38:39.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 get pod pause -L testing-label'
    Mar  2 12:38:39.849: INFO: stderr: ""
    Mar  2 12:38:39.849: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/02/23 12:38:39.849
    Mar  2 12:38:39.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 label pods pause testing-label-'
    Mar  2 12:38:40.131: INFO: stderr: ""
    Mar  2 12:38:40.131: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/02/23 12:38:40.142
    Mar  2 12:38:40.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 get pod pause -L testing-label'
    Mar  2 12:38:40.430: INFO: stderr: ""
    Mar  2 12:38:40.430: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 03/02/23 12:38:40.43
    Mar  2 12:38:40.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 delete --grace-period=0 --force -f -'
    Mar  2 12:38:40.664: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 12:38:40.664: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar  2 12:38:40.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 get rc,svc -l name=pause --no-headers'
    Mar  2 12:38:40.952: INFO: stderr: "No resources found in kubectl-3014 namespace.\n"
    Mar  2 12:38:40.952: INFO: stdout: ""
    Mar  2 12:38:40.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-3014 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  2 12:38:41.151: INFO: stderr: ""
    Mar  2 12:38:41.151: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 12:38:41.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3014" for this suite. 03/02/23 12:38:41.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:38:41.173
Mar  2 12:38:41.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 12:38:41.175
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:41.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:41.234
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-305a04f0-52d0-4b22-855b-1625d0970888 03/02/23 12:38:41.238
STEP: Creating a pod to test consume secrets 03/02/23 12:38:41.246
Mar  2 12:38:41.256: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581" in namespace "projected-7597" to be "Succeeded or Failed"
Mar  2 12:38:41.261: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581": Phase="Pending", Reason="", readiness=false. Elapsed: 4.419617ms
Mar  2 12:38:43.266: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009739339s
Mar  2 12:38:45.265: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008574193s
Mar  2 12:38:47.268: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011322482s
Mar  2 12:38:49.265: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00908509s
STEP: Saw pod success 03/02/23 12:38:49.266
Mar  2 12:38:49.266: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581" satisfied condition "Succeeded or Failed"
Mar  2 12:38:49.269: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/02/23 12:38:49.287
Mar  2 12:38:49.327: INFO: Waiting for pod pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581 to disappear
Mar  2 12:38:49.335: INFO: Pod pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 12:38:49.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7597" for this suite. 03/02/23 12:38:49.345
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":16,"skipped":300,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.197 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:38:41.173
    Mar  2 12:38:41.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 12:38:41.175
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:41.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:41.234
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-305a04f0-52d0-4b22-855b-1625d0970888 03/02/23 12:38:41.238
    STEP: Creating a pod to test consume secrets 03/02/23 12:38:41.246
    Mar  2 12:38:41.256: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581" in namespace "projected-7597" to be "Succeeded or Failed"
    Mar  2 12:38:41.261: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581": Phase="Pending", Reason="", readiness=false. Elapsed: 4.419617ms
    Mar  2 12:38:43.266: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009739339s
    Mar  2 12:38:45.265: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008574193s
    Mar  2 12:38:47.268: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011322482s
    Mar  2 12:38:49.265: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00908509s
    STEP: Saw pod success 03/02/23 12:38:49.266
    Mar  2 12:38:49.266: INFO: Pod "pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581" satisfied condition "Succeeded or Failed"
    Mar  2 12:38:49.269: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 12:38:49.287
    Mar  2 12:38:49.327: INFO: Waiting for pod pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581 to disappear
    Mar  2 12:38:49.335: INFO: Pod pod-projected-secrets-5f5734e9-e63f-4984-a204-6a441c45f581 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 12:38:49.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7597" for this suite. 03/02/23 12:38:49.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:38:49.373
Mar  2 12:38:49.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 12:38:49.375
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:49.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:49.432
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 12:38:49.45
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:38:50.071
STEP: Deploying the webhook pod 03/02/23 12:38:50.08
STEP: Wait for the deployment to be ready 03/02/23 12:38:50.105
Mar  2 12:38:50.122: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/02/23 12:38:52.139
STEP: Verifying the service has paired with the endpoint 03/02/23 12:38:52.151
Mar  2 12:38:53.152: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 03/02/23 12:38:53.215
STEP: Creating a configMap that should be mutated 03/02/23 12:38:53.231
STEP: Deleting the collection of validation webhooks 03/02/23 12:38:53.27
STEP: Creating a configMap that should not be mutated 03/02/23 12:38:53.295
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 12:38:53.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8119" for this suite. 03/02/23 12:38:53.319
STEP: Destroying namespace "webhook-8119-markers" for this suite. 03/02/23 12:38:53.325
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":17,"skipped":306,"failed":0}
------------------------------
â€¢ [4.023 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:38:49.373
    Mar  2 12:38:49.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 12:38:49.375
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:49.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:49.432
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 12:38:49.45
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:38:50.071
    STEP: Deploying the webhook pod 03/02/23 12:38:50.08
    STEP: Wait for the deployment to be ready 03/02/23 12:38:50.105
    Mar  2 12:38:50.122: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/02/23 12:38:52.139
    STEP: Verifying the service has paired with the endpoint 03/02/23 12:38:52.151
    Mar  2 12:38:53.152: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 03/02/23 12:38:53.215
    STEP: Creating a configMap that should be mutated 03/02/23 12:38:53.231
    STEP: Deleting the collection of validation webhooks 03/02/23 12:38:53.27
    STEP: Creating a configMap that should not be mutated 03/02/23 12:38:53.295
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 12:38:53.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8119" for this suite. 03/02/23 12:38:53.319
    STEP: Destroying namespace "webhook-8119-markers" for this suite. 03/02/23 12:38:53.325
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:38:53.411
Mar  2 12:38:53.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir-wrapper 03/02/23 12:38:53.413
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:53.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:53.501
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/02/23 12:38:53.509
STEP: Creating RC which spawns configmap-volume pods 03/02/23 12:38:54.026
Mar  2 12:38:54.044: INFO: Pod name wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375: Found 0 pods out of 5
Mar  2 12:38:59.065: INFO: Pod name wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/02/23 12:38:59.065
Mar  2 12:38:59.065: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:38:59.071: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.866016ms
Mar  2 12:39:01.097: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032162725s
Mar  2 12:39:03.110: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044918986s
Mar  2 12:39:05.082: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016789518s
Mar  2 12:39:07.084: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019505362s
Mar  2 12:39:09.090: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024634036s
Mar  2 12:39:11.086: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020614436s
Mar  2 12:39:13.079: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Running", Reason="", readiness=true. Elapsed: 14.013837055s
Mar  2 12:39:13.079: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d" satisfied condition "running"
Mar  2 12:39:13.080: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-pmnw7" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:13.090: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-pmnw7": Phase="Running", Reason="", readiness=true. Elapsed: 10.023213ms
Mar  2 12:39:13.090: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-pmnw7" satisfied condition "running"
Mar  2 12:39:13.090: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-r7k9h" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:13.101: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-r7k9h": Phase="Running", Reason="", readiness=true. Elapsed: 10.492384ms
Mar  2 12:39:13.101: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-r7k9h" satisfied condition "running"
Mar  2 12:39:13.101: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zkh7n" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:13.110: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zkh7n": Phase="Running", Reason="", readiness=true. Elapsed: 9.314314ms
Mar  2 12:39:13.110: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zkh7n" satisfied condition "running"
Mar  2 12:39:13.110: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zqmml" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:13.121: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zqmml": Phase="Running", Reason="", readiness=true. Elapsed: 9.860202ms
Mar  2 12:39:13.121: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zqmml" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375 in namespace emptydir-wrapper-705, will wait for the garbage collector to delete the pods 03/02/23 12:39:13.121
Mar  2 12:39:13.190: INFO: Deleting ReplicationController wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375 took: 7.861596ms
Mar  2 12:39:13.291: INFO: Terminating ReplicationController wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375 pods took: 100.994916ms
STEP: Creating RC which spawns configmap-volume pods 03/02/23 12:39:16.899
Mar  2 12:39:16.920: INFO: Pod name wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3: Found 0 pods out of 5
Mar  2 12:39:21.937: INFO: Pod name wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/02/23 12:39:21.937
Mar  2 12:39:21.937: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:21.943: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.461647ms
Mar  2 12:39:23.952: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014748502s
Mar  2 12:39:25.952: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014528142s
Mar  2 12:39:27.950: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012393576s
Mar  2 12:39:29.955: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017111572s
Mar  2 12:39:31.955: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017560018s
Mar  2 12:39:33.949: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Running", Reason="", readiness=true. Elapsed: 12.011519177s
Mar  2 12:39:33.949: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh" satisfied condition "running"
Mar  2 12:39:33.950: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-ccjrf" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:33.955: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-ccjrf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.289936ms
Mar  2 12:39:35.961: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-ccjrf": Phase="Running", Reason="", readiness=true. Elapsed: 2.011702838s
Mar  2 12:39:35.961: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-ccjrf" satisfied condition "running"
Mar  2 12:39:35.961: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-jqrl6" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:35.965: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-jqrl6": Phase="Running", Reason="", readiness=true. Elapsed: 3.898521ms
Mar  2 12:39:35.965: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-jqrl6" satisfied condition "running"
Mar  2 12:39:35.965: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pcpk8" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:35.969: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pcpk8": Phase="Running", Reason="", readiness=true. Elapsed: 3.298222ms
Mar  2 12:39:35.969: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pcpk8" satisfied condition "running"
Mar  2 12:39:35.969: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pm42v" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:35.999: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pm42v": Phase="Running", Reason="", readiness=true. Elapsed: 30.30597ms
Mar  2 12:39:36.000: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pm42v" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3 in namespace emptydir-wrapper-705, will wait for the garbage collector to delete the pods 03/02/23 12:39:36.01
Mar  2 12:39:36.086: INFO: Deleting ReplicationController wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3 took: 20.288104ms
Mar  2 12:39:36.199: INFO: Terminating ReplicationController wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3 pods took: 113.403581ms
STEP: Creating RC which spawns configmap-volume pods 03/02/23 12:39:40.813
Mar  2 12:39:40.832: INFO: Pod name wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725: Found 0 pods out of 5
Mar  2 12:39:45.854: INFO: Pod name wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/02/23 12:39:45.854
Mar  2 12:39:45.854: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:45.867: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.467444ms
Mar  2 12:39:47.876: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021715505s
Mar  2 12:39:49.873: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019290592s
Mar  2 12:39:51.872: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018382737s
Mar  2 12:39:53.891: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037091925s
Mar  2 12:39:55.889: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.035339796s
Mar  2 12:39:57.874: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Running", Reason="", readiness=true. Elapsed: 12.020167976s
Mar  2 12:39:57.874: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7" satisfied condition "running"
Mar  2 12:39:57.874: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-9lc46" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:57.878: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-9lc46": Phase="Running", Reason="", readiness=true. Elapsed: 3.998819ms
Mar  2 12:39:57.878: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-9lc46" satisfied condition "running"
Mar  2 12:39:57.878: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-jbpks" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:57.882: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-jbpks": Phase="Running", Reason="", readiness=true. Elapsed: 3.398797ms
Mar  2 12:39:57.882: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-jbpks" satisfied condition "running"
Mar  2 12:39:57.882: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-nq56j" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:57.885: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-nq56j": Phase="Running", Reason="", readiness=true. Elapsed: 3.290639ms
Mar  2 12:39:57.885: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-nq56j" satisfied condition "running"
Mar  2 12:39:57.885: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-v87pm" in namespace "emptydir-wrapper-705" to be "running"
Mar  2 12:39:57.888: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-v87pm": Phase="Running", Reason="", readiness=true. Elapsed: 2.988891ms
Mar  2 12:39:57.888: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-v87pm" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725 in namespace emptydir-wrapper-705, will wait for the garbage collector to delete the pods 03/02/23 12:39:57.888
Mar  2 12:39:57.949: INFO: Deleting ReplicationController wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725 took: 6.766952ms
Mar  2 12:39:58.050: INFO: Terminating ReplicationController wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725 pods took: 101.179877ms
STEP: Cleaning up the configMaps 03/02/23 12:40:01.052
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar  2 12:40:01.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-705" for this suite. 03/02/23 12:40:01.656
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":18,"skipped":316,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.253 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:38:53.411
    Mar  2 12:38:53.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir-wrapper 03/02/23 12:38:53.413
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:38:53.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:38:53.501
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/02/23 12:38:53.509
    STEP: Creating RC which spawns configmap-volume pods 03/02/23 12:38:54.026
    Mar  2 12:38:54.044: INFO: Pod name wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375: Found 0 pods out of 5
    Mar  2 12:38:59.065: INFO: Pod name wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/02/23 12:38:59.065
    Mar  2 12:38:59.065: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:38:59.071: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.866016ms
    Mar  2 12:39:01.097: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032162725s
    Mar  2 12:39:03.110: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044918986s
    Mar  2 12:39:05.082: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016789518s
    Mar  2 12:39:07.084: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019505362s
    Mar  2 12:39:09.090: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024634036s
    Mar  2 12:39:11.086: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020614436s
    Mar  2 12:39:13.079: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d": Phase="Running", Reason="", readiness=true. Elapsed: 14.013837055s
    Mar  2 12:39:13.079: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-5mq9d" satisfied condition "running"
    Mar  2 12:39:13.080: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-pmnw7" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:13.090: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-pmnw7": Phase="Running", Reason="", readiness=true. Elapsed: 10.023213ms
    Mar  2 12:39:13.090: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-pmnw7" satisfied condition "running"
    Mar  2 12:39:13.090: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-r7k9h" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:13.101: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-r7k9h": Phase="Running", Reason="", readiness=true. Elapsed: 10.492384ms
    Mar  2 12:39:13.101: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-r7k9h" satisfied condition "running"
    Mar  2 12:39:13.101: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zkh7n" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:13.110: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zkh7n": Phase="Running", Reason="", readiness=true. Elapsed: 9.314314ms
    Mar  2 12:39:13.110: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zkh7n" satisfied condition "running"
    Mar  2 12:39:13.110: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zqmml" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:13.121: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zqmml": Phase="Running", Reason="", readiness=true. Elapsed: 9.860202ms
    Mar  2 12:39:13.121: INFO: Pod "wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375-zqmml" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375 in namespace emptydir-wrapper-705, will wait for the garbage collector to delete the pods 03/02/23 12:39:13.121
    Mar  2 12:39:13.190: INFO: Deleting ReplicationController wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375 took: 7.861596ms
    Mar  2 12:39:13.291: INFO: Terminating ReplicationController wrapped-volume-race-f72af490-ed91-43c2-b20d-174b19674375 pods took: 100.994916ms
    STEP: Creating RC which spawns configmap-volume pods 03/02/23 12:39:16.899
    Mar  2 12:39:16.920: INFO: Pod name wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3: Found 0 pods out of 5
    Mar  2 12:39:21.937: INFO: Pod name wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/02/23 12:39:21.937
    Mar  2 12:39:21.937: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:21.943: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.461647ms
    Mar  2 12:39:23.952: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014748502s
    Mar  2 12:39:25.952: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014528142s
    Mar  2 12:39:27.950: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012393576s
    Mar  2 12:39:29.955: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017111572s
    Mar  2 12:39:31.955: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017560018s
    Mar  2 12:39:33.949: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh": Phase="Running", Reason="", readiness=true. Elapsed: 12.011519177s
    Mar  2 12:39:33.949: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-4mcsh" satisfied condition "running"
    Mar  2 12:39:33.950: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-ccjrf" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:33.955: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-ccjrf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.289936ms
    Mar  2 12:39:35.961: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-ccjrf": Phase="Running", Reason="", readiness=true. Elapsed: 2.011702838s
    Mar  2 12:39:35.961: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-ccjrf" satisfied condition "running"
    Mar  2 12:39:35.961: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-jqrl6" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:35.965: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-jqrl6": Phase="Running", Reason="", readiness=true. Elapsed: 3.898521ms
    Mar  2 12:39:35.965: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-jqrl6" satisfied condition "running"
    Mar  2 12:39:35.965: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pcpk8" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:35.969: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pcpk8": Phase="Running", Reason="", readiness=true. Elapsed: 3.298222ms
    Mar  2 12:39:35.969: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pcpk8" satisfied condition "running"
    Mar  2 12:39:35.969: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pm42v" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:35.999: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pm42v": Phase="Running", Reason="", readiness=true. Elapsed: 30.30597ms
    Mar  2 12:39:36.000: INFO: Pod "wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3-pm42v" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3 in namespace emptydir-wrapper-705, will wait for the garbage collector to delete the pods 03/02/23 12:39:36.01
    Mar  2 12:39:36.086: INFO: Deleting ReplicationController wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3 took: 20.288104ms
    Mar  2 12:39:36.199: INFO: Terminating ReplicationController wrapped-volume-race-538eabcc-5f1f-4f12-9829-fccc382b44f3 pods took: 113.403581ms
    STEP: Creating RC which spawns configmap-volume pods 03/02/23 12:39:40.813
    Mar  2 12:39:40.832: INFO: Pod name wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725: Found 0 pods out of 5
    Mar  2 12:39:45.854: INFO: Pod name wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/02/23 12:39:45.854
    Mar  2 12:39:45.854: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:45.867: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.467444ms
    Mar  2 12:39:47.876: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021715505s
    Mar  2 12:39:49.873: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019290592s
    Mar  2 12:39:51.872: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018382737s
    Mar  2 12:39:53.891: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037091925s
    Mar  2 12:39:55.889: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.035339796s
    Mar  2 12:39:57.874: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7": Phase="Running", Reason="", readiness=true. Elapsed: 12.020167976s
    Mar  2 12:39:57.874: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-422l7" satisfied condition "running"
    Mar  2 12:39:57.874: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-9lc46" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:57.878: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-9lc46": Phase="Running", Reason="", readiness=true. Elapsed: 3.998819ms
    Mar  2 12:39:57.878: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-9lc46" satisfied condition "running"
    Mar  2 12:39:57.878: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-jbpks" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:57.882: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-jbpks": Phase="Running", Reason="", readiness=true. Elapsed: 3.398797ms
    Mar  2 12:39:57.882: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-jbpks" satisfied condition "running"
    Mar  2 12:39:57.882: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-nq56j" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:57.885: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-nq56j": Phase="Running", Reason="", readiness=true. Elapsed: 3.290639ms
    Mar  2 12:39:57.885: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-nq56j" satisfied condition "running"
    Mar  2 12:39:57.885: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-v87pm" in namespace "emptydir-wrapper-705" to be "running"
    Mar  2 12:39:57.888: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-v87pm": Phase="Running", Reason="", readiness=true. Elapsed: 2.988891ms
    Mar  2 12:39:57.888: INFO: Pod "wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725-v87pm" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725 in namespace emptydir-wrapper-705, will wait for the garbage collector to delete the pods 03/02/23 12:39:57.888
    Mar  2 12:39:57.949: INFO: Deleting ReplicationController wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725 took: 6.766952ms
    Mar  2 12:39:58.050: INFO: Terminating ReplicationController wrapped-volume-race-23bfe66f-e0cd-4946-aa96-c9e6dda0f725 pods took: 101.179877ms
    STEP: Cleaning up the configMaps 03/02/23 12:40:01.052
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar  2 12:40:01.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-705" for this suite. 03/02/23 12:40:01.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:40:01.665
Mar  2 12:40:01.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-probe 03/02/23 12:40:01.667
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:40:01.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:40:01.749
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 in namespace container-probe-9608 03/02/23 12:40:01.753
Mar  2 12:40:01.764: INFO: Waiting up to 5m0s for pod "liveness-6d31f245-220d-484e-b6ff-baf1066c5f59" in namespace "container-probe-9608" to be "not pending"
Mar  2 12:40:01.830: INFO: Pod "liveness-6d31f245-220d-484e-b6ff-baf1066c5f59": Phase="Pending", Reason="", readiness=false. Elapsed: 65.859137ms
Mar  2 12:40:03.834: INFO: Pod "liveness-6d31f245-220d-484e-b6ff-baf1066c5f59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069820267s
Mar  2 12:40:05.839: INFO: Pod "liveness-6d31f245-220d-484e-b6ff-baf1066c5f59": Phase="Running", Reason="", readiness=true. Elapsed: 4.075348942s
Mar  2 12:40:05.840: INFO: Pod "liveness-6d31f245-220d-484e-b6ff-baf1066c5f59" satisfied condition "not pending"
Mar  2 12:40:05.840: INFO: Started pod liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 in namespace container-probe-9608
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 12:40:05.84
Mar  2 12:40:05.843: INFO: Initial restart count of pod liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is 0
Mar  2 12:40:23.899: INFO: Restart count of pod container-probe-9608/liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is now 1 (18.055855841s elapsed)
Mar  2 12:40:43.988: INFO: Restart count of pod container-probe-9608/liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is now 2 (38.145242319s elapsed)
Mar  2 12:41:04.059: INFO: Restart count of pod container-probe-9608/liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is now 3 (58.215593193s elapsed)
Mar  2 12:41:24.140: INFO: Restart count of pod container-probe-9608/liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is now 4 (1m18.297318261s elapsed)
Mar  2 12:42:36.435: INFO: Restart count of pod container-probe-9608/liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is now 5 (2m30.592376348s elapsed)
STEP: deleting the pod 03/02/23 12:42:36.436
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 12:42:36.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9608" for this suite. 03/02/23 12:42:36.473
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":19,"skipped":321,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.855 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:40:01.665
    Mar  2 12:40:01.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-probe 03/02/23 12:40:01.667
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:40:01.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:40:01.749
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 in namespace container-probe-9608 03/02/23 12:40:01.753
    Mar  2 12:40:01.764: INFO: Waiting up to 5m0s for pod "liveness-6d31f245-220d-484e-b6ff-baf1066c5f59" in namespace "container-probe-9608" to be "not pending"
    Mar  2 12:40:01.830: INFO: Pod "liveness-6d31f245-220d-484e-b6ff-baf1066c5f59": Phase="Pending", Reason="", readiness=false. Elapsed: 65.859137ms
    Mar  2 12:40:03.834: INFO: Pod "liveness-6d31f245-220d-484e-b6ff-baf1066c5f59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069820267s
    Mar  2 12:40:05.839: INFO: Pod "liveness-6d31f245-220d-484e-b6ff-baf1066c5f59": Phase="Running", Reason="", readiness=true. Elapsed: 4.075348942s
    Mar  2 12:40:05.840: INFO: Pod "liveness-6d31f245-220d-484e-b6ff-baf1066c5f59" satisfied condition "not pending"
    Mar  2 12:40:05.840: INFO: Started pod liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 in namespace container-probe-9608
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 12:40:05.84
    Mar  2 12:40:05.843: INFO: Initial restart count of pod liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is 0
    Mar  2 12:40:23.899: INFO: Restart count of pod container-probe-9608/liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is now 1 (18.055855841s elapsed)
    Mar  2 12:40:43.988: INFO: Restart count of pod container-probe-9608/liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is now 2 (38.145242319s elapsed)
    Mar  2 12:41:04.059: INFO: Restart count of pod container-probe-9608/liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is now 3 (58.215593193s elapsed)
    Mar  2 12:41:24.140: INFO: Restart count of pod container-probe-9608/liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is now 4 (1m18.297318261s elapsed)
    Mar  2 12:42:36.435: INFO: Restart count of pod container-probe-9608/liveness-6d31f245-220d-484e-b6ff-baf1066c5f59 is now 5 (2m30.592376348s elapsed)
    STEP: deleting the pod 03/02/23 12:42:36.436
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 12:42:36.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9608" for this suite. 03/02/23 12:42:36.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:42:36.529
Mar  2 12:42:36.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename deployment 03/02/23 12:42:36.531
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:42:36.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:42:36.564
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar  2 12:42:36.568: INFO: Creating simple deployment test-new-deployment
Mar  2 12:42:36.580: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 03/02/23 12:42:38.604
STEP: updating a scale subresource 03/02/23 12:42:38.606
STEP: verifying the deployment Spec.Replicas was modified 03/02/23 12:42:38.612
STEP: Patch a scale subresource 03/02/23 12:42:38.62
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 12:42:38.664: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8200  85e29fc0-8ea9-4fb1-a6bc-300e08c1b812 1923121 3 2023-03-02 12:42:36 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-02 12:42:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d04bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-02 12:42:38 +0000 UTC,LastTransitionTime:2023-03-02 12:42:36 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 12:42:38 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 12:42:38.695: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-8200  0a244401-b097-4542-aa83-6dac0162b9a2 1923123 3 2023-03-02 12:42:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 85e29fc0-8ea9-4fb1-a6bc-300e08c1b812 0xc001eb6677 0xc001eb6678}] [] [{kube-controller-manager Update apps/v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85e29fc0-8ea9-4fb1-a6bc-300e08c1b812\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001eb6728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 12:42:38.711: INFO: Pod "test-new-deployment-845c8977d9-6ht2l" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-6ht2l test-new-deployment-845c8977d9- deployment-8200  14dd8071-1802-49ca-ae15-2452c2339cf7 1923124 0 2023-03-02 12:42:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 0a244401-b097-4542-aa83-6dac0162b9a2 0xc001eb6b17 0xc001eb6b18}] [] [{kube-controller-manager Update v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a244401-b097-4542-aa83-6dac0162b9a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbf4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbf4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:,StartTime:2023-03-02 12:42:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 12:42:38.712: INFO: Pod "test-new-deployment-845c8977d9-pfvh8" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-pfvh8 test-new-deployment-845c8977d9- deployment-8200  38a1465e-8108-427c-b63a-97e99c572bfb 1923127 0 2023-03-02 12:42:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 0a244401-b097-4542-aa83-6dac0162b9a2 0xc001eb6cd7 0xc001eb6cd8}] [] [{kube-controller-manager Update v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a244401-b097-4542-aa83-6dac0162b9a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ckk6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ckk6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 12:42:38.714: INFO: Pod "test-new-deployment-845c8977d9-ptpsr" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-ptpsr test-new-deployment-845c8977d9- deployment-8200  fb1dc3ca-3903-44ca-adab-8cfb429728ec 1923108 0 2023-03-02 12:42:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:87e0361cb949f5b2b7fa2d2276d93ebf5ecaad564b7e77aa0f3cfa9d748c7204 cni.projectcalico.org/podIP:10.233.123.122/32 cni.projectcalico.org/podIPs:10.233.123.122/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 0a244401-b097-4542-aa83-6dac0162b9a2 0xc001eb6e20 0xc001eb6e21}] [] [{kube-controller-manager Update v1 2023-03-02 12:42:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a244401-b097-4542-aa83-6dac0162b9a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 12:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ghscf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ghscf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.122,StartTime:2023-03-02 12:42:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 12:42:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://02932ed3ae91d4ef759e121a75e16d7c0faae1c732996d28cf4d356cfa3721fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 12:42:38.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8200" for this suite. 03/02/23 12:42:38.742
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":20,"skipped":344,"failed":0}
------------------------------
â€¢ [2.237 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:42:36.529
    Mar  2 12:42:36.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename deployment 03/02/23 12:42:36.531
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:42:36.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:42:36.564
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar  2 12:42:36.568: INFO: Creating simple deployment test-new-deployment
    Mar  2 12:42:36.580: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 03/02/23 12:42:38.604
    STEP: updating a scale subresource 03/02/23 12:42:38.606
    STEP: verifying the deployment Spec.Replicas was modified 03/02/23 12:42:38.612
    STEP: Patch a scale subresource 03/02/23 12:42:38.62
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 12:42:38.664: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-8200  85e29fc0-8ea9-4fb1-a6bc-300e08c1b812 1923121 3 2023-03-02 12:42:36 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-02 12:42:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d04bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-02 12:42:38 +0000 UTC,LastTransitionTime:2023-03-02 12:42:36 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 12:42:38 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  2 12:42:38.695: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-8200  0a244401-b097-4542-aa83-6dac0162b9a2 1923123 3 2023-03-02 12:42:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 85e29fc0-8ea9-4fb1-a6bc-300e08c1b812 0xc001eb6677 0xc001eb6678}] [] [{kube-controller-manager Update apps/v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85e29fc0-8ea9-4fb1-a6bc-300e08c1b812\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001eb6728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 12:42:38.711: INFO: Pod "test-new-deployment-845c8977d9-6ht2l" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-6ht2l test-new-deployment-845c8977d9- deployment-8200  14dd8071-1802-49ca-ae15-2452c2339cf7 1923124 0 2023-03-02 12:42:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 0a244401-b097-4542-aa83-6dac0162b9a2 0xc001eb6b17 0xc001eb6b18}] [] [{kube-controller-manager Update v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a244401-b097-4542-aa83-6dac0162b9a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbf4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbf4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:,StartTime:2023-03-02 12:42:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 12:42:38.712: INFO: Pod "test-new-deployment-845c8977d9-pfvh8" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-pfvh8 test-new-deployment-845c8977d9- deployment-8200  38a1465e-8108-427c-b63a-97e99c572bfb 1923127 0 2023-03-02 12:42:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 0a244401-b097-4542-aa83-6dac0162b9a2 0xc001eb6cd7 0xc001eb6cd8}] [] [{kube-controller-manager Update v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a244401-b097-4542-aa83-6dac0162b9a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ckk6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ckk6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 12:42:38.714: INFO: Pod "test-new-deployment-845c8977d9-ptpsr" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-ptpsr test-new-deployment-845c8977d9- deployment-8200  fb1dc3ca-3903-44ca-adab-8cfb429728ec 1923108 0 2023-03-02 12:42:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:87e0361cb949f5b2b7fa2d2276d93ebf5ecaad564b7e77aa0f3cfa9d748c7204 cni.projectcalico.org/podIP:10.233.123.122/32 cni.projectcalico.org/podIPs:10.233.123.122/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 0a244401-b097-4542-aa83-6dac0162b9a2 0xc001eb6e20 0xc001eb6e21}] [] [{kube-controller-manager Update v1 2023-03-02 12:42:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a244401-b097-4542-aa83-6dac0162b9a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 12:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 12:42:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ghscf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ghscf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 12:42:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.122,StartTime:2023-03-02 12:42:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 12:42:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://02932ed3ae91d4ef759e121a75e16d7c0faae1c732996d28cf4d356cfa3721fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 12:42:38.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8200" for this suite. 03/02/23 12:42:38.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:42:38.77
Mar  2 12:42:38.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename init-container 03/02/23 12:42:38.787
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:42:38.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:42:38.829
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 03/02/23 12:42:38.837
Mar  2 12:42:38.837: INFO: PodSpec: initContainers in spec.initContainers
Mar  2 12:43:26.548: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-adfc412d-8f02-4bf0-8333-d75abb06c145", GenerateName:"", Namespace:"init-container-8190", SelfLink:"", UID:"94aa0699-9212-4fb8-a53b-8d49173c5594", ResourceVersion:"1923404", Generation:0, CreationTimestamp:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"837667528"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"dabc3eeb7d5b48cd073819e491ac6884183b3c14bd87b7bb24389ff5fac1c532", "cni.projectcalico.org/podIP":"10.233.123.95/32", "cni.projectcalico.org/podIPs":"10.233.123.95/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012bb788), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 12, 42, 39, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012bb7b8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 12, 43, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012bb7e8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-wddl9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000b2c880), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wddl9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wddl9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wddl9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001f05ab8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"aarnq-sc-k8s-node-srv2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000420380), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f05b30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f05b50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001f05b58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001f05b5c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0009df310), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.0.192", PodIP:"10.233.123.95", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.123.95"}}, StartTime:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004205b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000420620)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://f0cc78dfe0467b7280ee48f48d5fb098eb6a382b1e983d244941ddef6d971ffd", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b2c980), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b2c940), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc001f05bdf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 12:43:26.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8190" for this suite. 03/02/23 12:43:26.582
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":21,"skipped":355,"failed":0}
------------------------------
â€¢ [SLOW TEST] [47.819 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:42:38.77
    Mar  2 12:42:38.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename init-container 03/02/23 12:42:38.787
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:42:38.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:42:38.829
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 03/02/23 12:42:38.837
    Mar  2 12:42:38.837: INFO: PodSpec: initContainers in spec.initContainers
    Mar  2 12:43:26.548: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-adfc412d-8f02-4bf0-8333-d75abb06c145", GenerateName:"", Namespace:"init-container-8190", SelfLink:"", UID:"94aa0699-9212-4fb8-a53b-8d49173c5594", ResourceVersion:"1923404", Generation:0, CreationTimestamp:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"837667528"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"dabc3eeb7d5b48cd073819e491ac6884183b3c14bd87b7bb24389ff5fac1c532", "cni.projectcalico.org/podIP":"10.233.123.95/32", "cni.projectcalico.org/podIPs":"10.233.123.95/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012bb788), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 12, 42, 39, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012bb7b8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 12, 43, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012bb7e8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-wddl9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000b2c880), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wddl9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wddl9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wddl9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001f05ab8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"aarnq-sc-k8s-node-srv2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000420380), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f05b30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f05b50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001f05b58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001f05b5c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0009df310), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.0.192", PodIP:"10.233.123.95", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.123.95"}}, StartTime:time.Date(2023, time.March, 2, 12, 42, 38, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004205b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000420620)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://f0cc78dfe0467b7280ee48f48d5fb098eb6a382b1e983d244941ddef6d971ffd", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b2c980), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b2c940), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc001f05bdf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 12:43:26.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-8190" for this suite. 03/02/23 12:43:26.582
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:43:26.591
Mar  2 12:43:26.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename init-container 03/02/23 12:43:26.594
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:43:26.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:43:26.615
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 03/02/23 12:43:26.621
Mar  2 12:43:26.621: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 12:43:31.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1522" for this suite. 03/02/23 12:43:31.625
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":22,"skipped":356,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.042 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:43:26.591
    Mar  2 12:43:26.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename init-container 03/02/23 12:43:26.594
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:43:26.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:43:26.615
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 03/02/23 12:43:26.621
    Mar  2 12:43:26.621: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 12:43:31.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1522" for this suite. 03/02/23 12:43:31.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:43:31.646
Mar  2 12:43:31.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename statefulset 03/02/23 12:43:31.65
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:43:31.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:43:31.671
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7751 03/02/23 12:43:31.687
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-7751 03/02/23 12:43:31.694
Mar  2 12:43:31.710: INFO: Found 0 stateful pods, waiting for 1
Mar  2 12:43:41.739: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/02/23 12:43:41.751
STEP: updating a scale subresource 03/02/23 12:43:41.754
STEP: verifying the statefulset Spec.Replicas was modified 03/02/23 12:43:41.758
STEP: Patch a scale subresource 03/02/23 12:43:41.761
STEP: verifying the statefulset Spec.Replicas was modified 03/02/23 12:43:41.78
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 12:43:41.822: INFO: Deleting all statefulset in ns statefulset-7751
Mar  2 12:43:41.825: INFO: Scaling statefulset ss to 0
Mar  2 12:44:01.845: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 12:44:01.850: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 12:44:01.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7751" for this suite. 03/02/23 12:44:01.916
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":23,"skipped":372,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.285 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:43:31.646
    Mar  2 12:43:31.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename statefulset 03/02/23 12:43:31.65
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:43:31.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:43:31.671
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7751 03/02/23 12:43:31.687
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-7751 03/02/23 12:43:31.694
    Mar  2 12:43:31.710: INFO: Found 0 stateful pods, waiting for 1
    Mar  2 12:43:41.739: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/02/23 12:43:41.751
    STEP: updating a scale subresource 03/02/23 12:43:41.754
    STEP: verifying the statefulset Spec.Replicas was modified 03/02/23 12:43:41.758
    STEP: Patch a scale subresource 03/02/23 12:43:41.761
    STEP: verifying the statefulset Spec.Replicas was modified 03/02/23 12:43:41.78
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 12:43:41.822: INFO: Deleting all statefulset in ns statefulset-7751
    Mar  2 12:43:41.825: INFO: Scaling statefulset ss to 0
    Mar  2 12:44:01.845: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 12:44:01.850: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 12:44:01.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7751" for this suite. 03/02/23 12:44:01.916
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:44:01.936
Mar  2 12:44:01.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename replication-controller 03/02/23 12:44:01.938
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:01.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:01.955
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 03/02/23 12:44:01.958
STEP: When the matched label of one of its pods change 03/02/23 12:44:01.963
Mar  2 12:44:01.968: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  2 12:44:07.030: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/02/23 12:44:07.042
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  2 12:44:08.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3044" for this suite. 03/02/23 12:44:08.063
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":24,"skipped":373,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.186 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:44:01.936
    Mar  2 12:44:01.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename replication-controller 03/02/23 12:44:01.938
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:01.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:01.955
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 03/02/23 12:44:01.958
    STEP: When the matched label of one of its pods change 03/02/23 12:44:01.963
    Mar  2 12:44:01.968: INFO: Pod name pod-release: Found 0 pods out of 1
    Mar  2 12:44:07.030: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/02/23 12:44:07.042
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  2 12:44:08.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3044" for this suite. 03/02/23 12:44:08.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:44:08.129
Mar  2 12:44:08.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename replicaset 03/02/23 12:44:08.13
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:08.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:08.158
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/02/23 12:44:08.161
Mar  2 12:44:08.169: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 12:44:13.264: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 12:44:13.264
Mar  2 12:44:13.264: INFO: Waiting up to 5m0s for pod "test-rs-zw54r" in namespace "replicaset-7758" to be "running"
Mar  2 12:44:13.268: INFO: Pod "test-rs-zw54r": Phase="Pending", Reason="", readiness=false. Elapsed: 3.917364ms
Mar  2 12:44:15.276: INFO: Pod "test-rs-zw54r": Phase="Running", Reason="", readiness=true. Elapsed: 2.011533895s
Mar  2 12:44:15.276: INFO: Pod "test-rs-zw54r" satisfied condition "running"
STEP: getting scale subresource 03/02/23 12:44:15.276
STEP: updating a scale subresource 03/02/23 12:44:15.281
STEP: verifying the replicaset Spec.Replicas was modified 03/02/23 12:44:15.295
STEP: Patch a scale subresource 03/02/23 12:44:15.304
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 12:44:15.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7758" for this suite. 03/02/23 12:44:15.333
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":25,"skipped":424,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.219 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:44:08.129
    Mar  2 12:44:08.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename replicaset 03/02/23 12:44:08.13
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:08.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:08.158
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/02/23 12:44:08.161
    Mar  2 12:44:08.169: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  2 12:44:13.264: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 12:44:13.264
    Mar  2 12:44:13.264: INFO: Waiting up to 5m0s for pod "test-rs-zw54r" in namespace "replicaset-7758" to be "running"
    Mar  2 12:44:13.268: INFO: Pod "test-rs-zw54r": Phase="Pending", Reason="", readiness=false. Elapsed: 3.917364ms
    Mar  2 12:44:15.276: INFO: Pod "test-rs-zw54r": Phase="Running", Reason="", readiness=true. Elapsed: 2.011533895s
    Mar  2 12:44:15.276: INFO: Pod "test-rs-zw54r" satisfied condition "running"
    STEP: getting scale subresource 03/02/23 12:44:15.276
    STEP: updating a scale subresource 03/02/23 12:44:15.281
    STEP: verifying the replicaset Spec.Replicas was modified 03/02/23 12:44:15.295
    STEP: Patch a scale subresource 03/02/23 12:44:15.304
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 12:44:15.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7758" for this suite. 03/02/23 12:44:15.333
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:44:15.348
Mar  2 12:44:15.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 12:44:15.353
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:15.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:15.376
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 12:44:15.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1707" for this suite. 03/02/23 12:44:15.421
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":26,"skipped":426,"failed":0}
------------------------------
â€¢ [0.081 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:44:15.348
    Mar  2 12:44:15.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 12:44:15.353
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:15.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:15.376
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 12:44:15.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1707" for this suite. 03/02/23 12:44:15.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:44:15.431
Mar  2 12:44:15.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename replication-controller 03/02/23 12:44:15.44
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:15.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:15.488
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 03/02/23 12:44:15.496
Mar  2 12:44:15.531: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7123" to be "running and ready"
Mar  2 12:44:15.544: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 11.206969ms
Mar  2 12:44:15.544: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:44:17.558: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02461999s
Mar  2 12:44:17.558: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:44:19.551: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.017994323s
Mar  2 12:44:19.551: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar  2 12:44:19.551: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/02/23 12:44:19.555
STEP: Then the orphan pod is adopted 03/02/23 12:44:19.562
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  2 12:44:20.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7123" for this suite. 03/02/23 12:44:20.613
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":27,"skipped":445,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.193 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:44:15.431
    Mar  2 12:44:15.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename replication-controller 03/02/23 12:44:15.44
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:15.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:15.488
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/02/23 12:44:15.496
    Mar  2 12:44:15.531: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7123" to be "running and ready"
    Mar  2 12:44:15.544: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 11.206969ms
    Mar  2 12:44:15.544: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:44:17.558: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02461999s
    Mar  2 12:44:17.558: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:44:19.551: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.017994323s
    Mar  2 12:44:19.551: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar  2 12:44:19.551: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/02/23 12:44:19.555
    STEP: Then the orphan pod is adopted 03/02/23 12:44:19.562
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  2 12:44:20.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7123" for this suite. 03/02/23 12:44:20.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:44:20.625
Mar  2 12:44:20.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 12:44:20.627
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:20.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:20.657
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 03/02/23 12:44:20.661
Mar  2 12:44:20.670: INFO: Waiting up to 5m0s for pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba" in namespace "downward-api-4984" to be "Succeeded or Failed"
Mar  2 12:44:20.715: INFO: Pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba": Phase="Pending", Reason="", readiness=false. Elapsed: 45.053916ms
Mar  2 12:44:22.726: INFO: Pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.055344964s
Mar  2 12:44:24.759: INFO: Pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba": Phase="Running", Reason="", readiness=false. Elapsed: 4.088647744s
Mar  2 12:44:26.723: INFO: Pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052489727s
STEP: Saw pod success 03/02/23 12:44:26.723
Mar  2 12:44:26.724: INFO: Pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba" satisfied condition "Succeeded or Failed"
Mar  2 12:44:26.729: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba container dapi-container: <nil>
STEP: delete the pod 03/02/23 12:44:26.959
Mar  2 12:44:26.991: INFO: Waiting for pod downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba to disappear
Mar  2 12:44:26.994: INFO: Pod downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  2 12:44:26.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4984" for this suite. 03/02/23 12:44:27
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":28,"skipped":459,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.384 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:44:20.625
    Mar  2 12:44:20.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 12:44:20.627
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:20.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:20.657
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 03/02/23 12:44:20.661
    Mar  2 12:44:20.670: INFO: Waiting up to 5m0s for pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba" in namespace "downward-api-4984" to be "Succeeded or Failed"
    Mar  2 12:44:20.715: INFO: Pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba": Phase="Pending", Reason="", readiness=false. Elapsed: 45.053916ms
    Mar  2 12:44:22.726: INFO: Pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.055344964s
    Mar  2 12:44:24.759: INFO: Pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba": Phase="Running", Reason="", readiness=false. Elapsed: 4.088647744s
    Mar  2 12:44:26.723: INFO: Pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052489727s
    STEP: Saw pod success 03/02/23 12:44:26.723
    Mar  2 12:44:26.724: INFO: Pod "downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba" satisfied condition "Succeeded or Failed"
    Mar  2 12:44:26.729: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba container dapi-container: <nil>
    STEP: delete the pod 03/02/23 12:44:26.959
    Mar  2 12:44:26.991: INFO: Waiting for pod downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba to disappear
    Mar  2 12:44:26.994: INFO: Pod downward-api-0d8eb354-c85a-4c74-bfd5-abd7c48834ba no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  2 12:44:26.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4984" for this suite. 03/02/23 12:44:27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:44:27.012
Mar  2 12:44:27.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 12:44:27.013
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:27.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:27.041
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/02/23 12:44:27.048
Mar  2 12:44:27.067: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8173" to be "running and ready"
Mar  2 12:44:27.078: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.03811ms
Mar  2 12:44:27.078: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:44:29.091: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023513937s
Mar  2 12:44:29.091: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:44:31.083: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.015915015s
Mar  2 12:44:31.083: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  2 12:44:31.083: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 03/02/23 12:44:31.086
Mar  2 12:44:31.092: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-8173" to be "running and ready"
Mar  2 12:44:31.096: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.723924ms
Mar  2 12:44:31.096: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:44:33.106: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013993959s
Mar  2 12:44:33.106: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:44:35.105: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.012689087s
Mar  2 12:44:35.105: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar  2 12:44:35.106: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/02/23 12:44:35.114
STEP: delete the pod with lifecycle hook 03/02/23 12:44:35.135
Mar  2 12:44:35.152: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 12:44:35.165: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 12:44:37.166: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 12:44:37.173: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  2 12:44:37.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8173" for this suite. 03/02/23 12:44:37.185
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":29,"skipped":471,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.184 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:44:27.012
    Mar  2 12:44:27.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 12:44:27.013
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:27.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:27.041
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/02/23 12:44:27.048
    Mar  2 12:44:27.067: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8173" to be "running and ready"
    Mar  2 12:44:27.078: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.03811ms
    Mar  2 12:44:27.078: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:44:29.091: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023513937s
    Mar  2 12:44:29.091: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:44:31.083: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.015915015s
    Mar  2 12:44:31.083: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  2 12:44:31.083: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 03/02/23 12:44:31.086
    Mar  2 12:44:31.092: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-8173" to be "running and ready"
    Mar  2 12:44:31.096: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.723924ms
    Mar  2 12:44:31.096: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:44:33.106: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013993959s
    Mar  2 12:44:33.106: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:44:35.105: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.012689087s
    Mar  2 12:44:35.105: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar  2 12:44:35.106: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/02/23 12:44:35.114
    STEP: delete the pod with lifecycle hook 03/02/23 12:44:35.135
    Mar  2 12:44:35.152: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar  2 12:44:35.165: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar  2 12:44:37.166: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar  2 12:44:37.173: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  2 12:44:37.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8173" for this suite. 03/02/23 12:44:37.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:44:37.206
Mar  2 12:44:37.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename svcaccounts 03/02/23 12:44:37.207
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:37.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:37.235
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Mar  2 12:44:37.267: INFO: Waiting up to 5m0s for pod "pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2" in namespace "svcaccounts-8478" to be "running"
Mar  2 12:44:37.299: INFO: Pod "pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 31.85122ms
Mar  2 12:44:39.304: INFO: Pod "pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2": Phase="Running", Reason="", readiness=true. Elapsed: 2.036247596s
Mar  2 12:44:39.304: INFO: Pod "pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2" satisfied condition "running"
STEP: reading a file in the container 03/02/23 12:44:39.304
Mar  2 12:44:39.304: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8478 pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/02/23 12:44:39.53
Mar  2 12:44:39.530: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8478 pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/02/23 12:44:39.739
Mar  2 12:44:39.740: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8478 pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar  2 12:44:39.953: INFO: Got root ca configmap in namespace "svcaccounts-8478"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 12:44:39.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8478" for this suite. 03/02/23 12:44:39.965
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":30,"skipped":489,"failed":0}
------------------------------
â€¢ [2.764 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:44:37.206
    Mar  2 12:44:37.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 12:44:37.207
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:37.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:37.235
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Mar  2 12:44:37.267: INFO: Waiting up to 5m0s for pod "pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2" in namespace "svcaccounts-8478" to be "running"
    Mar  2 12:44:37.299: INFO: Pod "pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 31.85122ms
    Mar  2 12:44:39.304: INFO: Pod "pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2": Phase="Running", Reason="", readiness=true. Elapsed: 2.036247596s
    Mar  2 12:44:39.304: INFO: Pod "pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2" satisfied condition "running"
    STEP: reading a file in the container 03/02/23 12:44:39.304
    Mar  2 12:44:39.304: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8478 pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/02/23 12:44:39.53
    Mar  2 12:44:39.530: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8478 pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/02/23 12:44:39.739
    Mar  2 12:44:39.740: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8478 pod-service-account-0d998209-aa3a-4a59-af4a-f66aef197ca2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar  2 12:44:39.953: INFO: Got root ca configmap in namespace "svcaccounts-8478"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 12:44:39.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8478" for this suite. 03/02/23 12:44:39.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:44:39.975
Mar  2 12:44:39.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename statefulset 03/02/23 12:44:39.979
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:40.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:40.035
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-840 03/02/23 12:44:40.04
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 03/02/23 12:44:40.071
STEP: Creating stateful set ss in namespace statefulset-840 03/02/23 12:44:40.109
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-840 03/02/23 12:44:40.12
Mar  2 12:44:40.162: INFO: Found 0 stateful pods, waiting for 1
Mar  2 12:44:50.168: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/02/23 12:44:50.168
Mar  2 12:44:50.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 12:44:50.525: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 12:44:50.525: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 12:44:50.525: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 12:44:50.530: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 12:45:00.566: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 12:45:00.566: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 12:45:00.650: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999677s
Mar  2 12:45:01.751: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.876563132s
Mar  2 12:45:02.758: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.863390422s
Mar  2 12:45:03.763: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.856694622s
Mar  2 12:45:04.770: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.852830528s
Mar  2 12:45:05.826: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.844971095s
Mar  2 12:45:06.839: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.786076771s
Mar  2 12:45:07.843: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.77590615s
Mar  2 12:45:08.848: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.772052192s
Mar  2 12:45:09.853: INFO: Verifying statefulset ss doesn't scale past 1 for another 767.282643ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-840 03/02/23 12:45:10.854
Mar  2 12:45:10.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 12:45:11.112: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 12:45:11.112: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 12:45:11.112: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 12:45:11.116: INFO: Found 1 stateful pods, waiting for 3
Mar  2 12:45:21.126: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 12:45:21.126: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 12:45:21.126: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 12:45:31.124: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 12:45:31.124: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 12:45:31.124: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/02/23 12:45:31.124
STEP: Scale down will halt with unhealthy stateful pod 03/02/23 12:45:31.125
Mar  2 12:45:31.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 12:45:31.337: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 12:45:31.337: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 12:45:31.337: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 12:45:31.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 12:45:31.643: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 12:45:31.643: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 12:45:31.643: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 12:45:31.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 12:45:31.866: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 12:45:31.866: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 12:45:31.866: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 12:45:31.866: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 12:45:31.870: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  2 12:45:41.893: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 12:45:41.893: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 12:45:41.893: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 12:45:41.917: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999764s
Mar  2 12:45:42.925: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988650232s
Mar  2 12:45:43.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980740822s
Mar  2 12:45:44.936: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975891458s
Mar  2 12:45:45.941: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969944724s
Mar  2 12:45:46.949: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963328579s
Mar  2 12:45:47.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.955503386s
Mar  2 12:45:48.967: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.945607972s
Mar  2 12:45:49.976: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.938385181s
Mar  2 12:45:50.983: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.581856ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-840 03/02/23 12:45:51.983
Mar  2 12:45:51.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 12:45:52.219: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 12:45:52.219: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 12:45:52.219: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 12:45:52.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 12:45:52.449: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 12:45:52.449: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 12:45:52.449: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 12:45:52.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 12:45:52.685: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 12:45:52.685: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 12:45:52.685: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 12:45:52.685: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/02/23 12:46:02.743
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 12:46:02.744: INFO: Deleting all statefulset in ns statefulset-840
Mar  2 12:46:02.830: INFO: Scaling statefulset ss to 0
Mar  2 12:46:02.904: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 12:46:02.921: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 12:46:02.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-840" for this suite. 03/02/23 12:46:02.984
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":31,"skipped":514,"failed":0}
------------------------------
â€¢ [SLOW TEST] [83.053 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:44:39.975
    Mar  2 12:44:39.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename statefulset 03/02/23 12:44:39.979
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:44:40.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:44:40.035
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-840 03/02/23 12:44:40.04
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/02/23 12:44:40.071
    STEP: Creating stateful set ss in namespace statefulset-840 03/02/23 12:44:40.109
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-840 03/02/23 12:44:40.12
    Mar  2 12:44:40.162: INFO: Found 0 stateful pods, waiting for 1
    Mar  2 12:44:50.168: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/02/23 12:44:50.168
    Mar  2 12:44:50.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 12:44:50.525: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 12:44:50.525: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 12:44:50.525: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 12:44:50.530: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar  2 12:45:00.566: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 12:45:00.566: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 12:45:00.650: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999677s
    Mar  2 12:45:01.751: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.876563132s
    Mar  2 12:45:02.758: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.863390422s
    Mar  2 12:45:03.763: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.856694622s
    Mar  2 12:45:04.770: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.852830528s
    Mar  2 12:45:05.826: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.844971095s
    Mar  2 12:45:06.839: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.786076771s
    Mar  2 12:45:07.843: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.77590615s
    Mar  2 12:45:08.848: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.772052192s
    Mar  2 12:45:09.853: INFO: Verifying statefulset ss doesn't scale past 1 for another 767.282643ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-840 03/02/23 12:45:10.854
    Mar  2 12:45:10.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 12:45:11.112: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 12:45:11.112: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 12:45:11.112: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 12:45:11.116: INFO: Found 1 stateful pods, waiting for 3
    Mar  2 12:45:21.126: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 12:45:21.126: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 12:45:21.126: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
    Mar  2 12:45:31.124: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 12:45:31.124: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 12:45:31.124: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/02/23 12:45:31.124
    STEP: Scale down will halt with unhealthy stateful pod 03/02/23 12:45:31.125
    Mar  2 12:45:31.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 12:45:31.337: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 12:45:31.337: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 12:45:31.337: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 12:45:31.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 12:45:31.643: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 12:45:31.643: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 12:45:31.643: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 12:45:31.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 12:45:31.866: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 12:45:31.866: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 12:45:31.866: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 12:45:31.866: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 12:45:31.870: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Mar  2 12:45:41.893: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 12:45:41.893: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 12:45:41.893: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 12:45:41.917: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999764s
    Mar  2 12:45:42.925: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988650232s
    Mar  2 12:45:43.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980740822s
    Mar  2 12:45:44.936: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975891458s
    Mar  2 12:45:45.941: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969944724s
    Mar  2 12:45:46.949: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963328579s
    Mar  2 12:45:47.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.955503386s
    Mar  2 12:45:48.967: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.945607972s
    Mar  2 12:45:49.976: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.938385181s
    Mar  2 12:45:50.983: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.581856ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-840 03/02/23 12:45:51.983
    Mar  2 12:45:51.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 12:45:52.219: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 12:45:52.219: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 12:45:52.219: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 12:45:52.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 12:45:52.449: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 12:45:52.449: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 12:45:52.449: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 12:45:52.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-840 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 12:45:52.685: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 12:45:52.685: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 12:45:52.685: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 12:45:52.685: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/02/23 12:46:02.743
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 12:46:02.744: INFO: Deleting all statefulset in ns statefulset-840
    Mar  2 12:46:02.830: INFO: Scaling statefulset ss to 0
    Mar  2 12:46:02.904: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 12:46:02.921: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 12:46:02.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-840" for this suite. 03/02/23 12:46:02.984
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:46:03.036
Mar  2 12:46:03.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 12:46:03.039
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:46:03.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:46:03.079
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 03/02/23 12:46:03.106
Mar  2 12:46:03.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: rename a version 03/02/23 12:46:20.435
STEP: check the new version name is served 03/02/23 12:46:20.477
STEP: check the old version name is removed 03/02/23 12:46:25.297
STEP: check the other version is not changed 03/02/23 12:46:27.614
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 12:46:37.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-105" for this suite. 03/02/23 12:46:37.274
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":32,"skipped":514,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.250 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:46:03.036
    Mar  2 12:46:03.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 12:46:03.039
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:46:03.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:46:03.079
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 03/02/23 12:46:03.106
    Mar  2 12:46:03.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: rename a version 03/02/23 12:46:20.435
    STEP: check the new version name is served 03/02/23 12:46:20.477
    STEP: check the old version name is removed 03/02/23 12:46:25.297
    STEP: check the other version is not changed 03/02/23 12:46:27.614
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 12:46:37.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-105" for this suite. 03/02/23 12:46:37.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:46:37.295
Mar  2 12:46:37.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 12:46:37.303
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:46:37.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:46:37.341
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 03/02/23 12:46:37.345
Mar  2 12:46:37.356: INFO: Waiting up to 5m0s for pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3" in namespace "emptydir-61" to be "Succeeded or Failed"
Mar  2 12:46:37.360: INFO: Pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047699ms
Mar  2 12:46:39.368: INFO: Pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011443909s
Mar  2 12:46:41.366: INFO: Pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009545869s
Mar  2 12:46:43.370: INFO: Pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013408767s
STEP: Saw pod success 03/02/23 12:46:43.37
Mar  2 12:46:43.371: INFO: Pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3" satisfied condition "Succeeded or Failed"
Mar  2 12:46:43.386: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3 container test-container: <nil>
STEP: delete the pod 03/02/23 12:46:43.402
Mar  2 12:46:43.428: INFO: Waiting for pod pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3 to disappear
Mar  2 12:46:43.434: INFO: Pod pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 12:46:43.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-61" for this suite. 03/02/23 12:46:43.439
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":33,"skipped":520,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.154 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:46:37.295
    Mar  2 12:46:37.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 12:46:37.303
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:46:37.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:46:37.341
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/02/23 12:46:37.345
    Mar  2 12:46:37.356: INFO: Waiting up to 5m0s for pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3" in namespace "emptydir-61" to be "Succeeded or Failed"
    Mar  2 12:46:37.360: INFO: Pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047699ms
    Mar  2 12:46:39.368: INFO: Pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011443909s
    Mar  2 12:46:41.366: INFO: Pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009545869s
    Mar  2 12:46:43.370: INFO: Pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013408767s
    STEP: Saw pod success 03/02/23 12:46:43.37
    Mar  2 12:46:43.371: INFO: Pod "pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3" satisfied condition "Succeeded or Failed"
    Mar  2 12:46:43.386: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3 container test-container: <nil>
    STEP: delete the pod 03/02/23 12:46:43.402
    Mar  2 12:46:43.428: INFO: Waiting for pod pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3 to disappear
    Mar  2 12:46:43.434: INFO: Pod pod-14cedb9a-d3c0-449b-9570-5ac96739e8c3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 12:46:43.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-61" for this suite. 03/02/23 12:46:43.439
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:46:43.45
Mar  2 12:46:43.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 12:46:43.452
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:46:43.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:46:43.527
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 03/02/23 12:46:43.53
Mar  2 12:46:43.538: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb" in namespace "emptydir-5881" to be "running"
Mar  2 12:46:43.546: INFO: Pod "pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.352518ms
Mar  2 12:46:45.553: INFO: Pod "pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013993173s
Mar  2 12:46:47.552: INFO: Pod "pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb": Phase="Running", Reason="", readiness=false. Elapsed: 4.013178234s
Mar  2 12:46:47.552: INFO: Pod "pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/02/23 12:46:47.552
Mar  2 12:46:47.553: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5881 PodName:pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 12:46:47.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 12:46:47.554: INFO: ExecWithOptions: Clientset creation
Mar  2 12:46:47.554: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-5881/pods/pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar  2 12:46:47.641: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 12:46:47.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5881" for this suite. 03/02/23 12:46:47.646
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":34,"skipped":522,"failed":0}
------------------------------
â€¢ [4.202 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:46:43.45
    Mar  2 12:46:43.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 12:46:43.452
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:46:43.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:46:43.527
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 03/02/23 12:46:43.53
    Mar  2 12:46:43.538: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb" in namespace "emptydir-5881" to be "running"
    Mar  2 12:46:43.546: INFO: Pod "pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.352518ms
    Mar  2 12:46:45.553: INFO: Pod "pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013993173s
    Mar  2 12:46:47.552: INFO: Pod "pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb": Phase="Running", Reason="", readiness=false. Elapsed: 4.013178234s
    Mar  2 12:46:47.552: INFO: Pod "pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/02/23 12:46:47.552
    Mar  2 12:46:47.553: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5881 PodName:pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 12:46:47.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 12:46:47.554: INFO: ExecWithOptions: Clientset creation
    Mar  2 12:46:47.554: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-5881/pods/pod-sharedvolume-690f4237-ba54-4c2c-8525-052b704f55bb/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar  2 12:46:47.641: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 12:46:47.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5881" for this suite. 03/02/23 12:46:47.646
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:46:47.653
Mar  2 12:46:47.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 12:46:47.655
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:46:47.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:46:47.673
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 12:46:47.697
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:46:48.588
STEP: Deploying the webhook pod 03/02/23 12:46:48.596
STEP: Wait for the deployment to be ready 03/02/23 12:46:48.623
Mar  2 12:46:48.639: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 12:46:50.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 46, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 46, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 46, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 46, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 12:46:52.661
STEP: Verifying the service has paired with the endpoint 03/02/23 12:46:52.707
Mar  2 12:46:53.709: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Mar  2 12:46:53.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-325-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 12:46:59.229
STEP: Creating a custom resource that should be mutated by the webhook 03/02/23 12:46:59.247
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 12:47:01.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4417" for this suite. 03/02/23 12:47:01.862
STEP: Destroying namespace "webhook-4417-markers" for this suite. 03/02/23 12:47:01.871
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":35,"skipped":523,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.611 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:46:47.653
    Mar  2 12:46:47.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 12:46:47.655
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:46:47.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:46:47.673
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 12:46:47.697
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:46:48.588
    STEP: Deploying the webhook pod 03/02/23 12:46:48.596
    STEP: Wait for the deployment to be ready 03/02/23 12:46:48.623
    Mar  2 12:46:48.639: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 12:46:50.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 46, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 46, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 46, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 46, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 12:46:52.661
    STEP: Verifying the service has paired with the endpoint 03/02/23 12:46:52.707
    Mar  2 12:46:53.709: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Mar  2 12:46:53.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-325-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 12:46:59.229
    STEP: Creating a custom resource that should be mutated by the webhook 03/02/23 12:46:59.247
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 12:47:01.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4417" for this suite. 03/02/23 12:47:01.862
    STEP: Destroying namespace "webhook-4417-markers" for this suite. 03/02/23 12:47:01.871
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:47:02.355
Mar  2 12:47:02.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 12:47:02.361
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:02.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:02.549
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/02/23 12:47:02.57
Mar  2 12:47:02.639: INFO: Waiting up to 5m0s for pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4" in namespace "emptydir-4142" to be "Succeeded or Failed"
Mar  2 12:47:02.722: INFO: Pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 82.815399ms
Mar  2 12:47:04.758: INFO: Pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.11924657s
Mar  2 12:47:06.738: INFO: Pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09858362s
Mar  2 12:47:08.731: INFO: Pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.091500939s
STEP: Saw pod success 03/02/23 12:47:08.731
Mar  2 12:47:08.731: INFO: Pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4" satisfied condition "Succeeded or Failed"
Mar  2 12:47:08.735: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4 container test-container: <nil>
STEP: delete the pod 03/02/23 12:47:08.746
Mar  2 12:47:08.761: INFO: Waiting for pod pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4 to disappear
Mar  2 12:47:08.829: INFO: Pod pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 12:47:08.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4142" for this suite. 03/02/23 12:47:08.834
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":36,"skipped":550,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.484 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:47:02.355
    Mar  2 12:47:02.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 12:47:02.361
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:02.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:02.549
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/02/23 12:47:02.57
    Mar  2 12:47:02.639: INFO: Waiting up to 5m0s for pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4" in namespace "emptydir-4142" to be "Succeeded or Failed"
    Mar  2 12:47:02.722: INFO: Pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 82.815399ms
    Mar  2 12:47:04.758: INFO: Pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.11924657s
    Mar  2 12:47:06.738: INFO: Pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09858362s
    Mar  2 12:47:08.731: INFO: Pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.091500939s
    STEP: Saw pod success 03/02/23 12:47:08.731
    Mar  2 12:47:08.731: INFO: Pod "pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4" satisfied condition "Succeeded or Failed"
    Mar  2 12:47:08.735: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4 container test-container: <nil>
    STEP: delete the pod 03/02/23 12:47:08.746
    Mar  2 12:47:08.761: INFO: Waiting for pod pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4 to disappear
    Mar  2 12:47:08.829: INFO: Pod pod-ec7c9606-6770-4a3e-a902-53ee4a73a9a4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 12:47:08.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4142" for this suite. 03/02/23 12:47:08.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:47:08.842
Mar  2 12:47:08.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 12:47:08.844
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:08.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:08.864
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Mar  2 12:47:08.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 12:47:20.571
Mar  2 12:47:20.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9051 --namespace=crd-publish-openapi-9051 create -f -'
Mar  2 12:47:21.698: INFO: stderr: ""
Mar  2 12:47:21.698: INFO: stdout: "e2e-test-crd-publish-openapi-5913-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 12:47:21.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9051 --namespace=crd-publish-openapi-9051 delete e2e-test-crd-publish-openapi-5913-crds test-cr'
Mar  2 12:47:21.804: INFO: stderr: ""
Mar  2 12:47:21.804: INFO: stdout: "e2e-test-crd-publish-openapi-5913-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  2 12:47:21.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9051 --namespace=crd-publish-openapi-9051 apply -f -'
Mar  2 12:47:22.197: INFO: stderr: ""
Mar  2 12:47:22.197: INFO: stdout: "e2e-test-crd-publish-openapi-5913-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 12:47:22.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9051 --namespace=crd-publish-openapi-9051 delete e2e-test-crd-publish-openapi-5913-crds test-cr'
Mar  2 12:47:22.337: INFO: stderr: ""
Mar  2 12:47:22.337: INFO: stdout: "e2e-test-crd-publish-openapi-5913-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/02/23 12:47:22.337
Mar  2 12:47:22.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9051 explain e2e-test-crd-publish-openapi-5913-crds'
Mar  2 12:47:22.721: INFO: stderr: ""
Mar  2 12:47:22.721: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5913-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 12:47:28.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9051" for this suite. 03/02/23 12:47:28.445
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":37,"skipped":559,"failed":0}
------------------------------
â€¢ [SLOW TEST] [19.610 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:47:08.842
    Mar  2 12:47:08.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 12:47:08.844
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:08.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:08.864
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Mar  2 12:47:08.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 12:47:20.571
    Mar  2 12:47:20.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9051 --namespace=crd-publish-openapi-9051 create -f -'
    Mar  2 12:47:21.698: INFO: stderr: ""
    Mar  2 12:47:21.698: INFO: stdout: "e2e-test-crd-publish-openapi-5913-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar  2 12:47:21.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9051 --namespace=crd-publish-openapi-9051 delete e2e-test-crd-publish-openapi-5913-crds test-cr'
    Mar  2 12:47:21.804: INFO: stderr: ""
    Mar  2 12:47:21.804: INFO: stdout: "e2e-test-crd-publish-openapi-5913-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar  2 12:47:21.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9051 --namespace=crd-publish-openapi-9051 apply -f -'
    Mar  2 12:47:22.197: INFO: stderr: ""
    Mar  2 12:47:22.197: INFO: stdout: "e2e-test-crd-publish-openapi-5913-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar  2 12:47:22.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9051 --namespace=crd-publish-openapi-9051 delete e2e-test-crd-publish-openapi-5913-crds test-cr'
    Mar  2 12:47:22.337: INFO: stderr: ""
    Mar  2 12:47:22.337: INFO: stdout: "e2e-test-crd-publish-openapi-5913-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/02/23 12:47:22.337
    Mar  2 12:47:22.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9051 explain e2e-test-crd-publish-openapi-5913-crds'
    Mar  2 12:47:22.721: INFO: stderr: ""
    Mar  2 12:47:22.721: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5913-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 12:47:28.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9051" for this suite. 03/02/23 12:47:28.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:47:28.459
Mar  2 12:47:28.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 12:47:28.461
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:28.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:28.484
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-b1ba124d-b1a6-4329-b2d7-dbf3183f405d 03/02/23 12:47:28.494
STEP: Creating a pod to test consume secrets 03/02/23 12:47:28.505
Mar  2 12:47:28.518: INFO: Waiting up to 5m0s for pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d" in namespace "secrets-3886" to be "Succeeded or Failed"
Mar  2 12:47:28.529: INFO: Pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.187761ms
Mar  2 12:47:30.534: INFO: Pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016051095s
Mar  2 12:47:32.533: INFO: Pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d": Phase="Running", Reason="", readiness=false. Elapsed: 4.014664701s
Mar  2 12:47:34.540: INFO: Pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021969806s
STEP: Saw pod success 03/02/23 12:47:34.54
Mar  2 12:47:34.541: INFO: Pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d" satisfied condition "Succeeded or Failed"
Mar  2 12:47:34.551: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d container secret-env-test: <nil>
STEP: delete the pod 03/02/23 12:47:34.557
Mar  2 12:47:34.586: INFO: Waiting for pod pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d to disappear
Mar  2 12:47:34.610: INFO: Pod pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  2 12:47:34.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3886" for this suite. 03/02/23 12:47:34.659
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":38,"skipped":566,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.204 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:47:28.459
    Mar  2 12:47:28.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 12:47:28.461
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:28.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:28.484
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-b1ba124d-b1a6-4329-b2d7-dbf3183f405d 03/02/23 12:47:28.494
    STEP: Creating a pod to test consume secrets 03/02/23 12:47:28.505
    Mar  2 12:47:28.518: INFO: Waiting up to 5m0s for pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d" in namespace "secrets-3886" to be "Succeeded or Failed"
    Mar  2 12:47:28.529: INFO: Pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.187761ms
    Mar  2 12:47:30.534: INFO: Pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016051095s
    Mar  2 12:47:32.533: INFO: Pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d": Phase="Running", Reason="", readiness=false. Elapsed: 4.014664701s
    Mar  2 12:47:34.540: INFO: Pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021969806s
    STEP: Saw pod success 03/02/23 12:47:34.54
    Mar  2 12:47:34.541: INFO: Pod "pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d" satisfied condition "Succeeded or Failed"
    Mar  2 12:47:34.551: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d container secret-env-test: <nil>
    STEP: delete the pod 03/02/23 12:47:34.557
    Mar  2 12:47:34.586: INFO: Waiting for pod pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d to disappear
    Mar  2 12:47:34.610: INFO: Pod pod-secrets-35a2e8aa-0b76-4882-af7b-8afe08fb612d no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 12:47:34.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3886" for this suite. 03/02/23 12:47:34.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:47:34.738
Mar  2 12:47:34.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 12:47:34.74
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:34.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:34.76
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 03/02/23 12:47:34.764
Mar  2 12:47:34.781: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5" in namespace "downward-api-7923" to be "Succeeded or Failed"
Mar  2 12:47:34.909: INFO: Pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 128.154886ms
Mar  2 12:47:36.915: INFO: Pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133542996s
Mar  2 12:47:38.926: INFO: Pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.145413653s
Mar  2 12:47:40.915: INFO: Pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.134345134s
STEP: Saw pod success 03/02/23 12:47:40.915
Mar  2 12:47:40.916: INFO: Pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5" satisfied condition "Succeeded or Failed"
Mar  2 12:47:40.919: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5 container client-container: <nil>
STEP: delete the pod 03/02/23 12:47:40.931
Mar  2 12:47:40.952: INFO: Waiting for pod downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5 to disappear
Mar  2 12:47:40.956: INFO: Pod downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 12:47:40.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7923" for this suite. 03/02/23 12:47:40.967
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":39,"skipped":596,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.240 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:47:34.738
    Mar  2 12:47:34.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 12:47:34.74
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:34.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:34.76
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 03/02/23 12:47:34.764
    Mar  2 12:47:34.781: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5" in namespace "downward-api-7923" to be "Succeeded or Failed"
    Mar  2 12:47:34.909: INFO: Pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 128.154886ms
    Mar  2 12:47:36.915: INFO: Pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133542996s
    Mar  2 12:47:38.926: INFO: Pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.145413653s
    Mar  2 12:47:40.915: INFO: Pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.134345134s
    STEP: Saw pod success 03/02/23 12:47:40.915
    Mar  2 12:47:40.916: INFO: Pod "downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5" satisfied condition "Succeeded or Failed"
    Mar  2 12:47:40.919: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5 container client-container: <nil>
    STEP: delete the pod 03/02/23 12:47:40.931
    Mar  2 12:47:40.952: INFO: Waiting for pod downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5 to disappear
    Mar  2 12:47:40.956: INFO: Pod downwardapi-volume-5ff34574-757b-41b7-9552-ee64a4e14bb5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 12:47:40.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7923" for this suite. 03/02/23 12:47:40.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:47:40.98
Mar  2 12:47:40.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 12:47:40.981
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:41.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:41.009
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 03/02/23 12:47:41.05
Mar  2 12:47:41.069: INFO: Waiting up to 5m0s for pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac" in namespace "downward-api-9143" to be "Succeeded or Failed"
Mar  2 12:47:41.096: INFO: Pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac": Phase="Pending", Reason="", readiness=false. Elapsed: 26.896288ms
Mar  2 12:47:43.112: INFO: Pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042145992s
Mar  2 12:47:45.109: INFO: Pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039847994s
Mar  2 12:47:47.106: INFO: Pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036426178s
STEP: Saw pod success 03/02/23 12:47:47.107
Mar  2 12:47:47.108: INFO: Pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac" satisfied condition "Succeeded or Failed"
Mar  2 12:47:47.114: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac container dapi-container: <nil>
STEP: delete the pod 03/02/23 12:47:47.123
Mar  2 12:47:47.144: INFO: Waiting for pod downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac to disappear
Mar  2 12:47:47.170: INFO: Pod downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  2 12:47:47.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9143" for this suite. 03/02/23 12:47:47.202
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":40,"skipped":602,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.238 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:47:40.98
    Mar  2 12:47:40.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 12:47:40.981
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:41.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:41.009
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 03/02/23 12:47:41.05
    Mar  2 12:47:41.069: INFO: Waiting up to 5m0s for pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac" in namespace "downward-api-9143" to be "Succeeded or Failed"
    Mar  2 12:47:41.096: INFO: Pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac": Phase="Pending", Reason="", readiness=false. Elapsed: 26.896288ms
    Mar  2 12:47:43.112: INFO: Pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042145992s
    Mar  2 12:47:45.109: INFO: Pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039847994s
    Mar  2 12:47:47.106: INFO: Pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036426178s
    STEP: Saw pod success 03/02/23 12:47:47.107
    Mar  2 12:47:47.108: INFO: Pod "downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac" satisfied condition "Succeeded or Failed"
    Mar  2 12:47:47.114: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac container dapi-container: <nil>
    STEP: delete the pod 03/02/23 12:47:47.123
    Mar  2 12:47:47.144: INFO: Waiting for pod downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac to disappear
    Mar  2 12:47:47.170: INFO: Pod downward-api-7cd6ae91-d351-4f13-96b0-a6c227728aac no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  2 12:47:47.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9143" for this suite. 03/02/23 12:47:47.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:47:47.23
Mar  2 12:47:47.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 12:47:47.231
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:47.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:47.253
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 03/02/23 12:47:47.258
Mar  2 12:47:47.266: INFO: Waiting up to 5m0s for pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc" in namespace "emptydir-7159" to be "Succeeded or Failed"
Mar  2 12:47:47.466: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 198.406735ms
Mar  2 12:47:49.538: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.270860262s
Mar  2 12:47:51.519: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc": Phase="Running", Reason="", readiness=true. Elapsed: 4.251465269s
Mar  2 12:47:53.469: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc": Phase="Running", Reason="", readiness=false. Elapsed: 6.201350039s
Mar  2 12:47:55.472: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.20417493s
STEP: Saw pod success 03/02/23 12:47:55.524
Mar  2 12:47:55.524: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc" satisfied condition "Succeeded or Failed"
Mar  2 12:47:55.528: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc container test-container: <nil>
STEP: delete the pod 03/02/23 12:47:55.534
Mar  2 12:47:55.549: INFO: Waiting for pod pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc to disappear
Mar  2 12:47:55.554: INFO: Pod pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 12:47:55.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7159" for this suite. 03/02/23 12:47:55.564
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":41,"skipped":639,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.340 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:47:47.23
    Mar  2 12:47:47.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 12:47:47.231
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:47.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:47.253
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/02/23 12:47:47.258
    Mar  2 12:47:47.266: INFO: Waiting up to 5m0s for pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc" in namespace "emptydir-7159" to be "Succeeded or Failed"
    Mar  2 12:47:47.466: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 198.406735ms
    Mar  2 12:47:49.538: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.270860262s
    Mar  2 12:47:51.519: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc": Phase="Running", Reason="", readiness=true. Elapsed: 4.251465269s
    Mar  2 12:47:53.469: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc": Phase="Running", Reason="", readiness=false. Elapsed: 6.201350039s
    Mar  2 12:47:55.472: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.20417493s
    STEP: Saw pod success 03/02/23 12:47:55.524
    Mar  2 12:47:55.524: INFO: Pod "pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc" satisfied condition "Succeeded or Failed"
    Mar  2 12:47:55.528: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc container test-container: <nil>
    STEP: delete the pod 03/02/23 12:47:55.534
    Mar  2 12:47:55.549: INFO: Waiting for pod pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc to disappear
    Mar  2 12:47:55.554: INFO: Pod pod-23109c8d-57fe-4dd7-92ac-8f2e8ddffbfc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 12:47:55.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7159" for this suite. 03/02/23 12:47:55.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:47:55.626
Mar  2 12:47:55.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename endpointslice 03/02/23 12:47:55.634
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:55.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:55.656
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 03/02/23 12:48:00.86
STEP: referencing matching pods with named port 03/02/23 12:48:05.901
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/02/23 12:48:10.914
STEP: recreating EndpointSlices after they've been deleted 03/02/23 12:48:15.931
Mar  2 12:48:15.961: INFO: EndpointSlice for Service endpointslice-8403/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  2 12:48:26.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8403" for this suite. 03/02/23 12:48:26.018
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":42,"skipped":649,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.400 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:47:55.626
    Mar  2 12:47:55.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename endpointslice 03/02/23 12:47:55.634
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:47:55.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:47:55.656
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 03/02/23 12:48:00.86
    STEP: referencing matching pods with named port 03/02/23 12:48:05.901
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/02/23 12:48:10.914
    STEP: recreating EndpointSlices after they've been deleted 03/02/23 12:48:15.931
    Mar  2 12:48:15.961: INFO: EndpointSlice for Service endpointslice-8403/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  2 12:48:26.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8403" for this suite. 03/02/23 12:48:26.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:48:26.037
Mar  2 12:48:26.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-probe 03/02/23 12:48:26.046
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:48:26.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:48:26.067
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-25adf043-a523-4a23-9c15-c5405a4dbe16 in namespace container-probe-2429 03/02/23 12:48:26.07
Mar  2 12:48:26.081: INFO: Waiting up to 5m0s for pod "busybox-25adf043-a523-4a23-9c15-c5405a4dbe16" in namespace "container-probe-2429" to be "not pending"
Mar  2 12:48:26.086: INFO: Pod "busybox-25adf043-a523-4a23-9c15-c5405a4dbe16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.937038ms
Mar  2 12:48:28.138: INFO: Pod "busybox-25adf043-a523-4a23-9c15-c5405a4dbe16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056994636s
Mar  2 12:48:30.129: INFO: Pod "busybox-25adf043-a523-4a23-9c15-c5405a4dbe16": Phase="Running", Reason="", readiness=true. Elapsed: 4.047680092s
Mar  2 12:48:30.129: INFO: Pod "busybox-25adf043-a523-4a23-9c15-c5405a4dbe16" satisfied condition "not pending"
Mar  2 12:48:30.129: INFO: Started pod busybox-25adf043-a523-4a23-9c15-c5405a4dbe16 in namespace container-probe-2429
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 12:48:30.129
Mar  2 12:48:30.134: INFO: Initial restart count of pod busybox-25adf043-a523-4a23-9c15-c5405a4dbe16 is 0
Mar  2 12:49:18.346: INFO: Restart count of pod container-probe-2429/busybox-25adf043-a523-4a23-9c15-c5405a4dbe16 is now 1 (48.211685563s elapsed)
STEP: deleting the pod 03/02/23 12:49:18.346
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 12:49:18.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2429" for this suite. 03/02/23 12:49:18.428
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":43,"skipped":659,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.413 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:48:26.037
    Mar  2 12:48:26.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-probe 03/02/23 12:48:26.046
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:48:26.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:48:26.067
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-25adf043-a523-4a23-9c15-c5405a4dbe16 in namespace container-probe-2429 03/02/23 12:48:26.07
    Mar  2 12:48:26.081: INFO: Waiting up to 5m0s for pod "busybox-25adf043-a523-4a23-9c15-c5405a4dbe16" in namespace "container-probe-2429" to be "not pending"
    Mar  2 12:48:26.086: INFO: Pod "busybox-25adf043-a523-4a23-9c15-c5405a4dbe16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.937038ms
    Mar  2 12:48:28.138: INFO: Pod "busybox-25adf043-a523-4a23-9c15-c5405a4dbe16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056994636s
    Mar  2 12:48:30.129: INFO: Pod "busybox-25adf043-a523-4a23-9c15-c5405a4dbe16": Phase="Running", Reason="", readiness=true. Elapsed: 4.047680092s
    Mar  2 12:48:30.129: INFO: Pod "busybox-25adf043-a523-4a23-9c15-c5405a4dbe16" satisfied condition "not pending"
    Mar  2 12:48:30.129: INFO: Started pod busybox-25adf043-a523-4a23-9c15-c5405a4dbe16 in namespace container-probe-2429
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 12:48:30.129
    Mar  2 12:48:30.134: INFO: Initial restart count of pod busybox-25adf043-a523-4a23-9c15-c5405a4dbe16 is 0
    Mar  2 12:49:18.346: INFO: Restart count of pod container-probe-2429/busybox-25adf043-a523-4a23-9c15-c5405a4dbe16 is now 1 (48.211685563s elapsed)
    STEP: deleting the pod 03/02/23 12:49:18.346
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 12:49:18.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2429" for this suite. 03/02/23 12:49:18.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:49:18.479
Mar  2 12:49:18.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename namespaces 03/02/23 12:49:18.48
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:18.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:49:18.547
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 03/02/23 12:49:18.552
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:18.631
STEP: Creating a service in the namespace 03/02/23 12:49:18.642
STEP: Deleting the namespace 03/02/23 12:49:18.653
STEP: Waiting for the namespace to be removed. 03/02/23 12:49:18.668
STEP: Recreating the namespace 03/02/23 12:49:24.672
STEP: Verifying there is no service in the namespace 03/02/23 12:49:24.731
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  2 12:49:24.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6365" for this suite. 03/02/23 12:49:24.74
STEP: Destroying namespace "nsdeletetest-5977" for this suite. 03/02/23 12:49:24.745
Mar  2 12:49:24.752: INFO: Namespace nsdeletetest-5977 was already deleted
STEP: Destroying namespace "nsdeletetest-783" for this suite. 03/02/23 12:49:24.753
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":44,"skipped":689,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.284 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:49:18.479
    Mar  2 12:49:18.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename namespaces 03/02/23 12:49:18.48
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:18.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:49:18.547
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 03/02/23 12:49:18.552
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:18.631
    STEP: Creating a service in the namespace 03/02/23 12:49:18.642
    STEP: Deleting the namespace 03/02/23 12:49:18.653
    STEP: Waiting for the namespace to be removed. 03/02/23 12:49:18.668
    STEP: Recreating the namespace 03/02/23 12:49:24.672
    STEP: Verifying there is no service in the namespace 03/02/23 12:49:24.731
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 12:49:24.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6365" for this suite. 03/02/23 12:49:24.74
    STEP: Destroying namespace "nsdeletetest-5977" for this suite. 03/02/23 12:49:24.745
    Mar  2 12:49:24.752: INFO: Namespace nsdeletetest-5977 was already deleted
    STEP: Destroying namespace "nsdeletetest-783" for this suite. 03/02/23 12:49:24.753
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:49:24.767
Mar  2 12:49:24.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 12:49:24.77
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:24.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:49:24.823
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-3759fc48-1b8e-4a4b-af39-caa431252bbf 03/02/23 12:49:24.828
STEP: Creating a pod to test consume secrets 03/02/23 12:49:24.835
Mar  2 12:49:24.844: INFO: Waiting up to 5m0s for pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092" in namespace "secrets-8044" to be "Succeeded or Failed"
Mar  2 12:49:24.859: INFO: Pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092": Phase="Pending", Reason="", readiness=false. Elapsed: 15.121727ms
Mar  2 12:49:26.866: INFO: Pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022757932s
Mar  2 12:49:28.863: INFO: Pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019850703s
Mar  2 12:49:30.862: INFO: Pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018387326s
STEP: Saw pod success 03/02/23 12:49:30.862
Mar  2 12:49:30.863: INFO: Pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092" satisfied condition "Succeeded or Failed"
Mar  2 12:49:30.866: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 12:49:30.899
Mar  2 12:49:30.941: INFO: Waiting for pod pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092 to disappear
Mar  2 12:49:30.944: INFO: Pod pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 12:49:30.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8044" for this suite. 03/02/23 12:49:30.952
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":45,"skipped":692,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.191 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:49:24.767
    Mar  2 12:49:24.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 12:49:24.77
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:24.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:49:24.823
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-3759fc48-1b8e-4a4b-af39-caa431252bbf 03/02/23 12:49:24.828
    STEP: Creating a pod to test consume secrets 03/02/23 12:49:24.835
    Mar  2 12:49:24.844: INFO: Waiting up to 5m0s for pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092" in namespace "secrets-8044" to be "Succeeded or Failed"
    Mar  2 12:49:24.859: INFO: Pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092": Phase="Pending", Reason="", readiness=false. Elapsed: 15.121727ms
    Mar  2 12:49:26.866: INFO: Pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022757932s
    Mar  2 12:49:28.863: INFO: Pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019850703s
    Mar  2 12:49:30.862: INFO: Pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018387326s
    STEP: Saw pod success 03/02/23 12:49:30.862
    Mar  2 12:49:30.863: INFO: Pod "pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092" satisfied condition "Succeeded or Failed"
    Mar  2 12:49:30.866: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 12:49:30.899
    Mar  2 12:49:30.941: INFO: Waiting for pod pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092 to disappear
    Mar  2 12:49:30.944: INFO: Pod pod-secrets-db192e55-5179-45ed-93cf-3ca6b4727092 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 12:49:30.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8044" for this suite. 03/02/23 12:49:30.952
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:49:30.96
Mar  2 12:49:30.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pods 03/02/23 12:49:30.962
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:30.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:49:30.995
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 03/02/23 12:49:31.002
STEP: submitting the pod to kubernetes 03/02/23 12:49:31.002
Mar  2 12:49:31.018: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482" in namespace "pods-7205" to be "running and ready"
Mar  2 12:49:31.024: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Pending", Reason="", readiness=false. Elapsed: 6.503742ms
Mar  2 12:49:31.024: INFO: The phase of Pod pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:49:33.054: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Running", Reason="", readiness=true. Elapsed: 2.036086752s
Mar  2 12:49:33.054: INFO: The phase of Pod pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482 is Running (Ready = true)
Mar  2 12:49:33.054: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/02/23 12:49:33.057
STEP: updating the pod 03/02/23 12:49:33.06
Mar  2 12:49:33.631: INFO: Successfully updated pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482"
Mar  2 12:49:33.631: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482" in namespace "pods-7205" to be "terminated with reason DeadlineExceeded"
Mar  2 12:49:33.637: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Running", Reason="", readiness=true. Elapsed: 5.548811ms
Mar  2 12:49:35.642: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Running", Reason="", readiness=true. Elapsed: 2.011445989s
Mar  2 12:49:37.643: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Running", Reason="", readiness=false. Elapsed: 4.012238065s
Mar  2 12:49:39.643: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Running", Reason="", readiness=false. Elapsed: 6.012414721s
Mar  2 12:49:41.642: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 8.010859569s
Mar  2 12:49:41.642: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 12:49:41.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7205" for this suite. 03/02/23 12:49:41.648
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":46,"skipped":694,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.694 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:49:30.96
    Mar  2 12:49:30.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pods 03/02/23 12:49:30.962
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:30.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:49:30.995
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 03/02/23 12:49:31.002
    STEP: submitting the pod to kubernetes 03/02/23 12:49:31.002
    Mar  2 12:49:31.018: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482" in namespace "pods-7205" to be "running and ready"
    Mar  2 12:49:31.024: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Pending", Reason="", readiness=false. Elapsed: 6.503742ms
    Mar  2 12:49:31.024: INFO: The phase of Pod pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:49:33.054: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Running", Reason="", readiness=true. Elapsed: 2.036086752s
    Mar  2 12:49:33.054: INFO: The phase of Pod pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482 is Running (Ready = true)
    Mar  2 12:49:33.054: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/02/23 12:49:33.057
    STEP: updating the pod 03/02/23 12:49:33.06
    Mar  2 12:49:33.631: INFO: Successfully updated pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482"
    Mar  2 12:49:33.631: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482" in namespace "pods-7205" to be "terminated with reason DeadlineExceeded"
    Mar  2 12:49:33.637: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Running", Reason="", readiness=true. Elapsed: 5.548811ms
    Mar  2 12:49:35.642: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Running", Reason="", readiness=true. Elapsed: 2.011445989s
    Mar  2 12:49:37.643: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Running", Reason="", readiness=false. Elapsed: 4.012238065s
    Mar  2 12:49:39.643: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Running", Reason="", readiness=false. Elapsed: 6.012414721s
    Mar  2 12:49:41.642: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 8.010859569s
    Mar  2 12:49:41.642: INFO: Pod "pod-update-activedeadlineseconds-58a0c34a-de50-46b0-8272-75173d2ae482" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 12:49:41.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7205" for this suite. 03/02/23 12:49:41.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:49:41.659
Mar  2 12:49:41.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 12:49:41.66
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:41.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:49:41.691
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 12:49:41.695
Mar  2 12:49:41.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6150 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Mar  2 12:49:41.829: INFO: stderr: ""
Mar  2 12:49:41.830: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/02/23 12:49:41.832
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Mar  2 12:49:41.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6150 delete pods e2e-test-httpd-pod'
Mar  2 12:49:44.637: INFO: stderr: ""
Mar  2 12:49:44.638: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 12:49:44.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6150" for this suite. 03/02/23 12:49:44.647
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":47,"skipped":715,"failed":0}
------------------------------
â€¢ [2.993 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:49:41.659
    Mar  2 12:49:41.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 12:49:41.66
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:41.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:49:41.691
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 12:49:41.695
    Mar  2 12:49:41.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6150 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Mar  2 12:49:41.829: INFO: stderr: ""
    Mar  2 12:49:41.830: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/02/23 12:49:41.832
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Mar  2 12:49:41.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6150 delete pods e2e-test-httpd-pod'
    Mar  2 12:49:44.637: INFO: stderr: ""
    Mar  2 12:49:44.638: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 12:49:44.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6150" for this suite. 03/02/23 12:49:44.647
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:49:44.652
Mar  2 12:49:44.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename cronjob 03/02/23 12:49:44.654
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:44.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:49:44.734
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/02/23 12:49:44.737
STEP: Ensuring a job is scheduled 03/02/23 12:49:44.741
STEP: Ensuring exactly one is scheduled 03/02/23 12:50:00.744
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/02/23 12:50:00.746
STEP: Ensuring the job is replaced with a new one 03/02/23 12:50:00.748
STEP: Removing cronjob 03/02/23 12:51:00.759
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  2 12:51:00.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9804" for this suite. 03/02/23 12:51:00.784
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":48,"skipped":716,"failed":0}
------------------------------
â€¢ [SLOW TEST] [76.181 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:49:44.652
    Mar  2 12:49:44.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename cronjob 03/02/23 12:49:44.654
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:49:44.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:49:44.734
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/02/23 12:49:44.737
    STEP: Ensuring a job is scheduled 03/02/23 12:49:44.741
    STEP: Ensuring exactly one is scheduled 03/02/23 12:50:00.744
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/02/23 12:50:00.746
    STEP: Ensuring the job is replaced with a new one 03/02/23 12:50:00.748
    STEP: Removing cronjob 03/02/23 12:51:00.759
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  2 12:51:00.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9804" for this suite. 03/02/23 12:51:00.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:51:00.835
Mar  2 12:51:00.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 12:51:00.837
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:51:00.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:51:00.935
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3036 03/02/23 12:51:00.938
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/02/23 12:51:01.017
STEP: creating service externalsvc in namespace services-3036 03/02/23 12:51:01.017
STEP: creating replication controller externalsvc in namespace services-3036 03/02/23 12:51:01.058
I0302 12:51:01.117950      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3036, replica count: 2
I0302 12:51:04.171316      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/02/23 12:51:04.178
Mar  2 12:51:04.208: INFO: Creating new exec pod
Mar  2 12:51:04.213: INFO: Waiting up to 5m0s for pod "execpodt8z56" in namespace "services-3036" to be "running"
Mar  2 12:51:04.219: INFO: Pod "execpodt8z56": Phase="Pending", Reason="", readiness=false. Elapsed: 5.969179ms
Mar  2 12:51:06.230: INFO: Pod "execpodt8z56": Phase="Running", Reason="", readiness=true. Elapsed: 2.017251173s
Mar  2 12:51:06.230: INFO: Pod "execpodt8z56" satisfied condition "running"
Mar  2 12:51:06.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-3036 exec execpodt8z56 -- /bin/sh -x -c nslookup nodeport-service.services-3036.svc.cluster.local'
Mar  2 12:51:06.505: INFO: stderr: "+ nslookup nodeport-service.services-3036.svc.cluster.local\n"
Mar  2 12:51:06.506: INFO: stdout: "Server:\t\t10.233.0.3\nAddress:\t10.233.0.3#53\n\nnodeport-service.services-3036.svc.cluster.local\tcanonical name = externalsvc.services-3036.svc.cluster.local.\nName:\texternalsvc.services-3036.svc.cluster.local\nAddress: 10.233.10.101\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3036, will wait for the garbage collector to delete the pods 03/02/23 12:51:06.506
Mar  2 12:51:06.576: INFO: Deleting ReplicationController externalsvc took: 13.657965ms
Mar  2 12:51:06.689: INFO: Terminating ReplicationController externalsvc pods took: 112.873772ms
Mar  2 12:51:10.438: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 12:51:10.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3036" for this suite. 03/02/23 12:51:10.479
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":49,"skipped":728,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.651 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:51:00.835
    Mar  2 12:51:00.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 12:51:00.837
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:51:00.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:51:00.935
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3036 03/02/23 12:51:00.938
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/02/23 12:51:01.017
    STEP: creating service externalsvc in namespace services-3036 03/02/23 12:51:01.017
    STEP: creating replication controller externalsvc in namespace services-3036 03/02/23 12:51:01.058
    I0302 12:51:01.117950      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3036, replica count: 2
    I0302 12:51:04.171316      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/02/23 12:51:04.178
    Mar  2 12:51:04.208: INFO: Creating new exec pod
    Mar  2 12:51:04.213: INFO: Waiting up to 5m0s for pod "execpodt8z56" in namespace "services-3036" to be "running"
    Mar  2 12:51:04.219: INFO: Pod "execpodt8z56": Phase="Pending", Reason="", readiness=false. Elapsed: 5.969179ms
    Mar  2 12:51:06.230: INFO: Pod "execpodt8z56": Phase="Running", Reason="", readiness=true. Elapsed: 2.017251173s
    Mar  2 12:51:06.230: INFO: Pod "execpodt8z56" satisfied condition "running"
    Mar  2 12:51:06.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-3036 exec execpodt8z56 -- /bin/sh -x -c nslookup nodeport-service.services-3036.svc.cluster.local'
    Mar  2 12:51:06.505: INFO: stderr: "+ nslookup nodeport-service.services-3036.svc.cluster.local\n"
    Mar  2 12:51:06.506: INFO: stdout: "Server:\t\t10.233.0.3\nAddress:\t10.233.0.3#53\n\nnodeport-service.services-3036.svc.cluster.local\tcanonical name = externalsvc.services-3036.svc.cluster.local.\nName:\texternalsvc.services-3036.svc.cluster.local\nAddress: 10.233.10.101\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3036, will wait for the garbage collector to delete the pods 03/02/23 12:51:06.506
    Mar  2 12:51:06.576: INFO: Deleting ReplicationController externalsvc took: 13.657965ms
    Mar  2 12:51:06.689: INFO: Terminating ReplicationController externalsvc pods took: 112.873772ms
    Mar  2 12:51:10.438: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 12:51:10.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3036" for this suite. 03/02/23 12:51:10.479
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:51:10.488
Mar  2 12:51:10.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 12:51:10.489
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:51:10.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:51:10.516
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 12:51:10.532
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:51:10.995
STEP: Deploying the webhook pod 03/02/23 12:51:11.024
STEP: Wait for the deployment to be ready 03/02/23 12:51:11.052
Mar  2 12:51:11.075: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/02/23 12:51:13.124
STEP: Verifying the service has paired with the endpoint 03/02/23 12:51:13.133
Mar  2 12:51:14.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/02/23 12:51:14.139
STEP: create a configmap that should be updated by the webhook 03/02/23 12:51:14.164
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 12:51:14.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7138" for this suite. 03/02/23 12:51:14.242
STEP: Destroying namespace "webhook-7138-markers" for this suite. 03/02/23 12:51:14.249
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":50,"skipped":743,"failed":0}
------------------------------
â€¢ [3.845 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:51:10.488
    Mar  2 12:51:10.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 12:51:10.489
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:51:10.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:51:10.516
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 12:51:10.532
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:51:10.995
    STEP: Deploying the webhook pod 03/02/23 12:51:11.024
    STEP: Wait for the deployment to be ready 03/02/23 12:51:11.052
    Mar  2 12:51:11.075: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/02/23 12:51:13.124
    STEP: Verifying the service has paired with the endpoint 03/02/23 12:51:13.133
    Mar  2 12:51:14.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/02/23 12:51:14.139
    STEP: create a configmap that should be updated by the webhook 03/02/23 12:51:14.164
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 12:51:14.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7138" for this suite. 03/02/23 12:51:14.242
    STEP: Destroying namespace "webhook-7138-markers" for this suite. 03/02/23 12:51:14.249
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:51:14.335
Mar  2 12:51:14.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename subpath 03/02/23 12:51:14.336
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:51:14.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:51:14.363
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/02/23 12:51:14.366
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-dxs9 03/02/23 12:51:14.41
STEP: Creating a pod to test atomic-volume-subpath 03/02/23 12:51:14.416
Mar  2 12:51:14.434: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dxs9" in namespace "subpath-3776" to be "Succeeded or Failed"
Mar  2 12:51:14.443: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.848716ms
Mar  2 12:51:16.450: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016328538s
Mar  2 12:51:18.452: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 4.018069641s
Mar  2 12:51:20.452: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 6.017975572s
Mar  2 12:51:22.449: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 8.015035776s
Mar  2 12:51:24.450: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 10.016155394s
Mar  2 12:51:26.455: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 12.020876077s
Mar  2 12:51:28.447: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 14.013494778s
Mar  2 12:51:30.448: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 16.014597043s
Mar  2 12:51:32.449: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 18.015349475s
Mar  2 12:51:34.448: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 20.014030903s
Mar  2 12:51:36.445: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 22.011419539s
Mar  2 12:51:38.448: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=false. Elapsed: 24.013775582s
Mar  2 12:51:40.452: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.017645888s
STEP: Saw pod success 03/02/23 12:51:40.452
Mar  2 12:51:40.453: INFO: Pod "pod-subpath-test-downwardapi-dxs9" satisfied condition "Succeeded or Failed"
Mar  2 12:51:40.467: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-subpath-test-downwardapi-dxs9 container test-container-subpath-downwardapi-dxs9: <nil>
STEP: delete the pod 03/02/23 12:51:40.478
Mar  2 12:51:40.512: INFO: Waiting for pod pod-subpath-test-downwardapi-dxs9 to disappear
Mar  2 12:51:40.519: INFO: Pod pod-subpath-test-downwardapi-dxs9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-dxs9 03/02/23 12:51:40.523
Mar  2 12:51:40.523: INFO: Deleting pod "pod-subpath-test-downwardapi-dxs9" in namespace "subpath-3776"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  2 12:51:40.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3776" for this suite. 03/02/23 12:51:40.535
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":51,"skipped":761,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.206 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:51:14.335
    Mar  2 12:51:14.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename subpath 03/02/23 12:51:14.336
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:51:14.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:51:14.363
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/02/23 12:51:14.366
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-dxs9 03/02/23 12:51:14.41
    STEP: Creating a pod to test atomic-volume-subpath 03/02/23 12:51:14.416
    Mar  2 12:51:14.434: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dxs9" in namespace "subpath-3776" to be "Succeeded or Failed"
    Mar  2 12:51:14.443: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.848716ms
    Mar  2 12:51:16.450: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016328538s
    Mar  2 12:51:18.452: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 4.018069641s
    Mar  2 12:51:20.452: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 6.017975572s
    Mar  2 12:51:22.449: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 8.015035776s
    Mar  2 12:51:24.450: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 10.016155394s
    Mar  2 12:51:26.455: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 12.020876077s
    Mar  2 12:51:28.447: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 14.013494778s
    Mar  2 12:51:30.448: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 16.014597043s
    Mar  2 12:51:32.449: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 18.015349475s
    Mar  2 12:51:34.448: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 20.014030903s
    Mar  2 12:51:36.445: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=true. Elapsed: 22.011419539s
    Mar  2 12:51:38.448: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Running", Reason="", readiness=false. Elapsed: 24.013775582s
    Mar  2 12:51:40.452: INFO: Pod "pod-subpath-test-downwardapi-dxs9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.017645888s
    STEP: Saw pod success 03/02/23 12:51:40.452
    Mar  2 12:51:40.453: INFO: Pod "pod-subpath-test-downwardapi-dxs9" satisfied condition "Succeeded or Failed"
    Mar  2 12:51:40.467: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-subpath-test-downwardapi-dxs9 container test-container-subpath-downwardapi-dxs9: <nil>
    STEP: delete the pod 03/02/23 12:51:40.478
    Mar  2 12:51:40.512: INFO: Waiting for pod pod-subpath-test-downwardapi-dxs9 to disappear
    Mar  2 12:51:40.519: INFO: Pod pod-subpath-test-downwardapi-dxs9 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-dxs9 03/02/23 12:51:40.523
    Mar  2 12:51:40.523: INFO: Deleting pod "pod-subpath-test-downwardapi-dxs9" in namespace "subpath-3776"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  2 12:51:40.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3776" for this suite. 03/02/23 12:51:40.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:51:40.56
Mar  2 12:51:40.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename statefulset 03/02/23 12:51:40.562
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:51:40.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:51:40.592
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4150 03/02/23 12:51:40.596
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 03/02/23 12:51:40.602
Mar  2 12:51:40.617: INFO: Found 0 stateful pods, waiting for 3
Mar  2 12:51:50.630: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 12:51:50.631: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 12:51:50.631: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/02/23 12:51:50.641
Mar  2 12:51:50.660: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/02/23 12:51:50.66
STEP: Not applying an update when the partition is greater than the number of replicas 03/02/23 12:52:00.683
STEP: Performing a canary update 03/02/23 12:52:00.684
Mar  2 12:52:00.705: INFO: Updating stateful set ss2
Mar  2 12:52:00.718: INFO: Waiting for Pod statefulset-4150/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 03/02/23 12:52:10.732
Mar  2 12:52:10.801: INFO: Found 2 stateful pods, waiting for 3
Mar  2 12:52:20.816: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 12:52:20.816: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 12:52:20.817: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/02/23 12:52:20.827
Mar  2 12:52:20.847: INFO: Updating stateful set ss2
Mar  2 12:52:20.863: INFO: Waiting for Pod statefulset-4150/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar  2 12:52:30.928: INFO: Updating stateful set ss2
Mar  2 12:52:30.968: INFO: Waiting for StatefulSet statefulset-4150/ss2 to complete update
Mar  2 12:52:30.969: INFO: Waiting for Pod statefulset-4150/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 12:52:40.994: INFO: Deleting all statefulset in ns statefulset-4150
Mar  2 12:52:40.998: INFO: Scaling statefulset ss2 to 0
Mar  2 12:52:51.029: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 12:52:51.034: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 12:52:51.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4150" for this suite. 03/02/23 12:52:51.063
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":52,"skipped":793,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.526 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:51:40.56
    Mar  2 12:51:40.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename statefulset 03/02/23 12:51:40.562
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:51:40.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:51:40.592
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4150 03/02/23 12:51:40.596
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 03/02/23 12:51:40.602
    Mar  2 12:51:40.617: INFO: Found 0 stateful pods, waiting for 3
    Mar  2 12:51:50.630: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 12:51:50.631: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 12:51:50.631: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/02/23 12:51:50.641
    Mar  2 12:51:50.660: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/02/23 12:51:50.66
    STEP: Not applying an update when the partition is greater than the number of replicas 03/02/23 12:52:00.683
    STEP: Performing a canary update 03/02/23 12:52:00.684
    Mar  2 12:52:00.705: INFO: Updating stateful set ss2
    Mar  2 12:52:00.718: INFO: Waiting for Pod statefulset-4150/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 03/02/23 12:52:10.732
    Mar  2 12:52:10.801: INFO: Found 2 stateful pods, waiting for 3
    Mar  2 12:52:20.816: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 12:52:20.816: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 12:52:20.817: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/02/23 12:52:20.827
    Mar  2 12:52:20.847: INFO: Updating stateful set ss2
    Mar  2 12:52:20.863: INFO: Waiting for Pod statefulset-4150/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar  2 12:52:30.928: INFO: Updating stateful set ss2
    Mar  2 12:52:30.968: INFO: Waiting for StatefulSet statefulset-4150/ss2 to complete update
    Mar  2 12:52:30.969: INFO: Waiting for Pod statefulset-4150/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 12:52:40.994: INFO: Deleting all statefulset in ns statefulset-4150
    Mar  2 12:52:40.998: INFO: Scaling statefulset ss2 to 0
    Mar  2 12:52:51.029: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 12:52:51.034: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 12:52:51.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4150" for this suite. 03/02/23 12:52:51.063
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:52:51.088
Mar  2 12:52:51.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename job 03/02/23 12:52:51.093
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:52:51.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:52:51.157
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 03/02/23 12:52:51.16
STEP: Ensuring active pods == parallelism 03/02/23 12:52:51.166
STEP: Orphaning one of the Job's Pods 03/02/23 12:52:55.176
Mar  2 12:52:55.765: INFO: Successfully updated pod "adopt-release-gmh9l"
STEP: Checking that the Job readopts the Pod 03/02/23 12:52:55.765
Mar  2 12:52:55.765: INFO: Waiting up to 15m0s for pod "adopt-release-gmh9l" in namespace "job-8317" to be "adopted"
Mar  2 12:52:55.769: INFO: Pod "adopt-release-gmh9l": Phase="Running", Reason="", readiness=true. Elapsed: 3.988228ms
Mar  2 12:52:57.824: INFO: Pod "adopt-release-gmh9l": Phase="Running", Reason="", readiness=true. Elapsed: 2.059041016s
Mar  2 12:52:57.827: INFO: Pod "adopt-release-gmh9l" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/02/23 12:52:57.827
Mar  2 12:52:58.340: INFO: Successfully updated pod "adopt-release-gmh9l"
STEP: Checking that the Job releases the Pod 03/02/23 12:52:58.34
Mar  2 12:52:58.340: INFO: Waiting up to 15m0s for pod "adopt-release-gmh9l" in namespace "job-8317" to be "released"
Mar  2 12:52:58.351: INFO: Pod "adopt-release-gmh9l": Phase="Running", Reason="", readiness=true. Elapsed: 10.611734ms
Mar  2 12:53:00.358: INFO: Pod "adopt-release-gmh9l": Phase="Running", Reason="", readiness=true. Elapsed: 2.017690458s
Mar  2 12:53:00.358: INFO: Pod "adopt-release-gmh9l" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 12:53:00.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8317" for this suite. 03/02/23 12:53:00.382
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":53,"skipped":797,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.304 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:52:51.088
    Mar  2 12:52:51.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename job 03/02/23 12:52:51.093
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:52:51.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:52:51.157
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 03/02/23 12:52:51.16
    STEP: Ensuring active pods == parallelism 03/02/23 12:52:51.166
    STEP: Orphaning one of the Job's Pods 03/02/23 12:52:55.176
    Mar  2 12:52:55.765: INFO: Successfully updated pod "adopt-release-gmh9l"
    STEP: Checking that the Job readopts the Pod 03/02/23 12:52:55.765
    Mar  2 12:52:55.765: INFO: Waiting up to 15m0s for pod "adopt-release-gmh9l" in namespace "job-8317" to be "adopted"
    Mar  2 12:52:55.769: INFO: Pod "adopt-release-gmh9l": Phase="Running", Reason="", readiness=true. Elapsed: 3.988228ms
    Mar  2 12:52:57.824: INFO: Pod "adopt-release-gmh9l": Phase="Running", Reason="", readiness=true. Elapsed: 2.059041016s
    Mar  2 12:52:57.827: INFO: Pod "adopt-release-gmh9l" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/02/23 12:52:57.827
    Mar  2 12:52:58.340: INFO: Successfully updated pod "adopt-release-gmh9l"
    STEP: Checking that the Job releases the Pod 03/02/23 12:52:58.34
    Mar  2 12:52:58.340: INFO: Waiting up to 15m0s for pod "adopt-release-gmh9l" in namespace "job-8317" to be "released"
    Mar  2 12:52:58.351: INFO: Pod "adopt-release-gmh9l": Phase="Running", Reason="", readiness=true. Elapsed: 10.611734ms
    Mar  2 12:53:00.358: INFO: Pod "adopt-release-gmh9l": Phase="Running", Reason="", readiness=true. Elapsed: 2.017690458s
    Mar  2 12:53:00.358: INFO: Pod "adopt-release-gmh9l" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 12:53:00.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8317" for this suite. 03/02/23 12:53:00.382
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:53:00.393
Mar  2 12:53:00.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 12:53:00.394
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:00.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:00.414
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/02/23 12:53:00.444
Mar  2 12:53:00.471: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9739" to be "running and ready"
Mar  2 12:53:00.474: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229365ms
Mar  2 12:53:00.474: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:53:02.485: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013463055s
Mar  2 12:53:02.485: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:53:04.493: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021274321s
Mar  2 12:53:04.493: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:53:06.483: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 6.010988176s
Mar  2 12:53:06.483: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  2 12:53:06.483: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 03/02/23 12:53:06.49
Mar  2 12:53:06.496: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9739" to be "running and ready"
Mar  2 12:53:06.504: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.179448ms
Mar  2 12:53:06.504: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:53:08.511: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014743283s
Mar  2 12:53:08.511: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar  2 12:53:08.511: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/02/23 12:53:08.515
STEP: delete the pod with lifecycle hook 03/02/23 12:53:08.532
Mar  2 12:53:08.539: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 12:53:08.542: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 12:53:10.543: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 12:53:10.547: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 12:53:12.543: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 12:53:12.546: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  2 12:53:12.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9739" for this suite. 03/02/23 12:53:12.552
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":54,"skipped":799,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.165 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:53:00.393
    Mar  2 12:53:00.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 12:53:00.394
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:00.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:00.414
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/02/23 12:53:00.444
    Mar  2 12:53:00.471: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9739" to be "running and ready"
    Mar  2 12:53:00.474: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229365ms
    Mar  2 12:53:00.474: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:53:02.485: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013463055s
    Mar  2 12:53:02.485: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:53:04.493: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021274321s
    Mar  2 12:53:04.493: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:53:06.483: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 6.010988176s
    Mar  2 12:53:06.483: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  2 12:53:06.483: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 03/02/23 12:53:06.49
    Mar  2 12:53:06.496: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9739" to be "running and ready"
    Mar  2 12:53:06.504: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.179448ms
    Mar  2 12:53:06.504: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:53:08.511: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014743283s
    Mar  2 12:53:08.511: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar  2 12:53:08.511: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/02/23 12:53:08.515
    STEP: delete the pod with lifecycle hook 03/02/23 12:53:08.532
    Mar  2 12:53:08.539: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  2 12:53:08.542: INFO: Pod pod-with-poststart-http-hook still exists
    Mar  2 12:53:10.543: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  2 12:53:10.547: INFO: Pod pod-with-poststart-http-hook still exists
    Mar  2 12:53:12.543: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  2 12:53:12.546: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  2 12:53:12.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9739" for this suite. 03/02/23 12:53:12.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:53:12.561
Mar  2 12:53:12.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename resourcequota 03/02/23 12:53:12.563
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:12.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:12.63
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 03/02/23 12:53:12.635
STEP: Ensuring ResourceQuota status is calculated 03/02/23 12:53:12.639
STEP: Creating a ResourceQuota with not terminating scope 03/02/23 12:53:14.649
STEP: Ensuring ResourceQuota status is calculated 03/02/23 12:53:14.666
STEP: Creating a long running pod 03/02/23 12:53:16.676
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/02/23 12:53:16.703
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/02/23 12:53:18.714
STEP: Deleting the pod 03/02/23 12:53:20.719
STEP: Ensuring resource quota status released the pod usage 03/02/23 12:53:20.733
STEP: Creating a terminating pod 03/02/23 12:53:22.738
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/02/23 12:53:22.753
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/02/23 12:53:24.76
STEP: Deleting the pod 03/02/23 12:53:26.767
STEP: Ensuring resource quota status released the pod usage 03/02/23 12:53:26.826
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 12:53:28.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7440" for this suite. 03/02/23 12:53:28.837
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":55,"skipped":810,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.284 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:53:12.561
    Mar  2 12:53:12.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename resourcequota 03/02/23 12:53:12.563
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:12.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:12.63
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 03/02/23 12:53:12.635
    STEP: Ensuring ResourceQuota status is calculated 03/02/23 12:53:12.639
    STEP: Creating a ResourceQuota with not terminating scope 03/02/23 12:53:14.649
    STEP: Ensuring ResourceQuota status is calculated 03/02/23 12:53:14.666
    STEP: Creating a long running pod 03/02/23 12:53:16.676
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/02/23 12:53:16.703
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/02/23 12:53:18.714
    STEP: Deleting the pod 03/02/23 12:53:20.719
    STEP: Ensuring resource quota status released the pod usage 03/02/23 12:53:20.733
    STEP: Creating a terminating pod 03/02/23 12:53:22.738
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/02/23 12:53:22.753
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/02/23 12:53:24.76
    STEP: Deleting the pod 03/02/23 12:53:26.767
    STEP: Ensuring resource quota status released the pod usage 03/02/23 12:53:26.826
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 12:53:28.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7440" for this suite. 03/02/23 12:53:28.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:53:28.853
Mar  2 12:53:28.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 12:53:28.855
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:28.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:28.887
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-d9de42ad-e743-4cb3-9378-7d009c4f0fc1 03/02/23 12:53:28.891
STEP: Creating a pod to test consume configMaps 03/02/23 12:53:28.903
Mar  2 12:53:28.919: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688" in namespace "projected-8570" to be "Succeeded or Failed"
Mar  2 12:53:28.928: INFO: Pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688": Phase="Pending", Reason="", readiness=false. Elapsed: 9.092327ms
Mar  2 12:53:30.958: INFO: Pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038890404s
Mar  2 12:53:32.934: INFO: Pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688": Phase="Running", Reason="", readiness=false. Elapsed: 4.014878096s
Mar  2 12:53:34.933: INFO: Pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014353094s
STEP: Saw pod success 03/02/23 12:53:34.934
Mar  2 12:53:34.934: INFO: Pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688" satisfied condition "Succeeded or Failed"
Mar  2 12:53:34.938: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 12:53:34.945
Mar  2 12:53:34.957: INFO: Waiting for pod pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688 to disappear
Mar  2 12:53:34.961: INFO: Pod pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 12:53:34.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8570" for this suite. 03/02/23 12:53:34.966
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":56,"skipped":830,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.122 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:53:28.853
    Mar  2 12:53:28.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 12:53:28.855
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:28.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:28.887
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-d9de42ad-e743-4cb3-9378-7d009c4f0fc1 03/02/23 12:53:28.891
    STEP: Creating a pod to test consume configMaps 03/02/23 12:53:28.903
    Mar  2 12:53:28.919: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688" in namespace "projected-8570" to be "Succeeded or Failed"
    Mar  2 12:53:28.928: INFO: Pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688": Phase="Pending", Reason="", readiness=false. Elapsed: 9.092327ms
    Mar  2 12:53:30.958: INFO: Pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038890404s
    Mar  2 12:53:32.934: INFO: Pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688": Phase="Running", Reason="", readiness=false. Elapsed: 4.014878096s
    Mar  2 12:53:34.933: INFO: Pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014353094s
    STEP: Saw pod success 03/02/23 12:53:34.934
    Mar  2 12:53:34.934: INFO: Pod "pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688" satisfied condition "Succeeded or Failed"
    Mar  2 12:53:34.938: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 12:53:34.945
    Mar  2 12:53:34.957: INFO: Waiting for pod pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688 to disappear
    Mar  2 12:53:34.961: INFO: Pod pod-projected-configmaps-1139e07f-17ba-4860-ac95-cd6b084e8688 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 12:53:34.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8570" for this suite. 03/02/23 12:53:34.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:53:34.98
Mar  2 12:53:34.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename replicaset 03/02/23 12:53:34.996
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:35.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:35.063
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/02/23 12:53:35.067
STEP: Verify that the required pods have come up 03/02/23 12:53:35.071
Mar  2 12:53:35.164: INFO: Pod name sample-pod: Found 1 pods out of 3
Mar  2 12:53:40.189: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/02/23 12:53:40.189
Mar  2 12:53:40.220: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/02/23 12:53:40.221
STEP: DeleteCollection of the ReplicaSets 03/02/23 12:53:40.237
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/02/23 12:53:40.247
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 12:53:40.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5921" for this suite. 03/02/23 12:53:40.275
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":57,"skipped":845,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.310 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:53:34.98
    Mar  2 12:53:34.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename replicaset 03/02/23 12:53:34.996
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:35.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:35.063
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/02/23 12:53:35.067
    STEP: Verify that the required pods have come up 03/02/23 12:53:35.071
    Mar  2 12:53:35.164: INFO: Pod name sample-pod: Found 1 pods out of 3
    Mar  2 12:53:40.189: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/02/23 12:53:40.189
    Mar  2 12:53:40.220: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/02/23 12:53:40.221
    STEP: DeleteCollection of the ReplicaSets 03/02/23 12:53:40.237
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/02/23 12:53:40.247
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 12:53:40.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5921" for this suite. 03/02/23 12:53:40.275
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:53:40.379
Mar  2 12:53:40.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 12:53:40.382
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:40.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:40.439
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 12:53:40.469
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:53:42.646
STEP: Deploying the webhook pod 03/02/23 12:53:42.663
STEP: Wait for the deployment to be ready 03/02/23 12:53:42.751
Mar  2 12:53:42.763: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 12:53:44.788: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 12:53:46.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 12:53:48.802
STEP: Verifying the service has paired with the endpoint 03/02/23 12:53:48.835
Mar  2 12:53:49.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Mar  2 12:53:49.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7723-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 12:53:55.355
STEP: Creating a custom resource that should be mutated by the webhook 03/02/23 12:53:55.374
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 12:53:57.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3272" for this suite. 03/02/23 12:53:57.969
STEP: Destroying namespace "webhook-3272-markers" for this suite. 03/02/23 12:53:57.981
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":58,"skipped":848,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.673 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:53:40.379
    Mar  2 12:53:40.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 12:53:40.382
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:40.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:40.439
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 12:53:40.469
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:53:42.646
    STEP: Deploying the webhook pod 03/02/23 12:53:42.663
    STEP: Wait for the deployment to be ready 03/02/23 12:53:42.751
    Mar  2 12:53:42.763: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 12:53:44.788: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 12:53:46.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 12, 53, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 12:53:48.802
    STEP: Verifying the service has paired with the endpoint 03/02/23 12:53:48.835
    Mar  2 12:53:49.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Mar  2 12:53:49.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7723-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 12:53:55.355
    STEP: Creating a custom resource that should be mutated by the webhook 03/02/23 12:53:55.374
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 12:53:57.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3272" for this suite. 03/02/23 12:53:57.969
    STEP: Destroying namespace "webhook-3272-markers" for this suite. 03/02/23 12:53:57.981
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:53:58.063
Mar  2 12:53:58.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 12:53:58.064
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:58.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:58.149
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/02/23 12:53:58.163
Mar  2 12:53:58.223: INFO: Waiting up to 5m0s for pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19" in namespace "emptydir-7232" to be "Succeeded or Failed"
Mar  2 12:53:58.240: INFO: Pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19": Phase="Pending", Reason="", readiness=false. Elapsed: 16.689573ms
Mar  2 12:54:00.243: INFO: Pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020392932s
Mar  2 12:54:02.244: INFO: Pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021482609s
Mar  2 12:54:04.244: INFO: Pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021015907s
STEP: Saw pod success 03/02/23 12:54:04.244
Mar  2 12:54:04.244: INFO: Pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19" satisfied condition "Succeeded or Failed"
Mar  2 12:54:04.248: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-e22b95ed-b68c-4cbd-83b7-57a487352d19 container test-container: <nil>
STEP: delete the pod 03/02/23 12:54:04.258
Mar  2 12:54:04.272: INFO: Waiting for pod pod-e22b95ed-b68c-4cbd-83b7-57a487352d19 to disappear
Mar  2 12:54:04.332: INFO: Pod pod-e22b95ed-b68c-4cbd-83b7-57a487352d19 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 12:54:04.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7232" for this suite. 03/02/23 12:54:04.337
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":59,"skipped":864,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.280 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:53:58.063
    Mar  2 12:53:58.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 12:53:58.064
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:53:58.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:53:58.149
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/02/23 12:53:58.163
    Mar  2 12:53:58.223: INFO: Waiting up to 5m0s for pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19" in namespace "emptydir-7232" to be "Succeeded or Failed"
    Mar  2 12:53:58.240: INFO: Pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19": Phase="Pending", Reason="", readiness=false. Elapsed: 16.689573ms
    Mar  2 12:54:00.243: INFO: Pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020392932s
    Mar  2 12:54:02.244: INFO: Pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021482609s
    Mar  2 12:54:04.244: INFO: Pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021015907s
    STEP: Saw pod success 03/02/23 12:54:04.244
    Mar  2 12:54:04.244: INFO: Pod "pod-e22b95ed-b68c-4cbd-83b7-57a487352d19" satisfied condition "Succeeded or Failed"
    Mar  2 12:54:04.248: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-e22b95ed-b68c-4cbd-83b7-57a487352d19 container test-container: <nil>
    STEP: delete the pod 03/02/23 12:54:04.258
    Mar  2 12:54:04.272: INFO: Waiting for pod pod-e22b95ed-b68c-4cbd-83b7-57a487352d19 to disappear
    Mar  2 12:54:04.332: INFO: Pod pod-e22b95ed-b68c-4cbd-83b7-57a487352d19 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 12:54:04.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7232" for this suite. 03/02/23 12:54:04.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:54:04.347
Mar  2 12:54:04.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 12:54:04.349
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:54:04.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:54:04.367
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-d224b41b-f737-457b-9163-468fe15e8529 03/02/23 12:54:04.37
STEP: Creating a pod to test consume configMaps 03/02/23 12:54:04.406
Mar  2 12:54:04.434: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e" in namespace "configmap-3386" to be "Succeeded or Failed"
Mar  2 12:54:04.454: INFO: Pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.950555ms
Mar  2 12:54:06.460: INFO: Pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025463921s
Mar  2 12:54:08.458: INFO: Pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023466841s
Mar  2 12:54:10.462: INFO: Pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026807858s
STEP: Saw pod success 03/02/23 12:54:10.462
Mar  2 12:54:10.463: INFO: Pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e" satisfied condition "Succeeded or Failed"
Mar  2 12:54:10.524: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e container agnhost-container: <nil>
STEP: delete the pod 03/02/23 12:54:10.535
Mar  2 12:54:10.547: INFO: Waiting for pod pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e to disappear
Mar  2 12:54:10.549: INFO: Pod pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 12:54:10.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3386" for this suite. 03/02/23 12:54:10.556
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":60,"skipped":882,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.215 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:54:04.347
    Mar  2 12:54:04.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 12:54:04.349
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:54:04.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:54:04.367
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-d224b41b-f737-457b-9163-468fe15e8529 03/02/23 12:54:04.37
    STEP: Creating a pod to test consume configMaps 03/02/23 12:54:04.406
    Mar  2 12:54:04.434: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e" in namespace "configmap-3386" to be "Succeeded or Failed"
    Mar  2 12:54:04.454: INFO: Pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.950555ms
    Mar  2 12:54:06.460: INFO: Pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025463921s
    Mar  2 12:54:08.458: INFO: Pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023466841s
    Mar  2 12:54:10.462: INFO: Pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026807858s
    STEP: Saw pod success 03/02/23 12:54:10.462
    Mar  2 12:54:10.463: INFO: Pod "pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e" satisfied condition "Succeeded or Failed"
    Mar  2 12:54:10.524: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 12:54:10.535
    Mar  2 12:54:10.547: INFO: Waiting for pod pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e to disappear
    Mar  2 12:54:10.549: INFO: Pod pod-configmaps-e7e55dac-f62a-4499-98e5-c2b82585c19e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 12:54:10.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3386" for this suite. 03/02/23 12:54:10.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:54:10.571
Mar  2 12:54:10.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename cronjob 03/02/23 12:54:10.573
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:54:10.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:54:10.635
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/02/23 12:54:10.646
STEP: Ensuring no jobs are scheduled 03/02/23 12:54:10.653
STEP: Ensuring no job exists by listing jobs explicitly 03/02/23 12:59:10.669
STEP: Removing cronjob 03/02/23 12:59:10.674
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  2 12:59:10.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4033" for this suite. 03/02/23 12:59:10.714
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":61,"skipped":900,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.154 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:54:10.571
    Mar  2 12:54:10.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename cronjob 03/02/23 12:54:10.573
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:54:10.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:54:10.635
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/02/23 12:54:10.646
    STEP: Ensuring no jobs are scheduled 03/02/23 12:54:10.653
    STEP: Ensuring no job exists by listing jobs explicitly 03/02/23 12:59:10.669
    STEP: Removing cronjob 03/02/23 12:59:10.674
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  2 12:59:10.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4033" for this suite. 03/02/23 12:59:10.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:59:10.744
Mar  2 12:59:10.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sysctl 03/02/23 12:59:10.746
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:59:10.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:59:10.782
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/02/23 12:59:10.791
STEP: Watching for error events or started pod 03/02/23 12:59:10.799
STEP: Waiting for pod completion 03/02/23 12:59:12.81
Mar  2 12:59:12.810: INFO: Waiting up to 3m0s for pod "sysctl-7052c00a-9e63-42a2-bfdb-001c6ebcba03" in namespace "sysctl-4856" to be "completed"
Mar  2 12:59:12.814: INFO: Pod "sysctl-7052c00a-9e63-42a2-bfdb-001c6ebcba03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.252199ms
Mar  2 12:59:14.822: INFO: Pod "sysctl-7052c00a-9e63-42a2-bfdb-001c6ebcba03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012662587s
Mar  2 12:59:16.825: INFO: Pod "sysctl-7052c00a-9e63-42a2-bfdb-001c6ebcba03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015758506s
Mar  2 12:59:16.826: INFO: Pod "sysctl-7052c00a-9e63-42a2-bfdb-001c6ebcba03" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/02/23 12:59:16.833
STEP: Getting logs from the pod 03/02/23 12:59:16.833
STEP: Checking that the sysctl is actually updated 03/02/23 12:59:16.856
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 12:59:16.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4856" for this suite. 03/02/23 12:59:16.862
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":62,"skipped":945,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.123 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:59:10.744
    Mar  2 12:59:10.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sysctl 03/02/23 12:59:10.746
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:59:10.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:59:10.782
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/02/23 12:59:10.791
    STEP: Watching for error events or started pod 03/02/23 12:59:10.799
    STEP: Waiting for pod completion 03/02/23 12:59:12.81
    Mar  2 12:59:12.810: INFO: Waiting up to 3m0s for pod "sysctl-7052c00a-9e63-42a2-bfdb-001c6ebcba03" in namespace "sysctl-4856" to be "completed"
    Mar  2 12:59:12.814: INFO: Pod "sysctl-7052c00a-9e63-42a2-bfdb-001c6ebcba03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.252199ms
    Mar  2 12:59:14.822: INFO: Pod "sysctl-7052c00a-9e63-42a2-bfdb-001c6ebcba03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012662587s
    Mar  2 12:59:16.825: INFO: Pod "sysctl-7052c00a-9e63-42a2-bfdb-001c6ebcba03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015758506s
    Mar  2 12:59:16.826: INFO: Pod "sysctl-7052c00a-9e63-42a2-bfdb-001c6ebcba03" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/02/23 12:59:16.833
    STEP: Getting logs from the pod 03/02/23 12:59:16.833
    STEP: Checking that the sysctl is actually updated 03/02/23 12:59:16.856
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 12:59:16.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-4856" for this suite. 03/02/23 12:59:16.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:59:16.883
Mar  2 12:59:16.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 12:59:16.885
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:59:16.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:59:16.922
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-6502deca-f161-4900-8936-4357cc9f3624 03/02/23 12:59:16.929
STEP: Creating a pod to test consume configMaps 03/02/23 12:59:16.939
Mar  2 12:59:16.947: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c" in namespace "configmap-3040" to be "Succeeded or Failed"
Mar  2 12:59:16.952: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.814734ms
Mar  2 12:59:18.958: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01066526s
Mar  2 12:59:20.957: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c": Phase="Running", Reason="", readiness=true. Elapsed: 4.010079136s
Mar  2 12:59:22.955: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c": Phase="Running", Reason="", readiness=false. Elapsed: 6.008521055s
Mar  2 12:59:24.966: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018828666s
STEP: Saw pod success 03/02/23 12:59:24.966
Mar  2 12:59:24.967: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c" satisfied condition "Succeeded or Failed"
Mar  2 12:59:24.989: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c container agnhost-container: <nil>
STEP: delete the pod 03/02/23 12:59:24.997
Mar  2 12:59:25.016: INFO: Waiting for pod pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c to disappear
Mar  2 12:59:25.021: INFO: Pod pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 12:59:25.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3040" for this suite. 03/02/23 12:59:25.031
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":63,"skipped":996,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.154 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:59:16.883
    Mar  2 12:59:16.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 12:59:16.885
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:59:16.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:59:16.922
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-6502deca-f161-4900-8936-4357cc9f3624 03/02/23 12:59:16.929
    STEP: Creating a pod to test consume configMaps 03/02/23 12:59:16.939
    Mar  2 12:59:16.947: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c" in namespace "configmap-3040" to be "Succeeded or Failed"
    Mar  2 12:59:16.952: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.814734ms
    Mar  2 12:59:18.958: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01066526s
    Mar  2 12:59:20.957: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c": Phase="Running", Reason="", readiness=true. Elapsed: 4.010079136s
    Mar  2 12:59:22.955: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c": Phase="Running", Reason="", readiness=false. Elapsed: 6.008521055s
    Mar  2 12:59:24.966: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018828666s
    STEP: Saw pod success 03/02/23 12:59:24.966
    Mar  2 12:59:24.967: INFO: Pod "pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c" satisfied condition "Succeeded or Failed"
    Mar  2 12:59:24.989: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 12:59:24.997
    Mar  2 12:59:25.016: INFO: Waiting for pod pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c to disappear
    Mar  2 12:59:25.021: INFO: Pod pod-configmaps-7f58e92c-6766-4e34-ad9e-2b195cb2551c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 12:59:25.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3040" for this suite. 03/02/23 12:59:25.031
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:59:25.04
Mar  2 12:59:25.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 12:59:25.042
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:59:25.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:59:25.065
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 12:59:25.089
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:59:26.027
STEP: Deploying the webhook pod 03/02/23 12:59:26.038
STEP: Wait for the deployment to be ready 03/02/23 12:59:26.058
Mar  2 12:59:26.070: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/02/23 12:59:28.099
STEP: Verifying the service has paired with the endpoint 03/02/23 12:59:28.118
Mar  2 12:59:29.120: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 03/02/23 12:59:29.191
STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 12:59:29.249
STEP: Deleting the collection of validation webhooks 03/02/23 12:59:29.327
STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 12:59:29.355
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 12:59:29.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8862" for this suite. 03/02/23 12:59:29.37
STEP: Destroying namespace "webhook-8862-markers" for this suite. 03/02/23 12:59:29.375
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":64,"skipped":996,"failed":0}
------------------------------
â€¢ [4.442 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:59:25.04
    Mar  2 12:59:25.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 12:59:25.042
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:59:25.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:59:25.065
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 12:59:25.089
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 12:59:26.027
    STEP: Deploying the webhook pod 03/02/23 12:59:26.038
    STEP: Wait for the deployment to be ready 03/02/23 12:59:26.058
    Mar  2 12:59:26.070: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/02/23 12:59:28.099
    STEP: Verifying the service has paired with the endpoint 03/02/23 12:59:28.118
    Mar  2 12:59:29.120: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 03/02/23 12:59:29.191
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 12:59:29.249
    STEP: Deleting the collection of validation webhooks 03/02/23 12:59:29.327
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 12:59:29.355
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 12:59:29.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8862" for this suite. 03/02/23 12:59:29.37
    STEP: Destroying namespace "webhook-8862-markers" for this suite. 03/02/23 12:59:29.375
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:59:29.551
Mar  2 12:59:29.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename containers 03/02/23 12:59:29.554
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:59:29.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:59:29.66
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 03/02/23 12:59:29.667
Mar  2 12:59:29.734: INFO: Waiting up to 5m0s for pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80" in namespace "containers-4609" to be "Succeeded or Failed"
Mar  2 12:59:29.744: INFO: Pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80": Phase="Pending", Reason="", readiness=false. Elapsed: 10.233928ms
Mar  2 12:59:31.752: INFO: Pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01748616s
Mar  2 12:59:33.754: INFO: Pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80": Phase="Running", Reason="", readiness=false. Elapsed: 4.019962748s
Mar  2 12:59:35.751: INFO: Pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016623321s
STEP: Saw pod success 03/02/23 12:59:35.751
Mar  2 12:59:35.751: INFO: Pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80" satisfied condition "Succeeded or Failed"
Mar  2 12:59:35.756: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 12:59:35.766
Mar  2 12:59:35.810: INFO: Waiting for pod client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80 to disappear
Mar  2 12:59:35.826: INFO: Pod client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  2 12:59:35.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4609" for this suite. 03/02/23 12:59:35.838
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":65,"skipped":1045,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.295 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:59:29.551
    Mar  2 12:59:29.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename containers 03/02/23 12:59:29.554
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:59:29.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:59:29.66
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 03/02/23 12:59:29.667
    Mar  2 12:59:29.734: INFO: Waiting up to 5m0s for pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80" in namespace "containers-4609" to be "Succeeded or Failed"
    Mar  2 12:59:29.744: INFO: Pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80": Phase="Pending", Reason="", readiness=false. Elapsed: 10.233928ms
    Mar  2 12:59:31.752: INFO: Pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01748616s
    Mar  2 12:59:33.754: INFO: Pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80": Phase="Running", Reason="", readiness=false. Elapsed: 4.019962748s
    Mar  2 12:59:35.751: INFO: Pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016623321s
    STEP: Saw pod success 03/02/23 12:59:35.751
    Mar  2 12:59:35.751: INFO: Pod "client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80" satisfied condition "Succeeded or Failed"
    Mar  2 12:59:35.756: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 12:59:35.766
    Mar  2 12:59:35.810: INFO: Waiting for pod client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80 to disappear
    Mar  2 12:59:35.826: INFO: Pod client-containers-d3d45101-5195-4b20-8ecb-aa8fdb799d80 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  2 12:59:35.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4609" for this suite. 03/02/23 12:59:35.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 12:59:35.85
Mar  2 12:59:35.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pod-network-test 03/02/23 12:59:35.851
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:59:35.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:59:35.956
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-1050 03/02/23 12:59:35.967
STEP: creating a selector 03/02/23 12:59:35.967
STEP: Creating the service pods in kubernetes 03/02/23 12:59:35.967
Mar  2 12:59:35.967: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 12:59:36.052: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1050" to be "running and ready"
Mar  2 12:59:36.065: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.609253ms
Mar  2 12:59:36.065: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:59:38.098: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045664502s
Mar  2 12:59:38.098: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 12:59:40.073: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.021116752s
Mar  2 12:59:40.073: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 12:59:42.071: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019242855s
Mar  2 12:59:42.071: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 12:59:44.076: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.02371111s
Mar  2 12:59:44.076: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 12:59:46.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.021632883s
Mar  2 12:59:46.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 12:59:48.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022685929s
Mar  2 12:59:48.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 12:59:50.072: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019817579s
Mar  2 12:59:50.072: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 12:59:52.073: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.02036719s
Mar  2 12:59:52.073: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 12:59:54.077: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.025063967s
Mar  2 12:59:54.077: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 12:59:56.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023183709s
Mar  2 12:59:56.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 12:59:58.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.027555805s
Mar  2 12:59:58.080: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  2 12:59:58.080: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  2 12:59:58.088: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1050" to be "running and ready"
Mar  2 12:59:58.097: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.89477ms
Mar  2 12:59:58.097: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  2 12:59:58.097: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  2 12:59:58.104: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1050" to be "running and ready"
Mar  2 12:59:58.108: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.570802ms
Mar  2 12:59:58.108: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  2 12:59:58.108: INFO: Pod "netserver-2" satisfied condition "running and ready"
Mar  2 12:59:58.113: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1050" to be "running and ready"
Mar  2 12:59:58.117: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.776508ms
Mar  2 12:59:58.117: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Mar  2 12:59:58.117: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 03/02/23 12:59:58.121
Mar  2 12:59:58.138: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1050" to be "running"
Mar  2 12:59:58.167: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 28.783252ms
Mar  2 13:00:00.200: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062146343s
Mar  2 13:00:02.173: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.034888969s
Mar  2 13:00:02.173: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  2 13:00:02.177: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1050" to be "running"
Mar  2 13:00:02.180: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.829604ms
Mar  2 13:00:02.180: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar  2 13:00:02.214: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Mar  2 13:00:02.214: INFO: Going to poll 10.233.123.4 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Mar  2 13:00:02.222: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.123.4:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1050 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:00:02.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:00:02.224: INFO: ExecWithOptions: Clientset creation
Mar  2 13:00:02.224: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1050/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.123.4%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 13:00:02.348: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 13:00:02.352: INFO: Going to poll 10.233.92.65 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Mar  2 13:00:02.359: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.92.65:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1050 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:00:02.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:00:02.361: INFO: ExecWithOptions: Clientset creation
Mar  2 13:00:02.361: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1050/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.92.65%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 13:00:02.461: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 13:00:02.461: INFO: Going to poll 10.233.123.112 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Mar  2 13:00:02.469: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.123.112:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1050 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:00:02.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:00:02.489: INFO: ExecWithOptions: Clientset creation
Mar  2 13:00:02.489: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1050/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.123.112%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 13:00:02.630: INFO: Found all 1 expected endpoints: [netserver-2]
Mar  2 13:00:02.631: INFO: Going to poll 10.233.126.111 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Mar  2 13:00:02.637: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.126.111:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1050 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:00:02.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:00:02.651: INFO: ExecWithOptions: Clientset creation
Mar  2 13:00:02.651: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1050/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.126.111%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 13:00:02.889: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  2 13:00:02.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1050" for this suite. 03/02/23 13:00:02.899
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":66,"skipped":1094,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.068 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 12:59:35.85
    Mar  2 12:59:35.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pod-network-test 03/02/23 12:59:35.851
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 12:59:35.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 12:59:35.956
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-1050 03/02/23 12:59:35.967
    STEP: creating a selector 03/02/23 12:59:35.967
    STEP: Creating the service pods in kubernetes 03/02/23 12:59:35.967
    Mar  2 12:59:35.967: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  2 12:59:36.052: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1050" to be "running and ready"
    Mar  2 12:59:36.065: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.609253ms
    Mar  2 12:59:36.065: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:59:38.098: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045664502s
    Mar  2 12:59:38.098: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 12:59:40.073: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.021116752s
    Mar  2 12:59:40.073: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 12:59:42.071: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019242855s
    Mar  2 12:59:42.071: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 12:59:44.076: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.02371111s
    Mar  2 12:59:44.076: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 12:59:46.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.021632883s
    Mar  2 12:59:46.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 12:59:48.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022685929s
    Mar  2 12:59:48.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 12:59:50.072: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019817579s
    Mar  2 12:59:50.072: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 12:59:52.073: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.02036719s
    Mar  2 12:59:52.073: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 12:59:54.077: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.025063967s
    Mar  2 12:59:54.077: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 12:59:56.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023183709s
    Mar  2 12:59:56.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 12:59:58.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.027555805s
    Mar  2 12:59:58.080: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  2 12:59:58.080: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  2 12:59:58.088: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1050" to be "running and ready"
    Mar  2 12:59:58.097: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.89477ms
    Mar  2 12:59:58.097: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  2 12:59:58.097: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  2 12:59:58.104: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1050" to be "running and ready"
    Mar  2 12:59:58.108: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.570802ms
    Mar  2 12:59:58.108: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  2 12:59:58.108: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Mar  2 12:59:58.113: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1050" to be "running and ready"
    Mar  2 12:59:58.117: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.776508ms
    Mar  2 12:59:58.117: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Mar  2 12:59:58.117: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 03/02/23 12:59:58.121
    Mar  2 12:59:58.138: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1050" to be "running"
    Mar  2 12:59:58.167: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 28.783252ms
    Mar  2 13:00:00.200: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062146343s
    Mar  2 13:00:02.173: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.034888969s
    Mar  2 13:00:02.173: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  2 13:00:02.177: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1050" to be "running"
    Mar  2 13:00:02.180: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.829604ms
    Mar  2 13:00:02.180: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar  2 13:00:02.214: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Mar  2 13:00:02.214: INFO: Going to poll 10.233.123.4 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Mar  2 13:00:02.222: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.123.4:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1050 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:00:02.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:00:02.224: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:00:02.224: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1050/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.123.4%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 13:00:02.348: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar  2 13:00:02.352: INFO: Going to poll 10.233.92.65 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Mar  2 13:00:02.359: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.92.65:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1050 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:00:02.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:00:02.361: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:00:02.361: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1050/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.92.65%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 13:00:02.461: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar  2 13:00:02.461: INFO: Going to poll 10.233.123.112 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Mar  2 13:00:02.469: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.123.112:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1050 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:00:02.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:00:02.489: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:00:02.489: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1050/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.123.112%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 13:00:02.630: INFO: Found all 1 expected endpoints: [netserver-2]
    Mar  2 13:00:02.631: INFO: Going to poll 10.233.126.111 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Mar  2 13:00:02.637: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.126.111:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1050 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:00:02.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:00:02.651: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:00:02.651: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1050/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.126.111%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 13:00:02.889: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  2 13:00:02.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1050" for this suite. 03/02/23 13:00:02.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:00:02.923
Mar  2 13:00:02.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 13:00:02.925
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:00:02.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:00:02.954
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 03/02/23 13:00:02.958
Mar  2 13:00:03.016: INFO: Waiting up to 5m0s for pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6" in namespace "emptydir-2557" to be "Succeeded or Failed"
Mar  2 13:00:03.022: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.948055ms
Mar  2 13:00:05.030: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013561578s
Mar  2 13:00:07.028: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011661283s
Mar  2 13:00:09.051: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035283195s
Mar  2 13:00:11.028: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01150711s
STEP: Saw pod success 03/02/23 13:00:11.028
Mar  2 13:00:11.028: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6" satisfied condition "Succeeded or Failed"
Mar  2 13:00:11.031: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6 container test-container: <nil>
STEP: delete the pod 03/02/23 13:00:11.037
Mar  2 13:00:11.052: INFO: Waiting for pod pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6 to disappear
Mar  2 13:00:11.062: INFO: Pod pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 13:00:11.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2557" for this suite. 03/02/23 13:00:11.067
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":67,"skipped":1099,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.156 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:00:02.923
    Mar  2 13:00:02.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 13:00:02.925
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:00:02.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:00:02.954
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/02/23 13:00:02.958
    Mar  2 13:00:03.016: INFO: Waiting up to 5m0s for pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6" in namespace "emptydir-2557" to be "Succeeded or Failed"
    Mar  2 13:00:03.022: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.948055ms
    Mar  2 13:00:05.030: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013561578s
    Mar  2 13:00:07.028: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011661283s
    Mar  2 13:00:09.051: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035283195s
    Mar  2 13:00:11.028: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01150711s
    STEP: Saw pod success 03/02/23 13:00:11.028
    Mar  2 13:00:11.028: INFO: Pod "pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6" satisfied condition "Succeeded or Failed"
    Mar  2 13:00:11.031: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6 container test-container: <nil>
    STEP: delete the pod 03/02/23 13:00:11.037
    Mar  2 13:00:11.052: INFO: Waiting for pod pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6 to disappear
    Mar  2 13:00:11.062: INFO: Pod pod-c5c0c83a-c7d7-43aa-bbd5-0452bb1b7dd6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 13:00:11.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2557" for this suite. 03/02/23 13:00:11.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:00:11.088
Mar  2 13:00:11.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:00:11.09
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:00:11.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:00:11.112
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-43ad3680-e645-41a0-8422-c515a905521f 03/02/23 13:00:11.116
STEP: Creating a pod to test consume configMaps 03/02/23 13:00:11.12
Mar  2 13:00:11.127: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d" in namespace "projected-4472" to be "Succeeded or Failed"
Mar  2 13:00:11.130: INFO: Pod "pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.789779ms
Mar  2 13:00:13.138: INFO: Pod "pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010956451s
Mar  2 13:00:15.142: INFO: Pod "pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014855012s
STEP: Saw pod success 03/02/23 13:00:15.142
Mar  2 13:00:15.143: INFO: Pod "pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d" satisfied condition "Succeeded or Failed"
Mar  2 13:00:15.145: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d container agnhost-container: <nil>
STEP: delete the pod 03/02/23 13:00:15.156
Mar  2 13:00:15.215: INFO: Waiting for pod pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d to disappear
Mar  2 13:00:15.219: INFO: Pod pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 13:00:15.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4472" for this suite. 03/02/23 13:00:15.226
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":68,"skipped":1109,"failed":0}
------------------------------
â€¢ [4.144 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:00:11.088
    Mar  2 13:00:11.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:00:11.09
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:00:11.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:00:11.112
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-43ad3680-e645-41a0-8422-c515a905521f 03/02/23 13:00:11.116
    STEP: Creating a pod to test consume configMaps 03/02/23 13:00:11.12
    Mar  2 13:00:11.127: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d" in namespace "projected-4472" to be "Succeeded or Failed"
    Mar  2 13:00:11.130: INFO: Pod "pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.789779ms
    Mar  2 13:00:13.138: INFO: Pod "pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010956451s
    Mar  2 13:00:15.142: INFO: Pod "pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014855012s
    STEP: Saw pod success 03/02/23 13:00:15.142
    Mar  2 13:00:15.143: INFO: Pod "pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d" satisfied condition "Succeeded or Failed"
    Mar  2 13:00:15.145: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 13:00:15.156
    Mar  2 13:00:15.215: INFO: Waiting for pod pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d to disappear
    Mar  2 13:00:15.219: INFO: Pod pod-projected-configmaps-59efc7ae-3f29-4727-bbe6-c9229ab3224d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 13:00:15.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4472" for this suite. 03/02/23 13:00:15.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:00:15.24
Mar  2 13:00:15.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename var-expansion 03/02/23 13:00:15.241
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:00:15.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:00:15.268
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Mar  2 13:00:15.278: INFO: Waiting up to 2m0s for pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd" in namespace "var-expansion-2704" to be "container 0 failed with reason CreateContainerConfigError"
Mar  2 13:00:15.307: INFO: Pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 28.979088ms
Mar  2 13:00:17.318: INFO: Pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040436791s
Mar  2 13:00:17.318: INFO: Pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar  2 13:00:17.318: INFO: Deleting pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd" in namespace "var-expansion-2704"
Mar  2 13:00:17.334: INFO: Wait up to 5m0s for pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 13:00:23.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2704" for this suite. 03/02/23 13:00:23.349
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":69,"skipped":1130,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.122 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:00:15.24
    Mar  2 13:00:15.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename var-expansion 03/02/23 13:00:15.241
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:00:15.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:00:15.268
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Mar  2 13:00:15.278: INFO: Waiting up to 2m0s for pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd" in namespace "var-expansion-2704" to be "container 0 failed with reason CreateContainerConfigError"
    Mar  2 13:00:15.307: INFO: Pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 28.979088ms
    Mar  2 13:00:17.318: INFO: Pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040436791s
    Mar  2 13:00:17.318: INFO: Pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar  2 13:00:17.318: INFO: Deleting pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd" in namespace "var-expansion-2704"
    Mar  2 13:00:17.334: INFO: Wait up to 5m0s for pod "var-expansion-3e224134-c218-4df2-835b-a8be4b433ecd" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 13:00:23.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2704" for this suite. 03/02/23 13:00:23.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:00:23.365
Mar  2 13:00:23.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename replicaset 03/02/23 13:00:23.366
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:00:23.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:00:23.391
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/02/23 13:00:23.394
Mar  2 13:00:23.408: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9401" to be "running and ready"
Mar  2 13:00:23.415: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.325018ms
Mar  2 13:00:23.415: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:00:25.420: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.011176668s
Mar  2 13:00:25.420: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar  2 13:00:25.420: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/02/23 13:00:25.424
STEP: Then the orphan pod is adopted 03/02/23 13:00:25.432
STEP: When the matched label of one of its pods change 03/02/23 13:00:26.445
Mar  2 13:00:26.452: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/02/23 13:00:26.463
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 13:00:27.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9401" for this suite. 03/02/23 13:00:27.544
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":70,"skipped":1136,"failed":0}
------------------------------
â€¢ [4.184 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:00:23.365
    Mar  2 13:00:23.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename replicaset 03/02/23 13:00:23.366
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:00:23.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:00:23.391
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/02/23 13:00:23.394
    Mar  2 13:00:23.408: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9401" to be "running and ready"
    Mar  2 13:00:23.415: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.325018ms
    Mar  2 13:00:23.415: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:00:25.420: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.011176668s
    Mar  2 13:00:25.420: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar  2 13:00:25.420: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/02/23 13:00:25.424
    STEP: Then the orphan pod is adopted 03/02/23 13:00:25.432
    STEP: When the matched label of one of its pods change 03/02/23 13:00:26.445
    Mar  2 13:00:26.452: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/02/23 13:00:26.463
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 13:00:27.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9401" for this suite. 03/02/23 13:00:27.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:00:27.555
Mar  2 13:00:27.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 13:00:27.556
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:00:27.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:00:27.578
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 03/02/23 13:00:27.58
Mar  2 13:00:27.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: mark a version not serverd 03/02/23 13:00:49.305
STEP: check the unserved version gets removed 03/02/23 13:00:49.323
STEP: check the other version is not changed 03/02/23 13:00:53.642
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:01:02.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8913" for this suite. 03/02/23 13:01:02.508
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":71,"skipped":1181,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.961 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:00:27.555
    Mar  2 13:00:27.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 13:00:27.556
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:00:27.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:00:27.578
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 03/02/23 13:00:27.58
    Mar  2 13:00:27.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: mark a version not serverd 03/02/23 13:00:49.305
    STEP: check the unserved version gets removed 03/02/23 13:00:49.323
    STEP: check the other version is not changed 03/02/23 13:00:53.642
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:01:02.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8913" for this suite. 03/02/23 13:01:02.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:01:02.517
Mar  2 13:01:02.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename runtimeclass 03/02/23 13:01:02.518
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:02.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:02.542
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/02/23 13:01:02.545
STEP: getting /apis/node.k8s.io 03/02/23 13:01:02.547
STEP: getting /apis/node.k8s.io/v1 03/02/23 13:01:02.548
STEP: creating 03/02/23 13:01:02.55
STEP: watching 03/02/23 13:01:02.563
Mar  2 13:01:02.563: INFO: starting watch
STEP: getting 03/02/23 13:01:02.567
STEP: listing 03/02/23 13:01:02.569
STEP: patching 03/02/23 13:01:02.574
STEP: updating 03/02/23 13:01:02.586
Mar  2 13:01:02.610: INFO: waiting for watch events with expected annotations
STEP: deleting 03/02/23 13:01:02.611
STEP: deleting a collection 03/02/23 13:01:02.62
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  2 13:01:02.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8012" for this suite. 03/02/23 13:01:02.635
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":72,"skipped":1187,"failed":0}
------------------------------
â€¢ [0.122 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:01:02.517
    Mar  2 13:01:02.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename runtimeclass 03/02/23 13:01:02.518
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:02.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:02.542
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/02/23 13:01:02.545
    STEP: getting /apis/node.k8s.io 03/02/23 13:01:02.547
    STEP: getting /apis/node.k8s.io/v1 03/02/23 13:01:02.548
    STEP: creating 03/02/23 13:01:02.55
    STEP: watching 03/02/23 13:01:02.563
    Mar  2 13:01:02.563: INFO: starting watch
    STEP: getting 03/02/23 13:01:02.567
    STEP: listing 03/02/23 13:01:02.569
    STEP: patching 03/02/23 13:01:02.574
    STEP: updating 03/02/23 13:01:02.586
    Mar  2 13:01:02.610: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/02/23 13:01:02.611
    STEP: deleting a collection 03/02/23 13:01:02.62
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  2 13:01:02.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8012" for this suite. 03/02/23 13:01:02.635
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:01:02.64
Mar  2 13:01:02.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-webhook 03/02/23 13:01:02.641
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:02.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:02.676
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/02/23 13:01:02.681
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/02/23 13:01:03.432
STEP: Deploying the custom resource conversion webhook pod 03/02/23 13:01:03.447
STEP: Wait for the deployment to be ready 03/02/23 13:01:03.466
Mar  2 13:01:03.477: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/02/23 13:01:05.509
STEP: Verifying the service has paired with the endpoint 03/02/23 13:01:05.523
Mar  2 13:01:06.526: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar  2 13:01:06.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Creating a v1 custom resource 03/02/23 13:01:14.223
STEP: v2 custom resource should be converted 03/02/23 13:01:14.231
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:01:14.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5910" for this suite. 03/02/23 13:01:14.769
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":73,"skipped":1188,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.289 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:01:02.64
    Mar  2 13:01:02.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-webhook 03/02/23 13:01:02.641
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:02.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:02.676
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/02/23 13:01:02.681
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/02/23 13:01:03.432
    STEP: Deploying the custom resource conversion webhook pod 03/02/23 13:01:03.447
    STEP: Wait for the deployment to be ready 03/02/23 13:01:03.466
    Mar  2 13:01:03.477: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/02/23 13:01:05.509
    STEP: Verifying the service has paired with the endpoint 03/02/23 13:01:05.523
    Mar  2 13:01:06.526: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar  2 13:01:06.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Creating a v1 custom resource 03/02/23 13:01:14.223
    STEP: v2 custom resource should be converted 03/02/23 13:01:14.231
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:01:14.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5910" for this suite. 03/02/23 13:01:14.769
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:01:14.936
Mar  2 13:01:14.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 13:01:14.939
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:14.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:15.002
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 13:01:15.144
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:01:16.645
STEP: Deploying the webhook pod 03/02/23 13:01:16.655
STEP: Wait for the deployment to be ready 03/02/23 13:01:16.671
Mar  2 13:01:16.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 1, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 1, 16, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 13, 1, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 1, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 13:01:18.76
STEP: Verifying the service has paired with the endpoint 03/02/23 13:01:18.795
Mar  2 13:01:19.795: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 03/02/23 13:01:19.811
STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 13:01:19.844
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/02/23 13:01:19.911
STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 13:01:19.982
STEP: Patching a validating webhook configuration's rules to include the create operation 03/02/23 13:01:20.013
STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 13:01:20.024
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:01:20.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7200" for this suite. 03/02/23 13:01:20.058
STEP: Destroying namespace "webhook-7200-markers" for this suite. 03/02/23 13:01:20.075
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":74,"skipped":1192,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.197 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:01:14.936
    Mar  2 13:01:14.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 13:01:14.939
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:14.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:15.002
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 13:01:15.144
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:01:16.645
    STEP: Deploying the webhook pod 03/02/23 13:01:16.655
    STEP: Wait for the deployment to be ready 03/02/23 13:01:16.671
    Mar  2 13:01:16.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 1, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 1, 16, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 13, 1, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 1, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 13:01:18.76
    STEP: Verifying the service has paired with the endpoint 03/02/23 13:01:18.795
    Mar  2 13:01:19.795: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 03/02/23 13:01:19.811
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 13:01:19.844
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/02/23 13:01:19.911
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 13:01:19.982
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/02/23 13:01:20.013
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 13:01:20.024
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:01:20.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7200" for this suite. 03/02/23 13:01:20.058
    STEP: Destroying namespace "webhook-7200-markers" for this suite. 03/02/23 13:01:20.075
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:01:20.15
Mar  2 13:01:20.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename security-context-test 03/02/23 13:01:20.151
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:20.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:20.214
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Mar  2 13:01:20.231: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b" in namespace "security-context-test-6400" to be "Succeeded or Failed"
Mar  2 13:01:20.241: INFO: Pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.331813ms
Mar  2 13:01:22.263: INFO: Pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031553897s
Mar  2 13:01:24.248: INFO: Pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016345365s
Mar  2 13:01:26.251: INFO: Pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019323853s
Mar  2 13:01:26.251: INFO: Pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b" satisfied condition "Succeeded or Failed"
Mar  2 13:01:26.260: INFO: Got logs for pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 13:01:26.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6400" for this suite. 03/02/23 13:01:26.266
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":75,"skipped":1231,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.164 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:01:20.15
    Mar  2 13:01:20.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename security-context-test 03/02/23 13:01:20.151
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:20.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:20.214
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Mar  2 13:01:20.231: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b" in namespace "security-context-test-6400" to be "Succeeded or Failed"
    Mar  2 13:01:20.241: INFO: Pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.331813ms
    Mar  2 13:01:22.263: INFO: Pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031553897s
    Mar  2 13:01:24.248: INFO: Pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016345365s
    Mar  2 13:01:26.251: INFO: Pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019323853s
    Mar  2 13:01:26.251: INFO: Pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b" satisfied condition "Succeeded or Failed"
    Mar  2 13:01:26.260: INFO: Got logs for pod "busybox-privileged-false-bc36efd9-8866-485f-91a7-ca35f4cde61b": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 13:01:26.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6400" for this suite. 03/02/23 13:01:26.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:01:26.316
Mar  2 13:01:26.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 13:01:26.318
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:26.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:26.353
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 03/02/23 13:01:26.356
Mar  2 13:01:26.362: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da" in namespace "downward-api-325" to be "Succeeded or Failed"
Mar  2 13:01:26.369: INFO: Pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da": Phase="Pending", Reason="", readiness=false. Elapsed: 5.682164ms
Mar  2 13:01:28.374: INFO: Pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010832924s
Mar  2 13:01:30.373: INFO: Pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009700518s
Mar  2 13:01:32.430: INFO: Pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066407838s
STEP: Saw pod success 03/02/23 13:01:32.43
Mar  2 13:01:32.430: INFO: Pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da" satisfied condition "Succeeded or Failed"
Mar  2 13:01:32.443: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da container client-container: <nil>
STEP: delete the pod 03/02/23 13:01:32.452
Mar  2 13:01:32.464: INFO: Waiting for pod downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da to disappear
Mar  2 13:01:32.467: INFO: Pod downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 13:01:32.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-325" for this suite. 03/02/23 13:01:32.471
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":76,"skipped":1257,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.175 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:01:26.316
    Mar  2 13:01:26.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 13:01:26.318
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:26.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:26.353
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 03/02/23 13:01:26.356
    Mar  2 13:01:26.362: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da" in namespace "downward-api-325" to be "Succeeded or Failed"
    Mar  2 13:01:26.369: INFO: Pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da": Phase="Pending", Reason="", readiness=false. Elapsed: 5.682164ms
    Mar  2 13:01:28.374: INFO: Pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010832924s
    Mar  2 13:01:30.373: INFO: Pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009700518s
    Mar  2 13:01:32.430: INFO: Pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066407838s
    STEP: Saw pod success 03/02/23 13:01:32.43
    Mar  2 13:01:32.430: INFO: Pod "downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da" satisfied condition "Succeeded or Failed"
    Mar  2 13:01:32.443: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da container client-container: <nil>
    STEP: delete the pod 03/02/23 13:01:32.452
    Mar  2 13:01:32.464: INFO: Waiting for pod downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da to disappear
    Mar  2 13:01:32.467: INFO: Pod downwardapi-volume-59c0998d-ccc8-45a7-b17e-493ee7a953da no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 13:01:32.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-325" for this suite. 03/02/23 13:01:32.471
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:01:32.499
Mar  2 13:01:32.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pods 03/02/23 13:01:32.515
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:32.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:32.547
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Mar  2 13:01:32.559: INFO: Waiting up to 5m0s for pod "server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c" in namespace "pods-1590" to be "running and ready"
Mar  2 13:01:32.566: INFO: Pod "server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.140318ms
Mar  2 13:01:32.566: INFO: The phase of Pod server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:01:34.634: INFO: Pod "server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c": Phase="Running", Reason="", readiness=true. Elapsed: 2.074443021s
Mar  2 13:01:34.634: INFO: The phase of Pod server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c is Running (Ready = true)
Mar  2 13:01:34.634: INFO: Pod "server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c" satisfied condition "running and ready"
Mar  2 13:01:34.666: INFO: Waiting up to 5m0s for pod "client-envvars-74145b72-ab8d-4d37-8861-58326fad2239" in namespace "pods-1590" to be "Succeeded or Failed"
Mar  2 13:01:34.671: INFO: Pod "client-envvars-74145b72-ab8d-4d37-8861-58326fad2239": Phase="Pending", Reason="", readiness=false. Elapsed: 4.633383ms
Mar  2 13:01:36.686: INFO: Pod "client-envvars-74145b72-ab8d-4d37-8861-58326fad2239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020203616s
Mar  2 13:01:38.722: INFO: Pod "client-envvars-74145b72-ab8d-4d37-8861-58326fad2239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056161798s
STEP: Saw pod success 03/02/23 13:01:38.722
Mar  2 13:01:38.723: INFO: Pod "client-envvars-74145b72-ab8d-4d37-8861-58326fad2239" satisfied condition "Succeeded or Failed"
Mar  2 13:01:38.729: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod client-envvars-74145b72-ab8d-4d37-8861-58326fad2239 container env3cont: <nil>
STEP: delete the pod 03/02/23 13:01:38.737
Mar  2 13:01:38.756: INFO: Waiting for pod client-envvars-74145b72-ab8d-4d37-8861-58326fad2239 to disappear
Mar  2 13:01:38.761: INFO: Pod client-envvars-74145b72-ab8d-4d37-8861-58326fad2239 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 13:01:38.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1590" for this suite. 03/02/23 13:01:38.767
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":77,"skipped":1258,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.322 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:01:32.499
    Mar  2 13:01:32.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pods 03/02/23 13:01:32.515
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:32.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:32.547
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Mar  2 13:01:32.559: INFO: Waiting up to 5m0s for pod "server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c" in namespace "pods-1590" to be "running and ready"
    Mar  2 13:01:32.566: INFO: Pod "server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.140318ms
    Mar  2 13:01:32.566: INFO: The phase of Pod server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:01:34.634: INFO: Pod "server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c": Phase="Running", Reason="", readiness=true. Elapsed: 2.074443021s
    Mar  2 13:01:34.634: INFO: The phase of Pod server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c is Running (Ready = true)
    Mar  2 13:01:34.634: INFO: Pod "server-envvars-59a51718-6a8b-4a3a-8914-07c294217a0c" satisfied condition "running and ready"
    Mar  2 13:01:34.666: INFO: Waiting up to 5m0s for pod "client-envvars-74145b72-ab8d-4d37-8861-58326fad2239" in namespace "pods-1590" to be "Succeeded or Failed"
    Mar  2 13:01:34.671: INFO: Pod "client-envvars-74145b72-ab8d-4d37-8861-58326fad2239": Phase="Pending", Reason="", readiness=false. Elapsed: 4.633383ms
    Mar  2 13:01:36.686: INFO: Pod "client-envvars-74145b72-ab8d-4d37-8861-58326fad2239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020203616s
    Mar  2 13:01:38.722: INFO: Pod "client-envvars-74145b72-ab8d-4d37-8861-58326fad2239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056161798s
    STEP: Saw pod success 03/02/23 13:01:38.722
    Mar  2 13:01:38.723: INFO: Pod "client-envvars-74145b72-ab8d-4d37-8861-58326fad2239" satisfied condition "Succeeded or Failed"
    Mar  2 13:01:38.729: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod client-envvars-74145b72-ab8d-4d37-8861-58326fad2239 container env3cont: <nil>
    STEP: delete the pod 03/02/23 13:01:38.737
    Mar  2 13:01:38.756: INFO: Waiting for pod client-envvars-74145b72-ab8d-4d37-8861-58326fad2239 to disappear
    Mar  2 13:01:38.761: INFO: Pod client-envvars-74145b72-ab8d-4d37-8861-58326fad2239 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 13:01:38.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1590" for this suite. 03/02/23 13:01:38.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:01:38.827
Mar  2 13:01:38.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:01:38.829
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:38.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:38.854
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-8c212a97-4216-4331-b060-d303a52757aa 03/02/23 13:01:38.861
STEP: Creating a pod to test consume secrets 03/02/23 13:01:38.867
Mar  2 13:01:38.885: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272" in namespace "projected-8580" to be "Succeeded or Failed"
Mar  2 13:01:38.912: INFO: Pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272": Phase="Pending", Reason="", readiness=false. Elapsed: 26.987996ms
Mar  2 13:01:40.918: INFO: Pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272": Phase="Running", Reason="", readiness=true. Elapsed: 2.033473449s
Mar  2 13:01:42.917: INFO: Pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272": Phase="Running", Reason="", readiness=false. Elapsed: 4.032122674s
Mar  2 13:01:44.924: INFO: Pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039857944s
STEP: Saw pod success 03/02/23 13:01:44.924
Mar  2 13:01:44.925: INFO: Pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272" satisfied condition "Succeeded or Failed"
Mar  2 13:01:44.929: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/02/23 13:01:44.942
Mar  2 13:01:44.956: INFO: Waiting for pod pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272 to disappear
Mar  2 13:01:44.958: INFO: Pod pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 13:01:44.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8580" for this suite. 03/02/23 13:01:44.97
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":78,"skipped":1286,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.193 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:01:38.827
    Mar  2 13:01:38.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:01:38.829
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:38.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:38.854
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-8c212a97-4216-4331-b060-d303a52757aa 03/02/23 13:01:38.861
    STEP: Creating a pod to test consume secrets 03/02/23 13:01:38.867
    Mar  2 13:01:38.885: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272" in namespace "projected-8580" to be "Succeeded or Failed"
    Mar  2 13:01:38.912: INFO: Pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272": Phase="Pending", Reason="", readiness=false. Elapsed: 26.987996ms
    Mar  2 13:01:40.918: INFO: Pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272": Phase="Running", Reason="", readiness=true. Elapsed: 2.033473449s
    Mar  2 13:01:42.917: INFO: Pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272": Phase="Running", Reason="", readiness=false. Elapsed: 4.032122674s
    Mar  2 13:01:44.924: INFO: Pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039857944s
    STEP: Saw pod success 03/02/23 13:01:44.924
    Mar  2 13:01:44.925: INFO: Pod "pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272" satisfied condition "Succeeded or Failed"
    Mar  2 13:01:44.929: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 13:01:44.942
    Mar  2 13:01:44.956: INFO: Waiting for pod pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272 to disappear
    Mar  2 13:01:44.958: INFO: Pod pod-projected-secrets-72d37a61-a43e-411b-b86b-8c5dee44e272 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 13:01:44.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8580" for this suite. 03/02/23 13:01:44.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:01:45.026
Mar  2 13:01:45.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:01:45.027
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:45.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:45.067
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-5c6947c4-580d-4c55-bee7-434657695fb6 03/02/23 13:01:45.102
STEP: Creating a pod to test consume configMaps 03/02/23 13:01:45.109
Mar  2 13:01:45.120: INFO: Waiting up to 5m0s for pod "pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9" in namespace "configmap-2395" to be "Succeeded or Failed"
Mar  2 13:01:45.126: INFO: Pod "pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.683818ms
Mar  2 13:01:47.133: INFO: Pod "pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012092473s
Mar  2 13:01:49.139: INFO: Pod "pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01823514s
STEP: Saw pod success 03/02/23 13:01:49.139
Mar  2 13:01:49.139: INFO: Pod "pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9" satisfied condition "Succeeded or Failed"
Mar  2 13:01:49.143: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 13:01:49.152
Mar  2 13:01:49.161: INFO: Waiting for pod pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9 to disappear
Mar  2 13:01:49.165: INFO: Pod pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:01:49.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2395" for this suite. 03/02/23 13:01:49.17
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":79,"skipped":1303,"failed":0}
------------------------------
â€¢ [4.150 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:01:45.026
    Mar  2 13:01:45.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:01:45.027
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:45.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:45.067
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-5c6947c4-580d-4c55-bee7-434657695fb6 03/02/23 13:01:45.102
    STEP: Creating a pod to test consume configMaps 03/02/23 13:01:45.109
    Mar  2 13:01:45.120: INFO: Waiting up to 5m0s for pod "pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9" in namespace "configmap-2395" to be "Succeeded or Failed"
    Mar  2 13:01:45.126: INFO: Pod "pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.683818ms
    Mar  2 13:01:47.133: INFO: Pod "pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012092473s
    Mar  2 13:01:49.139: INFO: Pod "pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01823514s
    STEP: Saw pod success 03/02/23 13:01:49.139
    Mar  2 13:01:49.139: INFO: Pod "pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9" satisfied condition "Succeeded or Failed"
    Mar  2 13:01:49.143: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 13:01:49.152
    Mar  2 13:01:49.161: INFO: Waiting for pod pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9 to disappear
    Mar  2 13:01:49.165: INFO: Pod pod-configmaps-dcd083f0-6c43-4a3c-aa6e-3e2e4c8cbee9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:01:49.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2395" for this suite. 03/02/23 13:01:49.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:01:49.177
Mar  2 13:01:49.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:01:49.179
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:49.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:49.206
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-4317 03/02/23 13:01:49.22
Mar  2 13:01:49.280: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4317" to be "running and ready"
Mar  2 13:01:49.317: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 36.980313ms
Mar  2 13:01:49.318: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:01:51.324: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.043540637s
Mar  2 13:01:51.324: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  2 13:01:51.324: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar  2 13:01:51.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 13:01:51.786: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  2 13:01:51.786: INFO: stdout: "iptables"
Mar  2 13:01:51.786: INFO: proxyMode: iptables
Mar  2 13:01:51.827: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 13:01:51.831: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4317 03/02/23 13:01:51.831
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4317 03/02/23 13:01:51.837
I0302 13:01:51.852330      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4317, replica count: 3
I0302 13:01:54.955029      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 13:01:54.965: INFO: Creating new exec pod
Mar  2 13:01:54.975: INFO: Waiting up to 5m0s for pod "execpod-affinityx7xkp" in namespace "services-4317" to be "running"
Mar  2 13:01:55.005: INFO: Pod "execpod-affinityx7xkp": Phase="Pending", Reason="", readiness=false. Elapsed: 29.848769ms
Mar  2 13:01:57.019: INFO: Pod "execpod-affinityx7xkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.043063684s
Mar  2 13:01:57.019: INFO: Pod "execpod-affinityx7xkp" satisfied condition "running"
Mar  2 13:01:58.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec execpod-affinityx7xkp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar  2 13:01:58.958: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 13:01:58.958: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:01:58.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec execpod-affinityx7xkp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.60.230 80'
Mar  2 13:01:59.161: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.60.230 80\nConnection to 10.233.60.230 80 port [tcp/http] succeeded!\n"
Mar  2 13:01:59.161: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:01:59.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec execpod-affinityx7xkp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.60.230:80/ ; done'
Mar  2 13:01:59.539: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n"
Mar  2 13:01:59.539: INFO: stdout: "\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb"
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
Mar  2 13:01:59.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec execpod-affinityx7xkp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.60.230:80/'
Mar  2 13:01:59.735: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n"
Mar  2 13:01:59.735: INFO: stdout: "affinity-clusterip-timeout-74slb"
Mar  2 13:02:19.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec execpod-affinityx7xkp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.60.230:80/'
Mar  2 13:02:19.939: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n"
Mar  2 13:02:19.939: INFO: stdout: "affinity-clusterip-timeout-l9fvc"
Mar  2 13:02:19.939: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4317, will wait for the garbage collector to delete the pods 03/02/23 13:02:19.964
Mar  2 13:02:20.042: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.565167ms
Mar  2 13:02:20.143: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.153003ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:02:22.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4317" for this suite. 03/02/23 13:02:22.688
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":80,"skipped":1335,"failed":0}
------------------------------
â€¢ [SLOW TEST] [33.516 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:01:49.177
    Mar  2 13:01:49.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:01:49.179
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:01:49.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:01:49.206
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-4317 03/02/23 13:01:49.22
    Mar  2 13:01:49.280: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4317" to be "running and ready"
    Mar  2 13:01:49.317: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 36.980313ms
    Mar  2 13:01:49.318: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:01:51.324: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.043540637s
    Mar  2 13:01:51.324: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar  2 13:01:51.324: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar  2 13:01:51.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar  2 13:01:51.786: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar  2 13:01:51.786: INFO: stdout: "iptables"
    Mar  2 13:01:51.786: INFO: proxyMode: iptables
    Mar  2 13:01:51.827: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar  2 13:01:51.831: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-4317 03/02/23 13:01:51.831
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-4317 03/02/23 13:01:51.837
    I0302 13:01:51.852330      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4317, replica count: 3
    I0302 13:01:54.955029      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 13:01:54.965: INFO: Creating new exec pod
    Mar  2 13:01:54.975: INFO: Waiting up to 5m0s for pod "execpod-affinityx7xkp" in namespace "services-4317" to be "running"
    Mar  2 13:01:55.005: INFO: Pod "execpod-affinityx7xkp": Phase="Pending", Reason="", readiness=false. Elapsed: 29.848769ms
    Mar  2 13:01:57.019: INFO: Pod "execpod-affinityx7xkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.043063684s
    Mar  2 13:01:57.019: INFO: Pod "execpod-affinityx7xkp" satisfied condition "running"
    Mar  2 13:01:58.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec execpod-affinityx7xkp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Mar  2 13:01:58.958: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Mar  2 13:01:58.958: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:01:58.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec execpod-affinityx7xkp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.60.230 80'
    Mar  2 13:01:59.161: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.60.230 80\nConnection to 10.233.60.230 80 port [tcp/http] succeeded!\n"
    Mar  2 13:01:59.161: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:01:59.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec execpod-affinityx7xkp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.60.230:80/ ; done'
    Mar  2 13:01:59.539: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n"
    Mar  2 13:01:59.539: INFO: stdout: "\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb\naffinity-clusterip-timeout-74slb"
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Received response from host: affinity-clusterip-timeout-74slb
    Mar  2 13:01:59.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec execpod-affinityx7xkp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.60.230:80/'
    Mar  2 13:01:59.735: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n"
    Mar  2 13:01:59.735: INFO: stdout: "affinity-clusterip-timeout-74slb"
    Mar  2 13:02:19.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4317 exec execpod-affinityx7xkp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.60.230:80/'
    Mar  2 13:02:19.939: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.60.230:80/\n"
    Mar  2 13:02:19.939: INFO: stdout: "affinity-clusterip-timeout-l9fvc"
    Mar  2 13:02:19.939: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4317, will wait for the garbage collector to delete the pods 03/02/23 13:02:19.964
    Mar  2 13:02:20.042: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.565167ms
    Mar  2 13:02:20.143: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.153003ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:02:22.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4317" for this suite. 03/02/23 13:02:22.688
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:02:22.7
Mar  2 13:02:22.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename proxy 03/02/23 13:02:22.703
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:22.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:22.755
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/02/23 13:02:22.77
STEP: creating replication controller proxy-service-fv7tp in namespace proxy-285 03/02/23 13:02:22.771
I0302 13:02:22.786802      20 runners.go:193] Created replication controller with name: proxy-service-fv7tp, namespace: proxy-285, replica count: 1
I0302 13:02:23.850020      20 runners.go:193] proxy-service-fv7tp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 13:02:24.850402      20 runners.go:193] proxy-service-fv7tp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 13:02:25.852125      20 runners.go:193] proxy-service-fv7tp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 13:02:26.852521      20 runners.go:193] proxy-service-fv7tp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 13:02:26.871: INFO: setup took 4.111754959s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/02/23 13:02:26.871
Mar  2 13:02:26.887: INFO: (0) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 15.250885ms)
Mar  2 13:02:26.897: INFO: (0) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 24.954471ms)
Mar  2 13:02:26.897: INFO: (0) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 13.16037ms)
Mar  2 13:02:26.897: INFO: (0) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 25.39617ms)
Mar  2 13:02:26.907: INFO: (0) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 21.870122ms)
Mar  2 13:02:26.908: INFO: (0) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 23.845265ms)
Mar  2 13:02:26.908: INFO: (0) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 23.628865ms)
Mar  2 13:02:26.910: INFO: (0) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 36.783781ms)
Mar  2 13:02:26.918: INFO: (0) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 32.974611ms)
Mar  2 13:02:26.918: INFO: (0) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 45.154134ms)
Mar  2 13:02:26.922: INFO: (0) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 48.979482ms)
Mar  2 13:02:26.922: INFO: (0) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 49.259842ms)
Mar  2 13:02:26.922: INFO: (0) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 37.570607ms)
Mar  2 13:02:26.922: INFO: (0) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 49.159916ms)
Mar  2 13:02:26.924: INFO: (0) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 50.37451ms)
Mar  2 13:02:26.924: INFO: (0) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 39.336949ms)
Mar  2 13:02:26.932: INFO: (1) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 7.600812ms)
Mar  2 13:02:26.933: INFO: (1) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 8.328336ms)
Mar  2 13:02:26.934: INFO: (1) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 9.272733ms)
Mar  2 13:02:26.939: INFO: (1) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 13.465122ms)
Mar  2 13:02:26.939: INFO: (1) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 14.316291ms)
Mar  2 13:02:26.940: INFO: (1) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 15.284386ms)
Mar  2 13:02:26.940: INFO: (1) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 15.742078ms)
Mar  2 13:02:26.941: INFO: (1) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 15.924153ms)
Mar  2 13:02:26.942: INFO: (1) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 17.00039ms)
Mar  2 13:02:26.942: INFO: (1) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 16.873828ms)
Mar  2 13:02:26.942: INFO: (1) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 17.601348ms)
Mar  2 13:02:26.943: INFO: (1) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 18.054576ms)
Mar  2 13:02:26.943: INFO: (1) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 18.882297ms)
Mar  2 13:02:26.943: INFO: (1) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 18.399492ms)
Mar  2 13:02:26.944: INFO: (1) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 18.942536ms)
Mar  2 13:02:26.944: INFO: (1) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 19.083376ms)
Mar  2 13:02:26.953: INFO: (2) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 7.631792ms)
Mar  2 13:02:26.954: INFO: (2) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 9.047637ms)
Mar  2 13:02:26.955: INFO: (2) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 10.37599ms)
Mar  2 13:02:26.955: INFO: (2) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 10.067102ms)
Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.605252ms)
Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 10.46863ms)
Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 11.036069ms)
Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 10.802592ms)
Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 11.797075ms)
Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 10.984914ms)
Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 10.264262ms)
Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 11.730609ms)
Mar  2 13:02:26.957: INFO: (2) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 12.129176ms)
Mar  2 13:02:26.957: INFO: (2) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.032674ms)
Mar  2 13:02:26.957: INFO: (2) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 11.514998ms)
Mar  2 13:02:26.958: INFO: (2) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 13.516279ms)
Mar  2 13:02:26.966: INFO: (3) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 7.710189ms)
Mar  2 13:02:26.967: INFO: (3) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 8.783237ms)
Mar  2 13:02:26.967: INFO: (3) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 8.73731ms)
Mar  2 13:02:26.986: INFO: (3) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 27.043493ms)
Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 27.058408ms)
Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 26.886856ms)
Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 27.102882ms)
Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 27.226404ms)
Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 27.803708ms)
Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 27.901689ms)
Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 28.052333ms)
Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 28.815678ms)
Mar  2 13:02:27.007: INFO: (3) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 46.873816ms)
Mar  2 13:02:27.009: INFO: (3) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 50.3184ms)
Mar  2 13:02:27.009: INFO: (3) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 50.194455ms)
Mar  2 13:02:27.011: INFO: (3) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 51.551508ms)
Mar  2 13:02:27.028: INFO: (4) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 15.705645ms)
Mar  2 13:02:27.028: INFO: (4) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 16.883771ms)
Mar  2 13:02:27.031: INFO: (4) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 19.149336ms)
Mar  2 13:02:27.034: INFO: (4) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 21.805406ms)
Mar  2 13:02:27.034: INFO: (4) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 22.182492ms)
Mar  2 13:02:27.034: INFO: (4) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 22.590911ms)
Mar  2 13:02:27.035: INFO: (4) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 22.755877ms)
Mar  2 13:02:27.035: INFO: (4) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 23.082903ms)
Mar  2 13:02:27.035: INFO: (4) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 23.581698ms)
Mar  2 13:02:27.035: INFO: (4) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 23.935154ms)
Mar  2 13:02:27.036: INFO: (4) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 23.866085ms)
Mar  2 13:02:27.036: INFO: (4) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 24.052101ms)
Mar  2 13:02:27.036: INFO: (4) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 24.533658ms)
Mar  2 13:02:27.036: INFO: (4) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 24.405576ms)
Mar  2 13:02:27.037: INFO: (4) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 24.769637ms)
Mar  2 13:02:27.037: INFO: (4) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 24.939643ms)
Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 9.286496ms)
Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 9.221664ms)
Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 10.214602ms)
Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 9.350314ms)
Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 9.57113ms)
Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 9.436454ms)
Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 9.681564ms)
Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 9.534141ms)
Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 9.67263ms)
Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 9.612718ms)
Mar  2 13:02:27.052: INFO: (5) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 15.101925ms)
Mar  2 13:02:27.053: INFO: (5) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 14.795802ms)
Mar  2 13:02:27.053: INFO: (5) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 15.242864ms)
Mar  2 13:02:27.053: INFO: (5) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 14.793304ms)
Mar  2 13:02:27.053: INFO: (5) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 14.992171ms)
Mar  2 13:02:27.053: INFO: (5) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 15.443726ms)
Mar  2 13:02:27.063: INFO: (6) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 9.523338ms)
Mar  2 13:02:27.065: INFO: (6) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 10.531559ms)
Mar  2 13:02:27.067: INFO: (6) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 13.225963ms)
Mar  2 13:02:27.068: INFO: (6) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 13.891351ms)
Mar  2 13:02:27.069: INFO: (6) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 14.854336ms)
Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 16.030679ms)
Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 15.855869ms)
Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 15.482843ms)
Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 15.824823ms)
Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 15.538651ms)
Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 15.617251ms)
Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.007354ms)
Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 15.97989ms)
Mar  2 13:02:27.071: INFO: (6) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 16.968637ms)
Mar  2 13:02:27.078: INFO: (6) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 24.414227ms)
Mar  2 13:02:27.078: INFO: (6) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 24.668494ms)
Mar  2 13:02:27.099: INFO: (7) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 19.314965ms)
Mar  2 13:02:27.100: INFO: (7) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 21.445342ms)
Mar  2 13:02:27.104: INFO: (7) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 24.32007ms)
Mar  2 13:02:27.105: INFO: (7) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 25.624145ms)
Mar  2 13:02:27.111: INFO: (7) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 30.609426ms)
Mar  2 13:02:27.111: INFO: (7) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 31.393995ms)
Mar  2 13:02:27.111: INFO: (7) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 31.266806ms)
Mar  2 13:02:27.112: INFO: (7) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 32.439829ms)
Mar  2 13:02:27.112: INFO: (7) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 32.703474ms)
Mar  2 13:02:27.112: INFO: (7) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 32.118458ms)
Mar  2 13:02:27.112: INFO: (7) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 32.333339ms)
Mar  2 13:02:27.113: INFO: (7) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 32.468239ms)
Mar  2 13:02:27.113: INFO: (7) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 32.607228ms)
Mar  2 13:02:27.113: INFO: (7) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 32.821488ms)
Mar  2 13:02:27.113: INFO: (7) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 32.898489ms)
Mar  2 13:02:27.113: INFO: (7) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 32.994983ms)
Mar  2 13:02:27.124: INFO: (8) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 10.414293ms)
Mar  2 13:02:27.125: INFO: (8) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 11.623279ms)
Mar  2 13:02:27.126: INFO: (8) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.515635ms)
Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 11.98427ms)
Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 13.366902ms)
Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 13.105892ms)
Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 12.154211ms)
Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 13.483646ms)
Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 12.766062ms)
Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 13.343413ms)
Mar  2 13:02:27.128: INFO: (8) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 12.4691ms)
Mar  2 13:02:27.130: INFO: (8) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 16.140853ms)
Mar  2 13:02:27.131: INFO: (8) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 15.899261ms)
Mar  2 13:02:27.131: INFO: (8) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.790741ms)
Mar  2 13:02:27.131: INFO: (8) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 16.172798ms)
Mar  2 13:02:27.135: INFO: (8) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 20.825318ms)
Mar  2 13:02:27.144: INFO: (9) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 8.446664ms)
Mar  2 13:02:27.144: INFO: (9) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 8.569278ms)
Mar  2 13:02:27.145: INFO: (9) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 9.131611ms)
Mar  2 13:02:27.145: INFO: (9) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 9.189649ms)
Mar  2 13:02:27.145: INFO: (9) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 9.742295ms)
Mar  2 13:02:27.145: INFO: (9) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 9.81939ms)
Mar  2 13:02:27.145: INFO: (9) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 9.570155ms)
Mar  2 13:02:27.146: INFO: (9) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 10.487439ms)
Mar  2 13:02:27.146: INFO: (9) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.43521ms)
Mar  2 13:02:27.147: INFO: (9) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 11.208228ms)
Mar  2 13:02:27.148: INFO: (9) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 12.564517ms)
Mar  2 13:02:27.149: INFO: (9) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 12.738349ms)
Mar  2 13:02:27.150: INFO: (9) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 13.820889ms)
Mar  2 13:02:27.150: INFO: (9) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 14.487525ms)
Mar  2 13:02:27.150: INFO: (9) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 14.051147ms)
Mar  2 13:02:27.150: INFO: (9) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 14.687991ms)
Mar  2 13:02:27.160: INFO: (10) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 8.762456ms)
Mar  2 13:02:27.161: INFO: (10) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 10.085913ms)
Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 11.182883ms)
Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.053676ms)
Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.457135ms)
Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 11.634281ms)
Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 11.491424ms)
Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 11.555073ms)
Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 11.754032ms)
Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 11.418537ms)
Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 12.003997ms)
Mar  2 13:02:27.164: INFO: (10) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 12.829015ms)
Mar  2 13:02:27.164: INFO: (10) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 13.209868ms)
Mar  2 13:02:27.164: INFO: (10) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 12.9437ms)
Mar  2 13:02:27.164: INFO: (10) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 13.066868ms)
Mar  2 13:02:27.164: INFO: (10) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 13.486484ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.091087ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 15.852908ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 16.121418ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 16.195147ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 16.826168ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 16.573633ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 15.945422ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 16.527224ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 16.265466ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 17.206953ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 16.825822ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 16.315221ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 16.377887ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 17.390056ms)
Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 16.654804ms)
Mar  2 13:02:27.185: INFO: (11) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 17.366943ms)
Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 10.434313ms)
Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 9.719282ms)
Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 10.468692ms)
Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 10.580085ms)
Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 10.377621ms)
Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 10.81044ms)
Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 10.54094ms)
Mar  2 13:02:27.197: INFO: (12) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 10.736875ms)
Mar  2 13:02:27.197: INFO: (12) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 10.749636ms)
Mar  2 13:02:27.200: INFO: (12) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 13.702809ms)
Mar  2 13:02:27.200: INFO: (12) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 13.697867ms)
Mar  2 13:02:27.201: INFO: (12) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 14.50131ms)
Mar  2 13:02:27.201: INFO: (12) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 14.669798ms)
Mar  2 13:02:27.201: INFO: (12) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 14.831691ms)
Mar  2 13:02:27.201: INFO: (12) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 14.822398ms)
Mar  2 13:02:27.203: INFO: (12) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 16.327604ms)
Mar  2 13:02:27.207: INFO: (13) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 4.579295ms)
Mar  2 13:02:27.215: INFO: (13) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 12.214627ms)
Mar  2 13:02:27.216: INFO: (13) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 12.143058ms)
Mar  2 13:02:27.216: INFO: (13) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 12.207591ms)
Mar  2 13:02:27.216: INFO: (13) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 11.955553ms)
Mar  2 13:02:27.217: INFO: (13) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 12.342371ms)
Mar  2 13:02:27.217: INFO: (13) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 13.068636ms)
Mar  2 13:02:27.217: INFO: (13) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 13.666509ms)
Mar  2 13:02:27.217: INFO: (13) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 13.965019ms)
Mar  2 13:02:27.217: INFO: (13) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 13.468258ms)
Mar  2 13:02:27.220: INFO: (13) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.307798ms)
Mar  2 13:02:27.220: INFO: (13) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 16.596237ms)
Mar  2 13:02:27.220: INFO: (13) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 16.570406ms)
Mar  2 13:02:27.220: INFO: (13) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 16.34168ms)
Mar  2 13:02:27.221: INFO: (13) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 17.471963ms)
Mar  2 13:02:27.222: INFO: (13) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 17.613714ms)
Mar  2 13:02:27.230: INFO: (14) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 7.477401ms)
Mar  2 13:02:27.230: INFO: (14) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 6.946741ms)
Mar  2 13:02:27.230: INFO: (14) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 7.665253ms)
Mar  2 13:02:27.230: INFO: (14) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 8.088671ms)
Mar  2 13:02:27.232: INFO: (14) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 9.780477ms)
Mar  2 13:02:27.232: INFO: (14) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 10.068799ms)
Mar  2 13:02:27.235: INFO: (14) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 12.223248ms)
Mar  2 13:02:27.235: INFO: (14) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 12.421057ms)
Mar  2 13:02:27.236: INFO: (14) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 13.249556ms)
Mar  2 13:02:27.236: INFO: (14) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 13.357946ms)
Mar  2 13:02:27.236: INFO: (14) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 13.450381ms)
Mar  2 13:02:27.239: INFO: (14) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 15.990271ms)
Mar  2 13:02:27.239: INFO: (14) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.494697ms)
Mar  2 13:02:27.240: INFO: (14) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 17.356351ms)
Mar  2 13:02:27.240: INFO: (14) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 17.035849ms)
Mar  2 13:02:27.240: INFO: (14) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 17.357465ms)
Mar  2 13:02:27.249: INFO: (15) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 8.936163ms)
Mar  2 13:02:27.251: INFO: (15) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 10.182738ms)
Mar  2 13:02:27.251: INFO: (15) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 10.883816ms)
Mar  2 13:02:27.251: INFO: (15) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 10.699063ms)
Mar  2 13:02:27.251: INFO: (15) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 11.012851ms)
Mar  2 13:02:27.251: INFO: (15) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.403107ms)
Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 11.683951ms)
Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.415015ms)
Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 11.454291ms)
Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 12.088795ms)
Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 11.912286ms)
Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 11.661387ms)
Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 12.353684ms)
Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 11.940243ms)
Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 12.042159ms)
Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 11.935733ms)
Mar  2 13:02:27.259: INFO: (16) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 5.166593ms)
Mar  2 13:02:27.260: INFO: (16) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 5.714808ms)
Mar  2 13:02:27.260: INFO: (16) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 6.12715ms)
Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 10.381578ms)
Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 10.346165ms)
Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 10.619994ms)
Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 10.515171ms)
Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 11.042044ms)
Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 10.42318ms)
Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.416325ms)
Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 11.076487ms)
Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 11.109076ms)
Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 11.492274ms)
Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 11.296976ms)
Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 11.470281ms)
Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 12.338945ms)
Mar  2 13:02:27.275: INFO: (17) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 8.428796ms)
Mar  2 13:02:27.279: INFO: (17) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 13.1484ms)
Mar  2 13:02:27.279: INFO: (17) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 12.591032ms)
Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 13.199611ms)
Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 13.110106ms)
Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 13.197199ms)
Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 13.558795ms)
Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 13.861045ms)
Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 13.984693ms)
Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 14.111045ms)
Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 16.496296ms)
Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 16.614529ms)
Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.667738ms)
Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 16.775738ms)
Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 16.515745ms)
Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 17.18221ms)
Mar  2 13:02:27.292: INFO: (18) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 8.101582ms)
Mar  2 13:02:27.293: INFO: (18) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 7.789585ms)
Mar  2 13:02:27.293: INFO: (18) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 7.892452ms)
Mar  2 13:02:27.293: INFO: (18) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 9.38098ms)
Mar  2 13:02:27.293: INFO: (18) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 9.155605ms)
Mar  2 13:02:27.294: INFO: (18) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 10.093423ms)
Mar  2 13:02:27.294: INFO: (18) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 9.733112ms)
Mar  2 13:02:27.294: INFO: (18) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 9.067682ms)
Mar  2 13:02:27.295: INFO: (18) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 10.696678ms)
Mar  2 13:02:27.295: INFO: (18) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 10.268029ms)
Mar  2 13:02:27.295: INFO: (18) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 10.582156ms)
Mar  2 13:02:27.295: INFO: (18) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 11.137785ms)
Mar  2 13:02:27.296: INFO: (18) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 11.435722ms)
Mar  2 13:02:27.296: INFO: (18) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 11.47551ms)
Mar  2 13:02:27.296: INFO: (18) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 11.491746ms)
Mar  2 13:02:27.297: INFO: (18) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 12.102347ms)
Mar  2 13:02:27.304: INFO: (19) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 6.739899ms)
Mar  2 13:02:27.304: INFO: (19) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 6.585696ms)
Mar  2 13:02:27.307: INFO: (19) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 9.755931ms)
Mar  2 13:02:27.307: INFO: (19) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 9.43037ms)
Mar  2 13:02:27.314: INFO: (19) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 13.005815ms)
Mar  2 13:02:27.314: INFO: (19) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 16.579247ms)
Mar  2 13:02:27.314: INFO: (19) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 12.719249ms)
Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 13.128448ms)
Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 13.474643ms)
Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 13.525968ms)
Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 18.24877ms)
Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 14.509573ms)
Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 14.438761ms)
Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 14.396168ms)
Mar  2 13:02:27.316: INFO: (19) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 14.599668ms)
Mar  2 13:02:27.316: INFO: (19) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 14.650832ms)
STEP: deleting ReplicationController proxy-service-fv7tp in namespace proxy-285, will wait for the garbage collector to delete the pods 03/02/23 13:02:27.316
Mar  2 13:02:27.383: INFO: Deleting ReplicationController proxy-service-fv7tp took: 12.976399ms
Mar  2 13:02:27.485: INFO: Terminating ReplicationController proxy-service-fv7tp pods took: 102.894704ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  2 13:02:31.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-285" for this suite. 03/02/23 13:02:31.101
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":81,"skipped":1363,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.413 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:02:22.7
    Mar  2 13:02:22.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename proxy 03/02/23 13:02:22.703
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:22.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:22.755
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/02/23 13:02:22.77
    STEP: creating replication controller proxy-service-fv7tp in namespace proxy-285 03/02/23 13:02:22.771
    I0302 13:02:22.786802      20 runners.go:193] Created replication controller with name: proxy-service-fv7tp, namespace: proxy-285, replica count: 1
    I0302 13:02:23.850020      20 runners.go:193] proxy-service-fv7tp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 13:02:24.850402      20 runners.go:193] proxy-service-fv7tp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 13:02:25.852125      20 runners.go:193] proxy-service-fv7tp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0302 13:02:26.852521      20 runners.go:193] proxy-service-fv7tp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 13:02:26.871: INFO: setup took 4.111754959s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/02/23 13:02:26.871
    Mar  2 13:02:26.887: INFO: (0) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 15.250885ms)
    Mar  2 13:02:26.897: INFO: (0) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 24.954471ms)
    Mar  2 13:02:26.897: INFO: (0) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 13.16037ms)
    Mar  2 13:02:26.897: INFO: (0) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 25.39617ms)
    Mar  2 13:02:26.907: INFO: (0) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 21.870122ms)
    Mar  2 13:02:26.908: INFO: (0) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 23.845265ms)
    Mar  2 13:02:26.908: INFO: (0) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 23.628865ms)
    Mar  2 13:02:26.910: INFO: (0) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 36.783781ms)
    Mar  2 13:02:26.918: INFO: (0) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 32.974611ms)
    Mar  2 13:02:26.918: INFO: (0) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 45.154134ms)
    Mar  2 13:02:26.922: INFO: (0) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 48.979482ms)
    Mar  2 13:02:26.922: INFO: (0) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 49.259842ms)
    Mar  2 13:02:26.922: INFO: (0) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 37.570607ms)
    Mar  2 13:02:26.922: INFO: (0) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 49.159916ms)
    Mar  2 13:02:26.924: INFO: (0) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 50.37451ms)
    Mar  2 13:02:26.924: INFO: (0) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 39.336949ms)
    Mar  2 13:02:26.932: INFO: (1) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 7.600812ms)
    Mar  2 13:02:26.933: INFO: (1) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 8.328336ms)
    Mar  2 13:02:26.934: INFO: (1) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 9.272733ms)
    Mar  2 13:02:26.939: INFO: (1) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 13.465122ms)
    Mar  2 13:02:26.939: INFO: (1) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 14.316291ms)
    Mar  2 13:02:26.940: INFO: (1) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 15.284386ms)
    Mar  2 13:02:26.940: INFO: (1) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 15.742078ms)
    Mar  2 13:02:26.941: INFO: (1) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 15.924153ms)
    Mar  2 13:02:26.942: INFO: (1) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 17.00039ms)
    Mar  2 13:02:26.942: INFO: (1) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 16.873828ms)
    Mar  2 13:02:26.942: INFO: (1) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 17.601348ms)
    Mar  2 13:02:26.943: INFO: (1) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 18.054576ms)
    Mar  2 13:02:26.943: INFO: (1) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 18.882297ms)
    Mar  2 13:02:26.943: INFO: (1) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 18.399492ms)
    Mar  2 13:02:26.944: INFO: (1) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 18.942536ms)
    Mar  2 13:02:26.944: INFO: (1) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 19.083376ms)
    Mar  2 13:02:26.953: INFO: (2) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 7.631792ms)
    Mar  2 13:02:26.954: INFO: (2) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 9.047637ms)
    Mar  2 13:02:26.955: INFO: (2) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 10.37599ms)
    Mar  2 13:02:26.955: INFO: (2) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 10.067102ms)
    Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.605252ms)
    Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 10.46863ms)
    Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 11.036069ms)
    Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 10.802592ms)
    Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 11.797075ms)
    Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 10.984914ms)
    Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 10.264262ms)
    Mar  2 13:02:26.956: INFO: (2) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 11.730609ms)
    Mar  2 13:02:26.957: INFO: (2) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 12.129176ms)
    Mar  2 13:02:26.957: INFO: (2) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.032674ms)
    Mar  2 13:02:26.957: INFO: (2) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 11.514998ms)
    Mar  2 13:02:26.958: INFO: (2) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 13.516279ms)
    Mar  2 13:02:26.966: INFO: (3) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 7.710189ms)
    Mar  2 13:02:26.967: INFO: (3) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 8.783237ms)
    Mar  2 13:02:26.967: INFO: (3) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 8.73731ms)
    Mar  2 13:02:26.986: INFO: (3) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 27.043493ms)
    Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 27.058408ms)
    Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 26.886856ms)
    Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 27.102882ms)
    Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 27.226404ms)
    Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 27.803708ms)
    Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 27.901689ms)
    Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 28.052333ms)
    Mar  2 13:02:26.987: INFO: (3) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 28.815678ms)
    Mar  2 13:02:27.007: INFO: (3) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 46.873816ms)
    Mar  2 13:02:27.009: INFO: (3) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 50.3184ms)
    Mar  2 13:02:27.009: INFO: (3) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 50.194455ms)
    Mar  2 13:02:27.011: INFO: (3) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 51.551508ms)
    Mar  2 13:02:27.028: INFO: (4) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 15.705645ms)
    Mar  2 13:02:27.028: INFO: (4) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 16.883771ms)
    Mar  2 13:02:27.031: INFO: (4) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 19.149336ms)
    Mar  2 13:02:27.034: INFO: (4) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 21.805406ms)
    Mar  2 13:02:27.034: INFO: (4) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 22.182492ms)
    Mar  2 13:02:27.034: INFO: (4) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 22.590911ms)
    Mar  2 13:02:27.035: INFO: (4) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 22.755877ms)
    Mar  2 13:02:27.035: INFO: (4) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 23.082903ms)
    Mar  2 13:02:27.035: INFO: (4) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 23.581698ms)
    Mar  2 13:02:27.035: INFO: (4) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 23.935154ms)
    Mar  2 13:02:27.036: INFO: (4) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 23.866085ms)
    Mar  2 13:02:27.036: INFO: (4) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 24.052101ms)
    Mar  2 13:02:27.036: INFO: (4) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 24.533658ms)
    Mar  2 13:02:27.036: INFO: (4) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 24.405576ms)
    Mar  2 13:02:27.037: INFO: (4) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 24.769637ms)
    Mar  2 13:02:27.037: INFO: (4) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 24.939643ms)
    Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 9.286496ms)
    Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 9.221664ms)
    Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 10.214602ms)
    Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 9.350314ms)
    Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 9.57113ms)
    Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 9.436454ms)
    Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 9.681564ms)
    Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 9.534141ms)
    Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 9.67263ms)
    Mar  2 13:02:27.047: INFO: (5) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 9.612718ms)
    Mar  2 13:02:27.052: INFO: (5) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 15.101925ms)
    Mar  2 13:02:27.053: INFO: (5) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 14.795802ms)
    Mar  2 13:02:27.053: INFO: (5) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 15.242864ms)
    Mar  2 13:02:27.053: INFO: (5) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 14.793304ms)
    Mar  2 13:02:27.053: INFO: (5) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 14.992171ms)
    Mar  2 13:02:27.053: INFO: (5) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 15.443726ms)
    Mar  2 13:02:27.063: INFO: (6) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 9.523338ms)
    Mar  2 13:02:27.065: INFO: (6) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 10.531559ms)
    Mar  2 13:02:27.067: INFO: (6) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 13.225963ms)
    Mar  2 13:02:27.068: INFO: (6) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 13.891351ms)
    Mar  2 13:02:27.069: INFO: (6) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 14.854336ms)
    Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 16.030679ms)
    Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 15.855869ms)
    Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 15.482843ms)
    Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 15.824823ms)
    Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 15.538651ms)
    Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 15.617251ms)
    Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.007354ms)
    Mar  2 13:02:27.070: INFO: (6) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 15.97989ms)
    Mar  2 13:02:27.071: INFO: (6) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 16.968637ms)
    Mar  2 13:02:27.078: INFO: (6) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 24.414227ms)
    Mar  2 13:02:27.078: INFO: (6) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 24.668494ms)
    Mar  2 13:02:27.099: INFO: (7) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 19.314965ms)
    Mar  2 13:02:27.100: INFO: (7) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 21.445342ms)
    Mar  2 13:02:27.104: INFO: (7) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 24.32007ms)
    Mar  2 13:02:27.105: INFO: (7) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 25.624145ms)
    Mar  2 13:02:27.111: INFO: (7) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 30.609426ms)
    Mar  2 13:02:27.111: INFO: (7) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 31.393995ms)
    Mar  2 13:02:27.111: INFO: (7) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 31.266806ms)
    Mar  2 13:02:27.112: INFO: (7) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 32.439829ms)
    Mar  2 13:02:27.112: INFO: (7) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 32.703474ms)
    Mar  2 13:02:27.112: INFO: (7) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 32.118458ms)
    Mar  2 13:02:27.112: INFO: (7) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 32.333339ms)
    Mar  2 13:02:27.113: INFO: (7) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 32.468239ms)
    Mar  2 13:02:27.113: INFO: (7) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 32.607228ms)
    Mar  2 13:02:27.113: INFO: (7) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 32.821488ms)
    Mar  2 13:02:27.113: INFO: (7) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 32.898489ms)
    Mar  2 13:02:27.113: INFO: (7) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 32.994983ms)
    Mar  2 13:02:27.124: INFO: (8) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 10.414293ms)
    Mar  2 13:02:27.125: INFO: (8) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 11.623279ms)
    Mar  2 13:02:27.126: INFO: (8) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.515635ms)
    Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 11.98427ms)
    Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 13.366902ms)
    Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 13.105892ms)
    Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 12.154211ms)
    Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 13.483646ms)
    Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 12.766062ms)
    Mar  2 13:02:27.127: INFO: (8) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 13.343413ms)
    Mar  2 13:02:27.128: INFO: (8) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 12.4691ms)
    Mar  2 13:02:27.130: INFO: (8) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 16.140853ms)
    Mar  2 13:02:27.131: INFO: (8) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 15.899261ms)
    Mar  2 13:02:27.131: INFO: (8) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.790741ms)
    Mar  2 13:02:27.131: INFO: (8) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 16.172798ms)
    Mar  2 13:02:27.135: INFO: (8) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 20.825318ms)
    Mar  2 13:02:27.144: INFO: (9) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 8.446664ms)
    Mar  2 13:02:27.144: INFO: (9) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 8.569278ms)
    Mar  2 13:02:27.145: INFO: (9) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 9.131611ms)
    Mar  2 13:02:27.145: INFO: (9) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 9.189649ms)
    Mar  2 13:02:27.145: INFO: (9) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 9.742295ms)
    Mar  2 13:02:27.145: INFO: (9) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 9.81939ms)
    Mar  2 13:02:27.145: INFO: (9) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 9.570155ms)
    Mar  2 13:02:27.146: INFO: (9) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 10.487439ms)
    Mar  2 13:02:27.146: INFO: (9) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.43521ms)
    Mar  2 13:02:27.147: INFO: (9) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 11.208228ms)
    Mar  2 13:02:27.148: INFO: (9) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 12.564517ms)
    Mar  2 13:02:27.149: INFO: (9) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 12.738349ms)
    Mar  2 13:02:27.150: INFO: (9) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 13.820889ms)
    Mar  2 13:02:27.150: INFO: (9) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 14.487525ms)
    Mar  2 13:02:27.150: INFO: (9) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 14.051147ms)
    Mar  2 13:02:27.150: INFO: (9) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 14.687991ms)
    Mar  2 13:02:27.160: INFO: (10) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 8.762456ms)
    Mar  2 13:02:27.161: INFO: (10) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 10.085913ms)
    Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 11.182883ms)
    Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.053676ms)
    Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.457135ms)
    Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 11.634281ms)
    Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 11.491424ms)
    Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 11.555073ms)
    Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 11.754032ms)
    Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 11.418537ms)
    Mar  2 13:02:27.162: INFO: (10) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 12.003997ms)
    Mar  2 13:02:27.164: INFO: (10) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 12.829015ms)
    Mar  2 13:02:27.164: INFO: (10) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 13.209868ms)
    Mar  2 13:02:27.164: INFO: (10) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 12.9437ms)
    Mar  2 13:02:27.164: INFO: (10) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 13.066868ms)
    Mar  2 13:02:27.164: INFO: (10) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 13.486484ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.091087ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 15.852908ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 16.121418ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 16.195147ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 16.826168ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 16.573633ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 15.945422ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 16.527224ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 16.265466ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 17.206953ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 16.825822ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 16.315221ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 16.377887ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 17.390056ms)
    Mar  2 13:02:27.184: INFO: (11) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 16.654804ms)
    Mar  2 13:02:27.185: INFO: (11) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 17.366943ms)
    Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 10.434313ms)
    Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 9.719282ms)
    Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 10.468692ms)
    Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 10.580085ms)
    Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 10.377621ms)
    Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 10.81044ms)
    Mar  2 13:02:27.196: INFO: (12) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 10.54094ms)
    Mar  2 13:02:27.197: INFO: (12) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 10.736875ms)
    Mar  2 13:02:27.197: INFO: (12) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 10.749636ms)
    Mar  2 13:02:27.200: INFO: (12) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 13.702809ms)
    Mar  2 13:02:27.200: INFO: (12) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 13.697867ms)
    Mar  2 13:02:27.201: INFO: (12) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 14.50131ms)
    Mar  2 13:02:27.201: INFO: (12) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 14.669798ms)
    Mar  2 13:02:27.201: INFO: (12) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 14.831691ms)
    Mar  2 13:02:27.201: INFO: (12) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 14.822398ms)
    Mar  2 13:02:27.203: INFO: (12) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 16.327604ms)
    Mar  2 13:02:27.207: INFO: (13) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 4.579295ms)
    Mar  2 13:02:27.215: INFO: (13) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 12.214627ms)
    Mar  2 13:02:27.216: INFO: (13) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 12.143058ms)
    Mar  2 13:02:27.216: INFO: (13) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 12.207591ms)
    Mar  2 13:02:27.216: INFO: (13) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 11.955553ms)
    Mar  2 13:02:27.217: INFO: (13) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 12.342371ms)
    Mar  2 13:02:27.217: INFO: (13) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 13.068636ms)
    Mar  2 13:02:27.217: INFO: (13) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 13.666509ms)
    Mar  2 13:02:27.217: INFO: (13) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 13.965019ms)
    Mar  2 13:02:27.217: INFO: (13) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 13.468258ms)
    Mar  2 13:02:27.220: INFO: (13) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.307798ms)
    Mar  2 13:02:27.220: INFO: (13) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 16.596237ms)
    Mar  2 13:02:27.220: INFO: (13) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 16.570406ms)
    Mar  2 13:02:27.220: INFO: (13) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 16.34168ms)
    Mar  2 13:02:27.221: INFO: (13) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 17.471963ms)
    Mar  2 13:02:27.222: INFO: (13) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 17.613714ms)
    Mar  2 13:02:27.230: INFO: (14) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 7.477401ms)
    Mar  2 13:02:27.230: INFO: (14) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 6.946741ms)
    Mar  2 13:02:27.230: INFO: (14) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 7.665253ms)
    Mar  2 13:02:27.230: INFO: (14) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 8.088671ms)
    Mar  2 13:02:27.232: INFO: (14) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 9.780477ms)
    Mar  2 13:02:27.232: INFO: (14) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 10.068799ms)
    Mar  2 13:02:27.235: INFO: (14) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 12.223248ms)
    Mar  2 13:02:27.235: INFO: (14) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 12.421057ms)
    Mar  2 13:02:27.236: INFO: (14) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 13.249556ms)
    Mar  2 13:02:27.236: INFO: (14) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 13.357946ms)
    Mar  2 13:02:27.236: INFO: (14) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 13.450381ms)
    Mar  2 13:02:27.239: INFO: (14) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 15.990271ms)
    Mar  2 13:02:27.239: INFO: (14) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.494697ms)
    Mar  2 13:02:27.240: INFO: (14) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 17.356351ms)
    Mar  2 13:02:27.240: INFO: (14) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 17.035849ms)
    Mar  2 13:02:27.240: INFO: (14) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 17.357465ms)
    Mar  2 13:02:27.249: INFO: (15) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 8.936163ms)
    Mar  2 13:02:27.251: INFO: (15) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 10.182738ms)
    Mar  2 13:02:27.251: INFO: (15) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 10.883816ms)
    Mar  2 13:02:27.251: INFO: (15) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 10.699063ms)
    Mar  2 13:02:27.251: INFO: (15) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 11.012851ms)
    Mar  2 13:02:27.251: INFO: (15) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.403107ms)
    Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 11.683951ms)
    Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.415015ms)
    Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 11.454291ms)
    Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 12.088795ms)
    Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 11.912286ms)
    Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 11.661387ms)
    Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 12.353684ms)
    Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 11.940243ms)
    Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 12.042159ms)
    Mar  2 13:02:27.252: INFO: (15) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 11.935733ms)
    Mar  2 13:02:27.259: INFO: (16) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 5.166593ms)
    Mar  2 13:02:27.260: INFO: (16) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 5.714808ms)
    Mar  2 13:02:27.260: INFO: (16) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 6.12715ms)
    Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 10.381578ms)
    Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 10.346165ms)
    Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 10.619994ms)
    Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 10.515171ms)
    Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 11.042044ms)
    Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 10.42318ms)
    Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 11.416325ms)
    Mar  2 13:02:27.265: INFO: (16) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 11.076487ms)
    Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 11.109076ms)
    Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 11.492274ms)
    Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 11.296976ms)
    Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 11.470281ms)
    Mar  2 13:02:27.266: INFO: (16) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 12.338945ms)
    Mar  2 13:02:27.275: INFO: (17) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 8.428796ms)
    Mar  2 13:02:27.279: INFO: (17) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 13.1484ms)
    Mar  2 13:02:27.279: INFO: (17) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 12.591032ms)
    Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 13.199611ms)
    Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 13.110106ms)
    Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 13.197199ms)
    Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 13.558795ms)
    Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 13.861045ms)
    Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 13.984693ms)
    Mar  2 13:02:27.280: INFO: (17) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 14.111045ms)
    Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 16.496296ms)
    Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 16.614529ms)
    Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 16.667738ms)
    Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 16.775738ms)
    Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 16.515745ms)
    Mar  2 13:02:27.283: INFO: (17) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 17.18221ms)
    Mar  2 13:02:27.292: INFO: (18) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 8.101582ms)
    Mar  2 13:02:27.293: INFO: (18) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 7.789585ms)
    Mar  2 13:02:27.293: INFO: (18) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 7.892452ms)
    Mar  2 13:02:27.293: INFO: (18) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 9.38098ms)
    Mar  2 13:02:27.293: INFO: (18) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 9.155605ms)
    Mar  2 13:02:27.294: INFO: (18) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 10.093423ms)
    Mar  2 13:02:27.294: INFO: (18) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 9.733112ms)
    Mar  2 13:02:27.294: INFO: (18) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 9.067682ms)
    Mar  2 13:02:27.295: INFO: (18) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 10.696678ms)
    Mar  2 13:02:27.295: INFO: (18) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 10.268029ms)
    Mar  2 13:02:27.295: INFO: (18) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 10.582156ms)
    Mar  2 13:02:27.295: INFO: (18) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 11.137785ms)
    Mar  2 13:02:27.296: INFO: (18) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 11.435722ms)
    Mar  2 13:02:27.296: INFO: (18) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 11.47551ms)
    Mar  2 13:02:27.296: INFO: (18) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 11.491746ms)
    Mar  2 13:02:27.297: INFO: (18) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 12.102347ms)
    Mar  2 13:02:27.304: INFO: (19) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 6.739899ms)
    Mar  2 13:02:27.304: INFO: (19) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 6.585696ms)
    Mar  2 13:02:27.307: INFO: (19) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname2/proxy/: bar (200; 9.755931ms)
    Mar  2 13:02:27.307: INFO: (19) /api/v1/namespaces/proxy-285/services/proxy-service-fv7tp:portname1/proxy/: foo (200; 9.43037ms)
    Mar  2 13:02:27.314: INFO: (19) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb/proxy/rewriteme">test</a> (200; 13.005815ms)
    Mar  2 13:02:27.314: INFO: (19) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname2/proxy/: bar (200; 16.579247ms)
    Mar  2 13:02:27.314: INFO: (19) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:460/proxy/: tls baz (200; 12.719249ms)
    Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">t... (200; 13.128448ms)
    Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/proxy-service-fv7tp-9j6qb:1080/proxy/rewriteme">test</... (200; 13.474643ms)
    Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:160/proxy/: foo (200; 13.525968ms)
    Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/http:proxy-service-fv7tp-9j6qb:162/proxy/: bar (200; 18.24877ms)
    Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/: <a href="/api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:443/proxy/tlsrewriteme... (200; 14.509573ms)
    Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/pods/https:proxy-service-fv7tp-9j6qb:462/proxy/: tls qux (200; 14.438761ms)
    Mar  2 13:02:27.315: INFO: (19) /api/v1/namespaces/proxy-285/services/http:proxy-service-fv7tp:portname1/proxy/: foo (200; 14.396168ms)
    Mar  2 13:02:27.316: INFO: (19) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname1/proxy/: tls baz (200; 14.599668ms)
    Mar  2 13:02:27.316: INFO: (19) /api/v1/namespaces/proxy-285/services/https:proxy-service-fv7tp:tlsportname2/proxy/: tls qux (200; 14.650832ms)
    STEP: deleting ReplicationController proxy-service-fv7tp in namespace proxy-285, will wait for the garbage collector to delete the pods 03/02/23 13:02:27.316
    Mar  2 13:02:27.383: INFO: Deleting ReplicationController proxy-service-fv7tp took: 12.976399ms
    Mar  2 13:02:27.485: INFO: Terminating ReplicationController proxy-service-fv7tp pods took: 102.894704ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  2 13:02:31.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-285" for this suite. 03/02/23 13:02:31.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:02:31.142
Mar  2 13:02:31.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename ingressclass 03/02/23 13:02:31.144
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:31.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:31.168
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/02/23 13:02:31.18
STEP: getting /apis/networking.k8s.io 03/02/23 13:02:31.183
STEP: getting /apis/networking.k8s.iov1 03/02/23 13:02:31.187
STEP: creating 03/02/23 13:02:31.188
STEP: getting 03/02/23 13:02:31.228
STEP: listing 03/02/23 13:02:31.232
STEP: watching 03/02/23 13:02:31.234
Mar  2 13:02:31.235: INFO: starting watch
STEP: patching 03/02/23 13:02:31.237
STEP: updating 03/02/23 13:02:31.243
Mar  2 13:02:31.257: INFO: waiting for watch events with expected annotations
Mar  2 13:02:31.257: INFO: saw patched and updated annotations
STEP: deleting 03/02/23 13:02:31.257
STEP: deleting a collection 03/02/23 13:02:31.269
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Mar  2 13:02:31.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7716" for this suite. 03/02/23 13:02:31.357
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":82,"skipped":1423,"failed":0}
------------------------------
â€¢ [0.221 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:02:31.142
    Mar  2 13:02:31.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename ingressclass 03/02/23 13:02:31.144
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:31.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:31.168
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/02/23 13:02:31.18
    STEP: getting /apis/networking.k8s.io 03/02/23 13:02:31.183
    STEP: getting /apis/networking.k8s.iov1 03/02/23 13:02:31.187
    STEP: creating 03/02/23 13:02:31.188
    STEP: getting 03/02/23 13:02:31.228
    STEP: listing 03/02/23 13:02:31.232
    STEP: watching 03/02/23 13:02:31.234
    Mar  2 13:02:31.235: INFO: starting watch
    STEP: patching 03/02/23 13:02:31.237
    STEP: updating 03/02/23 13:02:31.243
    Mar  2 13:02:31.257: INFO: waiting for watch events with expected annotations
    Mar  2 13:02:31.257: INFO: saw patched and updated annotations
    STEP: deleting 03/02/23 13:02:31.257
    STEP: deleting a collection 03/02/23 13:02:31.269
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Mar  2 13:02:31.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-7716" for this suite. 03/02/23 13:02:31.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:02:31.365
Mar  2 13:02:31.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename deployment 03/02/23 13:02:31.367
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:31.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:31.392
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar  2 13:02:31.402: INFO: Creating deployment "webserver-deployment"
Mar  2 13:02:31.413: INFO: Waiting for observed generation 1
Mar  2 13:02:33.447: INFO: Waiting for all required pods to come up
Mar  2 13:02:33.466: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/02/23 13:02:33.466
Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xhpzt" in namespace "deployment-7135" to be "running"
Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rv97z" in namespace "deployment-7135" to be "running"
Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-svg5d" in namespace "deployment-7135" to be "running"
Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-c8bdl" in namespace "deployment-7135" to be "running"
Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hpjxk" in namespace "deployment-7135" to be "running"
Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-l8mzd" in namespace "deployment-7135" to be "running"
Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-p422l" in namespace "deployment-7135" to be "running"
Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-sshgl" in namespace "deployment-7135" to be "running"
Mar  2 13:02:33.558: INFO: Pod "webserver-deployment-845c8977d9-xhpzt": Phase="Pending", Reason="", readiness=false. Elapsed: 89.506512ms
Mar  2 13:02:33.559: INFO: Pod "webserver-deployment-845c8977d9-rv97z": Phase="Pending", Reason="", readiness=false. Elapsed: 89.709442ms
Mar  2 13:02:33.598: INFO: Pod "webserver-deployment-845c8977d9-l8mzd": Phase="Pending", Reason="", readiness=false. Elapsed: 40.386171ms
Mar  2 13:02:33.598: INFO: Pod "webserver-deployment-845c8977d9-sshgl": Phase="Pending", Reason="", readiness=false. Elapsed: 40.356222ms
Mar  2 13:02:33.599: INFO: Pod "webserver-deployment-845c8977d9-p422l": Phase="Pending", Reason="", readiness=false. Elapsed: 40.538377ms
Mar  2 13:02:33.599: INFO: Pod "webserver-deployment-845c8977d9-hpjxk": Phase="Pending", Reason="", readiness=false. Elapsed: 128.876486ms
Mar  2 13:02:33.599: INFO: Pod "webserver-deployment-845c8977d9-c8bdl": Phase="Pending", Reason="", readiness=false. Elapsed: 129.131807ms
Mar  2 13:02:33.599: INFO: Pod "webserver-deployment-845c8977d9-svg5d": Phase="Pending", Reason="", readiness=false. Elapsed: 129.571401ms
Mar  2 13:02:35.569: INFO: Pod "webserver-deployment-845c8977d9-rv97z": Phase="Running", Reason="", readiness=true. Elapsed: 2.099531466s
Mar  2 13:02:35.569: INFO: Pod "webserver-deployment-845c8977d9-rv97z" satisfied condition "running"
Mar  2 13:02:35.594: INFO: Pod "webserver-deployment-845c8977d9-xhpzt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.124778367s
Mar  2 13:02:35.621: INFO: Pod "webserver-deployment-845c8977d9-l8mzd": Phase="Running", Reason="", readiness=true. Elapsed: 2.063190394s
Mar  2 13:02:35.621: INFO: Pod "webserver-deployment-845c8977d9-l8mzd" satisfied condition "running"
Mar  2 13:02:35.621: INFO: Pod "webserver-deployment-845c8977d9-svg5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.151962043s
Mar  2 13:02:35.621: INFO: Pod "webserver-deployment-845c8977d9-svg5d" satisfied condition "running"
Mar  2 13:02:35.627: INFO: Pod "webserver-deployment-845c8977d9-p422l": Phase="Running", Reason="", readiness=true. Elapsed: 2.068555459s
Mar  2 13:02:35.627: INFO: Pod "webserver-deployment-845c8977d9-p422l" satisfied condition "running"
Mar  2 13:02:35.627: INFO: Pod "webserver-deployment-845c8977d9-c8bdl": Phase="Running", Reason="", readiness=true. Elapsed: 2.157593916s
Mar  2 13:02:35.627: INFO: Pod "webserver-deployment-845c8977d9-c8bdl" satisfied condition "running"
Mar  2 13:02:35.627: INFO: Pod "webserver-deployment-845c8977d9-hpjxk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.157635003s
Mar  2 13:02:35.628: INFO: Pod "webserver-deployment-845c8977d9-sshgl": Phase="Running", Reason="", readiness=true. Elapsed: 2.06993377s
Mar  2 13:02:35.628: INFO: Pod "webserver-deployment-845c8977d9-sshgl" satisfied condition "running"
Mar  2 13:02:37.564: INFO: Pod "webserver-deployment-845c8977d9-xhpzt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09469113s
Mar  2 13:02:37.617: INFO: Pod "webserver-deployment-845c8977d9-hpjxk": Phase="Running", Reason="", readiness=true. Elapsed: 4.146829762s
Mar  2 13:02:37.617: INFO: Pod "webserver-deployment-845c8977d9-hpjxk" satisfied condition "running"
Mar  2 13:02:39.566: INFO: Pod "webserver-deployment-845c8977d9-xhpzt": Phase="Running", Reason="", readiness=true. Elapsed: 6.097216024s
Mar  2 13:02:39.566: INFO: Pod "webserver-deployment-845c8977d9-xhpzt" satisfied condition "running"
Mar  2 13:02:39.567: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  2 13:02:39.577: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  2 13:02:39.586: INFO: Updating deployment webserver-deployment
Mar  2 13:02:39.586: INFO: Waiting for observed generation 2
Mar  2 13:02:41.614: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  2 13:02:41.619: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  2 13:02:41.626: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 13:02:41.635: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  2 13:02:41.635: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  2 13:02:41.638: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 13:02:41.643: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  2 13:02:41.643: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  2 13:02:41.651: INFO: Updating deployment webserver-deployment
Mar  2 13:02:41.652: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  2 13:02:41.659: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  2 13:02:41.670: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 13:02:41.762: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7135  22ff50da-c0ee-425a-b457-e038d934ae76 1932156 3 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00345a808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-02 13:02:39 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 13:02:41 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  2 13:02:41.847: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-7135  eaaabedf-125e-4ff5-8535-32a1891cde5c 1932128 3 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 22ff50da-c0ee-425a-b457-e038d934ae76 0xc00360fc17 0xc00360fc18}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22ff50da-c0ee-425a-b457-e038d934ae76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00360fd18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 13:02:41.847: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  2 13:02:41.847: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-7135  07b02358-b599-4016-9b25-0976e66b91da 1932126 3 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 22ff50da-c0ee-425a-b457-e038d934ae76 0xc00360fe57 0xc00360fe58}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22ff50da-c0ee-425a-b457-e038d934ae76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00360ffc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  2 13:02:41.879: INFO: Pod "webserver-deployment-69b7448995-477mp" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-477mp webserver-deployment-69b7448995- deployment-7135  7d7bca25-58b1-4552-af71-95b96744c750 1932177 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e186a7 0xc003e186a8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6r6b5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6r6b5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.61,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.880: INFO: Pod "webserver-deployment-69b7448995-56677" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-56677 webserver-deployment-69b7448995- deployment-7135  606f6a60-5c6e-4aca-9582-de26b6106eee 1932099 0 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f8699797e20c5a237c613e65e95eb529caabc24601f9f9cac8393363addb461d cni.projectcalico.org/podIP:10.233.123.29/32 cni.projectcalico.org/podIPs:10.233.123.29/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e18977 0xc003e18978}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 13:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mqhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mqhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.61,PodIP:,StartTime:2023-03-02 13:02:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.880: INFO: Pod "webserver-deployment-69b7448995-6l8vg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6l8vg webserver-deployment-69b7448995- deployment-7135  1711cb6e-fb63-407f-892a-20954a8e777e 1932107 0 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4283cca1abe8080a146bf796ea3916dd25a7dca62c4d28836cc7223248c1e81e cni.projectcalico.org/podIP:10.233.123.100/32 cni.projectcalico.org/podIPs:10.233.123.100/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e18c17 0xc003e18c18}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 13:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rfr2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfr2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:,StartTime:2023-03-02 13:02:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.880: INFO: Pod "webserver-deployment-69b7448995-79jr9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-79jr9 webserver-deployment-69b7448995- deployment-7135  24b52f38-2a12-4380-bf9f-4c9e9bf26757 1932119 0 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:95c651bd95a477e54a90c6effd11d2ea5318b12991021c0483cc2b31d919787f cni.projectcalico.org/podIP:10.233.123.110/32 cni.projectcalico.org/podIPs:10.233.123.110/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e18f77 0xc003e18f78}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 13:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hh2z4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hh2z4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:,StartTime:2023-03-02 13:02:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.881: INFO: Pod "webserver-deployment-69b7448995-8hrsz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-8hrsz webserver-deployment-69b7448995- deployment-7135  44718aca-2759-48cc-ba1d-4d1d82eebf0a 1932110 0 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:eb28cec9d9344582cf8e31e3461b2508ac902c997d1de52cac5645ca2e8c65f3 cni.projectcalico.org/podIP:10.233.126.68/32 cni.projectcalico.org/podIPs:10.233.126.68/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e19477 0xc003e19478}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 13:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85fjx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85fjx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:,StartTime:2023-03-02 13:02:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.882: INFO: Pod "webserver-deployment-69b7448995-9gxrk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-9gxrk webserver-deployment-69b7448995- deployment-7135  223a1da4-1c1c-468f-9bd9-a9d5e14c5bc4 1932167 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e19aa7 0xc003e19aa8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5jbvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5jbvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.882: INFO: Pod "webserver-deployment-69b7448995-bbvtx" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-bbvtx webserver-deployment-69b7448995- deployment-7135  ce4f7407-cc4a-4c1e-80a0-43d1a4336d48 1932100 0 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4ef30a5c70c5bcc16388deec4a3f8147ed7e8100fe02d87270397c6a9fbb468e cni.projectcalico.org/podIP:10.233.92.115/32 cni.projectcalico.org/podIPs:10.233.92.115/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e19db7 0xc003e19db8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 13:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bm8jk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bm8jk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.138,PodIP:,StartTime:2023-03-02 13:02:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.883: INFO: Pod "webserver-deployment-69b7448995-bjrbt" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-bjrbt webserver-deployment-69b7448995- deployment-7135  b2d161fd-05fa-4439-9824-af6c58ff0b75 1932168 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc000c1c167 0xc000c1c168}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlc7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlc7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.883: INFO: Pod "webserver-deployment-69b7448995-hsfk9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hsfk9 webserver-deployment-69b7448995- deployment-7135  a36fb85a-3567-432e-8e74-aaf96800dc9d 1932189 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc000c1c9a7 0xc000c1c9a8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4jsln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4jsln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.883: INFO: Pod "webserver-deployment-69b7448995-jv5z2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jv5z2 webserver-deployment-69b7448995- deployment-7135  185ddc4a-c407-4c58-bf9a-feacab6c0f93 1932169 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc000c1d5e7 0xc000c1d5e8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9dvms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9dvms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.935: INFO: Pod "webserver-deployment-69b7448995-kdnm4" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kdnm4 webserver-deployment-69b7448995- deployment-7135  c2b8e7b3-7d97-4508-97cc-8611aa125ddc 1932188 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc000c1dc87 0xc000c1dc88}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t44s8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t44s8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.138,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.935: INFO: Pod "webserver-deployment-69b7448995-t9hmz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-t9hmz webserver-deployment-69b7448995- deployment-7135  c63afebd-cca1-43dd-9bd8-fad3f6d757c6 1932166 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc0034c6c27 0xc0034c6c28}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wvs8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wvs8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.936: INFO: Pod "webserver-deployment-69b7448995-wz7br" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-wz7br webserver-deployment-69b7448995- deployment-7135  5ac53ad6-cb41-42aa-97b6-5e1b959e4630 1932190 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc0034c6ef7 0xc0034c6ef8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf9hw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf9hw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.936: INFO: Pod "webserver-deployment-845c8977d9-6zd6z" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6zd6z webserver-deployment-845c8977d9- deployment-7135  5eda7566-418f-4cf6-a2ce-8ecbac39870b 1932184 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0034c7307 0xc0034c7308}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdt9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdt9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.936: INFO: Pod "webserver-deployment-845c8977d9-7kx4p" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7kx4p webserver-deployment-845c8977d9- deployment-7135  3cea21e5-a522-45cb-b5b1-6bf7e7a354f2 1932187 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0034c7537 0xc0034c7538}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grm9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grm9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.938: INFO: Pod "webserver-deployment-845c8977d9-84r9c" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-84r9c webserver-deployment-845c8977d9- deployment-7135  ebad4406-2a3b-46ca-a21b-9826ebd01082 1932191 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0034c7837 0xc0034c7838}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6dsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6dsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.938: INFO: Pod "webserver-deployment-845c8977d9-ct7sq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ct7sq webserver-deployment-845c8977d9- deployment-7135  bd7e9164-978a-420c-880f-1fd5c7c07ce6 1932155 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0034c7bc7 0xc0034c7bc8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqk26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqk26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.939: INFO: Pod "webserver-deployment-845c8977d9-g96qh" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-g96qh webserver-deployment-845c8977d9- deployment-7135  2c045091-0af4-4cad-8fbb-4383d68fb46d 1932159 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003230217 0xc003230218}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rq67z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rq67z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.940: INFO: Pod "webserver-deployment-845c8977d9-hpjxk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hpjxk webserver-deployment-845c8977d9- deployment-7135  b67a8f20-d769-4616-a4c7-b6b6e147d374 1932006 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9f0f3a1cddae6502abc486089ae18099887e411049770b7270ecbb2bb5ba073d cni.projectcalico.org/podIP:10.233.92.83/32 cni.projectcalico.org/podIPs:10.233.92.83/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0032303d7 0xc0032303d8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.92.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bp4h7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bp4h7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.138,PodIP:10.233.92.83,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f04599547f4f42cf05d0462ffcc1074217f39331e109813a039c614f7f9d4c34,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.92.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.940: INFO: Pod "webserver-deployment-845c8977d9-j2xt9" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-j2xt9 webserver-deployment-845c8977d9- deployment-7135  5aada907-b90f-4510-8638-926350780228 1932161 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0032306d7 0xc0032306d8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzrsd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzrsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.941: INFO: Pod "webserver-deployment-845c8977d9-kqtlj" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kqtlj webserver-deployment-845c8977d9- deployment-7135  55ab8442-1423-4298-adff-e6e1adcb1709 1932160 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003230947 0xc003230948}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvsxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvsxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.948: INFO: Pod "webserver-deployment-845c8977d9-l8mzd" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-l8mzd webserver-deployment-845c8977d9- deployment-7135  15664361-be64-4b89-b06c-f369a6583623 1931961 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1ca34a50ad332a1f116dfb72c7922bcde117d19a5a434f418ee8fc9c7cc203bb cni.projectcalico.org/podIP:10.233.123.65/32 cni.projectcalico.org/podIPs:10.233.123.65/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003230c47 0xc003230c48}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mjkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mjkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.65,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b73a96d2a326edfee120894b8a94cbedafb78b9a8a8557daff6a992f1d4cbfea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.949: INFO: Pod "webserver-deployment-845c8977d9-lc27n" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lc27n webserver-deployment-845c8977d9- deployment-7135  ab42670e-7df4-4c2e-988c-244d823a7b30 1932185 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003230f27 0xc003230f28}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6sh4f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6sh4f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.954: INFO: Pod "webserver-deployment-845c8977d9-lzlp2" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lzlp2 webserver-deployment-845c8977d9- deployment-7135  ac77695a-9c85-4ae5-8102-5b76be55e54b 1932186 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003231157 0xc003231158}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8q8vq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8q8vq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.955: INFO: Pod "webserver-deployment-845c8977d9-p422l" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-p422l webserver-deployment-845c8977d9- deployment-7135  e77cb0aa-18bb-41a0-ab02-90d196a3aa02 1931952 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:34d5377c45ff401ca2b50447f2d5809428edffd4b93b8d4234baaa04229ed3bd cni.projectcalico.org/podIP:10.233.126.117/32 cni.projectcalico.org/podIPs:10.233.126.117/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce01a7 0xc003ce01a8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.126.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wdptw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wdptw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:10.233.126.117,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://73d34d3a7333646497d9b99830e7cb80d70a09b334c551f839b50909ba051538,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.126.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.955: INFO: Pod "webserver-deployment-845c8977d9-qbndr" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qbndr webserver-deployment-845c8977d9- deployment-7135  c27e8bd7-56f1-4a5c-8b93-476f771c6dd1 1931919 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d297123ff3243cb1b1aa4a5529f391a24383dab775bfa67619000d554c5aa5eb cni.projectcalico.org/podIP:10.233.123.32/32 cni.projectcalico.org/podIPs:10.233.123.32/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce05e7 0xc003ce05e8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fcc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fcc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.61,PodIP:10.233.123.32,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e2d06dfbf59d905b837dff1efe9e294071a53c3408fa055947b31d97cf153cc8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.955: INFO: Pod "webserver-deployment-845c8977d9-rdvwj" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rdvwj webserver-deployment-845c8977d9- deployment-7135  b217ab97-344d-477a-a1c4-72513bd6b701 1932175 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce0ad7 0xc003ce0ad8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4krz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4krz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.956: INFO: Pod "webserver-deployment-845c8977d9-rv97z" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rv97z webserver-deployment-845c8977d9- deployment-7135  97fac98d-e6e5-4108-b95c-2cfaa575678c 1931958 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3b9cea19a863dfba08c5bc3c41ec4ea5783ba031641273fb11a6d175c76d89a9 cni.projectcalico.org/podIP:10.233.123.123/32 cni.projectcalico.org/podIPs:10.233.123.123/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce0f37 0xc003ce0f38}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7hzd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7hzd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.123,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://74d5bd551149f83dc6d3a0de938b5a3c01f38f0d390a7cc7fed29f5e70a615e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.956: INFO: Pod "webserver-deployment-845c8977d9-sshgl" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-sshgl webserver-deployment-845c8977d9- deployment-7135  88ce4e88-0370-4499-8c6c-66d69a6ed8e4 1931948 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f1363f649b24f630269bdfa59c1b87a21f45de9f9e0d9b407fbee9c6a04b9deb cni.projectcalico.org/podIP:10.233.126.70/32 cni.projectcalico.org/podIPs:10.233.126.70/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce13c7 0xc003ce13c8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.126.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jd4nj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jd4nj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:10.233.126.70,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5778c2c262e5100430e8f0e1e76d3418dc513af5c1d1688ca5a35502bcb16b39,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.126.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.956: INFO: Pod "webserver-deployment-845c8977d9-v6llk" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-v6llk webserver-deployment-845c8977d9- deployment-7135  7cecfc1c-811c-4cdc-8cc5-7d06641c0054 1932164 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce1837 0xc003ce1838}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2rfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2rfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.957: INFO: Pod "webserver-deployment-845c8977d9-v7pxx" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-v7pxx webserver-deployment-845c8977d9- deployment-7135  6af28964-a71b-4b74-97fb-4e14df1e1808 1931921 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4825732bb6ee95a5aa8f500e37966a910040d05067b8d6c3b559814b2a22ec85 cni.projectcalico.org/podIP:10.233.123.30/32 cni.projectcalico.org/podIPs:10.233.123.30/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce1c27 0xc003ce1c28}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lgwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lgwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.61,PodIP:10.233.123.30,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7f458312336205332609049a976150d0cad10f03eb4c97c9f564bcb72941a1d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.957: INFO: Pod "webserver-deployment-845c8977d9-xhpzt" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xhpzt webserver-deployment-845c8977d9- deployment-7135  47f8e05e-bfeb-4fd7-aa59-696b1bcf2fba 1932030 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:857222d2aec520a49ef0dcd614e66ccfb62fbeb84d7a26c657af27310763b110 cni.projectcalico.org/podIP:10.233.92.108/32 cni.projectcalico.org/podIPs:10.233.92.108/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc001d040b7 0xc001d040b8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.92.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8t4t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8t4t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.138,PodIP:10.233.92.108,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7ecd35ce5f1dbd7f8de1f8da0516c91b420885f27cea77bdbf855196db00263e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.92.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:02:41.958: INFO: Pod "webserver-deployment-845c8977d9-zlb5x" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zlb5x webserver-deployment-845c8977d9- deployment-7135  7eee4dbc-0c69-4a31-862e-c97ac67dbe6e 1932150 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc001d04467 0xc001d04468}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7wbwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7wbwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 13:02:41.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7135" for this suite. 03/02/23 13:02:41.971
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":83,"skipped":1428,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.681 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:02:31.365
    Mar  2 13:02:31.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename deployment 03/02/23 13:02:31.367
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:31.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:31.392
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar  2 13:02:31.402: INFO: Creating deployment "webserver-deployment"
    Mar  2 13:02:31.413: INFO: Waiting for observed generation 1
    Mar  2 13:02:33.447: INFO: Waiting for all required pods to come up
    Mar  2 13:02:33.466: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/02/23 13:02:33.466
    Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xhpzt" in namespace "deployment-7135" to be "running"
    Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rv97z" in namespace "deployment-7135" to be "running"
    Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-svg5d" in namespace "deployment-7135" to be "running"
    Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-c8bdl" in namespace "deployment-7135" to be "running"
    Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hpjxk" in namespace "deployment-7135" to be "running"
    Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-l8mzd" in namespace "deployment-7135" to be "running"
    Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-p422l" in namespace "deployment-7135" to be "running"
    Mar  2 13:02:33.469: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-sshgl" in namespace "deployment-7135" to be "running"
    Mar  2 13:02:33.558: INFO: Pod "webserver-deployment-845c8977d9-xhpzt": Phase="Pending", Reason="", readiness=false. Elapsed: 89.506512ms
    Mar  2 13:02:33.559: INFO: Pod "webserver-deployment-845c8977d9-rv97z": Phase="Pending", Reason="", readiness=false. Elapsed: 89.709442ms
    Mar  2 13:02:33.598: INFO: Pod "webserver-deployment-845c8977d9-l8mzd": Phase="Pending", Reason="", readiness=false. Elapsed: 40.386171ms
    Mar  2 13:02:33.598: INFO: Pod "webserver-deployment-845c8977d9-sshgl": Phase="Pending", Reason="", readiness=false. Elapsed: 40.356222ms
    Mar  2 13:02:33.599: INFO: Pod "webserver-deployment-845c8977d9-p422l": Phase="Pending", Reason="", readiness=false. Elapsed: 40.538377ms
    Mar  2 13:02:33.599: INFO: Pod "webserver-deployment-845c8977d9-hpjxk": Phase="Pending", Reason="", readiness=false. Elapsed: 128.876486ms
    Mar  2 13:02:33.599: INFO: Pod "webserver-deployment-845c8977d9-c8bdl": Phase="Pending", Reason="", readiness=false. Elapsed: 129.131807ms
    Mar  2 13:02:33.599: INFO: Pod "webserver-deployment-845c8977d9-svg5d": Phase="Pending", Reason="", readiness=false. Elapsed: 129.571401ms
    Mar  2 13:02:35.569: INFO: Pod "webserver-deployment-845c8977d9-rv97z": Phase="Running", Reason="", readiness=true. Elapsed: 2.099531466s
    Mar  2 13:02:35.569: INFO: Pod "webserver-deployment-845c8977d9-rv97z" satisfied condition "running"
    Mar  2 13:02:35.594: INFO: Pod "webserver-deployment-845c8977d9-xhpzt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.124778367s
    Mar  2 13:02:35.621: INFO: Pod "webserver-deployment-845c8977d9-l8mzd": Phase="Running", Reason="", readiness=true. Elapsed: 2.063190394s
    Mar  2 13:02:35.621: INFO: Pod "webserver-deployment-845c8977d9-l8mzd" satisfied condition "running"
    Mar  2 13:02:35.621: INFO: Pod "webserver-deployment-845c8977d9-svg5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.151962043s
    Mar  2 13:02:35.621: INFO: Pod "webserver-deployment-845c8977d9-svg5d" satisfied condition "running"
    Mar  2 13:02:35.627: INFO: Pod "webserver-deployment-845c8977d9-p422l": Phase="Running", Reason="", readiness=true. Elapsed: 2.068555459s
    Mar  2 13:02:35.627: INFO: Pod "webserver-deployment-845c8977d9-p422l" satisfied condition "running"
    Mar  2 13:02:35.627: INFO: Pod "webserver-deployment-845c8977d9-c8bdl": Phase="Running", Reason="", readiness=true. Elapsed: 2.157593916s
    Mar  2 13:02:35.627: INFO: Pod "webserver-deployment-845c8977d9-c8bdl" satisfied condition "running"
    Mar  2 13:02:35.627: INFO: Pod "webserver-deployment-845c8977d9-hpjxk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.157635003s
    Mar  2 13:02:35.628: INFO: Pod "webserver-deployment-845c8977d9-sshgl": Phase="Running", Reason="", readiness=true. Elapsed: 2.06993377s
    Mar  2 13:02:35.628: INFO: Pod "webserver-deployment-845c8977d9-sshgl" satisfied condition "running"
    Mar  2 13:02:37.564: INFO: Pod "webserver-deployment-845c8977d9-xhpzt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09469113s
    Mar  2 13:02:37.617: INFO: Pod "webserver-deployment-845c8977d9-hpjxk": Phase="Running", Reason="", readiness=true. Elapsed: 4.146829762s
    Mar  2 13:02:37.617: INFO: Pod "webserver-deployment-845c8977d9-hpjxk" satisfied condition "running"
    Mar  2 13:02:39.566: INFO: Pod "webserver-deployment-845c8977d9-xhpzt": Phase="Running", Reason="", readiness=true. Elapsed: 6.097216024s
    Mar  2 13:02:39.566: INFO: Pod "webserver-deployment-845c8977d9-xhpzt" satisfied condition "running"
    Mar  2 13:02:39.567: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar  2 13:02:39.577: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar  2 13:02:39.586: INFO: Updating deployment webserver-deployment
    Mar  2 13:02:39.586: INFO: Waiting for observed generation 2
    Mar  2 13:02:41.614: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar  2 13:02:41.619: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar  2 13:02:41.626: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar  2 13:02:41.635: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar  2 13:02:41.635: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar  2 13:02:41.638: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar  2 13:02:41.643: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar  2 13:02:41.643: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar  2 13:02:41.651: INFO: Updating deployment webserver-deployment
    Mar  2 13:02:41.652: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar  2 13:02:41.659: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar  2 13:02:41.670: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 13:02:41.762: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-7135  22ff50da-c0ee-425a-b457-e038d934ae76 1932156 3 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00345a808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-02 13:02:39 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 13:02:41 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Mar  2 13:02:41.847: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-7135  eaaabedf-125e-4ff5-8535-32a1891cde5c 1932128 3 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 22ff50da-c0ee-425a-b457-e038d934ae76 0xc00360fc17 0xc00360fc18}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22ff50da-c0ee-425a-b457-e038d934ae76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00360fd18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 13:02:41.847: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar  2 13:02:41.847: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-7135  07b02358-b599-4016-9b25-0976e66b91da 1932126 3 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 22ff50da-c0ee-425a-b457-e038d934ae76 0xc00360fe57 0xc00360fe58}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22ff50da-c0ee-425a-b457-e038d934ae76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00360ffc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 13:02:41.879: INFO: Pod "webserver-deployment-69b7448995-477mp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-477mp webserver-deployment-69b7448995- deployment-7135  7d7bca25-58b1-4552-af71-95b96744c750 1932177 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e186a7 0xc003e186a8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6r6b5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6r6b5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.61,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.880: INFO: Pod "webserver-deployment-69b7448995-56677" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-56677 webserver-deployment-69b7448995- deployment-7135  606f6a60-5c6e-4aca-9582-de26b6106eee 1932099 0 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f8699797e20c5a237c613e65e95eb529caabc24601f9f9cac8393363addb461d cni.projectcalico.org/podIP:10.233.123.29/32 cni.projectcalico.org/podIPs:10.233.123.29/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e18977 0xc003e18978}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 13:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mqhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mqhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.61,PodIP:,StartTime:2023-03-02 13:02:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.880: INFO: Pod "webserver-deployment-69b7448995-6l8vg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6l8vg webserver-deployment-69b7448995- deployment-7135  1711cb6e-fb63-407f-892a-20954a8e777e 1932107 0 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4283cca1abe8080a146bf796ea3916dd25a7dca62c4d28836cc7223248c1e81e cni.projectcalico.org/podIP:10.233.123.100/32 cni.projectcalico.org/podIPs:10.233.123.100/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e18c17 0xc003e18c18}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 13:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rfr2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfr2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:,StartTime:2023-03-02 13:02:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.880: INFO: Pod "webserver-deployment-69b7448995-79jr9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-79jr9 webserver-deployment-69b7448995- deployment-7135  24b52f38-2a12-4380-bf9f-4c9e9bf26757 1932119 0 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:95c651bd95a477e54a90c6effd11d2ea5318b12991021c0483cc2b31d919787f cni.projectcalico.org/podIP:10.233.123.110/32 cni.projectcalico.org/podIPs:10.233.123.110/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e18f77 0xc003e18f78}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 13:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hh2z4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hh2z4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:,StartTime:2023-03-02 13:02:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.881: INFO: Pod "webserver-deployment-69b7448995-8hrsz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-8hrsz webserver-deployment-69b7448995- deployment-7135  44718aca-2759-48cc-ba1d-4d1d82eebf0a 1932110 0 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:eb28cec9d9344582cf8e31e3461b2508ac902c997d1de52cac5645ca2e8c65f3 cni.projectcalico.org/podIP:10.233.126.68/32 cni.projectcalico.org/podIPs:10.233.126.68/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e19477 0xc003e19478}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 13:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85fjx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85fjx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:,StartTime:2023-03-02 13:02:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.882: INFO: Pod "webserver-deployment-69b7448995-9gxrk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-9gxrk webserver-deployment-69b7448995- deployment-7135  223a1da4-1c1c-468f-9bd9-a9d5e14c5bc4 1932167 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e19aa7 0xc003e19aa8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5jbvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5jbvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.882: INFO: Pod "webserver-deployment-69b7448995-bbvtx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-bbvtx webserver-deployment-69b7448995- deployment-7135  ce4f7407-cc4a-4c1e-80a0-43d1a4336d48 1932100 0 2023-03-02 13:02:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4ef30a5c70c5bcc16388deec4a3f8147ed7e8100fe02d87270397c6a9fbb468e cni.projectcalico.org/podIP:10.233.92.115/32 cni.projectcalico.org/podIPs:10.233.92.115/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc003e19db7 0xc003e19db8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 13:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bm8jk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bm8jk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.138,PodIP:,StartTime:2023-03-02 13:02:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.883: INFO: Pod "webserver-deployment-69b7448995-bjrbt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-bjrbt webserver-deployment-69b7448995- deployment-7135  b2d161fd-05fa-4439-9824-af6c58ff0b75 1932168 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc000c1c167 0xc000c1c168}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlc7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlc7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.883: INFO: Pod "webserver-deployment-69b7448995-hsfk9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hsfk9 webserver-deployment-69b7448995- deployment-7135  a36fb85a-3567-432e-8e74-aaf96800dc9d 1932189 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc000c1c9a7 0xc000c1c9a8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4jsln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4jsln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.883: INFO: Pod "webserver-deployment-69b7448995-jv5z2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jv5z2 webserver-deployment-69b7448995- deployment-7135  185ddc4a-c407-4c58-bf9a-feacab6c0f93 1932169 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc000c1d5e7 0xc000c1d5e8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9dvms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9dvms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.935: INFO: Pod "webserver-deployment-69b7448995-kdnm4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kdnm4 webserver-deployment-69b7448995- deployment-7135  c2b8e7b3-7d97-4508-97cc-8611aa125ddc 1932188 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc000c1dc87 0xc000c1dc88}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t44s8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t44s8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.138,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.935: INFO: Pod "webserver-deployment-69b7448995-t9hmz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-t9hmz webserver-deployment-69b7448995- deployment-7135  c63afebd-cca1-43dd-9bd8-fad3f6d757c6 1932166 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc0034c6c27 0xc0034c6c28}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wvs8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wvs8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.936: INFO: Pod "webserver-deployment-69b7448995-wz7br" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-wz7br webserver-deployment-69b7448995- deployment-7135  5ac53ad6-cb41-42aa-97b6-5e1b959e4630 1932190 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 eaaabedf-125e-4ff5-8535-32a1891cde5c 0xc0034c6ef7 0xc0034c6ef8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaaabedf-125e-4ff5-8535-32a1891cde5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf9hw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf9hw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.936: INFO: Pod "webserver-deployment-845c8977d9-6zd6z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6zd6z webserver-deployment-845c8977d9- deployment-7135  5eda7566-418f-4cf6-a2ce-8ecbac39870b 1932184 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0034c7307 0xc0034c7308}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdt9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdt9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.936: INFO: Pod "webserver-deployment-845c8977d9-7kx4p" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7kx4p webserver-deployment-845c8977d9- deployment-7135  3cea21e5-a522-45cb-b5b1-6bf7e7a354f2 1932187 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0034c7537 0xc0034c7538}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grm9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grm9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.938: INFO: Pod "webserver-deployment-845c8977d9-84r9c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-84r9c webserver-deployment-845c8977d9- deployment-7135  ebad4406-2a3b-46ca-a21b-9826ebd01082 1932191 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0034c7837 0xc0034c7838}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6dsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6dsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.938: INFO: Pod "webserver-deployment-845c8977d9-ct7sq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ct7sq webserver-deployment-845c8977d9- deployment-7135  bd7e9164-978a-420c-880f-1fd5c7c07ce6 1932155 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0034c7bc7 0xc0034c7bc8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqk26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqk26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.939: INFO: Pod "webserver-deployment-845c8977d9-g96qh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-g96qh webserver-deployment-845c8977d9- deployment-7135  2c045091-0af4-4cad-8fbb-4383d68fb46d 1932159 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003230217 0xc003230218}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rq67z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rq67z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.940: INFO: Pod "webserver-deployment-845c8977d9-hpjxk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hpjxk webserver-deployment-845c8977d9- deployment-7135  b67a8f20-d769-4616-a4c7-b6b6e147d374 1932006 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9f0f3a1cddae6502abc486089ae18099887e411049770b7270ecbb2bb5ba073d cni.projectcalico.org/podIP:10.233.92.83/32 cni.projectcalico.org/podIPs:10.233.92.83/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0032303d7 0xc0032303d8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.92.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bp4h7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bp4h7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.138,PodIP:10.233.92.83,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f04599547f4f42cf05d0462ffcc1074217f39331e109813a039c614f7f9d4c34,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.92.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.940: INFO: Pod "webserver-deployment-845c8977d9-j2xt9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-j2xt9 webserver-deployment-845c8977d9- deployment-7135  5aada907-b90f-4510-8638-926350780228 1932161 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc0032306d7 0xc0032306d8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzrsd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzrsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.941: INFO: Pod "webserver-deployment-845c8977d9-kqtlj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kqtlj webserver-deployment-845c8977d9- deployment-7135  55ab8442-1423-4298-adff-e6e1adcb1709 1932160 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003230947 0xc003230948}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvsxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvsxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.948: INFO: Pod "webserver-deployment-845c8977d9-l8mzd" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-l8mzd webserver-deployment-845c8977d9- deployment-7135  15664361-be64-4b89-b06c-f369a6583623 1931961 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1ca34a50ad332a1f116dfb72c7922bcde117d19a5a434f418ee8fc9c7cc203bb cni.projectcalico.org/podIP:10.233.123.65/32 cni.projectcalico.org/podIPs:10.233.123.65/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003230c47 0xc003230c48}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mjkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mjkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.65,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b73a96d2a326edfee120894b8a94cbedafb78b9a8a8557daff6a992f1d4cbfea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.949: INFO: Pod "webserver-deployment-845c8977d9-lc27n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lc27n webserver-deployment-845c8977d9- deployment-7135  ab42670e-7df4-4c2e-988c-244d823a7b30 1932185 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003230f27 0xc003230f28}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6sh4f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6sh4f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.954: INFO: Pod "webserver-deployment-845c8977d9-lzlp2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lzlp2 webserver-deployment-845c8977d9- deployment-7135  ac77695a-9c85-4ae5-8102-5b76be55e54b 1932186 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003231157 0xc003231158}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8q8vq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8q8vq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.955: INFO: Pod "webserver-deployment-845c8977d9-p422l" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-p422l webserver-deployment-845c8977d9- deployment-7135  e77cb0aa-18bb-41a0-ab02-90d196a3aa02 1931952 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:34d5377c45ff401ca2b50447f2d5809428edffd4b93b8d4234baaa04229ed3bd cni.projectcalico.org/podIP:10.233.126.117/32 cni.projectcalico.org/podIPs:10.233.126.117/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce01a7 0xc003ce01a8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.126.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wdptw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wdptw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:10.233.126.117,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://73d34d3a7333646497d9b99830e7cb80d70a09b334c551f839b50909ba051538,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.126.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.955: INFO: Pod "webserver-deployment-845c8977d9-qbndr" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qbndr webserver-deployment-845c8977d9- deployment-7135  c27e8bd7-56f1-4a5c-8b93-476f771c6dd1 1931919 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d297123ff3243cb1b1aa4a5529f391a24383dab775bfa67619000d554c5aa5eb cni.projectcalico.org/podIP:10.233.123.32/32 cni.projectcalico.org/podIPs:10.233.123.32/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce05e7 0xc003ce05e8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fcc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fcc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.61,PodIP:10.233.123.32,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e2d06dfbf59d905b837dff1efe9e294071a53c3408fa055947b31d97cf153cc8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.955: INFO: Pod "webserver-deployment-845c8977d9-rdvwj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rdvwj webserver-deployment-845c8977d9- deployment-7135  b217ab97-344d-477a-a1c4-72513bd6b701 1932175 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce0ad7 0xc003ce0ad8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4krz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4krz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.956: INFO: Pod "webserver-deployment-845c8977d9-rv97z" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rv97z webserver-deployment-845c8977d9- deployment-7135  97fac98d-e6e5-4108-b95c-2cfaa575678c 1931958 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3b9cea19a863dfba08c5bc3c41ec4ea5783ba031641273fb11a6d175c76d89a9 cni.projectcalico.org/podIP:10.233.123.123/32 cni.projectcalico.org/podIPs:10.233.123.123/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce0f37 0xc003ce0f38}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7hzd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7hzd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.123,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://74d5bd551149f83dc6d3a0de938b5a3c01f38f0d390a7cc7fed29f5e70a615e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.956: INFO: Pod "webserver-deployment-845c8977d9-sshgl" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-sshgl webserver-deployment-845c8977d9- deployment-7135  88ce4e88-0370-4499-8c6c-66d69a6ed8e4 1931948 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f1363f649b24f630269bdfa59c1b87a21f45de9f9e0d9b407fbee9c6a04b9deb cni.projectcalico.org/podIP:10.233.126.70/32 cni.projectcalico.org/podIPs:10.233.126.70/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce13c7 0xc003ce13c8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.126.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jd4nj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jd4nj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:10.233.126.70,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5778c2c262e5100430e8f0e1e76d3418dc513af5c1d1688ca5a35502bcb16b39,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.126.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.956: INFO: Pod "webserver-deployment-845c8977d9-v6llk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-v6llk webserver-deployment-845c8977d9- deployment-7135  7cecfc1c-811c-4cdc-8cc5-7d06641c0054 1932164 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce1837 0xc003ce1838}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2rfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2rfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.56,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.957: INFO: Pod "webserver-deployment-845c8977d9-v7pxx" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-v7pxx webserver-deployment-845c8977d9- deployment-7135  6af28964-a71b-4b74-97fb-4e14df1e1808 1931921 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4825732bb6ee95a5aa8f500e37966a910040d05067b8d6c3b559814b2a22ec85 cni.projectcalico.org/podIP:10.233.123.30/32 cni.projectcalico.org/podIPs:10.233.123.30/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc003ce1c27 0xc003ce1c28}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lgwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lgwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.61,PodIP:10.233.123.30,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7f458312336205332609049a976150d0cad10f03eb4c97c9f564bcb72941a1d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.957: INFO: Pod "webserver-deployment-845c8977d9-xhpzt" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xhpzt webserver-deployment-845c8977d9- deployment-7135  47f8e05e-bfeb-4fd7-aa59-696b1bcf2fba 1932030 0 2023-03-02 13:02:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:857222d2aec520a49ef0dcd614e66ccfb62fbeb84d7a26c657af27310763b110 cni.projectcalico.org/podIP:10.233.92.108/32 cni.projectcalico.org/podIPs:10.233.92.108/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc001d040b7 0xc001d040b8}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:02:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.92.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8t4t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8t4t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.138,PodIP:10.233.92.108,StartTime:2023-03-02 13:02:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:02:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7ecd35ce5f1dbd7f8de1f8da0516c91b420885f27cea77bdbf855196db00263e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.92.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:02:41.958: INFO: Pod "webserver-deployment-845c8977d9-zlb5x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zlb5x webserver-deployment-845c8977d9- deployment-7135  7eee4dbc-0c69-4a31-862e-c97ac67dbe6e 1932150 0 2023-03-02 13:02:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07b02358-b599-4016-9b25-0976e66b91da 0xc001d04467 0xc001d04468}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b02358-b599-4016-9b25-0976e66b91da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 13:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7wbwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7wbwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:,StartTime:2023-03-02 13:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 13:02:41.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7135" for this suite. 03/02/23 13:02:41.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:02:42.056
Mar  2 13:02:42.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubelet-test 03/02/23 13:02:42.057
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:42.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:42.153
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  2 13:02:42.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-806" for this suite. 03/02/23 13:02:42.271
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":84,"skipped":1444,"failed":0}
------------------------------
â€¢ [0.394 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:02:42.056
    Mar  2 13:02:42.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubelet-test 03/02/23 13:02:42.057
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:42.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:42.153
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  2 13:02:42.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-806" for this suite. 03/02/23 13:02:42.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:02:42.45
Mar  2 13:02:42.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename dns 03/02/23 13:02:42.451
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:42.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:42.546
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/02/23 13:02:42.548
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6250.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6250.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/02/23 13:02:42.58
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6250.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6250.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/02/23 13:02:42.58
STEP: creating a pod to probe DNS 03/02/23 13:02:42.58
STEP: submitting the pod to kubernetes 03/02/23 13:02:42.58
Mar  2 13:02:42.942: INFO: Waiting up to 15m0s for pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848" in namespace "dns-6250" to be "running"
Mar  2 13:02:43.122: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848": Phase="Pending", Reason="", readiness=false. Elapsed: 179.814088ms
Mar  2 13:02:45.128: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18581093s
Mar  2 13:02:47.136: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848": Phase="Pending", Reason="", readiness=false. Elapsed: 4.193258198s
Mar  2 13:02:49.138: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848": Phase="Pending", Reason="", readiness=false. Elapsed: 6.195391638s
Mar  2 13:02:51.136: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848": Phase="Running", Reason="", readiness=true. Elapsed: 8.193757154s
Mar  2 13:02:51.137: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848" satisfied condition "running"
STEP: retrieving the pod 03/02/23 13:02:51.137
STEP: looking for the results for each expected name from probers 03/02/23 13:02:51.155
Mar  2 13:02:51.207: INFO: DNS probes using dns-6250/dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848 succeeded

STEP: deleting the pod 03/02/23 13:02:51.207
STEP: deleting the test headless service 03/02/23 13:02:51.231
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 13:02:51.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6250" for this suite. 03/02/23 13:02:51.262
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":85,"skipped":1455,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.816 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:02:42.45
    Mar  2 13:02:42.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename dns 03/02/23 13:02:42.451
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:42.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:42.546
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/02/23 13:02:42.548
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6250.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6250.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/02/23 13:02:42.58
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6250.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6250.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/02/23 13:02:42.58
    STEP: creating a pod to probe DNS 03/02/23 13:02:42.58
    STEP: submitting the pod to kubernetes 03/02/23 13:02:42.58
    Mar  2 13:02:42.942: INFO: Waiting up to 15m0s for pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848" in namespace "dns-6250" to be "running"
    Mar  2 13:02:43.122: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848": Phase="Pending", Reason="", readiness=false. Elapsed: 179.814088ms
    Mar  2 13:02:45.128: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18581093s
    Mar  2 13:02:47.136: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848": Phase="Pending", Reason="", readiness=false. Elapsed: 4.193258198s
    Mar  2 13:02:49.138: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848": Phase="Pending", Reason="", readiness=false. Elapsed: 6.195391638s
    Mar  2 13:02:51.136: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848": Phase="Running", Reason="", readiness=true. Elapsed: 8.193757154s
    Mar  2 13:02:51.137: INFO: Pod "dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 13:02:51.137
    STEP: looking for the results for each expected name from probers 03/02/23 13:02:51.155
    Mar  2 13:02:51.207: INFO: DNS probes using dns-6250/dns-test-0fe299a9-893e-49db-98f4-9c5368fbf848 succeeded

    STEP: deleting the pod 03/02/23 13:02:51.207
    STEP: deleting the test headless service 03/02/23 13:02:51.231
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 13:02:51.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6250" for this suite. 03/02/23 13:02:51.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:02:51.275
Mar  2 13:02:51.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename deployment 03/02/23 13:02:51.287
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:51.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:51.329
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar  2 13:02:51.332: INFO: Creating deployment "test-recreate-deployment"
Mar  2 13:02:51.336: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  2 13:02:51.343: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  2 13:02:53.357: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  2 13:02:53.363: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 13, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 13:02:55.368: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  2 13:02:55.426: INFO: Updating deployment test-recreate-deployment
Mar  2 13:02:55.426: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 13:02:55.550: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8225  ae5f911e-5980-41a5-a532-70bac44b8c66 1932751 2 2023-03-02 13:02:51 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007622038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 13:02:55 +0000 UTC,LastTransitionTime:2023-03-02 13:02:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-02 13:02:55 +0000 UTC,LastTransitionTime:2023-03-02 13:02:51 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 13:02:55.553: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-8225  6f29ceb2-9a15-4ace-af58-7e459a2649f1 1932750 1 2023-03-02 13:02:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment ae5f911e-5980-41a5-a532-70bac44b8c66 0xc007622740 0xc007622741}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae5f911e-5980-41a5-a532-70bac44b8c66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007622828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 13:02:55.553: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  2 13:02:55.553: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-8225  85ba6e31-76ba-4c19-8e81-43a5f8d99153 1932739 2 2023-03-02 13:02:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment ae5f911e-5980-41a5-a532-70bac44b8c66 0xc0076225c7 0xc0076225c8}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae5f911e-5980-41a5-a532-70bac44b8c66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0076226b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 13:02:55.564: INFO: Pod "test-recreate-deployment-9d58999df-2q556" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-2q556 test-recreate-deployment-9d58999df- deployment-8225  b748fd4c-f097-4b03-aa3b-6c03ffa8f1dc 1932744 0 2023-03-02 13:02:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 6f29ceb2-9a15-4ace-af58-7e459a2649f1 0xc007622e90 0xc007622e91}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f29ceb2-9a15-4ace-af58-7e459a2649f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jx2r6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jx2r6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 13:02:55.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8225" for this suite. 03/02/23 13:02:55.569
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":86,"skipped":1522,"failed":0}
------------------------------
â€¢ [4.343 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:02:51.275
    Mar  2 13:02:51.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename deployment 03/02/23 13:02:51.287
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:51.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:51.329
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar  2 13:02:51.332: INFO: Creating deployment "test-recreate-deployment"
    Mar  2 13:02:51.336: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar  2 13:02:51.343: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Mar  2 13:02:53.357: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar  2 13:02:53.363: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 13, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 13:02:55.368: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar  2 13:02:55.426: INFO: Updating deployment test-recreate-deployment
    Mar  2 13:02:55.426: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 13:02:55.550: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-8225  ae5f911e-5980-41a5-a532-70bac44b8c66 1932751 2 2023-03-02 13:02:51 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007622038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 13:02:55 +0000 UTC,LastTransitionTime:2023-03-02 13:02:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-02 13:02:55 +0000 UTC,LastTransitionTime:2023-03-02 13:02:51 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar  2 13:02:55.553: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-8225  6f29ceb2-9a15-4ace-af58-7e459a2649f1 1932750 1 2023-03-02 13:02:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment ae5f911e-5980-41a5-a532-70bac44b8c66 0xc007622740 0xc007622741}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae5f911e-5980-41a5-a532-70bac44b8c66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007622828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 13:02:55.553: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar  2 13:02:55.553: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-8225  85ba6e31-76ba-4c19-8e81-43a5f8d99153 1932739 2 2023-03-02 13:02:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment ae5f911e-5980-41a5-a532-70bac44b8c66 0xc0076225c7 0xc0076225c8}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae5f911e-5980-41a5-a532-70bac44b8c66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0076226b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 13:02:55.564: INFO: Pod "test-recreate-deployment-9d58999df-2q556" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-2q556 test-recreate-deployment-9d58999df- deployment-8225  b748fd4c-f097-4b03-aa3b-6c03ffa8f1dc 1932744 0 2023-03-02 13:02:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 6f29ceb2-9a15-4ace-af58-7e459a2649f1 0xc007622e90 0xc007622e91}] [] [{kube-controller-manager Update v1 2023-03-02 13:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f29ceb2-9a15-4ace-af58-7e459a2649f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jx2r6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jx2r6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:02:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 13:02:55.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8225" for this suite. 03/02/23 13:02:55.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:02:55.619
Mar  2 13:02:55.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:02:55.621
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:55.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:55.642
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 03/02/23 13:02:55.645
Mar  2 13:02:55.652: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743" in namespace "projected-5102" to be "Succeeded or Failed"
Mar  2 13:02:55.661: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743": Phase="Pending", Reason="", readiness=false. Elapsed: 8.483625ms
Mar  2 13:02:57.704: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052126355s
Mar  2 13:02:59.712: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060366172s
Mar  2 13:03:01.699: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046663163s
Mar  2 13:03:03.697: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.044808011s
STEP: Saw pod success 03/02/23 13:03:03.697
Mar  2 13:03:03.697: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743" satisfied condition "Succeeded or Failed"
Mar  2 13:03:03.701: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743 container client-container: <nil>
STEP: delete the pod 03/02/23 13:03:03.707
Mar  2 13:03:03.727: INFO: Waiting for pod downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743 to disappear
Mar  2 13:03:03.729: INFO: Pod downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 13:03:03.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5102" for this suite. 03/02/23 13:03:03.744
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":87,"skipped":1542,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.135 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:02:55.619
    Mar  2 13:02:55.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:02:55.621
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:02:55.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:02:55.642
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 03/02/23 13:02:55.645
    Mar  2 13:02:55.652: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743" in namespace "projected-5102" to be "Succeeded or Failed"
    Mar  2 13:02:55.661: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743": Phase="Pending", Reason="", readiness=false. Elapsed: 8.483625ms
    Mar  2 13:02:57.704: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052126355s
    Mar  2 13:02:59.712: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060366172s
    Mar  2 13:03:01.699: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046663163s
    Mar  2 13:03:03.697: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.044808011s
    STEP: Saw pod success 03/02/23 13:03:03.697
    Mar  2 13:03:03.697: INFO: Pod "downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743" satisfied condition "Succeeded or Failed"
    Mar  2 13:03:03.701: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743 container client-container: <nil>
    STEP: delete the pod 03/02/23 13:03:03.707
    Mar  2 13:03:03.727: INFO: Waiting for pod downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743 to disappear
    Mar  2 13:03:03.729: INFO: Pod downwardapi-volume-d3f5927f-5a3f-4fcf-9278-8fdfa7fe5743 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 13:03:03.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5102" for this suite. 03/02/23 13:03:03.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:03:03.755
Mar  2 13:03:03.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pods 03/02/23 13:03:03.756
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:03.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:03.783
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 03/02/23 13:03:03.788
STEP: submitting the pod to kubernetes 03/02/23 13:03:03.788
Mar  2 13:03:03.799: INFO: Waiting up to 5m0s for pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc" in namespace "pods-510" to be "running and ready"
Mar  2 13:03:03.811: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.228705ms
Mar  2 13:03:03.811: INFO: The phase of Pod pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:03:05.819: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020270243s
Mar  2 13:03:05.819: INFO: The phase of Pod pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:03:07.819: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc": Phase="Running", Reason="", readiness=true. Elapsed: 4.020063501s
Mar  2 13:03:07.819: INFO: The phase of Pod pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc is Running (Ready = true)
Mar  2 13:03:07.819: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/02/23 13:03:07.826
STEP: updating the pod 03/02/23 13:03:07.831
Mar  2 13:03:08.358: INFO: Successfully updated pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc"
Mar  2 13:03:08.358: INFO: Waiting up to 5m0s for pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc" in namespace "pods-510" to be "running"
Mar  2 13:03:08.365: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc": Phase="Running", Reason="", readiness=true. Elapsed: 6.756796ms
Mar  2 13:03:08.365: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/02/23 13:03:08.365
Mar  2 13:03:08.370: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 13:03:08.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-510" for this suite. 03/02/23 13:03:08.38
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":88,"skipped":1561,"failed":0}
------------------------------
â€¢ [4.643 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:03:03.755
    Mar  2 13:03:03.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pods 03/02/23 13:03:03.756
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:03.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:03.783
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 03/02/23 13:03:03.788
    STEP: submitting the pod to kubernetes 03/02/23 13:03:03.788
    Mar  2 13:03:03.799: INFO: Waiting up to 5m0s for pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc" in namespace "pods-510" to be "running and ready"
    Mar  2 13:03:03.811: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.228705ms
    Mar  2 13:03:03.811: INFO: The phase of Pod pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:03:05.819: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020270243s
    Mar  2 13:03:05.819: INFO: The phase of Pod pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:03:07.819: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc": Phase="Running", Reason="", readiness=true. Elapsed: 4.020063501s
    Mar  2 13:03:07.819: INFO: The phase of Pod pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc is Running (Ready = true)
    Mar  2 13:03:07.819: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/02/23 13:03:07.826
    STEP: updating the pod 03/02/23 13:03:07.831
    Mar  2 13:03:08.358: INFO: Successfully updated pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc"
    Mar  2 13:03:08.358: INFO: Waiting up to 5m0s for pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc" in namespace "pods-510" to be "running"
    Mar  2 13:03:08.365: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc": Phase="Running", Reason="", readiness=true. Elapsed: 6.756796ms
    Mar  2 13:03:08.365: INFO: Pod "pod-update-941a11a9-92b9-40d8-a9c2-5d419de423fc" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/02/23 13:03:08.365
    Mar  2 13:03:08.370: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 13:03:08.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-510" for this suite. 03/02/23 13:03:08.38
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:03:08.4
Mar  2 13:03:08.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 13:03:08.402
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:08.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:08.433
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/02/23 13:03:08.446
Mar  2 13:03:08.453: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6352" to be "running and ready"
Mar  2 13:03:08.459: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.043887ms
Mar  2 13:03:08.459: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:03:10.470: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016259606s
Mar  2 13:03:10.470: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  2 13:03:10.470: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 03/02/23 13:03:10.476
Mar  2 13:03:10.483: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6352" to be "running and ready"
Mar  2 13:03:10.487: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.810369ms
Mar  2 13:03:10.487: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:03:12.497: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012509787s
Mar  2 13:03:12.497: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:03:14.538: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.053667453s
Mar  2 13:03:14.538: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar  2 13:03:14.538: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/02/23 13:03:14.547
Mar  2 13:03:14.556: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 13:03:14.562: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 13:03:16.563: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 13:03:16.588: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 13:03:18.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 13:03:18.569: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/02/23 13:03:18.57
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  2 13:03:18.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6352" for this suite. 03/02/23 13:03:18.607
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":89,"skipped":1562,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.217 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:03:08.4
    Mar  2 13:03:08.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 13:03:08.402
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:08.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:08.433
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/02/23 13:03:08.446
    Mar  2 13:03:08.453: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6352" to be "running and ready"
    Mar  2 13:03:08.459: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.043887ms
    Mar  2 13:03:08.459: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:03:10.470: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016259606s
    Mar  2 13:03:10.470: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  2 13:03:10.470: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 03/02/23 13:03:10.476
    Mar  2 13:03:10.483: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6352" to be "running and ready"
    Mar  2 13:03:10.487: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.810369ms
    Mar  2 13:03:10.487: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:03:12.497: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012509787s
    Mar  2 13:03:12.497: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:03:14.538: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.053667453s
    Mar  2 13:03:14.538: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar  2 13:03:14.538: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/02/23 13:03:14.547
    Mar  2 13:03:14.556: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  2 13:03:14.562: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar  2 13:03:16.563: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  2 13:03:16.588: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar  2 13:03:18.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  2 13:03:18.569: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/02/23 13:03:18.57
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  2 13:03:18.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6352" for this suite. 03/02/23 13:03:18.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:03:18.624
Mar  2 13:03:18.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-runtime 03/02/23 13:03:18.627
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:18.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:18.657
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 03/02/23 13:03:18.66
STEP: wait for the container to reach Succeeded 03/02/23 13:03:18.666
STEP: get the container status 03/02/23 13:03:22.83
STEP: the container should be terminated 03/02/23 13:03:22.849
STEP: the termination message should be set 03/02/23 13:03:22.853
Mar  2 13:03:22.853: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/02/23 13:03:22.853
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  2 13:03:22.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-855" for this suite. 03/02/23 13:03:22.898
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":90,"skipped":1571,"failed":0}
------------------------------
â€¢ [4.280 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:03:18.624
    Mar  2 13:03:18.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-runtime 03/02/23 13:03:18.627
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:18.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:18.657
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 03/02/23 13:03:18.66
    STEP: wait for the container to reach Succeeded 03/02/23 13:03:18.666
    STEP: get the container status 03/02/23 13:03:22.83
    STEP: the container should be terminated 03/02/23 13:03:22.849
    STEP: the termination message should be set 03/02/23 13:03:22.853
    Mar  2 13:03:22.853: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/02/23 13:03:22.853
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  2 13:03:22.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-855" for this suite. 03/02/23 13:03:22.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:03:22.911
Mar  2 13:03:22.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 13:03:22.921
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:22.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:22.946
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar  2 13:03:22.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:03:28.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9184" for this suite. 03/02/23 13:03:28.518
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":91,"skipped":1596,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.615 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:03:22.911
    Mar  2 13:03:22.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 13:03:22.921
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:22.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:22.946
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar  2 13:03:22.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:03:28.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9184" for this suite. 03/02/23 13:03:28.518
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:03:28.528
Mar  2 13:03:28.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename namespaces 03/02/23 13:03:28.531
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:28.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:28.557
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 03/02/23 13:03:28.562
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:28.579
STEP: Creating a pod in the namespace 03/02/23 13:03:28.583
STEP: Waiting for the pod to have running status 03/02/23 13:03:28.597
Mar  2 13:03:28.597: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1560" to be "running"
Mar  2 13:03:28.617: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.405809ms
Mar  2 13:03:30.624: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026267792s
Mar  2 13:03:32.640: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.042505735s
Mar  2 13:03:32.640: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/02/23 13:03:32.64
STEP: Waiting for the namespace to be removed. 03/02/23 13:03:32.647
STEP: Recreating the namespace 03/02/23 13:03:43.654
STEP: Verifying there are no pods in the namespace 03/02/23 13:03:43.677
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  2 13:03:43.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9329" for this suite. 03/02/23 13:03:43.686
STEP: Destroying namespace "nsdeletetest-1560" for this suite. 03/02/23 13:03:43.693
Mar  2 13:03:43.698: INFO: Namespace nsdeletetest-1560 was already deleted
STEP: Destroying namespace "nsdeletetest-83" for this suite. 03/02/23 13:03:43.698
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":92,"skipped":1598,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.176 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:03:28.528
    Mar  2 13:03:28.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename namespaces 03/02/23 13:03:28.531
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:28.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:28.557
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 03/02/23 13:03:28.562
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:28.579
    STEP: Creating a pod in the namespace 03/02/23 13:03:28.583
    STEP: Waiting for the pod to have running status 03/02/23 13:03:28.597
    Mar  2 13:03:28.597: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1560" to be "running"
    Mar  2 13:03:28.617: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.405809ms
    Mar  2 13:03:30.624: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026267792s
    Mar  2 13:03:32.640: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.042505735s
    Mar  2 13:03:32.640: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/02/23 13:03:32.64
    STEP: Waiting for the namespace to be removed. 03/02/23 13:03:32.647
    STEP: Recreating the namespace 03/02/23 13:03:43.654
    STEP: Verifying there are no pods in the namespace 03/02/23 13:03:43.677
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 13:03:43.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9329" for this suite. 03/02/23 13:03:43.686
    STEP: Destroying namespace "nsdeletetest-1560" for this suite. 03/02/23 13:03:43.693
    Mar  2 13:03:43.698: INFO: Namespace nsdeletetest-1560 was already deleted
    STEP: Destroying namespace "nsdeletetest-83" for this suite. 03/02/23 13:03:43.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:03:43.711
Mar  2 13:03:43.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubelet-test 03/02/23 13:03:43.712
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:43.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:43.729
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar  2 13:03:43.738: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee" in namespace "kubelet-test-4420" to be "running and ready"
Mar  2 13:03:43.755: INFO: Pod "busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee": Phase="Pending", Reason="", readiness=false. Elapsed: 14.312287ms
Mar  2 13:03:43.755: INFO: The phase of Pod busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:03:45.771: INFO: Pod "busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030933375s
Mar  2 13:03:45.772: INFO: The phase of Pod busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:03:47.768: INFO: Pod "busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee": Phase="Running", Reason="", readiness=true. Elapsed: 4.02734949s
Mar  2 13:03:47.768: INFO: The phase of Pod busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee is Running (Ready = true)
Mar  2 13:03:47.768: INFO: Pod "busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  2 13:03:47.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4420" for this suite. 03/02/23 13:03:47.793
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":93,"skipped":1609,"failed":0}
------------------------------
â€¢ [4.090 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:03:43.711
    Mar  2 13:03:43.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubelet-test 03/02/23 13:03:43.712
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:43.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:43.729
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar  2 13:03:43.738: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee" in namespace "kubelet-test-4420" to be "running and ready"
    Mar  2 13:03:43.755: INFO: Pod "busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee": Phase="Pending", Reason="", readiness=false. Elapsed: 14.312287ms
    Mar  2 13:03:43.755: INFO: The phase of Pod busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:03:45.771: INFO: Pod "busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030933375s
    Mar  2 13:03:45.772: INFO: The phase of Pod busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:03:47.768: INFO: Pod "busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee": Phase="Running", Reason="", readiness=true. Elapsed: 4.02734949s
    Mar  2 13:03:47.768: INFO: The phase of Pod busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee is Running (Ready = true)
    Mar  2 13:03:47.768: INFO: Pod "busybox-readonly-fsb9fe78ca-5c15-4136-a124-c4b2cf565cee" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  2 13:03:47.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4420" for this suite. 03/02/23 13:03:47.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:03:47.803
Mar  2 13:03:47.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 13:03:47.804
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:47.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:47.826
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 03/02/23 13:03:47.83
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/02/23 13:03:47.836
STEP: patching the secret 03/02/23 13:03:47.922
STEP: deleting the secret using a LabelSelector 03/02/23 13:03:47.934
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/02/23 13:03:47.941
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  2 13:03:48.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5725" for this suite. 03/02/23 13:03:48.075
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":94,"skipped":1654,"failed":0}
------------------------------
â€¢ [0.289 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:03:47.803
    Mar  2 13:03:47.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 13:03:47.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:47.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:47.826
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 03/02/23 13:03:47.83
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/02/23 13:03:47.836
    STEP: patching the secret 03/02/23 13:03:47.922
    STEP: deleting the secret using a LabelSelector 03/02/23 13:03:47.934
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/02/23 13:03:47.941
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 13:03:48.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5725" for this suite. 03/02/23 13:03:48.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:03:48.095
Mar  2 13:03:48.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename statefulset 03/02/23 13:03:48.096
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:48.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:48.116
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4801 03/02/23 13:03:48.119
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 03/02/23 13:03:48.13
Mar  2 13:03:48.190: INFO: Found 0 stateful pods, waiting for 3
Mar  2 13:03:58.197: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 13:03:58.197: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 13:03:58.197: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 13:03:58.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4801 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 13:03:58.428: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 13:03:58.428: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 13:03:58.428: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/02/23 13:04:08.452
Mar  2 13:04:08.477: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/02/23 13:04:08.477
STEP: Updating Pods in reverse ordinal order 03/02/23 13:04:18.504
Mar  2 13:04:18.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4801 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 13:04:18.724: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 13:04:18.724: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 13:04:18.724: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 13:04:28.761: INFO: Waiting for StatefulSet statefulset-4801/ss2 to complete update
STEP: Rolling back to a previous revision 03/02/23 13:04:38.779
Mar  2 13:04:38.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4801 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 13:04:38.993: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 13:04:38.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 13:04:38.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 13:04:49.053: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/02/23 13:04:59.085
Mar  2 13:04:59.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4801 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 13:04:59.321: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 13:04:59.321: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 13:04:59.321: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 13:05:09.439: INFO: Waiting for StatefulSet statefulset-4801/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 13:05:19.450: INFO: Deleting all statefulset in ns statefulset-4801
Mar  2 13:05:19.454: INFO: Scaling statefulset ss2 to 0
Mar  2 13:05:29.507: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 13:05:29.510: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 13:05:29.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4801" for this suite. 03/02/23 13:05:29.534
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":95,"skipped":1707,"failed":0}
------------------------------
â€¢ [SLOW TEST] [101.443 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:03:48.095
    Mar  2 13:03:48.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename statefulset 03/02/23 13:03:48.096
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:03:48.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:03:48.116
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4801 03/02/23 13:03:48.119
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 03/02/23 13:03:48.13
    Mar  2 13:03:48.190: INFO: Found 0 stateful pods, waiting for 3
    Mar  2 13:03:58.197: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 13:03:58.197: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 13:03:58.197: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 13:03:58.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4801 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 13:03:58.428: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 13:03:58.428: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 13:03:58.428: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/02/23 13:04:08.452
    Mar  2 13:04:08.477: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/02/23 13:04:08.477
    STEP: Updating Pods in reverse ordinal order 03/02/23 13:04:18.504
    Mar  2 13:04:18.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4801 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 13:04:18.724: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 13:04:18.724: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 13:04:18.724: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 13:04:28.761: INFO: Waiting for StatefulSet statefulset-4801/ss2 to complete update
    STEP: Rolling back to a previous revision 03/02/23 13:04:38.779
    Mar  2 13:04:38.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4801 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 13:04:38.993: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 13:04:38.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 13:04:38.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 13:04:49.053: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/02/23 13:04:59.085
    Mar  2 13:04:59.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4801 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 13:04:59.321: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 13:04:59.321: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 13:04:59.321: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 13:05:09.439: INFO: Waiting for StatefulSet statefulset-4801/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 13:05:19.450: INFO: Deleting all statefulset in ns statefulset-4801
    Mar  2 13:05:19.454: INFO: Scaling statefulset ss2 to 0
    Mar  2 13:05:29.507: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 13:05:29.510: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 13:05:29.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4801" for this suite. 03/02/23 13:05:29.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:05:29.546
Mar  2 13:05:29.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:05:29.548
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:29.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:29.576
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-4c294f08-41a8-477f-b554-55ba4940d7c8 03/02/23 13:05:29.58
STEP: Creating a pod to test consume configMaps 03/02/23 13:05:29.584
Mar  2 13:05:29.595: INFO: Waiting up to 5m0s for pod "pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5" in namespace "configmap-7298" to be "Succeeded or Failed"
Mar  2 13:05:29.601: INFO: Pod "pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.881193ms
Mar  2 13:05:31.632: INFO: Pod "pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036626651s
Mar  2 13:05:33.626: INFO: Pod "pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030566891s
STEP: Saw pod success 03/02/23 13:05:33.626
Mar  2 13:05:33.626: INFO: Pod "pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5" satisfied condition "Succeeded or Failed"
Mar  2 13:05:33.637: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 13:05:33.645
Mar  2 13:05:33.659: INFO: Waiting for pod pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5 to disappear
Mar  2 13:05:33.666: INFO: Pod pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:05:33.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7298" for this suite. 03/02/23 13:05:33.69
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":96,"skipped":1749,"failed":0}
------------------------------
â€¢ [4.169 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:05:29.546
    Mar  2 13:05:29.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:05:29.548
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:29.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:29.576
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-4c294f08-41a8-477f-b554-55ba4940d7c8 03/02/23 13:05:29.58
    STEP: Creating a pod to test consume configMaps 03/02/23 13:05:29.584
    Mar  2 13:05:29.595: INFO: Waiting up to 5m0s for pod "pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5" in namespace "configmap-7298" to be "Succeeded or Failed"
    Mar  2 13:05:29.601: INFO: Pod "pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.881193ms
    Mar  2 13:05:31.632: INFO: Pod "pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036626651s
    Mar  2 13:05:33.626: INFO: Pod "pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030566891s
    STEP: Saw pod success 03/02/23 13:05:33.626
    Mar  2 13:05:33.626: INFO: Pod "pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5" satisfied condition "Succeeded or Failed"
    Mar  2 13:05:33.637: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 13:05:33.645
    Mar  2 13:05:33.659: INFO: Waiting for pod pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5 to disappear
    Mar  2 13:05:33.666: INFO: Pod pod-configmaps-bfd7cb60-1064-4b1a-b4b2-c1b7fda3d1e5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:05:33.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7298" for this suite. 03/02/23 13:05:33.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:05:33.727
Mar  2 13:05:33.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:05:33.729
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:33.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:33.751
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 03/02/23 13:05:33.754
STEP: fetching the ConfigMap 03/02/23 13:05:33.758
STEP: patching the ConfigMap 03/02/23 13:05:33.761
STEP: listing all ConfigMaps in all namespaces with a label selector 03/02/23 13:05:33.765
STEP: deleting the ConfigMap by collection with a label selector 03/02/23 13:05:33.826
STEP: listing all ConfigMaps in test namespace 03/02/23 13:05:33.832
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:05:33.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9929" for this suite. 03/02/23 13:05:33.84
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":97,"skipped":1762,"failed":0}
------------------------------
â€¢ [0.118 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:05:33.727
    Mar  2 13:05:33.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:05:33.729
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:33.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:33.751
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 03/02/23 13:05:33.754
    STEP: fetching the ConfigMap 03/02/23 13:05:33.758
    STEP: patching the ConfigMap 03/02/23 13:05:33.761
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/02/23 13:05:33.765
    STEP: deleting the ConfigMap by collection with a label selector 03/02/23 13:05:33.826
    STEP: listing all ConfigMaps in test namespace 03/02/23 13:05:33.832
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:05:33.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9929" for this suite. 03/02/23 13:05:33.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:05:33.854
Mar  2 13:05:33.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename replicaset 03/02/23 13:05:33.856
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:33.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:33.961
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar  2 13:05:34.042: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 13:05:39.048: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 13:05:39.048
STEP: Scaling up "test-rs" replicaset  03/02/23 13:05:39.048
Mar  2 13:05:39.059: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/02/23 13:05:39.059
W0302 13:05:39.099158      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  2 13:05:39.136: INFO: observed ReplicaSet test-rs in namespace replicaset-9768 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 13:05:39.136: INFO: observed ReplicaSet test-rs in namespace replicaset-9768 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 13:05:39.136: INFO: observed ReplicaSet test-rs in namespace replicaset-9768 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 13:05:39.148: INFO: observed ReplicaSet test-rs in namespace replicaset-9768 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 13:05:40.951: INFO: observed ReplicaSet test-rs in namespace replicaset-9768 with ReadyReplicas 2, AvailableReplicas 2
Mar  2 13:05:41.137: INFO: observed Replicaset test-rs in namespace replicaset-9768 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 13:05:41.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9768" for this suite. 03/02/23 13:05:41.148
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":98,"skipped":1819,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.299 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:05:33.854
    Mar  2 13:05:33.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename replicaset 03/02/23 13:05:33.856
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:33.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:33.961
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar  2 13:05:34.042: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  2 13:05:39.048: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 13:05:39.048
    STEP: Scaling up "test-rs" replicaset  03/02/23 13:05:39.048
    Mar  2 13:05:39.059: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/02/23 13:05:39.059
    W0302 13:05:39.099158      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  2 13:05:39.136: INFO: observed ReplicaSet test-rs in namespace replicaset-9768 with ReadyReplicas 1, AvailableReplicas 1
    Mar  2 13:05:39.136: INFO: observed ReplicaSet test-rs in namespace replicaset-9768 with ReadyReplicas 1, AvailableReplicas 1
    Mar  2 13:05:39.136: INFO: observed ReplicaSet test-rs in namespace replicaset-9768 with ReadyReplicas 1, AvailableReplicas 1
    Mar  2 13:05:39.148: INFO: observed ReplicaSet test-rs in namespace replicaset-9768 with ReadyReplicas 1, AvailableReplicas 1
    Mar  2 13:05:40.951: INFO: observed ReplicaSet test-rs in namespace replicaset-9768 with ReadyReplicas 2, AvailableReplicas 2
    Mar  2 13:05:41.137: INFO: observed Replicaset test-rs in namespace replicaset-9768 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 13:05:41.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9768" for this suite. 03/02/23 13:05:41.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:05:41.162
Mar  2 13:05:41.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename svcaccounts 03/02/23 13:05:41.164
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:41.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:41.258
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Mar  2 13:05:41.265: INFO: Got root ca configmap in namespace "svcaccounts-8543"
Mar  2 13:05:41.270: INFO: Deleted root ca configmap in namespace "svcaccounts-8543"
STEP: waiting for a new root ca configmap created 03/02/23 13:05:41.805
Mar  2 13:05:41.843: INFO: Recreated root ca configmap in namespace "svcaccounts-8543"
Mar  2 13:05:41.857: INFO: Updated root ca configmap in namespace "svcaccounts-8543"
STEP: waiting for the root ca configmap reconciled 03/02/23 13:05:42.357
Mar  2 13:05:42.386: INFO: Reconciled root ca configmap in namespace "svcaccounts-8543"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 13:05:42.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8543" for this suite. 03/02/23 13:05:42.456
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":99,"skipped":1839,"failed":0}
------------------------------
â€¢ [1.303 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:05:41.162
    Mar  2 13:05:41.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 13:05:41.164
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:41.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:41.258
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Mar  2 13:05:41.265: INFO: Got root ca configmap in namespace "svcaccounts-8543"
    Mar  2 13:05:41.270: INFO: Deleted root ca configmap in namespace "svcaccounts-8543"
    STEP: waiting for a new root ca configmap created 03/02/23 13:05:41.805
    Mar  2 13:05:41.843: INFO: Recreated root ca configmap in namespace "svcaccounts-8543"
    Mar  2 13:05:41.857: INFO: Updated root ca configmap in namespace "svcaccounts-8543"
    STEP: waiting for the root ca configmap reconciled 03/02/23 13:05:42.357
    Mar  2 13:05:42.386: INFO: Reconciled root ca configmap in namespace "svcaccounts-8543"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 13:05:42.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8543" for this suite. 03/02/23 13:05:42.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:05:42.496
Mar  2 13:05:42.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 13:05:42.498
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:42.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:42.558
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-8570/secret-test-45e50e48-fa90-4867-9082-e5702a2cb40a 03/02/23 13:05:42.562
STEP: Creating a pod to test consume secrets 03/02/23 13:05:42.568
Mar  2 13:05:42.638: INFO: Waiting up to 5m0s for pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f" in namespace "secrets-8570" to be "Succeeded or Failed"
Mar  2 13:05:42.650: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.790085ms
Mar  2 13:05:44.663: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025502018s
Mar  2 13:05:46.658: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0200307s
Mar  2 13:05:48.656: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018420519s
Mar  2 13:05:50.660: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0227833s
STEP: Saw pod success 03/02/23 13:05:50.661
Mar  2 13:05:50.661: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f" satisfied condition "Succeeded or Failed"
Mar  2 13:05:50.666: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f container env-test: <nil>
STEP: delete the pod 03/02/23 13:05:50.678
Mar  2 13:05:50.750: INFO: Waiting for pod pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f to disappear
Mar  2 13:05:50.757: INFO: Pod pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  2 13:05:50.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8570" for this suite. 03/02/23 13:05:50.764
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":100,"skipped":1868,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.272 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:05:42.496
    Mar  2 13:05:42.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 13:05:42.498
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:42.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:42.558
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-8570/secret-test-45e50e48-fa90-4867-9082-e5702a2cb40a 03/02/23 13:05:42.562
    STEP: Creating a pod to test consume secrets 03/02/23 13:05:42.568
    Mar  2 13:05:42.638: INFO: Waiting up to 5m0s for pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f" in namespace "secrets-8570" to be "Succeeded or Failed"
    Mar  2 13:05:42.650: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.790085ms
    Mar  2 13:05:44.663: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025502018s
    Mar  2 13:05:46.658: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0200307s
    Mar  2 13:05:48.656: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018420519s
    Mar  2 13:05:50.660: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0227833s
    STEP: Saw pod success 03/02/23 13:05:50.661
    Mar  2 13:05:50.661: INFO: Pod "pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f" satisfied condition "Succeeded or Failed"
    Mar  2 13:05:50.666: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f container env-test: <nil>
    STEP: delete the pod 03/02/23 13:05:50.678
    Mar  2 13:05:50.750: INFO: Waiting for pod pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f to disappear
    Mar  2 13:05:50.757: INFO: Pod pod-configmaps-1fde44b8-e477-473f-b181-9f1bee9e5a6f no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 13:05:50.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8570" for this suite. 03/02/23 13:05:50.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:05:50.77
Mar  2 13:05:50.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pods 03/02/23 13:05:50.771
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:50.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:50.82
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/02/23 13:05:50.83
STEP: submitting the pod to kubernetes 03/02/23 13:05:50.83
STEP: verifying QOS class is set on the pod 03/02/23 13:05:50.838
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Mar  2 13:05:50.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-149" for this suite. 03/02/23 13:05:50.848
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":101,"skipped":1878,"failed":0}
------------------------------
â€¢ [0.100 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:05:50.77
    Mar  2 13:05:50.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pods 03/02/23 13:05:50.771
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:50.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:50.82
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/02/23 13:05:50.83
    STEP: submitting the pod to kubernetes 03/02/23 13:05:50.83
    STEP: verifying QOS class is set on the pod 03/02/23 13:05:50.838
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Mar  2 13:05:50.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-149" for this suite. 03/02/23 13:05:50.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:05:50.928
Mar  2 13:05:50.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:05:50.929
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:50.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:50.954
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Mar  2 13:05:50.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-577 version'
Mar  2 13:05:51.118: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar  2 13:05:51.118: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:15:26Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:05:51.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-577" for this suite. 03/02/23 13:05:51.131
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":102,"skipped":1911,"failed":0}
------------------------------
â€¢ [0.211 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:05:50.928
    Mar  2 13:05:50.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:05:50.929
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:50.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:50.954
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Mar  2 13:05:50.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-577 version'
    Mar  2 13:05:51.118: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar  2 13:05:51.118: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:15:26Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:05:51.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-577" for this suite. 03/02/23 13:05:51.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:05:51.142
Mar  2 13:05:51.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename podtemplate 03/02/23 13:05:51.143
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:51.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:51.165
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/02/23 13:05:51.167
STEP: Replace a pod template 03/02/23 13:05:51.178
Mar  2 13:05:51.234: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  2 13:05:51.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8765" for this suite. 03/02/23 13:05:51.239
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":103,"skipped":1919,"failed":0}
------------------------------
â€¢ [0.102 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:05:51.142
    Mar  2 13:05:51.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename podtemplate 03/02/23 13:05:51.143
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:51.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:51.165
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/02/23 13:05:51.167
    STEP: Replace a pod template 03/02/23 13:05:51.178
    Mar  2 13:05:51.234: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  2 13:05:51.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-8765" for this suite. 03/02/23 13:05:51.239
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:05:51.246
Mar  2 13:05:51.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename proxy 03/02/23 13:05:51.248
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:51.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:51.27
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar  2 13:05:51.276: INFO: Creating pod...
Mar  2 13:05:51.330: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7886" to be "running"
Mar  2 13:05:51.340: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 8.66006ms
Mar  2 13:05:53.343: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.012515445s
Mar  2 13:05:53.344: INFO: Pod "agnhost" satisfied condition "running"
Mar  2 13:05:53.344: INFO: Creating service...
Mar  2 13:05:53.361: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=DELETE
Mar  2 13:05:53.368: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 13:05:53.368: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=OPTIONS
Mar  2 13:05:53.414: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 13:05:53.414: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=PATCH
Mar  2 13:05:53.448: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 13:05:53.448: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=POST
Mar  2 13:05:53.455: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 13:05:53.456: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=PUT
Mar  2 13:05:53.460: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  2 13:05:53.460: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=DELETE
Mar  2 13:05:53.564: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 13:05:53.565: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar  2 13:05:53.571: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 13:05:53.571: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=PATCH
Mar  2 13:05:53.747: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 13:05:53.747: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=POST
Mar  2 13:05:53.754: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 13:05:53.754: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=PUT
Mar  2 13:05:53.763: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  2 13:05:53.763: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=GET
Mar  2 13:05:53.767: INFO: http.Client request:GET StatusCode:301
Mar  2 13:05:53.767: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=GET
Mar  2 13:05:53.771: INFO: http.Client request:GET StatusCode:301
Mar  2 13:05:53.771: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=HEAD
Mar  2 13:05:53.826: INFO: http.Client request:HEAD StatusCode:301
Mar  2 13:05:53.826: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=HEAD
Mar  2 13:05:53.850: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  2 13:05:53.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7886" for this suite. 03/02/23 13:05:53.863
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":104,"skipped":1922,"failed":0}
------------------------------
â€¢ [2.668 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:05:51.246
    Mar  2 13:05:51.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename proxy 03/02/23 13:05:51.248
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:51.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:51.27
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar  2 13:05:51.276: INFO: Creating pod...
    Mar  2 13:05:51.330: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7886" to be "running"
    Mar  2 13:05:51.340: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 8.66006ms
    Mar  2 13:05:53.343: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.012515445s
    Mar  2 13:05:53.344: INFO: Pod "agnhost" satisfied condition "running"
    Mar  2 13:05:53.344: INFO: Creating service...
    Mar  2 13:05:53.361: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=DELETE
    Mar  2 13:05:53.368: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  2 13:05:53.368: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=OPTIONS
    Mar  2 13:05:53.414: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  2 13:05:53.414: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=PATCH
    Mar  2 13:05:53.448: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  2 13:05:53.448: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=POST
    Mar  2 13:05:53.455: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  2 13:05:53.456: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=PUT
    Mar  2 13:05:53.460: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  2 13:05:53.460: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar  2 13:05:53.564: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  2 13:05:53.565: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar  2 13:05:53.571: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  2 13:05:53.571: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar  2 13:05:53.747: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  2 13:05:53.747: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=POST
    Mar  2 13:05:53.754: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  2 13:05:53.754: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=PUT
    Mar  2 13:05:53.763: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  2 13:05:53.763: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=GET
    Mar  2 13:05:53.767: INFO: http.Client request:GET StatusCode:301
    Mar  2 13:05:53.767: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=GET
    Mar  2 13:05:53.771: INFO: http.Client request:GET StatusCode:301
    Mar  2 13:05:53.771: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/pods/agnhost/proxy?method=HEAD
    Mar  2 13:05:53.826: INFO: http.Client request:HEAD StatusCode:301
    Mar  2 13:05:53.826: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7886/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar  2 13:05:53.850: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  2 13:05:53.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-7886" for this suite. 03/02/23 13:05:53.863
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:05:53.914
Mar  2 13:05:53.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename deployment 03/02/23 13:05:53.915
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:53.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:53.946
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/02/23 13:05:53.958
STEP: waiting for Deployment to be created 03/02/23 13:05:53.962
STEP: waiting for all Replicas to be Ready 03/02/23 13:05:53.965
Mar  2 13:05:53.969: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 13:05:53.969: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 13:05:54.031: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 13:05:54.031: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 13:05:54.031: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 13:05:54.031: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 13:05:54.070: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 13:05:54.070: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 13:05:56.255: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 13:05:56.255: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 13:05:58.200: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/02/23 13:05:58.2
W0302 13:05:58.234236      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  2 13:05:58.237: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/02/23 13:05:58.237
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:05:58.267: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:05:58.267: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:05:58.366: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:05:58.366: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:05:58.366: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
Mar  2 13:05:58.366: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
Mar  2 13:06:00.246: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:06:00.246: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:06:00.278: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
STEP: listing Deployments 03/02/23 13:06:00.33
Mar  2 13:06:00.345: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/02/23 13:06:00.346
Mar  2 13:06:00.358: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/02/23 13:06:00.358
Mar  2 13:06:00.425: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 13:06:00.430: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 13:06:00.430: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 13:06:00.458: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 13:06:00.483: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 13:06:03.282: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 13:06:03.315: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 13:06:03.344: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 13:06:03.403: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 13:06:05.431: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/02/23 13:06:05.452
STEP: fetching the DeploymentStatus 03/02/23 13:06:05.464
Mar  2 13:06:05.468: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
Mar  2 13:06:05.468: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
Mar  2 13:06:05.468: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
Mar  2 13:06:05.468: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 3
STEP: deleting the Deployment 03/02/23 13:06:05.469
Mar  2 13:06:05.496: INFO: observed event type MODIFIED
Mar  2 13:06:05.520: INFO: observed event type MODIFIED
Mar  2 13:06:05.521: INFO: observed event type MODIFIED
Mar  2 13:06:05.522: INFO: observed event type MODIFIED
Mar  2 13:06:05.522: INFO: observed event type MODIFIED
Mar  2 13:06:05.522: INFO: observed event type MODIFIED
Mar  2 13:06:05.523: INFO: observed event type MODIFIED
Mar  2 13:06:05.524: INFO: observed event type MODIFIED
Mar  2 13:06:05.525: INFO: observed event type MODIFIED
Mar  2 13:06:05.526: INFO: observed event type MODIFIED
Mar  2 13:06:05.526: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 13:06:05.548: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 13:06:05.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6835" for this suite. 03/02/23 13:06:05.575
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":105,"skipped":1923,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.700 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:05:53.914
    Mar  2 13:05:53.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename deployment 03/02/23 13:05:53.915
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:05:53.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:05:53.946
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/02/23 13:05:53.958
    STEP: waiting for Deployment to be created 03/02/23 13:05:53.962
    STEP: waiting for all Replicas to be Ready 03/02/23 13:05:53.965
    Mar  2 13:05:53.969: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 13:05:53.969: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 13:05:54.031: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 13:05:54.031: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 13:05:54.031: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 13:05:54.031: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 13:05:54.070: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 13:05:54.070: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 13:05:56.255: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar  2 13:05:56.255: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar  2 13:05:58.200: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/02/23 13:05:58.2
    W0302 13:05:58.234236      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  2 13:05:58.237: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/02/23 13:05:58.237
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 0
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:05:58.250: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:05:58.267: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:05:58.267: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:05:58.366: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:05:58.366: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:05:58.366: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
    Mar  2 13:05:58.366: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
    Mar  2 13:06:00.246: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:06:00.246: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:06:00.278: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
    STEP: listing Deployments 03/02/23 13:06:00.33
    Mar  2 13:06:00.345: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/02/23 13:06:00.346
    Mar  2 13:06:00.358: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/02/23 13:06:00.358
    Mar  2 13:06:00.425: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 13:06:00.430: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 13:06:00.430: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 13:06:00.458: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 13:06:00.483: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 13:06:03.282: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 13:06:03.315: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 13:06:03.344: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 13:06:03.403: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 13:06:05.431: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/02/23 13:06:05.452
    STEP: fetching the DeploymentStatus 03/02/23 13:06:05.464
    Mar  2 13:06:05.468: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
    Mar  2 13:06:05.468: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
    Mar  2 13:06:05.468: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
    Mar  2 13:06:05.468: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
    Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 1
    Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 2
    Mar  2 13:06:05.469: INFO: observed Deployment test-deployment in namespace deployment-6835 with ReadyReplicas 3
    STEP: deleting the Deployment 03/02/23 13:06:05.469
    Mar  2 13:06:05.496: INFO: observed event type MODIFIED
    Mar  2 13:06:05.520: INFO: observed event type MODIFIED
    Mar  2 13:06:05.521: INFO: observed event type MODIFIED
    Mar  2 13:06:05.522: INFO: observed event type MODIFIED
    Mar  2 13:06:05.522: INFO: observed event type MODIFIED
    Mar  2 13:06:05.522: INFO: observed event type MODIFIED
    Mar  2 13:06:05.523: INFO: observed event type MODIFIED
    Mar  2 13:06:05.524: INFO: observed event type MODIFIED
    Mar  2 13:06:05.525: INFO: observed event type MODIFIED
    Mar  2 13:06:05.526: INFO: observed event type MODIFIED
    Mar  2 13:06:05.526: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 13:06:05.548: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 13:06:05.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6835" for this suite. 03/02/23 13:06:05.575
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:06:05.628
Mar  2 13:06:05.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 13:06:05.629
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:06:05.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:06:05.664
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 03/02/23 13:06:05.668
Mar  2 13:06:05.733: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615" in namespace "downward-api-8619" to be "Succeeded or Failed"
Mar  2 13:06:05.740: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615": Phase="Pending", Reason="", readiness=false. Elapsed: 6.470254ms
Mar  2 13:06:07.754: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615": Phase="Running", Reason="", readiness=true. Elapsed: 2.020792803s
Mar  2 13:06:09.750: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615": Phase="Running", Reason="", readiness=false. Elapsed: 4.017244189s
Mar  2 13:06:11.754: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615": Phase="Running", Reason="", readiness=false. Elapsed: 6.020572373s
Mar  2 13:06:13.754: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020674227s
STEP: Saw pod success 03/02/23 13:06:13.754
Mar  2 13:06:13.754: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615" satisfied condition "Succeeded or Failed"
Mar  2 13:06:13.758: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615 container client-container: <nil>
STEP: delete the pod 03/02/23 13:06:13.828
Mar  2 13:06:13.842: INFO: Waiting for pod downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615 to disappear
Mar  2 13:06:13.845: INFO: Pod downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 13:06:13.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8619" for this suite. 03/02/23 13:06:13.853
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":106,"skipped":1927,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.231 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:06:05.628
    Mar  2 13:06:05.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 13:06:05.629
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:06:05.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:06:05.664
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 03/02/23 13:06:05.668
    Mar  2 13:06:05.733: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615" in namespace "downward-api-8619" to be "Succeeded or Failed"
    Mar  2 13:06:05.740: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615": Phase="Pending", Reason="", readiness=false. Elapsed: 6.470254ms
    Mar  2 13:06:07.754: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615": Phase="Running", Reason="", readiness=true. Elapsed: 2.020792803s
    Mar  2 13:06:09.750: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615": Phase="Running", Reason="", readiness=false. Elapsed: 4.017244189s
    Mar  2 13:06:11.754: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615": Phase="Running", Reason="", readiness=false. Elapsed: 6.020572373s
    Mar  2 13:06:13.754: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020674227s
    STEP: Saw pod success 03/02/23 13:06:13.754
    Mar  2 13:06:13.754: INFO: Pod "downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615" satisfied condition "Succeeded or Failed"
    Mar  2 13:06:13.758: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615 container client-container: <nil>
    STEP: delete the pod 03/02/23 13:06:13.828
    Mar  2 13:06:13.842: INFO: Waiting for pod downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615 to disappear
    Mar  2 13:06:13.845: INFO: Pod downwardapi-volume-37b29c30-c54f-4f77-bdbb-1894f4cd7615 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 13:06:13.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8619" for this suite. 03/02/23 13:06:13.853
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:06:13.86
Mar  2 13:06:13.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename dns 03/02/23 13:06:13.861
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:06:13.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:06:13.95
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/02/23 13:06:13.958
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7332.svc.cluster.local;sleep 1; done
 03/02/23 13:06:13.966
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7332.svc.cluster.local;sleep 1; done
 03/02/23 13:06:13.966
STEP: creating a pod to probe DNS 03/02/23 13:06:13.966
STEP: submitting the pod to kubernetes 03/02/23 13:06:13.966
Mar  2 13:06:14.034: INFO: Waiting up to 15m0s for pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1" in namespace "dns-7332" to be "running"
Mar  2 13:06:14.045: INFO: Pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.915106ms
Mar  2 13:06:16.370: INFO: Pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.336774671s
Mar  2 13:06:18.052: INFO: Pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017827296s
Mar  2 13:06:20.049: INFO: Pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1": Phase="Running", Reason="", readiness=true. Elapsed: 6.015152744s
Mar  2 13:06:20.049: INFO: Pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1" satisfied condition "running"
STEP: retrieving the pod 03/02/23 13:06:20.05
STEP: looking for the results for each expected name from probers 03/02/23 13:06:20.052
Mar  2 13:06:20.063: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
Mar  2 13:06:20.067: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
Mar  2 13:06:20.070: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
Mar  2 13:06:20.122: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
Mar  2 13:06:20.129: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
Mar  2 13:06:20.135: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
Mar  2 13:06:20.143: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
Mar  2 13:06:20.151: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
Mar  2 13:06:20.151: INFO: Lookups using dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7332.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7332.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local jessie_udp@dns-test-service-2.dns-7332.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7332.svc.cluster.local]

Mar  2 13:06:25.211: INFO: DNS probes using dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1 succeeded

STEP: deleting the pod 03/02/23 13:06:25.212
STEP: deleting the test headless service 03/02/23 13:06:25.25
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 13:06:25.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7332" for this suite. 03/02/23 13:06:25.341
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":107,"skipped":1928,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.492 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:06:13.86
    Mar  2 13:06:13.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename dns 03/02/23 13:06:13.861
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:06:13.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:06:13.95
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/02/23 13:06:13.958
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7332.svc.cluster.local;sleep 1; done
     03/02/23 13:06:13.966
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7332.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7332.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7332.svc.cluster.local;sleep 1; done
     03/02/23 13:06:13.966
    STEP: creating a pod to probe DNS 03/02/23 13:06:13.966
    STEP: submitting the pod to kubernetes 03/02/23 13:06:13.966
    Mar  2 13:06:14.034: INFO: Waiting up to 15m0s for pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1" in namespace "dns-7332" to be "running"
    Mar  2 13:06:14.045: INFO: Pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.915106ms
    Mar  2 13:06:16.370: INFO: Pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.336774671s
    Mar  2 13:06:18.052: INFO: Pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017827296s
    Mar  2 13:06:20.049: INFO: Pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1": Phase="Running", Reason="", readiness=true. Elapsed: 6.015152744s
    Mar  2 13:06:20.049: INFO: Pod "dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 13:06:20.05
    STEP: looking for the results for each expected name from probers 03/02/23 13:06:20.052
    Mar  2 13:06:20.063: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
    Mar  2 13:06:20.067: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
    Mar  2 13:06:20.070: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
    Mar  2 13:06:20.122: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
    Mar  2 13:06:20.129: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
    Mar  2 13:06:20.135: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
    Mar  2 13:06:20.143: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
    Mar  2 13:06:20.151: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7332.svc.cluster.local from pod dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1: the server could not find the requested resource (get pods dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1)
    Mar  2 13:06:20.151: INFO: Lookups using dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7332.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7332.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7332.svc.cluster.local jessie_udp@dns-test-service-2.dns-7332.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7332.svc.cluster.local]

    Mar  2 13:06:25.211: INFO: DNS probes using dns-7332/dns-test-c6fa7eb3-18e3-4895-98cc-6909fe1a19f1 succeeded

    STEP: deleting the pod 03/02/23 13:06:25.212
    STEP: deleting the test headless service 03/02/23 13:06:25.25
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 13:06:25.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7332" for this suite. 03/02/23 13:06:25.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:06:25.364
Mar  2 13:06:25.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename replication-controller 03/02/23 13:06:25.367
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:06:25.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:06:25.427
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Mar  2 13:06:25.435: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/02/23 13:06:26.454
STEP: Checking rc "condition-test" has the desired failure condition set 03/02/23 13:06:26.46
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/02/23 13:06:27.558
Mar  2 13:06:27.579: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/02/23 13:06:27.579
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  2 13:06:27.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4886" for this suite. 03/02/23 13:06:27.605
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":108,"skipped":1949,"failed":0}
------------------------------
â€¢ [2.266 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:06:25.364
    Mar  2 13:06:25.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename replication-controller 03/02/23 13:06:25.367
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:06:25.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:06:25.427
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Mar  2 13:06:25.435: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/02/23 13:06:26.454
    STEP: Checking rc "condition-test" has the desired failure condition set 03/02/23 13:06:26.46
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/02/23 13:06:27.558
    Mar  2 13:06:27.579: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/02/23 13:06:27.579
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  2 13:06:27.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4886" for this suite. 03/02/23 13:06:27.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:06:27.633
Mar  2 13:06:27.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename watch 03/02/23 13:06:27.636
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:06:27.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:06:27.662
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/02/23 13:06:27.667
STEP: modifying the configmap once 03/02/23 13:06:27.688
STEP: modifying the configmap a second time 03/02/23 13:06:27.709
STEP: deleting the configmap 03/02/23 13:06:27.753
STEP: creating a watch on configmaps from the resource version returned by the first update 03/02/23 13:06:27.759
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/02/23 13:06:27.762
Mar  2 13:06:27.762: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7891  fe3db04f-514f-442f-9430-6006c760ca3a 1935177 0 2023-03-02 13:06:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-02 13:06:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:06:27.763: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7891  fe3db04f-514f-442f-9430-6006c760ca3a 1935178 0 2023-03-02 13:06:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-02 13:06:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  2 13:06:27.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7891" for this suite. 03/02/23 13:06:27.768
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":109,"skipped":1956,"failed":0}
------------------------------
â€¢ [0.149 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:06:27.633
    Mar  2 13:06:27.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename watch 03/02/23 13:06:27.636
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:06:27.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:06:27.662
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/02/23 13:06:27.667
    STEP: modifying the configmap once 03/02/23 13:06:27.688
    STEP: modifying the configmap a second time 03/02/23 13:06:27.709
    STEP: deleting the configmap 03/02/23 13:06:27.753
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/02/23 13:06:27.759
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/02/23 13:06:27.762
    Mar  2 13:06:27.762: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7891  fe3db04f-514f-442f-9430-6006c760ca3a 1935177 0 2023-03-02 13:06:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-02 13:06:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:06:27.763: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7891  fe3db04f-514f-442f-9430-6006c760ca3a 1935178 0 2023-03-02 13:06:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-02 13:06:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  2 13:06:27.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7891" for this suite. 03/02/23 13:06:27.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:06:27.786
Mar  2 13:06:27.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename svcaccounts 03/02/23 13:06:27.794
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:06:27.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:06:27.836
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Mar  2 13:06:27.858: INFO: created pod
Mar  2 13:06:27.858: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2491" to be "Succeeded or Failed"
Mar  2 13:06:27.869: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.297284ms
Mar  2 13:06:29.878: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020256961s
Mar  2 13:06:31.922: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064104015s
STEP: Saw pod success 03/02/23 13:06:31.925
Mar  2 13:06:31.929: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar  2 13:07:01.934: INFO: polling logs
Mar  2 13:07:01.941: INFO: Pod logs: 
I0302 13:06:29.288558       1 log.go:195] OK: Got token
I0302 13:06:29.288604       1 log.go:195] validating with in-cluster discovery
I0302 13:06:29.289082       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0302 13:06:29.289106       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2491:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677762988, NotBefore:1677762388, IssuedAt:1677762388, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2491", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"68e0b906-2bf1-4752-8b6c-d5a60e0b6a23"}}}
I0302 13:06:29.310520       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0302 13:06:29.330602       1 log.go:195] OK: Validated signature on JWT
I0302 13:06:29.330926       1 log.go:195] OK: Got valid claims from token!
I0302 13:06:29.331048       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2491:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677762988, NotBefore:1677762388, IssuedAt:1677762388, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2491", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"68e0b906-2bf1-4752-8b6c-d5a60e0b6a23"}}}

Mar  2 13:07:01.941: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 13:07:01.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2491" for this suite. 03/02/23 13:07:01.952
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":110,"skipped":1966,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.171 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:06:27.786
    Mar  2 13:06:27.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 13:06:27.794
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:06:27.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:06:27.836
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Mar  2 13:06:27.858: INFO: created pod
    Mar  2 13:06:27.858: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2491" to be "Succeeded or Failed"
    Mar  2 13:06:27.869: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.297284ms
    Mar  2 13:06:29.878: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020256961s
    Mar  2 13:06:31.922: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064104015s
    STEP: Saw pod success 03/02/23 13:06:31.925
    Mar  2 13:06:31.929: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar  2 13:07:01.934: INFO: polling logs
    Mar  2 13:07:01.941: INFO: Pod logs: 
    I0302 13:06:29.288558       1 log.go:195] OK: Got token
    I0302 13:06:29.288604       1 log.go:195] validating with in-cluster discovery
    I0302 13:06:29.289082       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0302 13:06:29.289106       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2491:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677762988, NotBefore:1677762388, IssuedAt:1677762388, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2491", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"68e0b906-2bf1-4752-8b6c-d5a60e0b6a23"}}}
    I0302 13:06:29.310520       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0302 13:06:29.330602       1 log.go:195] OK: Validated signature on JWT
    I0302 13:06:29.330926       1 log.go:195] OK: Got valid claims from token!
    I0302 13:06:29.331048       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2491:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677762988, NotBefore:1677762388, IssuedAt:1677762388, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2491", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"68e0b906-2bf1-4752-8b6c-d5a60e0b6a23"}}}

    Mar  2 13:07:01.941: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 13:07:01.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2491" for this suite. 03/02/23 13:07:01.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:07:01.974
Mar  2 13:07:01.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename security-context 03/02/23 13:07:01.977
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:07:02.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:07:02.041
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/02/23 13:07:02.053
Mar  2 13:07:02.107: INFO: Waiting up to 5m0s for pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431" in namespace "security-context-5145" to be "Succeeded or Failed"
Mar  2 13:07:02.121: INFO: Pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431": Phase="Pending", Reason="", readiness=false. Elapsed: 13.757413ms
Mar  2 13:07:04.132: INFO: Pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431": Phase="Running", Reason="", readiness=true. Elapsed: 2.025567084s
Mar  2 13:07:06.170: INFO: Pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431": Phase="Running", Reason="", readiness=false. Elapsed: 4.06279246s
Mar  2 13:07:08.130: INFO: Pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022870671s
STEP: Saw pod success 03/02/23 13:07:08.13
Mar  2 13:07:08.130: INFO: Pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431" satisfied condition "Succeeded or Failed"
Mar  2 13:07:08.134: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod security-context-9e9b42fd-019f-4960-98b8-46374b3b0431 container test-container: <nil>
STEP: delete the pod 03/02/23 13:07:08.142
Mar  2 13:07:08.156: INFO: Waiting for pod security-context-9e9b42fd-019f-4960-98b8-46374b3b0431 to disappear
Mar  2 13:07:08.166: INFO: Pod security-context-9e9b42fd-019f-4960-98b8-46374b3b0431 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 13:07:08.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5145" for this suite. 03/02/23 13:07:08.229
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":111,"skipped":2008,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.270 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:07:01.974
    Mar  2 13:07:01.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename security-context 03/02/23 13:07:01.977
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:07:02.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:07:02.041
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/02/23 13:07:02.053
    Mar  2 13:07:02.107: INFO: Waiting up to 5m0s for pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431" in namespace "security-context-5145" to be "Succeeded or Failed"
    Mar  2 13:07:02.121: INFO: Pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431": Phase="Pending", Reason="", readiness=false. Elapsed: 13.757413ms
    Mar  2 13:07:04.132: INFO: Pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431": Phase="Running", Reason="", readiness=true. Elapsed: 2.025567084s
    Mar  2 13:07:06.170: INFO: Pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431": Phase="Running", Reason="", readiness=false. Elapsed: 4.06279246s
    Mar  2 13:07:08.130: INFO: Pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022870671s
    STEP: Saw pod success 03/02/23 13:07:08.13
    Mar  2 13:07:08.130: INFO: Pod "security-context-9e9b42fd-019f-4960-98b8-46374b3b0431" satisfied condition "Succeeded or Failed"
    Mar  2 13:07:08.134: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod security-context-9e9b42fd-019f-4960-98b8-46374b3b0431 container test-container: <nil>
    STEP: delete the pod 03/02/23 13:07:08.142
    Mar  2 13:07:08.156: INFO: Waiting for pod security-context-9e9b42fd-019f-4960-98b8-46374b3b0431 to disappear
    Mar  2 13:07:08.166: INFO: Pod security-context-9e9b42fd-019f-4960-98b8-46374b3b0431 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 13:07:08.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-5145" for this suite. 03/02/23 13:07:08.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:07:08.253
Mar  2 13:07:08.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename subpath 03/02/23 13:07:08.254
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:07:08.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:07:08.359
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/02/23 13:07:08.364
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-dg8h 03/02/23 13:07:08.449
STEP: Creating a pod to test atomic-volume-subpath 03/02/23 13:07:08.449
Mar  2 13:07:08.500: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-dg8h" in namespace "subpath-7858" to be "Succeeded or Failed"
Mar  2 13:07:08.535: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Pending", Reason="", readiness=false. Elapsed: 35.616336ms
Mar  2 13:07:10.544: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044484761s
Mar  2 13:07:12.541: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 4.041596142s
Mar  2 13:07:14.543: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 6.043084469s
Mar  2 13:07:16.539: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 8.039277752s
Mar  2 13:07:18.541: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 10.041297s
Mar  2 13:07:20.540: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 12.040250281s
Mar  2 13:07:22.543: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 14.043782641s
Mar  2 13:07:24.541: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 16.041334528s
Mar  2 13:07:26.541: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 18.040892985s
Mar  2 13:07:28.540: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 20.040772755s
Mar  2 13:07:30.540: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 22.040649328s
Mar  2 13:07:32.542: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 24.04257504s
Mar  2 13:07:34.718: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=false. Elapsed: 26.218333611s
Mar  2 13:07:36.539: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.039725528s
STEP: Saw pod success 03/02/23 13:07:36.54
Mar  2 13:07:36.540: INFO: Pod "pod-subpath-test-secret-dg8h" satisfied condition "Succeeded or Failed"
Mar  2 13:07:36.544: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-subpath-test-secret-dg8h container test-container-subpath-secret-dg8h: <nil>
STEP: delete the pod 03/02/23 13:07:36.551
Mar  2 13:07:36.568: INFO: Waiting for pod pod-subpath-test-secret-dg8h to disappear
Mar  2 13:07:36.571: INFO: Pod pod-subpath-test-secret-dg8h no longer exists
STEP: Deleting pod pod-subpath-test-secret-dg8h 03/02/23 13:07:36.622
Mar  2 13:07:36.622: INFO: Deleting pod "pod-subpath-test-secret-dg8h" in namespace "subpath-7858"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  2 13:07:36.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7858" for this suite. 03/02/23 13:07:36.642
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":112,"skipped":2032,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.404 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:07:08.253
    Mar  2 13:07:08.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename subpath 03/02/23 13:07:08.254
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:07:08.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:07:08.359
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/02/23 13:07:08.364
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-dg8h 03/02/23 13:07:08.449
    STEP: Creating a pod to test atomic-volume-subpath 03/02/23 13:07:08.449
    Mar  2 13:07:08.500: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-dg8h" in namespace "subpath-7858" to be "Succeeded or Failed"
    Mar  2 13:07:08.535: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Pending", Reason="", readiness=false. Elapsed: 35.616336ms
    Mar  2 13:07:10.544: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044484761s
    Mar  2 13:07:12.541: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 4.041596142s
    Mar  2 13:07:14.543: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 6.043084469s
    Mar  2 13:07:16.539: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 8.039277752s
    Mar  2 13:07:18.541: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 10.041297s
    Mar  2 13:07:20.540: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 12.040250281s
    Mar  2 13:07:22.543: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 14.043782641s
    Mar  2 13:07:24.541: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 16.041334528s
    Mar  2 13:07:26.541: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 18.040892985s
    Mar  2 13:07:28.540: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 20.040772755s
    Mar  2 13:07:30.540: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 22.040649328s
    Mar  2 13:07:32.542: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=true. Elapsed: 24.04257504s
    Mar  2 13:07:34.718: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Running", Reason="", readiness=false. Elapsed: 26.218333611s
    Mar  2 13:07:36.539: INFO: Pod "pod-subpath-test-secret-dg8h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.039725528s
    STEP: Saw pod success 03/02/23 13:07:36.54
    Mar  2 13:07:36.540: INFO: Pod "pod-subpath-test-secret-dg8h" satisfied condition "Succeeded or Failed"
    Mar  2 13:07:36.544: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-subpath-test-secret-dg8h container test-container-subpath-secret-dg8h: <nil>
    STEP: delete the pod 03/02/23 13:07:36.551
    Mar  2 13:07:36.568: INFO: Waiting for pod pod-subpath-test-secret-dg8h to disappear
    Mar  2 13:07:36.571: INFO: Pod pod-subpath-test-secret-dg8h no longer exists
    STEP: Deleting pod pod-subpath-test-secret-dg8h 03/02/23 13:07:36.622
    Mar  2 13:07:36.622: INFO: Deleting pod "pod-subpath-test-secret-dg8h" in namespace "subpath-7858"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  2 13:07:36.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7858" for this suite. 03/02/23 13:07:36.642
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:07:36.659
Mar  2 13:07:36.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename gc 03/02/23 13:07:36.665
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:07:36.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:07:36.736
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/02/23 13:07:36.749
STEP: create the rc2 03/02/23 13:07:36.753
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/02/23 13:07:42.07
STEP: delete the rc simpletest-rc-to-be-deleted 03/02/23 13:07:44.523
STEP: wait for the rc to be deleted 03/02/23 13:07:44.662
Mar  2 13:07:51.990: INFO: 68 pods remaining
Mar  2 13:07:51.990: INFO: 68 pods has nil DeletionTimestamp
Mar  2 13:07:51.990: INFO: 
STEP: Gathering metrics 03/02/23 13:07:54.742
Mar  2 13:07:54.884: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
Mar  2 13:07:54.936: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 51.744682ms
Mar  2 13:07:54.936: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
Mar  2 13:07:54.936: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
Mar  2 13:07:55.256: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar  2 13:07:55.257: INFO: Deleting pod "simpletest-rc-to-be-deleted-279fl" in namespace "gc-992"
Mar  2 13:07:55.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-2frfd" in namespace "gc-992"
Mar  2 13:07:55.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lr68" in namespace "gc-992"
Mar  2 13:07:55.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-2v8x4" in namespace "gc-992"
Mar  2 13:07:56.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vkb9" in namespace "gc-992"
Mar  2 13:07:56.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kvm5" in namespace "gc-992"
Mar  2 13:07:56.166: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p7b6" in namespace "gc-992"
Mar  2 13:07:56.328: INFO: Deleting pod "simpletest-rc-to-be-deleted-52sr2" in namespace "gc-992"
Mar  2 13:07:56.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gpx7" in namespace "gc-992"
Mar  2 13:07:56.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hxw7" in namespace "gc-992"
Mar  2 13:07:56.451: INFO: Deleting pod "simpletest-rc-to-be-deleted-69fg5" in namespace "gc-992"
Mar  2 13:07:56.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xdm9" in namespace "gc-992"
Mar  2 13:07:56.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bnlv" in namespace "gc-992"
Mar  2 13:07:56.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rmvs" in namespace "gc-992"
Mar  2 13:07:56.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-84w8q" in namespace "gc-992"
Mar  2 13:07:56.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fspv" in namespace "gc-992"
Mar  2 13:07:56.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ls9t" in namespace "gc-992"
Mar  2 13:07:56.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hrsf" in namespace "gc-992"
Mar  2 13:07:56.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-9khwx" in namespace "gc-992"
Mar  2 13:07:56.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kx9g" in namespace "gc-992"
Mar  2 13:07:57.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6ww2" in namespace "gc-992"
Mar  2 13:07:57.065: INFO: Deleting pod "simpletest-rc-to-be-deleted-cblmk" in namespace "gc-992"
Mar  2 13:07:57.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgmph" in namespace "gc-992"
Mar  2 13:07:57.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-cld46" in namespace "gc-992"
Mar  2 13:07:57.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwtdt" in namespace "gc-992"
Mar  2 13:07:57.243: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz2sb" in namespace "gc-992"
Mar  2 13:07:57.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz664" in namespace "gc-992"
Mar  2 13:07:57.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4rv8" in namespace "gc-992"
Mar  2 13:07:57.444: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7xqb" in namespace "gc-992"
Mar  2 13:07:57.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftcxh" in namespace "gc-992"
Mar  2 13:07:57.554: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw985" in namespace "gc-992"
Mar  2 13:07:57.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc5k9" in namespace "gc-992"
Mar  2 13:07:57.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjqbp" in namespace "gc-992"
Mar  2 13:07:57.966: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq7pv" in namespace "gc-992"
Mar  2 13:07:58.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvsj4" in namespace "gc-992"
Mar  2 13:07:58.106: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzjsf" in namespace "gc-992"
Mar  2 13:07:58.214: INFO: Deleting pod "simpletest-rc-to-be-deleted-h87cr" in namespace "gc-992"
Mar  2 13:07:58.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdzsn" in namespace "gc-992"
Mar  2 13:07:58.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-hf6f8" in namespace "gc-992"
Mar  2 13:07:58.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgn7r" in namespace "gc-992"
Mar  2 13:07:58.564: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkc2h" in namespace "gc-992"
Mar  2 13:07:58.660: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxpbq" in namespace "gc-992"
Mar  2 13:07:58.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6skg" in namespace "gc-992"
Mar  2 13:07:58.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7xst" in namespace "gc-992"
Mar  2 13:07:58.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8g7r" in namespace "gc-992"
Mar  2 13:07:58.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8rvq" in namespace "gc-992"
Mar  2 13:07:58.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-jc45j" in namespace "gc-992"
Mar  2 13:07:58.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgxqn" in namespace "gc-992"
Mar  2 13:07:58.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-k9cq8" in namespace "gc-992"
Mar  2 13:07:59.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbcqr" in namespace "gc-992"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 13:07:59.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-992" for this suite. 03/02/23 13:07:59.036
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":113,"skipped":2035,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.384 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:07:36.659
    Mar  2 13:07:36.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename gc 03/02/23 13:07:36.665
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:07:36.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:07:36.736
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/02/23 13:07:36.749
    STEP: create the rc2 03/02/23 13:07:36.753
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/02/23 13:07:42.07
    STEP: delete the rc simpletest-rc-to-be-deleted 03/02/23 13:07:44.523
    STEP: wait for the rc to be deleted 03/02/23 13:07:44.662
    Mar  2 13:07:51.990: INFO: 68 pods remaining
    Mar  2 13:07:51.990: INFO: 68 pods has nil DeletionTimestamp
    Mar  2 13:07:51.990: INFO: 
    STEP: Gathering metrics 03/02/23 13:07:54.742
    Mar  2 13:07:54.884: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
    Mar  2 13:07:54.936: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 51.744682ms
    Mar  2 13:07:54.936: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
    Mar  2 13:07:54.936: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
    Mar  2 13:07:55.256: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar  2 13:07:55.257: INFO: Deleting pod "simpletest-rc-to-be-deleted-279fl" in namespace "gc-992"
    Mar  2 13:07:55.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-2frfd" in namespace "gc-992"
    Mar  2 13:07:55.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lr68" in namespace "gc-992"
    Mar  2 13:07:55.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-2v8x4" in namespace "gc-992"
    Mar  2 13:07:56.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vkb9" in namespace "gc-992"
    Mar  2 13:07:56.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kvm5" in namespace "gc-992"
    Mar  2 13:07:56.166: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p7b6" in namespace "gc-992"
    Mar  2 13:07:56.328: INFO: Deleting pod "simpletest-rc-to-be-deleted-52sr2" in namespace "gc-992"
    Mar  2 13:07:56.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gpx7" in namespace "gc-992"
    Mar  2 13:07:56.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hxw7" in namespace "gc-992"
    Mar  2 13:07:56.451: INFO: Deleting pod "simpletest-rc-to-be-deleted-69fg5" in namespace "gc-992"
    Mar  2 13:07:56.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xdm9" in namespace "gc-992"
    Mar  2 13:07:56.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bnlv" in namespace "gc-992"
    Mar  2 13:07:56.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rmvs" in namespace "gc-992"
    Mar  2 13:07:56.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-84w8q" in namespace "gc-992"
    Mar  2 13:07:56.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fspv" in namespace "gc-992"
    Mar  2 13:07:56.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ls9t" in namespace "gc-992"
    Mar  2 13:07:56.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hrsf" in namespace "gc-992"
    Mar  2 13:07:56.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-9khwx" in namespace "gc-992"
    Mar  2 13:07:56.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kx9g" in namespace "gc-992"
    Mar  2 13:07:57.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6ww2" in namespace "gc-992"
    Mar  2 13:07:57.065: INFO: Deleting pod "simpletest-rc-to-be-deleted-cblmk" in namespace "gc-992"
    Mar  2 13:07:57.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgmph" in namespace "gc-992"
    Mar  2 13:07:57.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-cld46" in namespace "gc-992"
    Mar  2 13:07:57.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwtdt" in namespace "gc-992"
    Mar  2 13:07:57.243: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz2sb" in namespace "gc-992"
    Mar  2 13:07:57.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz664" in namespace "gc-992"
    Mar  2 13:07:57.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4rv8" in namespace "gc-992"
    Mar  2 13:07:57.444: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7xqb" in namespace "gc-992"
    Mar  2 13:07:57.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftcxh" in namespace "gc-992"
    Mar  2 13:07:57.554: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw985" in namespace "gc-992"
    Mar  2 13:07:57.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc5k9" in namespace "gc-992"
    Mar  2 13:07:57.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjqbp" in namespace "gc-992"
    Mar  2 13:07:57.966: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq7pv" in namespace "gc-992"
    Mar  2 13:07:58.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvsj4" in namespace "gc-992"
    Mar  2 13:07:58.106: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzjsf" in namespace "gc-992"
    Mar  2 13:07:58.214: INFO: Deleting pod "simpletest-rc-to-be-deleted-h87cr" in namespace "gc-992"
    Mar  2 13:07:58.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdzsn" in namespace "gc-992"
    Mar  2 13:07:58.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-hf6f8" in namespace "gc-992"
    Mar  2 13:07:58.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgn7r" in namespace "gc-992"
    Mar  2 13:07:58.564: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkc2h" in namespace "gc-992"
    Mar  2 13:07:58.660: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxpbq" in namespace "gc-992"
    Mar  2 13:07:58.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6skg" in namespace "gc-992"
    Mar  2 13:07:58.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7xst" in namespace "gc-992"
    Mar  2 13:07:58.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8g7r" in namespace "gc-992"
    Mar  2 13:07:58.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8rvq" in namespace "gc-992"
    Mar  2 13:07:58.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-jc45j" in namespace "gc-992"
    Mar  2 13:07:58.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgxqn" in namespace "gc-992"
    Mar  2 13:07:58.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-k9cq8" in namespace "gc-992"
    Mar  2 13:07:59.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbcqr" in namespace "gc-992"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 13:07:59.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-992" for this suite. 03/02/23 13:07:59.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:07:59.048
Mar  2 13:07:59.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sched-preemption 03/02/23 13:07:59.054
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:07:59.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:07:59.088
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 13:07:59.106: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 13:08:59.321: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 03/02/23 13:08:59.324
Mar  2 13:08:59.355: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  2 13:08:59.367: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  2 13:08:59.408: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  2 13:08:59.423: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar  2 13:08:59.486: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar  2 13:08:59.526: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Mar  2 13:08:59.584: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Mar  2 13:08:59.598: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/02/23 13:08:59.598
Mar  2 13:08:59.598: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3053" to be "running"
Mar  2 13:08:59.624: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 25.652281ms
Mar  2 13:09:01.766: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168006255s
Mar  2 13:09:03.658: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059830056s
Mar  2 13:09:05.649: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051461008s
Mar  2 13:09:07.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032383015s
Mar  2 13:09:09.631: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033128013s
Mar  2 13:09:11.649: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.051193351s
Mar  2 13:09:11.649: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar  2 13:09:11.649: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
Mar  2 13:09:11.654: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.135001ms
Mar  2 13:09:11.654: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 13:09:11.654: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
Mar  2 13:09:11.660: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.047508ms
Mar  2 13:09:11.660: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 13:09:11.660: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
Mar  2 13:09:11.664: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.670787ms
Mar  2 13:09:11.664: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 13:09:11.664: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
Mar  2 13:09:11.667: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.863701ms
Mar  2 13:09:13.674: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009242908s
Mar  2 13:09:13.674: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 13:09:13.674: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
Mar  2 13:09:13.678: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.211911ms
Mar  2 13:09:13.678: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 13:09:13.678: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
Mar  2 13:09:13.698: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.767526ms
Mar  2 13:09:13.698: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 13:09:13.698: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
Mar  2 13:09:13.702: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.125205ms
Mar  2 13:09:13.702: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/02/23 13:09:13.702
Mar  2 13:09:13.712: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3053" to be "running"
Mar  2 13:09:13.719: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.365897ms
Mar  2 13:09:15.735: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022003667s
Mar  2 13:09:17.750: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037167971s
Mar  2 13:09:19.724: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.011259331s
Mar  2 13:09:19.724: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  2 13:09:19.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3053" for this suite. 03/02/23 13:09:19.759
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":114,"skipped":2044,"failed":0}
------------------------------
â€¢ [SLOW TEST] [80.798 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:07:59.048
    Mar  2 13:07:59.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sched-preemption 03/02/23 13:07:59.054
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:07:59.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:07:59.088
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  2 13:07:59.106: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 13:08:59.321: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 03/02/23 13:08:59.324
    Mar  2 13:08:59.355: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar  2 13:08:59.367: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar  2 13:08:59.408: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar  2 13:08:59.423: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar  2 13:08:59.486: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar  2 13:08:59.526: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Mar  2 13:08:59.584: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Mar  2 13:08:59.598: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/02/23 13:08:59.598
    Mar  2 13:08:59.598: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3053" to be "running"
    Mar  2 13:08:59.624: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 25.652281ms
    Mar  2 13:09:01.766: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168006255s
    Mar  2 13:09:03.658: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059830056s
    Mar  2 13:09:05.649: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051461008s
    Mar  2 13:09:07.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032383015s
    Mar  2 13:09:09.631: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033128013s
    Mar  2 13:09:11.649: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.051193351s
    Mar  2 13:09:11.649: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar  2 13:09:11.649: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
    Mar  2 13:09:11.654: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.135001ms
    Mar  2 13:09:11.654: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 13:09:11.654: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
    Mar  2 13:09:11.660: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.047508ms
    Mar  2 13:09:11.660: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 13:09:11.660: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
    Mar  2 13:09:11.664: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.670787ms
    Mar  2 13:09:11.664: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 13:09:11.664: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
    Mar  2 13:09:11.667: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.863701ms
    Mar  2 13:09:13.674: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009242908s
    Mar  2 13:09:13.674: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 13:09:13.674: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
    Mar  2 13:09:13.678: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.211911ms
    Mar  2 13:09:13.678: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 13:09:13.678: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
    Mar  2 13:09:13.698: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.767526ms
    Mar  2 13:09:13.698: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 13:09:13.698: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-3053" to be "running"
    Mar  2 13:09:13.702: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.125205ms
    Mar  2 13:09:13.702: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/02/23 13:09:13.702
    Mar  2 13:09:13.712: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3053" to be "running"
    Mar  2 13:09:13.719: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.365897ms
    Mar  2 13:09:15.735: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022003667s
    Mar  2 13:09:17.750: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037167971s
    Mar  2 13:09:19.724: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.011259331s
    Mar  2 13:09:19.724: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 13:09:19.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3053" for this suite. 03/02/23 13:09:19.759
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:09:19.855
Mar  2 13:09:19.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:09:19.859
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:09:19.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:09:19.899
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:09:19.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8839" for this suite. 03/02/23 13:09:19.967
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":115,"skipped":2074,"failed":0}
------------------------------
â€¢ [0.168 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:09:19.855
    Mar  2 13:09:19.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:09:19.859
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:09:19.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:09:19.899
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:09:19.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8839" for this suite. 03/02/23 13:09:19.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:09:20.03
Mar  2 13:09:20.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename daemonsets 03/02/23 13:09:20.031
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:09:20.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:09:20.055
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Mar  2 13:09:20.143: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 13:09:20.147
Mar  2 13:09:20.156: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:20.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:09:20.163: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 13:09:21.171: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:21.253: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:09:21.253: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 13:09:22.171: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:22.190: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 13:09:22.190: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 13:09:23.171: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:23.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Mar  2 13:09:23.231: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Update daemon pods image. 03/02/23 13:09:23.25
STEP: Check that daemon pods images are updated. 03/02/23 13:09:23.315
Mar  2 13:09:23.327: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:23.327: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:23.327: INFO: Wrong image for pod: daemon-set-z6292. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:23.334: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:24.344: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:24.345: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:24.345: INFO: Wrong image for pod: daemon-set-z6292. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:24.352: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:25.354: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:25.355: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:25.355: INFO: Wrong image for pod: daemon-set-z6292. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:25.364: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:26.522: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:26.550: INFO: Pod daemon-set-hx68r is not available
Mar  2 13:09:26.550: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:26.550: INFO: Wrong image for pod: daemon-set-z6292. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:26.642: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:27.341: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:27.341: INFO: Pod daemon-set-hx68r is not available
Mar  2 13:09:27.341: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:27.341: INFO: Wrong image for pod: daemon-set-z6292. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:27.350: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:28.343: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:28.343: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:28.351: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:29.349: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:29.349: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:29.357: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:30.341: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:30.341: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:30.341: INFO: Pod daemon-set-zkgxv is not available
Mar  2 13:09:30.351: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:31.345: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:31.345: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:31.345: INFO: Pod daemon-set-zkgxv is not available
Mar  2 13:09:31.355: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:32.340: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:32.346: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:33.356: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:33.356: INFO: Pod daemon-set-s6h4k is not available
Mar  2 13:09:33.370: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:34.359: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 13:09:34.359: INFO: Pod daemon-set-s6h4k is not available
Mar  2 13:09:34.386: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:35.364: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:36.338: INFO: Pod daemon-set-w2ntl is not available
Mar  2 13:09:36.344: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 03/02/23 13:09:36.344
Mar  2 13:09:36.351: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:36.355: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 13:09:36.355: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
Mar  2 13:09:37.361: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:37.364: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 13:09:37.365: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
Mar  2 13:09:38.362: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:09:38.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Mar  2 13:09:38.367: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 13:09:38.433
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8820, will wait for the garbage collector to delete the pods 03/02/23 13:09:38.434
Mar  2 13:09:38.514: INFO: Deleting DaemonSet.extensions daemon-set took: 12.112244ms
Mar  2 13:09:38.650: INFO: Terminating DaemonSet.extensions daemon-set pods took: 135.479525ms
Mar  2 13:09:42.153: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:09:42.153: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 13:09:42.160: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1938808"},"items":null}

Mar  2 13:09:42.164: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1938808"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 13:09:42.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8820" for this suite. 03/02/23 13:09:42.198
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":116,"skipped":2080,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.182 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:09:20.03
    Mar  2 13:09:20.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename daemonsets 03/02/23 13:09:20.031
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:09:20.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:09:20.055
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Mar  2 13:09:20.143: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 13:09:20.147
    Mar  2 13:09:20.156: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:20.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:09:20.163: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 13:09:21.171: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:21.253: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:09:21.253: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 13:09:22.171: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:22.190: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  2 13:09:22.190: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 13:09:23.171: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:23.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Mar  2 13:09:23.231: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Update daemon pods image. 03/02/23 13:09:23.25
    STEP: Check that daemon pods images are updated. 03/02/23 13:09:23.315
    Mar  2 13:09:23.327: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:23.327: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:23.327: INFO: Wrong image for pod: daemon-set-z6292. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:23.334: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:24.344: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:24.345: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:24.345: INFO: Wrong image for pod: daemon-set-z6292. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:24.352: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:25.354: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:25.355: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:25.355: INFO: Wrong image for pod: daemon-set-z6292. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:25.364: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:26.522: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:26.550: INFO: Pod daemon-set-hx68r is not available
    Mar  2 13:09:26.550: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:26.550: INFO: Wrong image for pod: daemon-set-z6292. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:26.642: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:27.341: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:27.341: INFO: Pod daemon-set-hx68r is not available
    Mar  2 13:09:27.341: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:27.341: INFO: Wrong image for pod: daemon-set-z6292. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:27.350: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:28.343: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:28.343: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:28.351: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:29.349: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:29.349: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:29.357: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:30.341: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:30.341: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:30.341: INFO: Pod daemon-set-zkgxv is not available
    Mar  2 13:09:30.351: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:31.345: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:31.345: INFO: Wrong image for pod: daemon-set-vhnk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:31.345: INFO: Pod daemon-set-zkgxv is not available
    Mar  2 13:09:31.355: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:32.340: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:32.346: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:33.356: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:33.356: INFO: Pod daemon-set-s6h4k is not available
    Mar  2 13:09:33.370: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:34.359: INFO: Wrong image for pod: daemon-set-gtqrm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 13:09:34.359: INFO: Pod daemon-set-s6h4k is not available
    Mar  2 13:09:34.386: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:35.364: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:36.338: INFO: Pod daemon-set-w2ntl is not available
    Mar  2 13:09:36.344: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 03/02/23 13:09:36.344
    Mar  2 13:09:36.351: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:36.355: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 13:09:36.355: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
    Mar  2 13:09:37.361: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:37.364: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 13:09:37.365: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
    Mar  2 13:09:38.362: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:09:38.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Mar  2 13:09:38.367: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 13:09:38.433
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8820, will wait for the garbage collector to delete the pods 03/02/23 13:09:38.434
    Mar  2 13:09:38.514: INFO: Deleting DaemonSet.extensions daemon-set took: 12.112244ms
    Mar  2 13:09:38.650: INFO: Terminating DaemonSet.extensions daemon-set pods took: 135.479525ms
    Mar  2 13:09:42.153: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:09:42.153: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 13:09:42.160: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1938808"},"items":null}

    Mar  2 13:09:42.164: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1938808"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 13:09:42.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8820" for this suite. 03/02/23 13:09:42.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:09:42.22
Mar  2 13:09:42.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:09:42.222
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:09:42.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:09:42.26
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 03/02/23 13:09:42.265
Mar  2 13:09:42.318: INFO: Waiting up to 5m0s for pod "downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f" in namespace "projected-3831" to be "Succeeded or Failed"
Mar  2 13:09:42.327: INFO: Pod "downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.555034ms
Mar  2 13:09:44.351: INFO: Pod "downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032832294s
Mar  2 13:09:46.332: INFO: Pod "downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013583263s
STEP: Saw pod success 03/02/23 13:09:46.332
Mar  2 13:09:46.332: INFO: Pod "downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f" satisfied condition "Succeeded or Failed"
Mar  2 13:09:46.335: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f container client-container: <nil>
STEP: delete the pod 03/02/23 13:09:46.346
Mar  2 13:09:46.363: INFO: Waiting for pod downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f to disappear
Mar  2 13:09:46.366: INFO: Pod downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 13:09:46.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3831" for this suite. 03/02/23 13:09:46.414
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":117,"skipped":2088,"failed":0}
------------------------------
â€¢ [4.223 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:09:42.22
    Mar  2 13:09:42.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:09:42.222
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:09:42.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:09:42.26
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 03/02/23 13:09:42.265
    Mar  2 13:09:42.318: INFO: Waiting up to 5m0s for pod "downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f" in namespace "projected-3831" to be "Succeeded or Failed"
    Mar  2 13:09:42.327: INFO: Pod "downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.555034ms
    Mar  2 13:09:44.351: INFO: Pod "downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032832294s
    Mar  2 13:09:46.332: INFO: Pod "downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013583263s
    STEP: Saw pod success 03/02/23 13:09:46.332
    Mar  2 13:09:46.332: INFO: Pod "downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f" satisfied condition "Succeeded or Failed"
    Mar  2 13:09:46.335: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f container client-container: <nil>
    STEP: delete the pod 03/02/23 13:09:46.346
    Mar  2 13:09:46.363: INFO: Waiting for pod downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f to disappear
    Mar  2 13:09:46.366: INFO: Pod downwardapi-volume-547603f8-5729-4a50-9f61-0d00f124399f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 13:09:46.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3831" for this suite. 03/02/23 13:09:46.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:09:46.452
Mar  2 13:09:46.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename disruption 03/02/23 13:09:46.453
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:09:46.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:09:46.514
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 03/02/23 13:09:46.528
STEP: Waiting for all pods to be running 03/02/23 13:09:48.593
Mar  2 13:09:48.612: INFO: running pods: 0 < 3
Mar  2 13:09:50.622: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  2 13:09:52.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5113" for this suite. 03/02/23 13:09:52.633
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":118,"skipped":2161,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.189 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:09:46.452
    Mar  2 13:09:46.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename disruption 03/02/23 13:09:46.453
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:09:46.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:09:46.514
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 03/02/23 13:09:46.528
    STEP: Waiting for all pods to be running 03/02/23 13:09:48.593
    Mar  2 13:09:48.612: INFO: running pods: 0 < 3
    Mar  2 13:09:50.622: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  2 13:09:52.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5113" for this suite. 03/02/23 13:09:52.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:09:52.65
Mar  2 13:09:52.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:09:52.652
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:09:52.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:09:52.724
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 03/02/23 13:09:52.734
Mar  2 13:09:52.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 create -f -'
Mar  2 13:09:54.737: INFO: stderr: ""
Mar  2 13:09:54.737: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 13:09:54.737
Mar  2 13:09:54.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 13:09:54.890: INFO: stderr: ""
Mar  2 13:09:54.890: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-m662r "
Mar  2 13:09:54.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 13:09:55.129: INFO: stderr: ""
Mar  2 13:09:55.129: INFO: stdout: ""
Mar  2 13:09:55.129: INFO: update-demo-nautilus-cknbm is created but not running
Mar  2 13:10:00.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 13:10:00.541: INFO: stderr: ""
Mar  2 13:10:00.541: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-m662r "
Mar  2 13:10:00.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 13:10:01.463: INFO: stderr: ""
Mar  2 13:10:01.463: INFO: stdout: ""
Mar  2 13:10:01.463: INFO: update-demo-nautilus-cknbm is created but not running
Mar  2 13:10:06.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 13:10:06.664: INFO: stderr: ""
Mar  2 13:10:06.664: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-m662r "
Mar  2 13:10:06.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 13:10:06.938: INFO: stderr: ""
Mar  2 13:10:06.938: INFO: stdout: "true"
Mar  2 13:10:06.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 13:10:07.128: INFO: stderr: ""
Mar  2 13:10:07.128: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 13:10:07.128: INFO: validating pod update-demo-nautilus-cknbm
Mar  2 13:10:07.136: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 13:10:07.136: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 13:10:07.136: INFO: update-demo-nautilus-cknbm is verified up and running
Mar  2 13:10:07.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-m662r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 13:10:07.247: INFO: stderr: ""
Mar  2 13:10:07.247: INFO: stdout: ""
Mar  2 13:10:07.247: INFO: update-demo-nautilus-m662r is created but not running
Mar  2 13:10:12.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 13:10:12.399: INFO: stderr: ""
Mar  2 13:10:12.399: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-m662r "
Mar  2 13:10:12.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 13:10:12.559: INFO: stderr: ""
Mar  2 13:10:12.559: INFO: stdout: "true"
Mar  2 13:10:12.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 13:10:12.681: INFO: stderr: ""
Mar  2 13:10:12.682: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 13:10:12.682: INFO: validating pod update-demo-nautilus-cknbm
Mar  2 13:10:12.692: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 13:10:12.692: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 13:10:12.692: INFO: update-demo-nautilus-cknbm is verified up and running
Mar  2 13:10:12.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-m662r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 13:10:12.832: INFO: stderr: ""
Mar  2 13:10:12.832: INFO: stdout: "true"
Mar  2 13:10:12.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-m662r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 13:10:12.961: INFO: stderr: ""
Mar  2 13:10:12.961: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 13:10:12.961: INFO: validating pod update-demo-nautilus-m662r
Mar  2 13:10:12.967: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 13:10:12.967: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 13:10:12.967: INFO: update-demo-nautilus-m662r is verified up and running
STEP: scaling down the replication controller 03/02/23 13:10:12.967
Mar  2 13:10:12.970: INFO: scanned /root for discovery docs: <nil>
Mar  2 13:10:12.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar  2 13:10:14.156: INFO: stderr: ""
Mar  2 13:10:14.156: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 13:10:14.156
Mar  2 13:10:14.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 13:10:14.348: INFO: stderr: ""
Mar  2 13:10:14.348: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-m662r "
STEP: Replicas for name=update-demo: expected=1 actual=2 03/02/23 13:10:14.348
Mar  2 13:10:19.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 13:10:19.527: INFO: stderr: ""
Mar  2 13:10:19.527: INFO: stdout: "update-demo-nautilus-cknbm "
Mar  2 13:10:19.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 13:10:19.663: INFO: stderr: ""
Mar  2 13:10:19.663: INFO: stdout: "true"
Mar  2 13:10:19.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 13:10:19.841: INFO: stderr: ""
Mar  2 13:10:19.841: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 13:10:19.841: INFO: validating pod update-demo-nautilus-cknbm
Mar  2 13:10:19.856: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 13:10:19.856: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 13:10:19.856: INFO: update-demo-nautilus-cknbm is verified up and running
STEP: scaling up the replication controller 03/02/23 13:10:19.856
Mar  2 13:10:19.864: INFO: scanned /root for discovery docs: <nil>
Mar  2 13:10:19.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar  2 13:10:21.059: INFO: stderr: ""
Mar  2 13:10:21.059: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 13:10:21.059
Mar  2 13:10:21.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 13:10:21.192: INFO: stderr: ""
Mar  2 13:10:21.192: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-fsvpx "
Mar  2 13:10:21.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 13:10:21.288: INFO: stderr: ""
Mar  2 13:10:21.288: INFO: stdout: "true"
Mar  2 13:10:21.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 13:10:21.396: INFO: stderr: ""
Mar  2 13:10:21.396: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 13:10:21.396: INFO: validating pod update-demo-nautilus-cknbm
Mar  2 13:10:21.401: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 13:10:21.401: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 13:10:21.401: INFO: update-demo-nautilus-cknbm is verified up and running
Mar  2 13:10:21.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-fsvpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 13:10:21.574: INFO: stderr: ""
Mar  2 13:10:21.574: INFO: stdout: ""
Mar  2 13:10:21.574: INFO: update-demo-nautilus-fsvpx is created but not running
Mar  2 13:10:26.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 13:10:26.700: INFO: stderr: ""
Mar  2 13:10:26.700: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-fsvpx "
Mar  2 13:10:26.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 13:10:26.819: INFO: stderr: ""
Mar  2 13:10:26.819: INFO: stdout: "true"
Mar  2 13:10:26.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 13:10:26.938: INFO: stderr: ""
Mar  2 13:10:26.938: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 13:10:26.938: INFO: validating pod update-demo-nautilus-cknbm
Mar  2 13:10:26.943: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 13:10:26.943: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 13:10:26.943: INFO: update-demo-nautilus-cknbm is verified up and running
Mar  2 13:10:26.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-fsvpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 13:10:27.058: INFO: stderr: ""
Mar  2 13:10:27.058: INFO: stdout: "true"
Mar  2 13:10:27.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-fsvpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 13:10:27.145: INFO: stderr: ""
Mar  2 13:10:27.145: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 13:10:27.145: INFO: validating pod update-demo-nautilus-fsvpx
Mar  2 13:10:27.155: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 13:10:27.155: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 13:10:27.155: INFO: update-demo-nautilus-fsvpx is verified up and running
STEP: using delete to clean up resources 03/02/23 13:10:27.155
Mar  2 13:10:27.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 delete --grace-period=0 --force -f -'
Mar  2 13:10:27.269: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 13:10:27.269: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 13:10:27.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get rc,svc -l name=update-demo --no-headers'
Mar  2 13:10:27.449: INFO: stderr: "No resources found in kubectl-5746 namespace.\n"
Mar  2 13:10:27.449: INFO: stdout: ""
Mar  2 13:10:27.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 13:10:27.611: INFO: stderr: ""
Mar  2 13:10:27.611: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:10:27.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5746" for this suite. 03/02/23 13:10:27.618
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":119,"skipped":2179,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.985 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:09:52.65
    Mar  2 13:09:52.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:09:52.652
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:09:52.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:09:52.724
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 03/02/23 13:09:52.734
    Mar  2 13:09:52.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 create -f -'
    Mar  2 13:09:54.737: INFO: stderr: ""
    Mar  2 13:09:54.737: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 13:09:54.737
    Mar  2 13:09:54.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 13:09:54.890: INFO: stderr: ""
    Mar  2 13:09:54.890: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-m662r "
    Mar  2 13:09:54.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 13:09:55.129: INFO: stderr: ""
    Mar  2 13:09:55.129: INFO: stdout: ""
    Mar  2 13:09:55.129: INFO: update-demo-nautilus-cknbm is created but not running
    Mar  2 13:10:00.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 13:10:00.541: INFO: stderr: ""
    Mar  2 13:10:00.541: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-m662r "
    Mar  2 13:10:00.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 13:10:01.463: INFO: stderr: ""
    Mar  2 13:10:01.463: INFO: stdout: ""
    Mar  2 13:10:01.463: INFO: update-demo-nautilus-cknbm is created but not running
    Mar  2 13:10:06.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 13:10:06.664: INFO: stderr: ""
    Mar  2 13:10:06.664: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-m662r "
    Mar  2 13:10:06.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 13:10:06.938: INFO: stderr: ""
    Mar  2 13:10:06.938: INFO: stdout: "true"
    Mar  2 13:10:06.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 13:10:07.128: INFO: stderr: ""
    Mar  2 13:10:07.128: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 13:10:07.128: INFO: validating pod update-demo-nautilus-cknbm
    Mar  2 13:10:07.136: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 13:10:07.136: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 13:10:07.136: INFO: update-demo-nautilus-cknbm is verified up and running
    Mar  2 13:10:07.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-m662r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 13:10:07.247: INFO: stderr: ""
    Mar  2 13:10:07.247: INFO: stdout: ""
    Mar  2 13:10:07.247: INFO: update-demo-nautilus-m662r is created but not running
    Mar  2 13:10:12.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 13:10:12.399: INFO: stderr: ""
    Mar  2 13:10:12.399: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-m662r "
    Mar  2 13:10:12.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 13:10:12.559: INFO: stderr: ""
    Mar  2 13:10:12.559: INFO: stdout: "true"
    Mar  2 13:10:12.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 13:10:12.681: INFO: stderr: ""
    Mar  2 13:10:12.682: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 13:10:12.682: INFO: validating pod update-demo-nautilus-cknbm
    Mar  2 13:10:12.692: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 13:10:12.692: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 13:10:12.692: INFO: update-demo-nautilus-cknbm is verified up and running
    Mar  2 13:10:12.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-m662r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 13:10:12.832: INFO: stderr: ""
    Mar  2 13:10:12.832: INFO: stdout: "true"
    Mar  2 13:10:12.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-m662r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 13:10:12.961: INFO: stderr: ""
    Mar  2 13:10:12.961: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 13:10:12.961: INFO: validating pod update-demo-nautilus-m662r
    Mar  2 13:10:12.967: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 13:10:12.967: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 13:10:12.967: INFO: update-demo-nautilus-m662r is verified up and running
    STEP: scaling down the replication controller 03/02/23 13:10:12.967
    Mar  2 13:10:12.970: INFO: scanned /root for discovery docs: <nil>
    Mar  2 13:10:12.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar  2 13:10:14.156: INFO: stderr: ""
    Mar  2 13:10:14.156: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 13:10:14.156
    Mar  2 13:10:14.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 13:10:14.348: INFO: stderr: ""
    Mar  2 13:10:14.348: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-m662r "
    STEP: Replicas for name=update-demo: expected=1 actual=2 03/02/23 13:10:14.348
    Mar  2 13:10:19.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 13:10:19.527: INFO: stderr: ""
    Mar  2 13:10:19.527: INFO: stdout: "update-demo-nautilus-cknbm "
    Mar  2 13:10:19.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 13:10:19.663: INFO: stderr: ""
    Mar  2 13:10:19.663: INFO: stdout: "true"
    Mar  2 13:10:19.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 13:10:19.841: INFO: stderr: ""
    Mar  2 13:10:19.841: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 13:10:19.841: INFO: validating pod update-demo-nautilus-cknbm
    Mar  2 13:10:19.856: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 13:10:19.856: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 13:10:19.856: INFO: update-demo-nautilus-cknbm is verified up and running
    STEP: scaling up the replication controller 03/02/23 13:10:19.856
    Mar  2 13:10:19.864: INFO: scanned /root for discovery docs: <nil>
    Mar  2 13:10:19.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar  2 13:10:21.059: INFO: stderr: ""
    Mar  2 13:10:21.059: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 13:10:21.059
    Mar  2 13:10:21.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 13:10:21.192: INFO: stderr: ""
    Mar  2 13:10:21.192: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-fsvpx "
    Mar  2 13:10:21.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 13:10:21.288: INFO: stderr: ""
    Mar  2 13:10:21.288: INFO: stdout: "true"
    Mar  2 13:10:21.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 13:10:21.396: INFO: stderr: ""
    Mar  2 13:10:21.396: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 13:10:21.396: INFO: validating pod update-demo-nautilus-cknbm
    Mar  2 13:10:21.401: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 13:10:21.401: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 13:10:21.401: INFO: update-demo-nautilus-cknbm is verified up and running
    Mar  2 13:10:21.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-fsvpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 13:10:21.574: INFO: stderr: ""
    Mar  2 13:10:21.574: INFO: stdout: ""
    Mar  2 13:10:21.574: INFO: update-demo-nautilus-fsvpx is created but not running
    Mar  2 13:10:26.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 13:10:26.700: INFO: stderr: ""
    Mar  2 13:10:26.700: INFO: stdout: "update-demo-nautilus-cknbm update-demo-nautilus-fsvpx "
    Mar  2 13:10:26.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 13:10:26.819: INFO: stderr: ""
    Mar  2 13:10:26.819: INFO: stdout: "true"
    Mar  2 13:10:26.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-cknbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 13:10:26.938: INFO: stderr: ""
    Mar  2 13:10:26.938: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 13:10:26.938: INFO: validating pod update-demo-nautilus-cknbm
    Mar  2 13:10:26.943: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 13:10:26.943: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 13:10:26.943: INFO: update-demo-nautilus-cknbm is verified up and running
    Mar  2 13:10:26.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-fsvpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 13:10:27.058: INFO: stderr: ""
    Mar  2 13:10:27.058: INFO: stdout: "true"
    Mar  2 13:10:27.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods update-demo-nautilus-fsvpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 13:10:27.145: INFO: stderr: ""
    Mar  2 13:10:27.145: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 13:10:27.145: INFO: validating pod update-demo-nautilus-fsvpx
    Mar  2 13:10:27.155: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 13:10:27.155: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 13:10:27.155: INFO: update-demo-nautilus-fsvpx is verified up and running
    STEP: using delete to clean up resources 03/02/23 13:10:27.155
    Mar  2 13:10:27.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 delete --grace-period=0 --force -f -'
    Mar  2 13:10:27.269: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 13:10:27.269: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar  2 13:10:27.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get rc,svc -l name=update-demo --no-headers'
    Mar  2 13:10:27.449: INFO: stderr: "No resources found in kubectl-5746 namespace.\n"
    Mar  2 13:10:27.449: INFO: stdout: ""
    Mar  2 13:10:27.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5746 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  2 13:10:27.611: INFO: stderr: ""
    Mar  2 13:10:27.611: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:10:27.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5746" for this suite. 03/02/23 13:10:27.618
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:10:27.636
Mar  2 13:10:27.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename gc 03/02/23 13:10:27.64
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:10:27.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:10:27.658
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Mar  2 13:10:27.764: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5a6cb857-e9b5-4aa6-be92-fb75580dfcbf", Controller:(*bool)(0xc001d04fb2), BlockOwnerDeletion:(*bool)(0xc001d04fb3)}}
Mar  2 13:10:27.822: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1c9a6ac5-db28-4338-bd1a-6eda9c40879e", Controller:(*bool)(0xc0037f046a), BlockOwnerDeletion:(*bool)(0xc0037f046b)}}
Mar  2 13:10:27.851: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d4f7ac9f-f1f4-467f-a5e2-01ebf36c210f", Controller:(*bool)(0xc0037f0fc2), BlockOwnerDeletion:(*bool)(0xc0037f0fc3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 13:10:32.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4694" for this suite. 03/02/23 13:10:32.953
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":120,"skipped":2182,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.328 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:10:27.636
    Mar  2 13:10:27.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename gc 03/02/23 13:10:27.64
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:10:27.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:10:27.658
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Mar  2 13:10:27.764: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5a6cb857-e9b5-4aa6-be92-fb75580dfcbf", Controller:(*bool)(0xc001d04fb2), BlockOwnerDeletion:(*bool)(0xc001d04fb3)}}
    Mar  2 13:10:27.822: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1c9a6ac5-db28-4338-bd1a-6eda9c40879e", Controller:(*bool)(0xc0037f046a), BlockOwnerDeletion:(*bool)(0xc0037f046b)}}
    Mar  2 13:10:27.851: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d4f7ac9f-f1f4-467f-a5e2-01ebf36c210f", Controller:(*bool)(0xc0037f0fc2), BlockOwnerDeletion:(*bool)(0xc0037f0fc3)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 13:10:32.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4694" for this suite. 03/02/23 13:10:32.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:10:33.163
Mar  2 13:10:33.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename security-context-test 03/02/23 13:10:33.165
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:10:33.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:10:33.232
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Mar  2 13:10:33.261: INFO: Waiting up to 5m0s for pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560" in namespace "security-context-test-4871" to be "Succeeded or Failed"
Mar  2 13:10:33.269: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032837ms
Mar  2 13:10:35.282: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020736025s
Mar  2 13:10:37.281: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019785838s
Mar  2 13:10:39.294: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033086637s
Mar  2 13:10:41.282: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020590071s
Mar  2 13:10:43.274: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.012888917s
Mar  2 13:10:43.274: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 13:10:43.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4871" for this suite. 03/02/23 13:10:43.279
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":121,"skipped":2262,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.122 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:10:33.163
    Mar  2 13:10:33.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename security-context-test 03/02/23 13:10:33.165
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:10:33.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:10:33.232
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Mar  2 13:10:33.261: INFO: Waiting up to 5m0s for pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560" in namespace "security-context-test-4871" to be "Succeeded or Failed"
    Mar  2 13:10:33.269: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032837ms
    Mar  2 13:10:35.282: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020736025s
    Mar  2 13:10:37.281: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019785838s
    Mar  2 13:10:39.294: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033086637s
    Mar  2 13:10:41.282: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020590071s
    Mar  2 13:10:43.274: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.012888917s
    Mar  2 13:10:43.274: INFO: Pod "busybox-user-65534-90345c2f-61b6-40a6-926b-3bc7289e2560" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 13:10:43.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4871" for this suite. 03/02/23 13:10:43.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:10:43.287
Mar  2 13:10:43.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 13:10:43.288
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:10:43.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:10:43.342
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/02/23 13:10:43.349
Mar  2 13:10:43.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:10:53.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:11:19.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4588" for this suite. 03/02/23 13:11:19.901
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":122,"skipped":2286,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.619 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:10:43.287
    Mar  2 13:10:43.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 13:10:43.288
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:10:43.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:10:43.342
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/02/23 13:10:43.349
    Mar  2 13:10:43.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:10:53.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:11:19.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4588" for this suite. 03/02/23 13:11:19.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:11:19.925
Mar  2 13:11:19.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:11:19.928
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:11:19.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:11:19.953
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 03/02/23 13:11:19.956
Mar  2 13:11:19.968: INFO: Waiting up to 5m0s for pod "annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd" in namespace "projected-1756" to be "running and ready"
Mar  2 13:11:19.984: INFO: Pod "annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.496538ms
Mar  2 13:11:19.988: INFO: The phase of Pod annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:11:22.006: INFO: Pod "annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.037959018s
Mar  2 13:11:22.027: INFO: The phase of Pod annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd is Running (Ready = true)
Mar  2 13:11:22.027: INFO: Pod "annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd" satisfied condition "running and ready"
Mar  2 13:11:22.574: INFO: Successfully updated pod "annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 13:11:24.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1756" for this suite. 03/02/23 13:11:24.661
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":123,"skipped":2357,"failed":0}
------------------------------
â€¢ [4.744 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:11:19.925
    Mar  2 13:11:19.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:11:19.928
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:11:19.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:11:19.953
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 03/02/23 13:11:19.956
    Mar  2 13:11:19.968: INFO: Waiting up to 5m0s for pod "annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd" in namespace "projected-1756" to be "running and ready"
    Mar  2 13:11:19.984: INFO: Pod "annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.496538ms
    Mar  2 13:11:19.988: INFO: The phase of Pod annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:11:22.006: INFO: Pod "annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.037959018s
    Mar  2 13:11:22.027: INFO: The phase of Pod annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd is Running (Ready = true)
    Mar  2 13:11:22.027: INFO: Pod "annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd" satisfied condition "running and ready"
    Mar  2 13:11:22.574: INFO: Successfully updated pod "annotationupdate0c08ce48-355c-4d16-9bbe-6ef0a2d52bbd"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 13:11:24.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1756" for this suite. 03/02/23 13:11:24.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:11:24.672
Mar  2 13:11:24.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-probe 03/02/23 13:11:24.679
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:11:24.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:11:24.845
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7 in namespace container-probe-3324 03/02/23 13:11:24.856
Mar  2 13:11:24.865: INFO: Waiting up to 5m0s for pod "test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7" in namespace "container-probe-3324" to be "not pending"
Mar  2 13:11:24.888: INFO: Pod "test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.322465ms
Mar  2 13:11:26.941: INFO: Pod "test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.074983786s
Mar  2 13:11:26.941: INFO: Pod "test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7" satisfied condition "not pending"
Mar  2 13:11:26.941: INFO: Started pod test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7 in namespace container-probe-3324
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 13:11:26.941
Mar  2 13:11:26.947: INFO: Initial restart count of pod test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7 is 0
STEP: deleting the pod 03/02/23 13:15:28.076
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 13:15:28.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3324" for this suite. 03/02/23 13:15:28.124
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":124,"skipped":2365,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.460 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:11:24.672
    Mar  2 13:11:24.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-probe 03/02/23 13:11:24.679
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:11:24.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:11:24.845
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7 in namespace container-probe-3324 03/02/23 13:11:24.856
    Mar  2 13:11:24.865: INFO: Waiting up to 5m0s for pod "test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7" in namespace "container-probe-3324" to be "not pending"
    Mar  2 13:11:24.888: INFO: Pod "test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.322465ms
    Mar  2 13:11:26.941: INFO: Pod "test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.074983786s
    Mar  2 13:11:26.941: INFO: Pod "test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7" satisfied condition "not pending"
    Mar  2 13:11:26.941: INFO: Started pod test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7 in namespace container-probe-3324
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 13:11:26.941
    Mar  2 13:11:26.947: INFO: Initial restart count of pod test-webserver-dc9c85b2-fc99-484d-b622-422181bc53f7 is 0
    STEP: deleting the pod 03/02/23 13:15:28.076
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 13:15:28.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3324" for this suite. 03/02/23 13:15:28.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:15:28.136
Mar  2 13:15:28.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 13:15:28.14
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:15:28.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:15:28.163
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar  2 13:15:28.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:15:34.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1658" for this suite. 03/02/23 13:15:34.224
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":125,"skipped":2371,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.112 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:15:28.136
    Mar  2 13:15:28.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 13:15:28.14
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:15:28.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:15:28.163
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar  2 13:15:28.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:15:34.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1658" for this suite. 03/02/23 13:15:34.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:15:34.252
Mar  2 13:15:34.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename endpointslice 03/02/23 13:15:34.253
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:15:34.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:15:34.389
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Mar  2 13:15:34.446: INFO: Endpoints addresses: [172.16.0.114] , ports: [6443]
Mar  2 13:15:34.446: INFO: EndpointSlices addresses: [172.16.0.114] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  2 13:15:34.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3120" for this suite. 03/02/23 13:15:34.459
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":126,"skipped":2380,"failed":0}
------------------------------
â€¢ [0.240 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:15:34.252
    Mar  2 13:15:34.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename endpointslice 03/02/23 13:15:34.253
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:15:34.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:15:34.389
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Mar  2 13:15:34.446: INFO: Endpoints addresses: [172.16.0.114] , ports: [6443]
    Mar  2 13:15:34.446: INFO: EndpointSlices addresses: [172.16.0.114] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  2 13:15:34.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3120" for this suite. 03/02/23 13:15:34.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:15:34.495
Mar  2 13:15:34.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 13:15:34.503
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:15:34.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:15:34.565
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 13:15:34.595
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:15:35.473
STEP: Deploying the webhook pod 03/02/23 13:15:35.483
STEP: Wait for the deployment to be ready 03/02/23 13:15:35.494
Mar  2 13:15:35.502: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 03/02/23 13:15:37.515
STEP: Verifying the service has paired with the endpoint 03/02/23 13:15:37.526
Mar  2 13:15:38.529: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/02/23 13:15:38.534
STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 13:15:38.534
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/02/23 13:15:38.558
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/02/23 13:15:39.57
STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 13:15:39.571
STEP: Having no error when timeout is longer than webhook latency 03/02/23 13:15:40.607
STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 13:15:40.607
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/02/23 13:15:45.648
STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 13:15:45.648
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:15:50.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3917" for this suite. 03/02/23 13:15:50.694
STEP: Destroying namespace "webhook-3917-markers" for this suite. 03/02/23 13:15:50.701
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":127,"skipped":2410,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.270 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:15:34.495
    Mar  2 13:15:34.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 13:15:34.503
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:15:34.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:15:34.565
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 13:15:34.595
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:15:35.473
    STEP: Deploying the webhook pod 03/02/23 13:15:35.483
    STEP: Wait for the deployment to be ready 03/02/23 13:15:35.494
    Mar  2 13:15:35.502: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 03/02/23 13:15:37.515
    STEP: Verifying the service has paired with the endpoint 03/02/23 13:15:37.526
    Mar  2 13:15:38.529: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/02/23 13:15:38.534
    STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 13:15:38.534
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/02/23 13:15:38.558
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/02/23 13:15:39.57
    STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 13:15:39.571
    STEP: Having no error when timeout is longer than webhook latency 03/02/23 13:15:40.607
    STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 13:15:40.607
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/02/23 13:15:45.648
    STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 13:15:45.648
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:15:50.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3917" for this suite. 03/02/23 13:15:50.694
    STEP: Destroying namespace "webhook-3917-markers" for this suite. 03/02/23 13:15:50.701
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:15:50.812
Mar  2 13:15:50.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 13:15:50.816
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:15:50.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:15:50.851
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 03/02/23 13:15:50.87
Mar  2 13:15:50.884: INFO: Waiting up to 5m0s for pod "annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea" in namespace "downward-api-3833" to be "running and ready"
Mar  2 13:15:50.893: INFO: Pod "annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea": Phase="Pending", Reason="", readiness=false. Elapsed: 9.148029ms
Mar  2 13:15:50.893: INFO: The phase of Pod annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:15:52.897: INFO: Pod "annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea": Phase="Running", Reason="", readiness=true. Elapsed: 2.013050235s
Mar  2 13:15:52.897: INFO: The phase of Pod annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea is Running (Ready = true)
Mar  2 13:15:52.897: INFO: Pod "annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea" satisfied condition "running and ready"
Mar  2 13:15:53.440: INFO: Successfully updated pod "annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 13:15:57.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3833" for this suite. 03/02/23 13:15:57.482
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":128,"skipped":2419,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.683 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:15:50.812
    Mar  2 13:15:50.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 13:15:50.816
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:15:50.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:15:50.851
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 03/02/23 13:15:50.87
    Mar  2 13:15:50.884: INFO: Waiting up to 5m0s for pod "annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea" in namespace "downward-api-3833" to be "running and ready"
    Mar  2 13:15:50.893: INFO: Pod "annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea": Phase="Pending", Reason="", readiness=false. Elapsed: 9.148029ms
    Mar  2 13:15:50.893: INFO: The phase of Pod annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:15:52.897: INFO: Pod "annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea": Phase="Running", Reason="", readiness=true. Elapsed: 2.013050235s
    Mar  2 13:15:52.897: INFO: The phase of Pod annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea is Running (Ready = true)
    Mar  2 13:15:52.897: INFO: Pod "annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea" satisfied condition "running and ready"
    Mar  2 13:15:53.440: INFO: Successfully updated pod "annotationupdate1ea00b57-cbc2-4f2a-8b5c-58b37c4aedea"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 13:15:57.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3833" for this suite. 03/02/23 13:15:57.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:15:57.503
Mar  2 13:15:57.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename svc-latency 03/02/23 13:15:57.505
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:15:57.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:15:57.529
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar  2 13:15:57.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: creating replication controller svc-latency-rc in namespace svc-latency-443 03/02/23 13:15:57.534
I0302 13:15:57.539425      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-443, replica count: 1
I0302 13:15:58.594131      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 13:15:59.594604      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 13:15:59.726: INFO: Created: latency-svc-9hl4n
Mar  2 13:15:59.740: INFO: Got endpoints: latency-svc-9hl4n [42.094518ms]
Mar  2 13:15:59.763: INFO: Created: latency-svc-x4lxz
Mar  2 13:15:59.768: INFO: Got endpoints: latency-svc-x4lxz [28.14079ms]
Mar  2 13:15:59.790: INFO: Created: latency-svc-z9fq4
Mar  2 13:15:59.790: INFO: Created: latency-svc-2c4m7
Mar  2 13:15:59.790: INFO: Got endpoints: latency-svc-z9fq4 [48.897265ms]
Mar  2 13:15:59.793: INFO: Got endpoints: latency-svc-2c4m7 [52.025188ms]
Mar  2 13:15:59.800: INFO: Created: latency-svc-g2pz9
Mar  2 13:15:59.811: INFO: Got endpoints: latency-svc-g2pz9 [69.919266ms]
Mar  2 13:15:59.823: INFO: Created: latency-svc-slgbl
Mar  2 13:15:59.831: INFO: Created: latency-svc-drdx7
Mar  2 13:15:59.833: INFO: Got endpoints: latency-svc-slgbl [92.314754ms]
Mar  2 13:15:59.835: INFO: Got endpoints: latency-svc-drdx7 [94.601017ms]
Mar  2 13:15:59.842: INFO: Created: latency-svc-glj6z
Mar  2 13:15:59.852: INFO: Got endpoints: latency-svc-glj6z [110.527354ms]
Mar  2 13:15:59.859: INFO: Created: latency-svc-5nkgz
Mar  2 13:15:59.872: INFO: Got endpoints: latency-svc-5nkgz [130.809633ms]
Mar  2 13:15:59.880: INFO: Created: latency-svc-6sc6j
Mar  2 13:15:59.889: INFO: Got endpoints: latency-svc-6sc6j [148.218295ms]
Mar  2 13:15:59.898: INFO: Created: latency-svc-nbnvp
Mar  2 13:15:59.898: INFO: Got endpoints: latency-svc-nbnvp [157.168451ms]
Mar  2 13:15:59.905: INFO: Created: latency-svc-tjg86
Mar  2 13:15:59.911: INFO: Created: latency-svc-5tlkv
Mar  2 13:15:59.918: INFO: Got endpoints: latency-svc-tjg86 [176.432397ms]
Mar  2 13:15:59.925: INFO: Got endpoints: latency-svc-5tlkv [184.030014ms]
Mar  2 13:15:59.934: INFO: Created: latency-svc-x8zrd
Mar  2 13:15:59.957: INFO: Got endpoints: latency-svc-x8zrd [215.411831ms]
Mar  2 13:15:59.957: INFO: Created: latency-svc-g9whp
Mar  2 13:15:59.962: INFO: Created: latency-svc-zx6x5
Mar  2 13:15:59.975: INFO: Got endpoints: latency-svc-g9whp [233.922181ms]
Mar  2 13:15:59.989: INFO: Got endpoints: latency-svc-zx6x5 [247.440009ms]
Mar  2 13:15:59.991: INFO: Created: latency-svc-cvvlt
Mar  2 13:16:00.010: INFO: Got endpoints: latency-svc-cvvlt [241.766774ms]
Mar  2 13:16:00.018: INFO: Created: latency-svc-zdrfv
Mar  2 13:16:00.026: INFO: Created: latency-svc-ctlbl
Mar  2 13:16:00.038: INFO: Got endpoints: latency-svc-zdrfv [247.911434ms]
Mar  2 13:16:00.042: INFO: Got endpoints: latency-svc-ctlbl [248.536944ms]
Mar  2 13:16:00.042: INFO: Created: latency-svc-bfmt8
Mar  2 13:16:00.050: INFO: Got endpoints: latency-svc-bfmt8 [238.907567ms]
Mar  2 13:16:00.065: INFO: Created: latency-svc-gkgs6
Mar  2 13:16:00.065: INFO: Got endpoints: latency-svc-gkgs6 [229.864325ms]
Mar  2 13:16:00.068: INFO: Created: latency-svc-8ztvk
Mar  2 13:16:00.083: INFO: Created: latency-svc-5zdxx
Mar  2 13:16:00.084: INFO: Got endpoints: latency-svc-8ztvk [250.889593ms]
Mar  2 13:16:00.088: INFO: Got endpoints: latency-svc-5zdxx [236.145785ms]
Mar  2 13:16:00.092: INFO: Created: latency-svc-gnfs2
Mar  2 13:16:00.099: INFO: Created: latency-svc-fqjkh
Mar  2 13:16:00.104: INFO: Got endpoints: latency-svc-gnfs2 [232.052541ms]
Mar  2 13:16:00.127: INFO: Got endpoints: latency-svc-fqjkh [237.236328ms]
Mar  2 13:16:00.126: INFO: Created: latency-svc-2znbv
Mar  2 13:16:00.127: INFO: Got endpoints: latency-svc-2znbv [229.194355ms]
Mar  2 13:16:00.127: INFO: Created: latency-svc-zjrnz
Mar  2 13:16:00.136: INFO: Created: latency-svc-skm4m
Mar  2 13:16:00.140: INFO: Got endpoints: latency-svc-zjrnz [222.009269ms]
Mar  2 13:16:00.151: INFO: Got endpoints: latency-svc-skm4m [225.386594ms]
Mar  2 13:16:00.154: INFO: Created: latency-svc-nc22j
Mar  2 13:16:00.159: INFO: Created: latency-svc-wbtgz
Mar  2 13:16:00.164: INFO: Got endpoints: latency-svc-nc22j [206.708477ms]
Mar  2 13:16:00.171: INFO: Created: latency-svc-vdmtx
Mar  2 13:16:00.200: INFO: Created: latency-svc-9hgrn
Mar  2 13:16:00.200: INFO: Created: latency-svc-jr2fg
Mar  2 13:16:00.200: INFO: Got endpoints: latency-svc-9hgrn [189.816365ms]
Mar  2 13:16:00.200: INFO: Got endpoints: latency-svc-wbtgz [224.805999ms]
Mar  2 13:16:00.201: INFO: Got endpoints: latency-svc-vdmtx [210.427472ms]
Mar  2 13:16:00.207: INFO: Got endpoints: latency-svc-jr2fg [165.342961ms]
Mar  2 13:16:00.209: INFO: Created: latency-svc-2l449
Mar  2 13:16:00.210: INFO: Got endpoints: latency-svc-2l449 [171.678432ms]
Mar  2 13:16:00.214: INFO: Created: latency-svc-vpwnf
Mar  2 13:16:00.224: INFO: Got endpoints: latency-svc-vpwnf [173.612371ms]
Mar  2 13:16:00.232: INFO: Created: latency-svc-ds4b4
Mar  2 13:16:00.240: INFO: Created: latency-svc-mxsz6
Mar  2 13:16:00.242: INFO: Got endpoints: latency-svc-ds4b4 [177.256663ms]
Mar  2 13:16:00.244: INFO: Created: latency-svc-7r7q2
Mar  2 13:16:00.249: INFO: Got endpoints: latency-svc-mxsz6 [164.591184ms]
Mar  2 13:16:00.254: INFO: Got endpoints: latency-svc-7r7q2 [165.906814ms]
Mar  2 13:16:00.262: INFO: Created: latency-svc-c6zhp
Mar  2 13:16:00.270: INFO: Got endpoints: latency-svc-c6zhp [143.483904ms]
Mar  2 13:16:00.356: INFO: Created: latency-svc-s8ggh
Mar  2 13:16:00.370: INFO: Created: latency-svc-s4hrm
Mar  2 13:16:00.370: INFO: Created: latency-svc-l4ll2
Mar  2 13:16:00.371: INFO: Created: latency-svc-l72hr
Mar  2 13:16:00.371: INFO: Created: latency-svc-ppfrn
Mar  2 13:16:00.371: INFO: Created: latency-svc-7cw7s
Mar  2 13:16:00.371: INFO: Created: latency-svc-48d4t
Mar  2 13:16:00.393: INFO: Created: latency-svc-st6z5
Mar  2 13:16:00.395: INFO: Created: latency-svc-tfg5b
Mar  2 13:16:00.395: INFO: Created: latency-svc-jfwvm
Mar  2 13:16:00.395: INFO: Created: latency-svc-jpxk9
Mar  2 13:16:00.395: INFO: Created: latency-svc-dfk45
Mar  2 13:16:00.427: INFO: Got endpoints: latency-svc-dfk45 [156.787349ms]
Mar  2 13:16:00.426: INFO: Created: latency-svc-ng8cc
Mar  2 13:16:00.427: INFO: Created: latency-svc-f68k4
Mar  2 13:16:00.427: INFO: Got endpoints: latency-svc-7cw7s [177.920744ms]
Mar  2 13:16:00.427: INFO: Got endpoints: latency-svc-s8ggh [299.799703ms]
Mar  2 13:16:00.427: INFO: Got endpoints: latency-svc-s4hrm [172.587698ms]
Mar  2 13:16:00.427: INFO: Created: latency-svc-9dqmd
Mar  2 13:16:00.444: INFO: Got endpoints: latency-svc-f68k4 [316.731039ms]
Mar  2 13:16:00.495: INFO: Created: latency-svc-lfhpc
Mar  2 13:16:00.507: INFO: Created: latency-svc-v2j2v
Mar  2 13:16:00.519: INFO: Got endpoints: latency-svc-ng8cc [379.451915ms]
Mar  2 13:16:00.531: INFO: Created: latency-svc-cslqm
Mar  2 13:16:00.548: INFO: Got endpoints: latency-svc-9dqmd [397.609397ms]
Mar  2 13:16:00.551: INFO: Created: latency-svc-c8c2z
Mar  2 13:16:00.557: INFO: Created: latency-svc-9vrzp
Mar  2 13:16:00.563: INFO: Created: latency-svc-jzs49
Mar  2 13:16:00.570: INFO: Created: latency-svc-n5x5r
Mar  2 13:16:00.594: INFO: Got endpoints: latency-svc-st6z5 [429.854264ms]
Mar  2 13:16:00.622: INFO: Created: latency-svc-tn5kb
Mar  2 13:16:00.642: INFO: Got endpoints: latency-svc-48d4t [442.348577ms]
Mar  2 13:16:00.653: INFO: Created: latency-svc-jwr8r
Mar  2 13:16:00.690: INFO: Got endpoints: latency-svc-jfwvm [489.146899ms]
Mar  2 13:16:00.734: INFO: Created: latency-svc-f9t8l
Mar  2 13:16:00.749: INFO: Got endpoints: latency-svc-tfg5b [549.042897ms]
Mar  2 13:16:00.760: INFO: Created: latency-svc-tq9j2
Mar  2 13:16:00.840: INFO: Got endpoints: latency-svc-jpxk9 [632.784372ms]
Mar  2 13:16:00.844: INFO: Got endpoints: latency-svc-l4ll2 [633.840135ms]
Mar  2 13:16:00.855: INFO: Created: latency-svc-nvbgv
Mar  2 13:16:00.867: INFO: Created: latency-svc-rx985
Mar  2 13:16:00.916: INFO: Got endpoints: latency-svc-l72hr [691.398603ms]
Mar  2 13:16:00.935: INFO: Created: latency-svc-tpmmz
Mar  2 13:16:00.943: INFO: Got endpoints: latency-svc-ppfrn [700.408416ms]
Mar  2 13:16:00.959: INFO: Created: latency-svc-d4gpc
Mar  2 13:16:01.016: INFO: Got endpoints: latency-svc-lfhpc [588.884837ms]
Mar  2 13:16:01.026: INFO: Created: latency-svc-r99dw
Mar  2 13:16:01.037: INFO: Got endpoints: latency-svc-v2j2v [609.549031ms]
Mar  2 13:16:01.048: INFO: Created: latency-svc-626dq
Mar  2 13:16:01.122: INFO: Got endpoints: latency-svc-cslqm [693.990036ms]
Mar  2 13:16:01.130: INFO: Created: latency-svc-6k9cn
Mar  2 13:16:01.140: INFO: Got endpoints: latency-svc-c8c2z [711.85158ms]
Mar  2 13:16:01.148: INFO: Created: latency-svc-v2vx9
Mar  2 13:16:01.218: INFO: Got endpoints: latency-svc-9vrzp [773.884102ms]
Mar  2 13:16:01.228: INFO: Created: latency-svc-9z6fx
Mar  2 13:16:01.240: INFO: Got endpoints: latency-svc-jzs49 [719.807177ms]
Mar  2 13:16:01.248: INFO: Created: latency-svc-wk2wq
Mar  2 13:16:01.298: INFO: Got endpoints: latency-svc-n5x5r [748.679703ms]
Mar  2 13:16:01.327: INFO: Created: latency-svc-bp7sk
Mar  2 13:16:01.337: INFO: Got endpoints: latency-svc-tn5kb [743.745585ms]
Mar  2 13:16:01.347: INFO: Created: latency-svc-5wjwv
Mar  2 13:16:01.422: INFO: Got endpoints: latency-svc-jwr8r [779.562301ms]
Mar  2 13:16:01.443: INFO: Got endpoints: latency-svc-f9t8l [753.249546ms]
Mar  2 13:16:01.446: INFO: Created: latency-svc-dsjg4
Mar  2 13:16:01.518: INFO: Got endpoints: latency-svc-tq9j2 [768.071804ms]
Mar  2 13:16:01.518: INFO: Created: latency-svc-55nmr
Mar  2 13:16:01.533: INFO: Created: latency-svc-vhfsx
Mar  2 13:16:01.543: INFO: Got endpoints: latency-svc-nvbgv [702.859354ms]
Mar  2 13:16:01.555: INFO: Created: latency-svc-blcxl
Mar  2 13:16:01.618: INFO: Got endpoints: latency-svc-rx985 [773.833736ms]
Mar  2 13:16:01.627: INFO: Created: latency-svc-5b82d
Mar  2 13:16:01.639: INFO: Got endpoints: latency-svc-tpmmz [723.363121ms]
Mar  2 13:16:01.652: INFO: Created: latency-svc-dq6jp
Mar  2 13:16:01.735: INFO: Got endpoints: latency-svc-d4gpc [792.092426ms]
Mar  2 13:16:01.744: INFO: Got endpoints: latency-svc-r99dw [727.818241ms]
Mar  2 13:16:01.763: INFO: Created: latency-svc-pnsr5
Mar  2 13:16:01.763: INFO: Created: latency-svc-dcwzp
Mar  2 13:16:01.817: INFO: Got endpoints: latency-svc-626dq [779.531422ms]
Mar  2 13:16:01.842: INFO: Created: latency-svc-2xfpk
Mar  2 13:16:01.844: INFO: Got endpoints: latency-svc-6k9cn [722.43975ms]
Mar  2 13:16:01.862: INFO: Created: latency-svc-ksqnb
Mar  2 13:16:01.918: INFO: Got endpoints: latency-svc-v2vx9 [778.720723ms]
Mar  2 13:16:01.934: INFO: Created: latency-svc-cvk94
Mar  2 13:16:01.939: INFO: Got endpoints: latency-svc-9z6fx [720.981756ms]
Mar  2 13:16:01.952: INFO: Created: latency-svc-xtc9c
Mar  2 13:16:02.014: INFO: Got endpoints: latency-svc-wk2wq [773.938571ms]
Mar  2 13:16:02.031: INFO: Created: latency-svc-6h7wj
Mar  2 13:16:02.039: INFO: Got endpoints: latency-svc-bp7sk [740.878424ms]
Mar  2 13:16:02.055: INFO: Created: latency-svc-7xff4
Mar  2 13:16:02.094: INFO: Got endpoints: latency-svc-5wjwv [756.176791ms]
Mar  2 13:16:02.143: INFO: Got endpoints: latency-svc-dsjg4 [720.248375ms]
Mar  2 13:16:02.143: INFO: Created: latency-svc-474sf
Mar  2 13:16:02.156: INFO: Created: latency-svc-8jk8x
Mar  2 13:16:02.214: INFO: Got endpoints: latency-svc-55nmr [771.22966ms]
Mar  2 13:16:02.230: INFO: Created: latency-svc-nh4mb
Mar  2 13:16:02.243: INFO: Got endpoints: latency-svc-vhfsx [721.928831ms]
Mar  2 13:16:02.256: INFO: Created: latency-svc-h5wsc
Mar  2 13:16:02.318: INFO: Got endpoints: latency-svc-blcxl [774.667097ms]
Mar  2 13:16:02.332: INFO: Created: latency-svc-qml5t
Mar  2 13:16:02.341: INFO: Got endpoints: latency-svc-5b82d [722.943215ms]
Mar  2 13:16:02.352: INFO: Created: latency-svc-vl2wx
Mar  2 13:16:02.398: INFO: Got endpoints: latency-svc-dq6jp [758.354229ms]
Mar  2 13:16:02.427: INFO: Created: latency-svc-rhg4w
Mar  2 13:16:02.442: INFO: Got endpoints: latency-svc-pnsr5 [706.563739ms]
Mar  2 13:16:02.451: INFO: Created: latency-svc-gqp6d
Mar  2 13:16:02.516: INFO: Got endpoints: latency-svc-dcwzp [771.653379ms]
Mar  2 13:16:02.535: INFO: Created: latency-svc-pbxvt
Mar  2 13:16:02.543: INFO: Got endpoints: latency-svc-2xfpk [726.422859ms]
Mar  2 13:16:02.558: INFO: Created: latency-svc-rvd42
Mar  2 13:16:02.590: INFO: Got endpoints: latency-svc-ksqnb [745.437247ms]
Mar  2 13:16:02.636: INFO: Created: latency-svc-6ctq5
Mar  2 13:16:02.642: INFO: Got endpoints: latency-svc-cvk94 [723.272697ms]
Mar  2 13:16:02.659: INFO: Created: latency-svc-c9swm
Mar  2 13:16:02.690: INFO: Got endpoints: latency-svc-xtc9c [750.690317ms]
Mar  2 13:16:02.760: INFO: Got endpoints: latency-svc-6h7wj [742.056302ms]
Mar  2 13:16:02.814: INFO: Got endpoints: latency-svc-7xff4 [774.871591ms]
Mar  2 13:16:02.846: INFO: Created: latency-svc-zj8zd
Mar  2 13:16:02.848: INFO: Got endpoints: latency-svc-474sf [732.554315ms]
Mar  2 13:16:02.863: INFO: Created: latency-svc-lxplv
Mar  2 13:16:02.916: INFO: Got endpoints: latency-svc-8jk8x [772.310573ms]
Mar  2 13:16:02.916: INFO: Created: latency-svc-92v2c
Mar  2 13:16:02.916: INFO: Created: latency-svc-z924k
Mar  2 13:16:02.934: INFO: Created: latency-svc-lpfgn
Mar  2 13:16:02.944: INFO: Got endpoints: latency-svc-nh4mb [729.368478ms]
Mar  2 13:16:02.961: INFO: Created: latency-svc-h7zqk
Mar  2 13:16:03.026: INFO: Got endpoints: latency-svc-h5wsc [782.434971ms]
Mar  2 13:16:03.046: INFO: Got endpoints: latency-svc-qml5t [727.902175ms]
Mar  2 13:16:03.052: INFO: Created: latency-svc-hh6gq
Mar  2 13:16:03.062: INFO: Created: latency-svc-45wdd
Mar  2 13:16:03.119: INFO: Got endpoints: latency-svc-vl2wx [777.956407ms]
Mar  2 13:16:03.132: INFO: Created: latency-svc-nfr5d
Mar  2 13:16:03.141: INFO: Got endpoints: latency-svc-rhg4w [726.999359ms]
Mar  2 13:16:03.156: INFO: Created: latency-svc-kw5cf
Mar  2 13:16:03.214: INFO: Got endpoints: latency-svc-gqp6d [771.396992ms]
Mar  2 13:16:03.236: INFO: Created: latency-svc-8f8l8
Mar  2 13:16:03.249: INFO: Got endpoints: latency-svc-pbxvt [732.168742ms]
Mar  2 13:16:03.267: INFO: Created: latency-svc-8r59h
Mar  2 13:16:03.329: INFO: Got endpoints: latency-svc-rvd42 [785.154088ms]
Mar  2 13:16:03.346: INFO: Got endpoints: latency-svc-6ctq5 [727.874395ms]
Mar  2 13:16:03.348: INFO: Created: latency-svc-2tf47
Mar  2 13:16:03.362: INFO: Created: latency-svc-fj66n
Mar  2 13:16:03.418: INFO: Got endpoints: latency-svc-c9swm [775.660133ms]
Mar  2 13:16:03.432: INFO: Created: latency-svc-qcm5w
Mar  2 13:16:03.445: INFO: Got endpoints: latency-svc-zj8zd [755.429565ms]
Mar  2 13:16:03.458: INFO: Created: latency-svc-g27cj
Mar  2 13:16:03.514: INFO: Got endpoints: latency-svc-lxplv [754.283528ms]
Mar  2 13:16:03.531: INFO: Created: latency-svc-f2tch
Mar  2 13:16:03.539: INFO: Got endpoints: latency-svc-92v2c [724.934664ms]
Mar  2 13:16:03.550: INFO: Created: latency-svc-764w9
Mar  2 13:16:03.618: INFO: Got endpoints: latency-svc-z924k [770.026245ms]
Mar  2 13:16:03.632: INFO: Created: latency-svc-5kfrh
Mar  2 13:16:03.639: INFO: Got endpoints: latency-svc-lpfgn [723.293369ms]
Mar  2 13:16:03.646: INFO: Created: latency-svc-zgdqk
Mar  2 13:16:03.712: INFO: Got endpoints: latency-svc-h7zqk [768.018709ms]
Mar  2 13:16:03.733: INFO: Created: latency-svc-xs44x
Mar  2 13:16:03.746: INFO: Got endpoints: latency-svc-hh6gq [719.765499ms]
Mar  2 13:16:03.761: INFO: Created: latency-svc-s8rzs
Mar  2 13:16:03.818: INFO: Got endpoints: latency-svc-45wdd [772.184944ms]
Mar  2 13:16:03.828: INFO: Created: latency-svc-6wkd4
Mar  2 13:16:03.839: INFO: Got endpoints: latency-svc-nfr5d [719.393045ms]
Mar  2 13:16:03.847: INFO: Created: latency-svc-dbp9h
Mar  2 13:16:03.914: INFO: Got endpoints: latency-svc-kw5cf [773.490667ms]
Mar  2 13:16:03.927: INFO: Created: latency-svc-98m64
Mar  2 13:16:03.943: INFO: Got endpoints: latency-svc-8f8l8 [729.010041ms]
Mar  2 13:16:03.954: INFO: Created: latency-svc-qsht9
Mar  2 13:16:04.008: INFO: Got endpoints: latency-svc-8r59h [759.213742ms]
Mar  2 13:16:04.021: INFO: Created: latency-svc-dlldf
Mar  2 13:16:04.038: INFO: Got endpoints: latency-svc-2tf47 [709.144821ms]
Mar  2 13:16:04.050: INFO: Created: latency-svc-k8225
Mar  2 13:16:04.098: INFO: Got endpoints: latency-svc-fj66n [751.586165ms]
Mar  2 13:16:04.109: INFO: Created: latency-svc-jnwk7
Mar  2 13:16:04.137: INFO: Got endpoints: latency-svc-qcm5w [719.091736ms]
Mar  2 13:16:04.148: INFO: Created: latency-svc-lbbrs
Mar  2 13:16:04.194: INFO: Got endpoints: latency-svc-g27cj [748.824423ms]
Mar  2 13:16:04.204: INFO: Created: latency-svc-xxwt9
Mar  2 13:16:04.239: INFO: Got endpoints: latency-svc-f2tch [724.818939ms]
Mar  2 13:16:04.249: INFO: Created: latency-svc-9kkdz
Mar  2 13:16:04.294: INFO: Got endpoints: latency-svc-764w9 [755.148355ms]
Mar  2 13:16:04.310: INFO: Created: latency-svc-9vp89
Mar  2 13:16:04.337: INFO: Got endpoints: latency-svc-5kfrh [718.434648ms]
Mar  2 13:16:04.345: INFO: Created: latency-svc-tf9hw
Mar  2 13:16:04.396: INFO: Got endpoints: latency-svc-zgdqk [756.670762ms]
Mar  2 13:16:04.420: INFO: Created: latency-svc-t5csq
Mar  2 13:16:04.440: INFO: Got endpoints: latency-svc-xs44x [727.360462ms]
Mar  2 13:16:04.451: INFO: Created: latency-svc-9gp7p
Mar  2 13:16:04.519: INFO: Got endpoints: latency-svc-s8rzs [772.739116ms]
Mar  2 13:16:04.539: INFO: Created: latency-svc-28bq7
Mar  2 13:16:04.540: INFO: Got endpoints: latency-svc-6wkd4 [721.491777ms]
Mar  2 13:16:04.551: INFO: Created: latency-svc-655vc
Mar  2 13:16:04.592: INFO: Got endpoints: latency-svc-dbp9h [753.291101ms]
Mar  2 13:16:04.617: INFO: Created: latency-svc-vrvgf
Mar  2 13:16:04.637: INFO: Got endpoints: latency-svc-98m64 [723.110012ms]
Mar  2 13:16:04.648: INFO: Created: latency-svc-c2t2n
Mar  2 13:16:04.714: INFO: Got endpoints: latency-svc-qsht9 [771.461818ms]
Mar  2 13:16:04.723: INFO: Created: latency-svc-z45d2
Mar  2 13:16:04.737: INFO: Got endpoints: latency-svc-dlldf [728.678341ms]
Mar  2 13:16:04.747: INFO: Created: latency-svc-246tv
Mar  2 13:16:04.792: INFO: Got endpoints: latency-svc-k8225 [754.335781ms]
Mar  2 13:16:04.821: INFO: Created: latency-svc-w2ngd
Mar  2 13:16:04.835: INFO: Got endpoints: latency-svc-jnwk7 [736.412253ms]
Mar  2 13:16:04.847: INFO: Created: latency-svc-696jf
Mar  2 13:16:04.887: INFO: Got endpoints: latency-svc-lbbrs [749.380394ms]
Mar  2 13:16:04.908: INFO: Created: latency-svc-p5zcx
Mar  2 13:16:04.941: INFO: Got endpoints: latency-svc-xxwt9 [746.501478ms]
Mar  2 13:16:04.949: INFO: Created: latency-svc-6kkgn
Mar  2 13:16:05.003: INFO: Got endpoints: latency-svc-9kkdz [763.086682ms]
Mar  2 13:16:05.015: INFO: Created: latency-svc-qtrn4
Mar  2 13:16:05.038: INFO: Got endpoints: latency-svc-9vp89 [744.078202ms]
Mar  2 13:16:05.050: INFO: Created: latency-svc-z6vtp
Mar  2 13:16:05.091: INFO: Got endpoints: latency-svc-tf9hw [753.409941ms]
Mar  2 13:16:05.110: INFO: Created: latency-svc-mgnmf
Mar  2 13:16:05.139: INFO: Got endpoints: latency-svc-t5csq [742.727893ms]
Mar  2 13:16:05.147: INFO: Created: latency-svc-g9fm7
Mar  2 13:16:05.214: INFO: Got endpoints: latency-svc-9gp7p [773.601589ms]
Mar  2 13:16:05.234: INFO: Created: latency-svc-6mmqg
Mar  2 13:16:05.240: INFO: Got endpoints: latency-svc-28bq7 [720.142474ms]
Mar  2 13:16:05.247: INFO: Created: latency-svc-d687x
Mar  2 13:16:05.308: INFO: Got endpoints: latency-svc-655vc [768.131181ms]
Mar  2 13:16:05.317: INFO: Created: latency-svc-wzr69
Mar  2 13:16:05.347: INFO: Got endpoints: latency-svc-vrvgf [754.039754ms]
Mar  2 13:16:05.360: INFO: Created: latency-svc-lshnx
Mar  2 13:16:05.416: INFO: Got endpoints: latency-svc-c2t2n [778.107478ms]
Mar  2 13:16:05.444: INFO: Created: latency-svc-t69r8
Mar  2 13:16:05.445: INFO: Got endpoints: latency-svc-z45d2 [730.429878ms]
Mar  2 13:16:05.462: INFO: Created: latency-svc-9n6c5
Mar  2 13:16:05.492: INFO: Got endpoints: latency-svc-246tv [755.00463ms]
Mar  2 13:16:05.527: INFO: Created: latency-svc-scvjc
Mar  2 13:16:05.539: INFO: Got endpoints: latency-svc-w2ngd [746.247719ms]
Mar  2 13:16:05.558: INFO: Created: latency-svc-8tq5f
Mar  2 13:16:05.654: INFO: Got endpoints: latency-svc-p5zcx [766.985681ms]
Mar  2 13:16:05.655: INFO: Got endpoints: latency-svc-696jf [819.656179ms]
Mar  2 13:16:05.667: INFO: Created: latency-svc-7wc24
Mar  2 13:16:05.732: INFO: Created: latency-svc-js8pj
Mar  2 13:16:05.732: INFO: Got endpoints: latency-svc-6kkgn [790.409563ms]
Mar  2 13:16:05.739: INFO: Got endpoints: latency-svc-qtrn4 [735.736779ms]
Mar  2 13:16:05.758: INFO: Created: latency-svc-l7cpr
Mar  2 13:16:05.761: INFO: Created: latency-svc-k8qzv
Mar  2 13:16:05.806: INFO: Got endpoints: latency-svc-z6vtp [767.517697ms]
Mar  2 13:16:05.853: INFO: Created: latency-svc-67kqt
Mar  2 13:16:05.853: INFO: Got endpoints: latency-svc-mgnmf [762.79875ms]
Mar  2 13:16:05.877: INFO: Created: latency-svc-6752f
Mar  2 13:16:05.889: INFO: Got endpoints: latency-svc-g9fm7 [749.948542ms]
Mar  2 13:16:05.912: INFO: Created: latency-svc-npb2w
Mar  2 13:16:05.939: INFO: Got endpoints: latency-svc-6mmqg [725.325762ms]
Mar  2 13:16:05.949: INFO: Created: latency-svc-q9jj5
Mar  2 13:16:05.993: INFO: Got endpoints: latency-svc-d687x [753.641603ms]
Mar  2 13:16:06.009: INFO: Created: latency-svc-58rt2
Mar  2 13:16:06.039: INFO: Got endpoints: latency-svc-wzr69 [730.831806ms]
Mar  2 13:16:06.048: INFO: Created: latency-svc-jrljt
Mar  2 13:16:06.090: INFO: Got endpoints: latency-svc-lshnx [742.744639ms]
Mar  2 13:16:06.111: INFO: Created: latency-svc-z68tx
Mar  2 13:16:06.141: INFO: Got endpoints: latency-svc-t69r8 [724.009724ms]
Mar  2 13:16:06.155: INFO: Created: latency-svc-bm4dq
Mar  2 13:16:06.191: INFO: Got endpoints: latency-svc-9n6c5 [746.227713ms]
Mar  2 13:16:06.209: INFO: Created: latency-svc-rjrdp
Mar  2 13:16:06.241: INFO: Got endpoints: latency-svc-scvjc [748.747056ms]
Mar  2 13:16:06.250: INFO: Created: latency-svc-6tr2c
Mar  2 13:16:06.294: INFO: Got endpoints: latency-svc-8tq5f [754.525403ms]
Mar  2 13:16:06.326: INFO: Created: latency-svc-hxjdz
Mar  2 13:16:06.336: INFO: Got endpoints: latency-svc-7wc24 [681.493622ms]
Mar  2 13:16:06.349: INFO: Created: latency-svc-kpsgd
Mar  2 13:16:06.426: INFO: Got endpoints: latency-svc-js8pj [771.598218ms]
Mar  2 13:16:06.446: INFO: Got endpoints: latency-svc-l7cpr [714.462479ms]
Mar  2 13:16:06.453: INFO: Created: latency-svc-wjm74
Mar  2 13:16:06.459: INFO: Created: latency-svc-c9bb7
Mar  2 13:16:06.492: INFO: Got endpoints: latency-svc-k8qzv [753.011764ms]
Mar  2 13:16:06.506: INFO: Created: latency-svc-c7rsc
Mar  2 13:16:06.540: INFO: Got endpoints: latency-svc-67kqt [733.877663ms]
Mar  2 13:16:06.549: INFO: Created: latency-svc-256fc
Mar  2 13:16:06.602: INFO: Got endpoints: latency-svc-6752f [748.18759ms]
Mar  2 13:16:06.616: INFO: Created: latency-svc-b2ddc
Mar  2 13:16:06.636: INFO: Got endpoints: latency-svc-npb2w [747.192571ms]
Mar  2 13:16:06.649: INFO: Created: latency-svc-nslsk
Mar  2 13:16:06.694: INFO: Got endpoints: latency-svc-q9jj5 [754.463009ms]
Mar  2 13:16:06.716: INFO: Created: latency-svc-jtlqq
Mar  2 13:16:06.738: INFO: Got endpoints: latency-svc-58rt2 [744.465223ms]
Mar  2 13:16:06.748: INFO: Created: latency-svc-2rj4v
Mar  2 13:16:06.793: INFO: Got endpoints: latency-svc-jrljt [754.110592ms]
Mar  2 13:16:06.819: INFO: Created: latency-svc-tzlfv
Mar  2 13:16:06.840: INFO: Got endpoints: latency-svc-z68tx [749.887795ms]
Mar  2 13:16:06.863: INFO: Created: latency-svc-5dnbk
Mar  2 13:16:06.891: INFO: Got endpoints: latency-svc-bm4dq [749.766444ms]
Mar  2 13:16:06.908: INFO: Created: latency-svc-vg8xd
Mar  2 13:16:06.940: INFO: Got endpoints: latency-svc-rjrdp [748.753749ms]
Mar  2 13:16:06.953: INFO: Created: latency-svc-swdgw
Mar  2 13:16:06.992: INFO: Got endpoints: latency-svc-6tr2c [750.764533ms]
Mar  2 13:16:07.009: INFO: Created: latency-svc-4htgn
Mar  2 13:16:07.036: INFO: Got endpoints: latency-svc-hxjdz [742.338901ms]
Mar  2 13:16:07.048: INFO: Created: latency-svc-zfd64
Mar  2 13:16:07.098: INFO: Got endpoints: latency-svc-kpsgd [762.088287ms]
Mar  2 13:16:07.115: INFO: Created: latency-svc-f5d98
Mar  2 13:16:07.141: INFO: Got endpoints: latency-svc-wjm74 [714.456447ms]
Mar  2 13:16:07.152: INFO: Created: latency-svc-khsvw
Mar  2 13:16:07.193: INFO: Got endpoints: latency-svc-c9bb7 [746.705141ms]
Mar  2 13:16:07.210: INFO: Created: latency-svc-577m4
Mar  2 13:16:07.240: INFO: Got endpoints: latency-svc-c7rsc [747.694176ms]
Mar  2 13:16:07.251: INFO: Created: latency-svc-lmbn7
Mar  2 13:16:07.290: INFO: Got endpoints: latency-svc-256fc [750.116597ms]
Mar  2 13:16:07.305: INFO: Created: latency-svc-xlgzp
Mar  2 13:16:07.339: INFO: Got endpoints: latency-svc-b2ddc [737.703271ms]
Mar  2 13:16:07.361: INFO: Created: latency-svc-24xr6
Mar  2 13:16:07.403: INFO: Got endpoints: latency-svc-nslsk [767.217987ms]
Mar  2 13:16:07.412: INFO: Created: latency-svc-n8g6x
Mar  2 13:16:07.441: INFO: Got endpoints: latency-svc-jtlqq [746.701175ms]
Mar  2 13:16:07.453: INFO: Created: latency-svc-hpw26
Mar  2 13:16:07.498: INFO: Got endpoints: latency-svc-2rj4v [760.279104ms]
Mar  2 13:16:07.510: INFO: Created: latency-svc-b65cm
Mar  2 13:16:07.542: INFO: Got endpoints: latency-svc-tzlfv [748.169722ms]
Mar  2 13:16:07.564: INFO: Created: latency-svc-lsh4l
Mar  2 13:16:07.597: INFO: Got endpoints: latency-svc-5dnbk [756.528083ms]
Mar  2 13:16:07.639: INFO: Got endpoints: latency-svc-vg8xd [747.814581ms]
Mar  2 13:16:07.714: INFO: Got endpoints: latency-svc-swdgw [773.239007ms]
Mar  2 13:16:07.738: INFO: Got endpoints: latency-svc-4htgn [745.560428ms]
Mar  2 13:16:07.793: INFO: Got endpoints: latency-svc-zfd64 [756.272444ms]
Mar  2 13:16:07.839: INFO: Got endpoints: latency-svc-f5d98 [740.891187ms]
Mar  2 13:16:07.900: INFO: Got endpoints: latency-svc-khsvw [759.141455ms]
Mar  2 13:16:07.940: INFO: Got endpoints: latency-svc-577m4 [745.947029ms]
Mar  2 13:16:08.000: INFO: Got endpoints: latency-svc-lmbn7 [759.577353ms]
Mar  2 13:16:08.041: INFO: Got endpoints: latency-svc-xlgzp [749.815623ms]
Mar  2 13:16:08.089: INFO: Got endpoints: latency-svc-24xr6 [749.35191ms]
Mar  2 13:16:08.143: INFO: Got endpoints: latency-svc-n8g6x [739.983053ms]
Mar  2 13:16:08.189: INFO: Got endpoints: latency-svc-hpw26 [747.932084ms]
Mar  2 13:16:08.242: INFO: Got endpoints: latency-svc-b65cm [743.339965ms]
Mar  2 13:16:08.296: INFO: Got endpoints: latency-svc-lsh4l [754.129912ms]
Mar  2 13:16:08.297: INFO: Latencies: [28.14079ms 48.897265ms 52.025188ms 69.919266ms 92.314754ms 94.601017ms 110.527354ms 130.809633ms 143.483904ms 148.218295ms 156.787349ms 157.168451ms 164.591184ms 165.342961ms 165.906814ms 171.678432ms 172.587698ms 173.612371ms 176.432397ms 177.256663ms 177.920744ms 184.030014ms 189.816365ms 206.708477ms 210.427472ms 215.411831ms 222.009269ms 224.805999ms 225.386594ms 229.194355ms 229.864325ms 232.052541ms 233.922181ms 236.145785ms 237.236328ms 238.907567ms 241.766774ms 247.440009ms 247.911434ms 248.536944ms 250.889593ms 299.799703ms 316.731039ms 379.451915ms 397.609397ms 429.854264ms 442.348577ms 489.146899ms 549.042897ms 588.884837ms 609.549031ms 632.784372ms 633.840135ms 681.493622ms 691.398603ms 693.990036ms 700.408416ms 702.859354ms 706.563739ms 709.144821ms 711.85158ms 714.456447ms 714.462479ms 718.434648ms 719.091736ms 719.393045ms 719.765499ms 719.807177ms 720.142474ms 720.248375ms 720.981756ms 721.491777ms 721.928831ms 722.43975ms 722.943215ms 723.110012ms 723.272697ms 723.293369ms 723.363121ms 724.009724ms 724.818939ms 724.934664ms 725.325762ms 726.422859ms 726.999359ms 727.360462ms 727.818241ms 727.874395ms 727.902175ms 728.678341ms 729.010041ms 729.368478ms 730.429878ms 730.831806ms 732.168742ms 732.554315ms 733.877663ms 735.736779ms 736.412253ms 737.703271ms 739.983053ms 740.878424ms 740.891187ms 742.056302ms 742.338901ms 742.727893ms 742.744639ms 743.339965ms 743.745585ms 744.078202ms 744.465223ms 745.437247ms 745.560428ms 745.947029ms 746.227713ms 746.247719ms 746.501478ms 746.701175ms 746.705141ms 747.192571ms 747.694176ms 747.814581ms 747.932084ms 748.169722ms 748.18759ms 748.679703ms 748.747056ms 748.753749ms 748.824423ms 749.35191ms 749.380394ms 749.766444ms 749.815623ms 749.887795ms 749.948542ms 750.116597ms 750.690317ms 750.764533ms 751.586165ms 753.011764ms 753.249546ms 753.291101ms 753.409941ms 753.641603ms 754.039754ms 754.110592ms 754.129912ms 754.283528ms 754.335781ms 754.463009ms 754.525403ms 755.00463ms 755.148355ms 755.429565ms 756.176791ms 756.272444ms 756.528083ms 756.670762ms 758.354229ms 759.141455ms 759.213742ms 759.577353ms 760.279104ms 762.088287ms 762.79875ms 763.086682ms 766.985681ms 767.217987ms 767.517697ms 768.018709ms 768.071804ms 768.131181ms 770.026245ms 771.22966ms 771.396992ms 771.461818ms 771.598218ms 771.653379ms 772.184944ms 772.310573ms 772.739116ms 773.239007ms 773.490667ms 773.601589ms 773.833736ms 773.884102ms 773.938571ms 774.667097ms 774.871591ms 775.660133ms 777.956407ms 778.107478ms 778.720723ms 779.531422ms 779.562301ms 782.434971ms 785.154088ms 790.409563ms 792.092426ms 819.656179ms]
Mar  2 13:16:08.298: INFO: 50 %ile: 739.983053ms
Mar  2 13:16:08.298: INFO: 90 %ile: 772.739116ms
Mar  2 13:16:08.298: INFO: 99 %ile: 792.092426ms
Mar  2 13:16:08.298: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Mar  2 13:16:08.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-443" for this suite. 03/02/23 13:16:08.313
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":129,"skipped":2432,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.816 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:15:57.503
    Mar  2 13:15:57.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename svc-latency 03/02/23 13:15:57.505
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:15:57.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:15:57.529
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar  2 13:15:57.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-443 03/02/23 13:15:57.534
    I0302 13:15:57.539425      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-443, replica count: 1
    I0302 13:15:58.594131      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 13:15:59.594604      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 13:15:59.726: INFO: Created: latency-svc-9hl4n
    Mar  2 13:15:59.740: INFO: Got endpoints: latency-svc-9hl4n [42.094518ms]
    Mar  2 13:15:59.763: INFO: Created: latency-svc-x4lxz
    Mar  2 13:15:59.768: INFO: Got endpoints: latency-svc-x4lxz [28.14079ms]
    Mar  2 13:15:59.790: INFO: Created: latency-svc-z9fq4
    Mar  2 13:15:59.790: INFO: Created: latency-svc-2c4m7
    Mar  2 13:15:59.790: INFO: Got endpoints: latency-svc-z9fq4 [48.897265ms]
    Mar  2 13:15:59.793: INFO: Got endpoints: latency-svc-2c4m7 [52.025188ms]
    Mar  2 13:15:59.800: INFO: Created: latency-svc-g2pz9
    Mar  2 13:15:59.811: INFO: Got endpoints: latency-svc-g2pz9 [69.919266ms]
    Mar  2 13:15:59.823: INFO: Created: latency-svc-slgbl
    Mar  2 13:15:59.831: INFO: Created: latency-svc-drdx7
    Mar  2 13:15:59.833: INFO: Got endpoints: latency-svc-slgbl [92.314754ms]
    Mar  2 13:15:59.835: INFO: Got endpoints: latency-svc-drdx7 [94.601017ms]
    Mar  2 13:15:59.842: INFO: Created: latency-svc-glj6z
    Mar  2 13:15:59.852: INFO: Got endpoints: latency-svc-glj6z [110.527354ms]
    Mar  2 13:15:59.859: INFO: Created: latency-svc-5nkgz
    Mar  2 13:15:59.872: INFO: Got endpoints: latency-svc-5nkgz [130.809633ms]
    Mar  2 13:15:59.880: INFO: Created: latency-svc-6sc6j
    Mar  2 13:15:59.889: INFO: Got endpoints: latency-svc-6sc6j [148.218295ms]
    Mar  2 13:15:59.898: INFO: Created: latency-svc-nbnvp
    Mar  2 13:15:59.898: INFO: Got endpoints: latency-svc-nbnvp [157.168451ms]
    Mar  2 13:15:59.905: INFO: Created: latency-svc-tjg86
    Mar  2 13:15:59.911: INFO: Created: latency-svc-5tlkv
    Mar  2 13:15:59.918: INFO: Got endpoints: latency-svc-tjg86 [176.432397ms]
    Mar  2 13:15:59.925: INFO: Got endpoints: latency-svc-5tlkv [184.030014ms]
    Mar  2 13:15:59.934: INFO: Created: latency-svc-x8zrd
    Mar  2 13:15:59.957: INFO: Got endpoints: latency-svc-x8zrd [215.411831ms]
    Mar  2 13:15:59.957: INFO: Created: latency-svc-g9whp
    Mar  2 13:15:59.962: INFO: Created: latency-svc-zx6x5
    Mar  2 13:15:59.975: INFO: Got endpoints: latency-svc-g9whp [233.922181ms]
    Mar  2 13:15:59.989: INFO: Got endpoints: latency-svc-zx6x5 [247.440009ms]
    Mar  2 13:15:59.991: INFO: Created: latency-svc-cvvlt
    Mar  2 13:16:00.010: INFO: Got endpoints: latency-svc-cvvlt [241.766774ms]
    Mar  2 13:16:00.018: INFO: Created: latency-svc-zdrfv
    Mar  2 13:16:00.026: INFO: Created: latency-svc-ctlbl
    Mar  2 13:16:00.038: INFO: Got endpoints: latency-svc-zdrfv [247.911434ms]
    Mar  2 13:16:00.042: INFO: Got endpoints: latency-svc-ctlbl [248.536944ms]
    Mar  2 13:16:00.042: INFO: Created: latency-svc-bfmt8
    Mar  2 13:16:00.050: INFO: Got endpoints: latency-svc-bfmt8 [238.907567ms]
    Mar  2 13:16:00.065: INFO: Created: latency-svc-gkgs6
    Mar  2 13:16:00.065: INFO: Got endpoints: latency-svc-gkgs6 [229.864325ms]
    Mar  2 13:16:00.068: INFO: Created: latency-svc-8ztvk
    Mar  2 13:16:00.083: INFO: Created: latency-svc-5zdxx
    Mar  2 13:16:00.084: INFO: Got endpoints: latency-svc-8ztvk [250.889593ms]
    Mar  2 13:16:00.088: INFO: Got endpoints: latency-svc-5zdxx [236.145785ms]
    Mar  2 13:16:00.092: INFO: Created: latency-svc-gnfs2
    Mar  2 13:16:00.099: INFO: Created: latency-svc-fqjkh
    Mar  2 13:16:00.104: INFO: Got endpoints: latency-svc-gnfs2 [232.052541ms]
    Mar  2 13:16:00.127: INFO: Got endpoints: latency-svc-fqjkh [237.236328ms]
    Mar  2 13:16:00.126: INFO: Created: latency-svc-2znbv
    Mar  2 13:16:00.127: INFO: Got endpoints: latency-svc-2znbv [229.194355ms]
    Mar  2 13:16:00.127: INFO: Created: latency-svc-zjrnz
    Mar  2 13:16:00.136: INFO: Created: latency-svc-skm4m
    Mar  2 13:16:00.140: INFO: Got endpoints: latency-svc-zjrnz [222.009269ms]
    Mar  2 13:16:00.151: INFO: Got endpoints: latency-svc-skm4m [225.386594ms]
    Mar  2 13:16:00.154: INFO: Created: latency-svc-nc22j
    Mar  2 13:16:00.159: INFO: Created: latency-svc-wbtgz
    Mar  2 13:16:00.164: INFO: Got endpoints: latency-svc-nc22j [206.708477ms]
    Mar  2 13:16:00.171: INFO: Created: latency-svc-vdmtx
    Mar  2 13:16:00.200: INFO: Created: latency-svc-9hgrn
    Mar  2 13:16:00.200: INFO: Created: latency-svc-jr2fg
    Mar  2 13:16:00.200: INFO: Got endpoints: latency-svc-9hgrn [189.816365ms]
    Mar  2 13:16:00.200: INFO: Got endpoints: latency-svc-wbtgz [224.805999ms]
    Mar  2 13:16:00.201: INFO: Got endpoints: latency-svc-vdmtx [210.427472ms]
    Mar  2 13:16:00.207: INFO: Got endpoints: latency-svc-jr2fg [165.342961ms]
    Mar  2 13:16:00.209: INFO: Created: latency-svc-2l449
    Mar  2 13:16:00.210: INFO: Got endpoints: latency-svc-2l449 [171.678432ms]
    Mar  2 13:16:00.214: INFO: Created: latency-svc-vpwnf
    Mar  2 13:16:00.224: INFO: Got endpoints: latency-svc-vpwnf [173.612371ms]
    Mar  2 13:16:00.232: INFO: Created: latency-svc-ds4b4
    Mar  2 13:16:00.240: INFO: Created: latency-svc-mxsz6
    Mar  2 13:16:00.242: INFO: Got endpoints: latency-svc-ds4b4 [177.256663ms]
    Mar  2 13:16:00.244: INFO: Created: latency-svc-7r7q2
    Mar  2 13:16:00.249: INFO: Got endpoints: latency-svc-mxsz6 [164.591184ms]
    Mar  2 13:16:00.254: INFO: Got endpoints: latency-svc-7r7q2 [165.906814ms]
    Mar  2 13:16:00.262: INFO: Created: latency-svc-c6zhp
    Mar  2 13:16:00.270: INFO: Got endpoints: latency-svc-c6zhp [143.483904ms]
    Mar  2 13:16:00.356: INFO: Created: latency-svc-s8ggh
    Mar  2 13:16:00.370: INFO: Created: latency-svc-s4hrm
    Mar  2 13:16:00.370: INFO: Created: latency-svc-l4ll2
    Mar  2 13:16:00.371: INFO: Created: latency-svc-l72hr
    Mar  2 13:16:00.371: INFO: Created: latency-svc-ppfrn
    Mar  2 13:16:00.371: INFO: Created: latency-svc-7cw7s
    Mar  2 13:16:00.371: INFO: Created: latency-svc-48d4t
    Mar  2 13:16:00.393: INFO: Created: latency-svc-st6z5
    Mar  2 13:16:00.395: INFO: Created: latency-svc-tfg5b
    Mar  2 13:16:00.395: INFO: Created: latency-svc-jfwvm
    Mar  2 13:16:00.395: INFO: Created: latency-svc-jpxk9
    Mar  2 13:16:00.395: INFO: Created: latency-svc-dfk45
    Mar  2 13:16:00.427: INFO: Got endpoints: latency-svc-dfk45 [156.787349ms]
    Mar  2 13:16:00.426: INFO: Created: latency-svc-ng8cc
    Mar  2 13:16:00.427: INFO: Created: latency-svc-f68k4
    Mar  2 13:16:00.427: INFO: Got endpoints: latency-svc-7cw7s [177.920744ms]
    Mar  2 13:16:00.427: INFO: Got endpoints: latency-svc-s8ggh [299.799703ms]
    Mar  2 13:16:00.427: INFO: Got endpoints: latency-svc-s4hrm [172.587698ms]
    Mar  2 13:16:00.427: INFO: Created: latency-svc-9dqmd
    Mar  2 13:16:00.444: INFO: Got endpoints: latency-svc-f68k4 [316.731039ms]
    Mar  2 13:16:00.495: INFO: Created: latency-svc-lfhpc
    Mar  2 13:16:00.507: INFO: Created: latency-svc-v2j2v
    Mar  2 13:16:00.519: INFO: Got endpoints: latency-svc-ng8cc [379.451915ms]
    Mar  2 13:16:00.531: INFO: Created: latency-svc-cslqm
    Mar  2 13:16:00.548: INFO: Got endpoints: latency-svc-9dqmd [397.609397ms]
    Mar  2 13:16:00.551: INFO: Created: latency-svc-c8c2z
    Mar  2 13:16:00.557: INFO: Created: latency-svc-9vrzp
    Mar  2 13:16:00.563: INFO: Created: latency-svc-jzs49
    Mar  2 13:16:00.570: INFO: Created: latency-svc-n5x5r
    Mar  2 13:16:00.594: INFO: Got endpoints: latency-svc-st6z5 [429.854264ms]
    Mar  2 13:16:00.622: INFO: Created: latency-svc-tn5kb
    Mar  2 13:16:00.642: INFO: Got endpoints: latency-svc-48d4t [442.348577ms]
    Mar  2 13:16:00.653: INFO: Created: latency-svc-jwr8r
    Mar  2 13:16:00.690: INFO: Got endpoints: latency-svc-jfwvm [489.146899ms]
    Mar  2 13:16:00.734: INFO: Created: latency-svc-f9t8l
    Mar  2 13:16:00.749: INFO: Got endpoints: latency-svc-tfg5b [549.042897ms]
    Mar  2 13:16:00.760: INFO: Created: latency-svc-tq9j2
    Mar  2 13:16:00.840: INFO: Got endpoints: latency-svc-jpxk9 [632.784372ms]
    Mar  2 13:16:00.844: INFO: Got endpoints: latency-svc-l4ll2 [633.840135ms]
    Mar  2 13:16:00.855: INFO: Created: latency-svc-nvbgv
    Mar  2 13:16:00.867: INFO: Created: latency-svc-rx985
    Mar  2 13:16:00.916: INFO: Got endpoints: latency-svc-l72hr [691.398603ms]
    Mar  2 13:16:00.935: INFO: Created: latency-svc-tpmmz
    Mar  2 13:16:00.943: INFO: Got endpoints: latency-svc-ppfrn [700.408416ms]
    Mar  2 13:16:00.959: INFO: Created: latency-svc-d4gpc
    Mar  2 13:16:01.016: INFO: Got endpoints: latency-svc-lfhpc [588.884837ms]
    Mar  2 13:16:01.026: INFO: Created: latency-svc-r99dw
    Mar  2 13:16:01.037: INFO: Got endpoints: latency-svc-v2j2v [609.549031ms]
    Mar  2 13:16:01.048: INFO: Created: latency-svc-626dq
    Mar  2 13:16:01.122: INFO: Got endpoints: latency-svc-cslqm [693.990036ms]
    Mar  2 13:16:01.130: INFO: Created: latency-svc-6k9cn
    Mar  2 13:16:01.140: INFO: Got endpoints: latency-svc-c8c2z [711.85158ms]
    Mar  2 13:16:01.148: INFO: Created: latency-svc-v2vx9
    Mar  2 13:16:01.218: INFO: Got endpoints: latency-svc-9vrzp [773.884102ms]
    Mar  2 13:16:01.228: INFO: Created: latency-svc-9z6fx
    Mar  2 13:16:01.240: INFO: Got endpoints: latency-svc-jzs49 [719.807177ms]
    Mar  2 13:16:01.248: INFO: Created: latency-svc-wk2wq
    Mar  2 13:16:01.298: INFO: Got endpoints: latency-svc-n5x5r [748.679703ms]
    Mar  2 13:16:01.327: INFO: Created: latency-svc-bp7sk
    Mar  2 13:16:01.337: INFO: Got endpoints: latency-svc-tn5kb [743.745585ms]
    Mar  2 13:16:01.347: INFO: Created: latency-svc-5wjwv
    Mar  2 13:16:01.422: INFO: Got endpoints: latency-svc-jwr8r [779.562301ms]
    Mar  2 13:16:01.443: INFO: Got endpoints: latency-svc-f9t8l [753.249546ms]
    Mar  2 13:16:01.446: INFO: Created: latency-svc-dsjg4
    Mar  2 13:16:01.518: INFO: Got endpoints: latency-svc-tq9j2 [768.071804ms]
    Mar  2 13:16:01.518: INFO: Created: latency-svc-55nmr
    Mar  2 13:16:01.533: INFO: Created: latency-svc-vhfsx
    Mar  2 13:16:01.543: INFO: Got endpoints: latency-svc-nvbgv [702.859354ms]
    Mar  2 13:16:01.555: INFO: Created: latency-svc-blcxl
    Mar  2 13:16:01.618: INFO: Got endpoints: latency-svc-rx985 [773.833736ms]
    Mar  2 13:16:01.627: INFO: Created: latency-svc-5b82d
    Mar  2 13:16:01.639: INFO: Got endpoints: latency-svc-tpmmz [723.363121ms]
    Mar  2 13:16:01.652: INFO: Created: latency-svc-dq6jp
    Mar  2 13:16:01.735: INFO: Got endpoints: latency-svc-d4gpc [792.092426ms]
    Mar  2 13:16:01.744: INFO: Got endpoints: latency-svc-r99dw [727.818241ms]
    Mar  2 13:16:01.763: INFO: Created: latency-svc-pnsr5
    Mar  2 13:16:01.763: INFO: Created: latency-svc-dcwzp
    Mar  2 13:16:01.817: INFO: Got endpoints: latency-svc-626dq [779.531422ms]
    Mar  2 13:16:01.842: INFO: Created: latency-svc-2xfpk
    Mar  2 13:16:01.844: INFO: Got endpoints: latency-svc-6k9cn [722.43975ms]
    Mar  2 13:16:01.862: INFO: Created: latency-svc-ksqnb
    Mar  2 13:16:01.918: INFO: Got endpoints: latency-svc-v2vx9 [778.720723ms]
    Mar  2 13:16:01.934: INFO: Created: latency-svc-cvk94
    Mar  2 13:16:01.939: INFO: Got endpoints: latency-svc-9z6fx [720.981756ms]
    Mar  2 13:16:01.952: INFO: Created: latency-svc-xtc9c
    Mar  2 13:16:02.014: INFO: Got endpoints: latency-svc-wk2wq [773.938571ms]
    Mar  2 13:16:02.031: INFO: Created: latency-svc-6h7wj
    Mar  2 13:16:02.039: INFO: Got endpoints: latency-svc-bp7sk [740.878424ms]
    Mar  2 13:16:02.055: INFO: Created: latency-svc-7xff4
    Mar  2 13:16:02.094: INFO: Got endpoints: latency-svc-5wjwv [756.176791ms]
    Mar  2 13:16:02.143: INFO: Got endpoints: latency-svc-dsjg4 [720.248375ms]
    Mar  2 13:16:02.143: INFO: Created: latency-svc-474sf
    Mar  2 13:16:02.156: INFO: Created: latency-svc-8jk8x
    Mar  2 13:16:02.214: INFO: Got endpoints: latency-svc-55nmr [771.22966ms]
    Mar  2 13:16:02.230: INFO: Created: latency-svc-nh4mb
    Mar  2 13:16:02.243: INFO: Got endpoints: latency-svc-vhfsx [721.928831ms]
    Mar  2 13:16:02.256: INFO: Created: latency-svc-h5wsc
    Mar  2 13:16:02.318: INFO: Got endpoints: latency-svc-blcxl [774.667097ms]
    Mar  2 13:16:02.332: INFO: Created: latency-svc-qml5t
    Mar  2 13:16:02.341: INFO: Got endpoints: latency-svc-5b82d [722.943215ms]
    Mar  2 13:16:02.352: INFO: Created: latency-svc-vl2wx
    Mar  2 13:16:02.398: INFO: Got endpoints: latency-svc-dq6jp [758.354229ms]
    Mar  2 13:16:02.427: INFO: Created: latency-svc-rhg4w
    Mar  2 13:16:02.442: INFO: Got endpoints: latency-svc-pnsr5 [706.563739ms]
    Mar  2 13:16:02.451: INFO: Created: latency-svc-gqp6d
    Mar  2 13:16:02.516: INFO: Got endpoints: latency-svc-dcwzp [771.653379ms]
    Mar  2 13:16:02.535: INFO: Created: latency-svc-pbxvt
    Mar  2 13:16:02.543: INFO: Got endpoints: latency-svc-2xfpk [726.422859ms]
    Mar  2 13:16:02.558: INFO: Created: latency-svc-rvd42
    Mar  2 13:16:02.590: INFO: Got endpoints: latency-svc-ksqnb [745.437247ms]
    Mar  2 13:16:02.636: INFO: Created: latency-svc-6ctq5
    Mar  2 13:16:02.642: INFO: Got endpoints: latency-svc-cvk94 [723.272697ms]
    Mar  2 13:16:02.659: INFO: Created: latency-svc-c9swm
    Mar  2 13:16:02.690: INFO: Got endpoints: latency-svc-xtc9c [750.690317ms]
    Mar  2 13:16:02.760: INFO: Got endpoints: latency-svc-6h7wj [742.056302ms]
    Mar  2 13:16:02.814: INFO: Got endpoints: latency-svc-7xff4 [774.871591ms]
    Mar  2 13:16:02.846: INFO: Created: latency-svc-zj8zd
    Mar  2 13:16:02.848: INFO: Got endpoints: latency-svc-474sf [732.554315ms]
    Mar  2 13:16:02.863: INFO: Created: latency-svc-lxplv
    Mar  2 13:16:02.916: INFO: Got endpoints: latency-svc-8jk8x [772.310573ms]
    Mar  2 13:16:02.916: INFO: Created: latency-svc-92v2c
    Mar  2 13:16:02.916: INFO: Created: latency-svc-z924k
    Mar  2 13:16:02.934: INFO: Created: latency-svc-lpfgn
    Mar  2 13:16:02.944: INFO: Got endpoints: latency-svc-nh4mb [729.368478ms]
    Mar  2 13:16:02.961: INFO: Created: latency-svc-h7zqk
    Mar  2 13:16:03.026: INFO: Got endpoints: latency-svc-h5wsc [782.434971ms]
    Mar  2 13:16:03.046: INFO: Got endpoints: latency-svc-qml5t [727.902175ms]
    Mar  2 13:16:03.052: INFO: Created: latency-svc-hh6gq
    Mar  2 13:16:03.062: INFO: Created: latency-svc-45wdd
    Mar  2 13:16:03.119: INFO: Got endpoints: latency-svc-vl2wx [777.956407ms]
    Mar  2 13:16:03.132: INFO: Created: latency-svc-nfr5d
    Mar  2 13:16:03.141: INFO: Got endpoints: latency-svc-rhg4w [726.999359ms]
    Mar  2 13:16:03.156: INFO: Created: latency-svc-kw5cf
    Mar  2 13:16:03.214: INFO: Got endpoints: latency-svc-gqp6d [771.396992ms]
    Mar  2 13:16:03.236: INFO: Created: latency-svc-8f8l8
    Mar  2 13:16:03.249: INFO: Got endpoints: latency-svc-pbxvt [732.168742ms]
    Mar  2 13:16:03.267: INFO: Created: latency-svc-8r59h
    Mar  2 13:16:03.329: INFO: Got endpoints: latency-svc-rvd42 [785.154088ms]
    Mar  2 13:16:03.346: INFO: Got endpoints: latency-svc-6ctq5 [727.874395ms]
    Mar  2 13:16:03.348: INFO: Created: latency-svc-2tf47
    Mar  2 13:16:03.362: INFO: Created: latency-svc-fj66n
    Mar  2 13:16:03.418: INFO: Got endpoints: latency-svc-c9swm [775.660133ms]
    Mar  2 13:16:03.432: INFO: Created: latency-svc-qcm5w
    Mar  2 13:16:03.445: INFO: Got endpoints: latency-svc-zj8zd [755.429565ms]
    Mar  2 13:16:03.458: INFO: Created: latency-svc-g27cj
    Mar  2 13:16:03.514: INFO: Got endpoints: latency-svc-lxplv [754.283528ms]
    Mar  2 13:16:03.531: INFO: Created: latency-svc-f2tch
    Mar  2 13:16:03.539: INFO: Got endpoints: latency-svc-92v2c [724.934664ms]
    Mar  2 13:16:03.550: INFO: Created: latency-svc-764w9
    Mar  2 13:16:03.618: INFO: Got endpoints: latency-svc-z924k [770.026245ms]
    Mar  2 13:16:03.632: INFO: Created: latency-svc-5kfrh
    Mar  2 13:16:03.639: INFO: Got endpoints: latency-svc-lpfgn [723.293369ms]
    Mar  2 13:16:03.646: INFO: Created: latency-svc-zgdqk
    Mar  2 13:16:03.712: INFO: Got endpoints: latency-svc-h7zqk [768.018709ms]
    Mar  2 13:16:03.733: INFO: Created: latency-svc-xs44x
    Mar  2 13:16:03.746: INFO: Got endpoints: latency-svc-hh6gq [719.765499ms]
    Mar  2 13:16:03.761: INFO: Created: latency-svc-s8rzs
    Mar  2 13:16:03.818: INFO: Got endpoints: latency-svc-45wdd [772.184944ms]
    Mar  2 13:16:03.828: INFO: Created: latency-svc-6wkd4
    Mar  2 13:16:03.839: INFO: Got endpoints: latency-svc-nfr5d [719.393045ms]
    Mar  2 13:16:03.847: INFO: Created: latency-svc-dbp9h
    Mar  2 13:16:03.914: INFO: Got endpoints: latency-svc-kw5cf [773.490667ms]
    Mar  2 13:16:03.927: INFO: Created: latency-svc-98m64
    Mar  2 13:16:03.943: INFO: Got endpoints: latency-svc-8f8l8 [729.010041ms]
    Mar  2 13:16:03.954: INFO: Created: latency-svc-qsht9
    Mar  2 13:16:04.008: INFO: Got endpoints: latency-svc-8r59h [759.213742ms]
    Mar  2 13:16:04.021: INFO: Created: latency-svc-dlldf
    Mar  2 13:16:04.038: INFO: Got endpoints: latency-svc-2tf47 [709.144821ms]
    Mar  2 13:16:04.050: INFO: Created: latency-svc-k8225
    Mar  2 13:16:04.098: INFO: Got endpoints: latency-svc-fj66n [751.586165ms]
    Mar  2 13:16:04.109: INFO: Created: latency-svc-jnwk7
    Mar  2 13:16:04.137: INFO: Got endpoints: latency-svc-qcm5w [719.091736ms]
    Mar  2 13:16:04.148: INFO: Created: latency-svc-lbbrs
    Mar  2 13:16:04.194: INFO: Got endpoints: latency-svc-g27cj [748.824423ms]
    Mar  2 13:16:04.204: INFO: Created: latency-svc-xxwt9
    Mar  2 13:16:04.239: INFO: Got endpoints: latency-svc-f2tch [724.818939ms]
    Mar  2 13:16:04.249: INFO: Created: latency-svc-9kkdz
    Mar  2 13:16:04.294: INFO: Got endpoints: latency-svc-764w9 [755.148355ms]
    Mar  2 13:16:04.310: INFO: Created: latency-svc-9vp89
    Mar  2 13:16:04.337: INFO: Got endpoints: latency-svc-5kfrh [718.434648ms]
    Mar  2 13:16:04.345: INFO: Created: latency-svc-tf9hw
    Mar  2 13:16:04.396: INFO: Got endpoints: latency-svc-zgdqk [756.670762ms]
    Mar  2 13:16:04.420: INFO: Created: latency-svc-t5csq
    Mar  2 13:16:04.440: INFO: Got endpoints: latency-svc-xs44x [727.360462ms]
    Mar  2 13:16:04.451: INFO: Created: latency-svc-9gp7p
    Mar  2 13:16:04.519: INFO: Got endpoints: latency-svc-s8rzs [772.739116ms]
    Mar  2 13:16:04.539: INFO: Created: latency-svc-28bq7
    Mar  2 13:16:04.540: INFO: Got endpoints: latency-svc-6wkd4 [721.491777ms]
    Mar  2 13:16:04.551: INFO: Created: latency-svc-655vc
    Mar  2 13:16:04.592: INFO: Got endpoints: latency-svc-dbp9h [753.291101ms]
    Mar  2 13:16:04.617: INFO: Created: latency-svc-vrvgf
    Mar  2 13:16:04.637: INFO: Got endpoints: latency-svc-98m64 [723.110012ms]
    Mar  2 13:16:04.648: INFO: Created: latency-svc-c2t2n
    Mar  2 13:16:04.714: INFO: Got endpoints: latency-svc-qsht9 [771.461818ms]
    Mar  2 13:16:04.723: INFO: Created: latency-svc-z45d2
    Mar  2 13:16:04.737: INFO: Got endpoints: latency-svc-dlldf [728.678341ms]
    Mar  2 13:16:04.747: INFO: Created: latency-svc-246tv
    Mar  2 13:16:04.792: INFO: Got endpoints: latency-svc-k8225 [754.335781ms]
    Mar  2 13:16:04.821: INFO: Created: latency-svc-w2ngd
    Mar  2 13:16:04.835: INFO: Got endpoints: latency-svc-jnwk7 [736.412253ms]
    Mar  2 13:16:04.847: INFO: Created: latency-svc-696jf
    Mar  2 13:16:04.887: INFO: Got endpoints: latency-svc-lbbrs [749.380394ms]
    Mar  2 13:16:04.908: INFO: Created: latency-svc-p5zcx
    Mar  2 13:16:04.941: INFO: Got endpoints: latency-svc-xxwt9 [746.501478ms]
    Mar  2 13:16:04.949: INFO: Created: latency-svc-6kkgn
    Mar  2 13:16:05.003: INFO: Got endpoints: latency-svc-9kkdz [763.086682ms]
    Mar  2 13:16:05.015: INFO: Created: latency-svc-qtrn4
    Mar  2 13:16:05.038: INFO: Got endpoints: latency-svc-9vp89 [744.078202ms]
    Mar  2 13:16:05.050: INFO: Created: latency-svc-z6vtp
    Mar  2 13:16:05.091: INFO: Got endpoints: latency-svc-tf9hw [753.409941ms]
    Mar  2 13:16:05.110: INFO: Created: latency-svc-mgnmf
    Mar  2 13:16:05.139: INFO: Got endpoints: latency-svc-t5csq [742.727893ms]
    Mar  2 13:16:05.147: INFO: Created: latency-svc-g9fm7
    Mar  2 13:16:05.214: INFO: Got endpoints: latency-svc-9gp7p [773.601589ms]
    Mar  2 13:16:05.234: INFO: Created: latency-svc-6mmqg
    Mar  2 13:16:05.240: INFO: Got endpoints: latency-svc-28bq7 [720.142474ms]
    Mar  2 13:16:05.247: INFO: Created: latency-svc-d687x
    Mar  2 13:16:05.308: INFO: Got endpoints: latency-svc-655vc [768.131181ms]
    Mar  2 13:16:05.317: INFO: Created: latency-svc-wzr69
    Mar  2 13:16:05.347: INFO: Got endpoints: latency-svc-vrvgf [754.039754ms]
    Mar  2 13:16:05.360: INFO: Created: latency-svc-lshnx
    Mar  2 13:16:05.416: INFO: Got endpoints: latency-svc-c2t2n [778.107478ms]
    Mar  2 13:16:05.444: INFO: Created: latency-svc-t69r8
    Mar  2 13:16:05.445: INFO: Got endpoints: latency-svc-z45d2 [730.429878ms]
    Mar  2 13:16:05.462: INFO: Created: latency-svc-9n6c5
    Mar  2 13:16:05.492: INFO: Got endpoints: latency-svc-246tv [755.00463ms]
    Mar  2 13:16:05.527: INFO: Created: latency-svc-scvjc
    Mar  2 13:16:05.539: INFO: Got endpoints: latency-svc-w2ngd [746.247719ms]
    Mar  2 13:16:05.558: INFO: Created: latency-svc-8tq5f
    Mar  2 13:16:05.654: INFO: Got endpoints: latency-svc-p5zcx [766.985681ms]
    Mar  2 13:16:05.655: INFO: Got endpoints: latency-svc-696jf [819.656179ms]
    Mar  2 13:16:05.667: INFO: Created: latency-svc-7wc24
    Mar  2 13:16:05.732: INFO: Created: latency-svc-js8pj
    Mar  2 13:16:05.732: INFO: Got endpoints: latency-svc-6kkgn [790.409563ms]
    Mar  2 13:16:05.739: INFO: Got endpoints: latency-svc-qtrn4 [735.736779ms]
    Mar  2 13:16:05.758: INFO: Created: latency-svc-l7cpr
    Mar  2 13:16:05.761: INFO: Created: latency-svc-k8qzv
    Mar  2 13:16:05.806: INFO: Got endpoints: latency-svc-z6vtp [767.517697ms]
    Mar  2 13:16:05.853: INFO: Created: latency-svc-67kqt
    Mar  2 13:16:05.853: INFO: Got endpoints: latency-svc-mgnmf [762.79875ms]
    Mar  2 13:16:05.877: INFO: Created: latency-svc-6752f
    Mar  2 13:16:05.889: INFO: Got endpoints: latency-svc-g9fm7 [749.948542ms]
    Mar  2 13:16:05.912: INFO: Created: latency-svc-npb2w
    Mar  2 13:16:05.939: INFO: Got endpoints: latency-svc-6mmqg [725.325762ms]
    Mar  2 13:16:05.949: INFO: Created: latency-svc-q9jj5
    Mar  2 13:16:05.993: INFO: Got endpoints: latency-svc-d687x [753.641603ms]
    Mar  2 13:16:06.009: INFO: Created: latency-svc-58rt2
    Mar  2 13:16:06.039: INFO: Got endpoints: latency-svc-wzr69 [730.831806ms]
    Mar  2 13:16:06.048: INFO: Created: latency-svc-jrljt
    Mar  2 13:16:06.090: INFO: Got endpoints: latency-svc-lshnx [742.744639ms]
    Mar  2 13:16:06.111: INFO: Created: latency-svc-z68tx
    Mar  2 13:16:06.141: INFO: Got endpoints: latency-svc-t69r8 [724.009724ms]
    Mar  2 13:16:06.155: INFO: Created: latency-svc-bm4dq
    Mar  2 13:16:06.191: INFO: Got endpoints: latency-svc-9n6c5 [746.227713ms]
    Mar  2 13:16:06.209: INFO: Created: latency-svc-rjrdp
    Mar  2 13:16:06.241: INFO: Got endpoints: latency-svc-scvjc [748.747056ms]
    Mar  2 13:16:06.250: INFO: Created: latency-svc-6tr2c
    Mar  2 13:16:06.294: INFO: Got endpoints: latency-svc-8tq5f [754.525403ms]
    Mar  2 13:16:06.326: INFO: Created: latency-svc-hxjdz
    Mar  2 13:16:06.336: INFO: Got endpoints: latency-svc-7wc24 [681.493622ms]
    Mar  2 13:16:06.349: INFO: Created: latency-svc-kpsgd
    Mar  2 13:16:06.426: INFO: Got endpoints: latency-svc-js8pj [771.598218ms]
    Mar  2 13:16:06.446: INFO: Got endpoints: latency-svc-l7cpr [714.462479ms]
    Mar  2 13:16:06.453: INFO: Created: latency-svc-wjm74
    Mar  2 13:16:06.459: INFO: Created: latency-svc-c9bb7
    Mar  2 13:16:06.492: INFO: Got endpoints: latency-svc-k8qzv [753.011764ms]
    Mar  2 13:16:06.506: INFO: Created: latency-svc-c7rsc
    Mar  2 13:16:06.540: INFO: Got endpoints: latency-svc-67kqt [733.877663ms]
    Mar  2 13:16:06.549: INFO: Created: latency-svc-256fc
    Mar  2 13:16:06.602: INFO: Got endpoints: latency-svc-6752f [748.18759ms]
    Mar  2 13:16:06.616: INFO: Created: latency-svc-b2ddc
    Mar  2 13:16:06.636: INFO: Got endpoints: latency-svc-npb2w [747.192571ms]
    Mar  2 13:16:06.649: INFO: Created: latency-svc-nslsk
    Mar  2 13:16:06.694: INFO: Got endpoints: latency-svc-q9jj5 [754.463009ms]
    Mar  2 13:16:06.716: INFO: Created: latency-svc-jtlqq
    Mar  2 13:16:06.738: INFO: Got endpoints: latency-svc-58rt2 [744.465223ms]
    Mar  2 13:16:06.748: INFO: Created: latency-svc-2rj4v
    Mar  2 13:16:06.793: INFO: Got endpoints: latency-svc-jrljt [754.110592ms]
    Mar  2 13:16:06.819: INFO: Created: latency-svc-tzlfv
    Mar  2 13:16:06.840: INFO: Got endpoints: latency-svc-z68tx [749.887795ms]
    Mar  2 13:16:06.863: INFO: Created: latency-svc-5dnbk
    Mar  2 13:16:06.891: INFO: Got endpoints: latency-svc-bm4dq [749.766444ms]
    Mar  2 13:16:06.908: INFO: Created: latency-svc-vg8xd
    Mar  2 13:16:06.940: INFO: Got endpoints: latency-svc-rjrdp [748.753749ms]
    Mar  2 13:16:06.953: INFO: Created: latency-svc-swdgw
    Mar  2 13:16:06.992: INFO: Got endpoints: latency-svc-6tr2c [750.764533ms]
    Mar  2 13:16:07.009: INFO: Created: latency-svc-4htgn
    Mar  2 13:16:07.036: INFO: Got endpoints: latency-svc-hxjdz [742.338901ms]
    Mar  2 13:16:07.048: INFO: Created: latency-svc-zfd64
    Mar  2 13:16:07.098: INFO: Got endpoints: latency-svc-kpsgd [762.088287ms]
    Mar  2 13:16:07.115: INFO: Created: latency-svc-f5d98
    Mar  2 13:16:07.141: INFO: Got endpoints: latency-svc-wjm74 [714.456447ms]
    Mar  2 13:16:07.152: INFO: Created: latency-svc-khsvw
    Mar  2 13:16:07.193: INFO: Got endpoints: latency-svc-c9bb7 [746.705141ms]
    Mar  2 13:16:07.210: INFO: Created: latency-svc-577m4
    Mar  2 13:16:07.240: INFO: Got endpoints: latency-svc-c7rsc [747.694176ms]
    Mar  2 13:16:07.251: INFO: Created: latency-svc-lmbn7
    Mar  2 13:16:07.290: INFO: Got endpoints: latency-svc-256fc [750.116597ms]
    Mar  2 13:16:07.305: INFO: Created: latency-svc-xlgzp
    Mar  2 13:16:07.339: INFO: Got endpoints: latency-svc-b2ddc [737.703271ms]
    Mar  2 13:16:07.361: INFO: Created: latency-svc-24xr6
    Mar  2 13:16:07.403: INFO: Got endpoints: latency-svc-nslsk [767.217987ms]
    Mar  2 13:16:07.412: INFO: Created: latency-svc-n8g6x
    Mar  2 13:16:07.441: INFO: Got endpoints: latency-svc-jtlqq [746.701175ms]
    Mar  2 13:16:07.453: INFO: Created: latency-svc-hpw26
    Mar  2 13:16:07.498: INFO: Got endpoints: latency-svc-2rj4v [760.279104ms]
    Mar  2 13:16:07.510: INFO: Created: latency-svc-b65cm
    Mar  2 13:16:07.542: INFO: Got endpoints: latency-svc-tzlfv [748.169722ms]
    Mar  2 13:16:07.564: INFO: Created: latency-svc-lsh4l
    Mar  2 13:16:07.597: INFO: Got endpoints: latency-svc-5dnbk [756.528083ms]
    Mar  2 13:16:07.639: INFO: Got endpoints: latency-svc-vg8xd [747.814581ms]
    Mar  2 13:16:07.714: INFO: Got endpoints: latency-svc-swdgw [773.239007ms]
    Mar  2 13:16:07.738: INFO: Got endpoints: latency-svc-4htgn [745.560428ms]
    Mar  2 13:16:07.793: INFO: Got endpoints: latency-svc-zfd64 [756.272444ms]
    Mar  2 13:16:07.839: INFO: Got endpoints: latency-svc-f5d98 [740.891187ms]
    Mar  2 13:16:07.900: INFO: Got endpoints: latency-svc-khsvw [759.141455ms]
    Mar  2 13:16:07.940: INFO: Got endpoints: latency-svc-577m4 [745.947029ms]
    Mar  2 13:16:08.000: INFO: Got endpoints: latency-svc-lmbn7 [759.577353ms]
    Mar  2 13:16:08.041: INFO: Got endpoints: latency-svc-xlgzp [749.815623ms]
    Mar  2 13:16:08.089: INFO: Got endpoints: latency-svc-24xr6 [749.35191ms]
    Mar  2 13:16:08.143: INFO: Got endpoints: latency-svc-n8g6x [739.983053ms]
    Mar  2 13:16:08.189: INFO: Got endpoints: latency-svc-hpw26 [747.932084ms]
    Mar  2 13:16:08.242: INFO: Got endpoints: latency-svc-b65cm [743.339965ms]
    Mar  2 13:16:08.296: INFO: Got endpoints: latency-svc-lsh4l [754.129912ms]
    Mar  2 13:16:08.297: INFO: Latencies: [28.14079ms 48.897265ms 52.025188ms 69.919266ms 92.314754ms 94.601017ms 110.527354ms 130.809633ms 143.483904ms 148.218295ms 156.787349ms 157.168451ms 164.591184ms 165.342961ms 165.906814ms 171.678432ms 172.587698ms 173.612371ms 176.432397ms 177.256663ms 177.920744ms 184.030014ms 189.816365ms 206.708477ms 210.427472ms 215.411831ms 222.009269ms 224.805999ms 225.386594ms 229.194355ms 229.864325ms 232.052541ms 233.922181ms 236.145785ms 237.236328ms 238.907567ms 241.766774ms 247.440009ms 247.911434ms 248.536944ms 250.889593ms 299.799703ms 316.731039ms 379.451915ms 397.609397ms 429.854264ms 442.348577ms 489.146899ms 549.042897ms 588.884837ms 609.549031ms 632.784372ms 633.840135ms 681.493622ms 691.398603ms 693.990036ms 700.408416ms 702.859354ms 706.563739ms 709.144821ms 711.85158ms 714.456447ms 714.462479ms 718.434648ms 719.091736ms 719.393045ms 719.765499ms 719.807177ms 720.142474ms 720.248375ms 720.981756ms 721.491777ms 721.928831ms 722.43975ms 722.943215ms 723.110012ms 723.272697ms 723.293369ms 723.363121ms 724.009724ms 724.818939ms 724.934664ms 725.325762ms 726.422859ms 726.999359ms 727.360462ms 727.818241ms 727.874395ms 727.902175ms 728.678341ms 729.010041ms 729.368478ms 730.429878ms 730.831806ms 732.168742ms 732.554315ms 733.877663ms 735.736779ms 736.412253ms 737.703271ms 739.983053ms 740.878424ms 740.891187ms 742.056302ms 742.338901ms 742.727893ms 742.744639ms 743.339965ms 743.745585ms 744.078202ms 744.465223ms 745.437247ms 745.560428ms 745.947029ms 746.227713ms 746.247719ms 746.501478ms 746.701175ms 746.705141ms 747.192571ms 747.694176ms 747.814581ms 747.932084ms 748.169722ms 748.18759ms 748.679703ms 748.747056ms 748.753749ms 748.824423ms 749.35191ms 749.380394ms 749.766444ms 749.815623ms 749.887795ms 749.948542ms 750.116597ms 750.690317ms 750.764533ms 751.586165ms 753.011764ms 753.249546ms 753.291101ms 753.409941ms 753.641603ms 754.039754ms 754.110592ms 754.129912ms 754.283528ms 754.335781ms 754.463009ms 754.525403ms 755.00463ms 755.148355ms 755.429565ms 756.176791ms 756.272444ms 756.528083ms 756.670762ms 758.354229ms 759.141455ms 759.213742ms 759.577353ms 760.279104ms 762.088287ms 762.79875ms 763.086682ms 766.985681ms 767.217987ms 767.517697ms 768.018709ms 768.071804ms 768.131181ms 770.026245ms 771.22966ms 771.396992ms 771.461818ms 771.598218ms 771.653379ms 772.184944ms 772.310573ms 772.739116ms 773.239007ms 773.490667ms 773.601589ms 773.833736ms 773.884102ms 773.938571ms 774.667097ms 774.871591ms 775.660133ms 777.956407ms 778.107478ms 778.720723ms 779.531422ms 779.562301ms 782.434971ms 785.154088ms 790.409563ms 792.092426ms 819.656179ms]
    Mar  2 13:16:08.298: INFO: 50 %ile: 739.983053ms
    Mar  2 13:16:08.298: INFO: 90 %ile: 772.739116ms
    Mar  2 13:16:08.298: INFO: 99 %ile: 792.092426ms
    Mar  2 13:16:08.298: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Mar  2 13:16:08.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-443" for this suite. 03/02/23 13:16:08.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:16:08.321
Mar  2 13:16:08.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sched-pred 03/02/23 13:16:08.323
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:16:08.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:16:08.341
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 13:16:08.344: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 13:16:08.353: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 13:16:08.356: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv0 before test
Mar  2 13:16:08.392: INFO: falco-exporter-r2zfx from falco started at 2023-02-27 15:51:02 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.392: INFO: 	Container falco-exporter ready: true, restart count 4
Mar  2 13:16:08.392: INFO: falco-falcosidekick-5d5c7d4db-r6952 from falco started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.392: INFO: 	Container falcosidekick ready: true, restart count 0
Mar  2 13:16:08.392: INFO: falco-trjnh from falco started at 2023-02-27 15:53:31 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.392: INFO: 	Container falco ready: true, restart count 2
Mar  2 13:16:08.393: INFO: fluentd-forwarder-qxbtj from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.393: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 13:16:08.393: INFO: ingress-nginx-controller-qprg2 from ingress-nginx started at 2023-02-27 13:57:01 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.393: INFO: 	Container controller ready: true, restart count 2
Mar  2 13:16:08.393: INFO: calico-accountant-k6t4p from kube-system started at 2023-02-27 13:50:57 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.393: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 13:16:08.393: INFO: calico-node-dz84l from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.394: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 13:16:08.394: INFO: coredns-588bb58b94-k6j76 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.394: INFO: 	Container coredns ready: true, restart count 0
Mar  2 13:16:08.394: INFO: csi-cinder-nodeplugin-9nct9 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
Mar  2 13:16:08.394: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
Mar  2 13:16:08.394: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 13:16:08.394: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 13:16:08.395: INFO: kube-proxy-7bm9z from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.395: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 13:16:08.395: INFO: nginx-proxy-aarnq-sc-k8s-node-srv0 from kube-system started at 2023-02-27 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.395: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 13:16:08.395: INFO: node-local-dns-dk8hd from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.395: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 13:16:08.395: INFO: snapshot-controller-7d445c66c9-6w4w5 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.395: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 13:16:08.395: INFO: kured-hdkrw from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.395: INFO: 	Container kured ready: true, restart count 3
Mar  2 13:16:08.396: INFO: alertmanager-kube-prometheus-stack-alertmanager-0 from monitoring started at 2023-03-01 07:32:10 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.396: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 13:16:08.396: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 13:16:08.396: INFO: kube-prometheus-stack-grafana-84f79f467b-sr7kl from monitoring started at 2023-02-28 08:03:49 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.396: INFO: 	Container grafana ready: true, restart count 0
Mar  2 13:16:08.396: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar  2 13:16:08.396: INFO: kube-prometheus-stack-prometheus-node-exporter-nl9pw from monitoring started at 2023-02-27 13:49:55 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.396: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 13:16:08.397: INFO: prometheus-blackbox-exporter-677b579798-7xm9d from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.397: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar  2 13:16:08.397: INFO: s3-exporter-867c5b9457-lfcsf from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.397: INFO: 	Container s3-exporter ready: true, restart count 0
Mar  2 13:16:08.397: INFO: opensearch-dashboards-58c8d95f7b-9spcv from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.397: INFO: 	Container dashboards ready: true, restart count 0
Mar  2 13:16:08.397: INFO: opensearch-master-2 from opensearch-system started at 2023-02-28 08:03:54 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.397: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 13:16:08.397: INFO: prometheus-opensearch-exporter-5688c84dcd-95vjh from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.397: INFO: 	Container exporter ready: true, restart count 0
Mar  2 13:16:08.397: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-qv9vz from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.398: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 13:16:08.398: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 13:16:08.398: INFO: thanos-query-query-69fc6f554b-db7w4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.398: INFO: 	Container query ready: true, restart count 0
Mar  2 13:16:08.398: INFO: thanos-receiver-bucketweb-b4955fcf8-8w2xg from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.398: INFO: 	Container bucketweb ready: true, restart count 0
Mar  2 13:16:08.398: INFO: thanos-receiver-compactor-848df7b5d7-z2jh4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.398: INFO: 	Container compactor ready: true, restart count 0
Mar  2 13:16:08.399: INFO: thanos-receiver-receive-0 from thanos started at 2023-02-28 08:04:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.399: INFO: 	Container receive ready: true, restart count 0
Mar  2 13:16:08.399: INFO: thanos-receiver-receive-distributor-779c5d74d8-7hhmb from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.399: INFO: 	Container receive ready: true, restart count 0
Mar  2 13:16:08.399: INFO: thanos-receiver-ruler-0 from thanos started at 2023-02-28 08:03:52 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.399: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 13:16:08.399: INFO: 	Container ruler ready: true, restart count 0
Mar  2 13:16:08.399: INFO: restic-k4z4s from velero started at 2023-02-27 13:13:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.399: INFO: 	Container restic ready: true, restart count 2
Mar  2 13:16:08.399: INFO: velero-7bbd458dfc-s8n2h from velero started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.399: INFO: 	Container velero ready: true, restart count 0
Mar  2 13:16:08.400: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv1 before test
Mar  2 13:16:08.449: INFO: cert-manager-754b766f8b-fvh5z from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.449: INFO: 	Container cert-manager-controller ready: true, restart count 0
Mar  2 13:16:08.449: INFO: cert-manager-webhook-875cdf98f-lfgn7 from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.449: INFO: 	Container cert-manager-webhook ready: true, restart count 0
Mar  2 13:16:08.449: INFO: dex-58d8c68494-flfvv from dex started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.449: INFO: 	Container dex ready: true, restart count 0
Mar  2 13:16:08.449: INFO: falco-8cz2x from falco started at 2023-02-27 15:54:18 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.449: INFO: 	Container falco ready: true, restart count 2
Mar  2 13:16:08.449: INFO: falco-exporter-srbrb from falco started at 2023-02-27 15:51:10 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.449: INFO: 	Container falco-exporter ready: true, restart count 4
Mar  2 13:16:08.449: INFO: fluentd-aggregator-0 from fluentd-system started at 2023-02-28 08:16:03 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.449: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 13:16:08.449: INFO: fluentd-forwarder-zgcds from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.449: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 13:16:08.449: INFO: harbor-chartmuseum-5c9477455d-hp9zb from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.449: INFO: 	Container chartmuseum ready: true, restart count 0
Mar  2 13:16:08.449: INFO: harbor-core-58dc955656-2vz5k from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.449: INFO: 	Container core ready: true, restart count 1
Mar  2 13:16:08.450: INFO: harbor-database-0 from harbor started at 2023-02-28 08:16:02 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container database ready: true, restart count 0
Mar  2 13:16:08.450: INFO: harbor-jobservice-69c4c778fb-8qt7s from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container jobservice ready: true, restart count 2
Mar  2 13:16:08.450: INFO: harbor-notary-server-6cfdf66b5-sxpqw from harbor started at 2023-02-28 08:15:44 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container notary-server ready: true, restart count 2
Mar  2 13:16:08.450: INFO: harbor-notary-signer-5d6d45f584-rfqm7 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container notary-signer ready: true, restart count 2
Mar  2 13:16:08.450: INFO: harbor-portal-77d6c78fd9-p7t57 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container portal ready: true, restart count 0
Mar  2 13:16:08.450: INFO: harbor-redis-0 from harbor started at 2023-02-28 08:16:04 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container redis ready: true, restart count 0
Mar  2 13:16:08.450: INFO: harbor-registry-787bfb74d7-9vbht from harbor started at 2023-02-28 08:15:42 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container registry ready: true, restart count 0
Mar  2 13:16:08.450: INFO: 	Container registryctl ready: true, restart count 0
Mar  2 13:16:08.450: INFO: harbor-trivy-0 from harbor started at 2023-02-28 08:15:58 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container trivy ready: true, restart count 0
Mar  2 13:16:08.450: INFO: ingress-nginx-controller-8jd6t from ingress-nginx started at 2023-02-27 13:52:07 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container controller ready: true, restart count 2
Mar  2 13:16:08.450: INFO: calico-accountant-sfvmv from kube-system started at 2023-02-27 13:50:54 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 13:16:08.450: INFO: calico-node-vj6gp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 13:16:08.450: INFO: csi-cinder-nodeplugin-lvpvh from kube-system started at 2023-02-27 13:28:22 +0000 UTC (3 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container cinder-csi-plugin ready: true, restart count 2
Mar  2 13:16:08.450: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 13:16:08.450: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 13:16:08.450: INFO: kube-proxy-nrgbs from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 13:16:08.450: INFO: metrics-server-d9dcc77d6-z4sx7 from kube-system started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container metrics-server ready: true, restart count 0
Mar  2 13:16:08.450: INFO: nginx-proxy-aarnq-sc-k8s-node-srv1 from kube-system started at 2023-02-27 14:17:16 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 13:16:08.450: INFO: node-local-dns-b8kzp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 13:16:08.450: INFO: kured-kbmf8 from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container kured ready: true, restart count 3
Mar  2 13:16:08.450: INFO: alertmanager-kube-prometheus-stack-alertmanager-1 from monitoring started at 2023-02-28 08:16:00 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.450: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 13:16:08.451: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 13:16:08.451: INFO: kube-prometheus-stack-prometheus-node-exporter-jmbsj from monitoring started at 2023-02-27 13:49:59 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.451: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 13:16:08.451: INFO: user-grafana-6f7c7d589-q6clc from monitoring started at 2023-02-28 08:15:43 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.451: INFO: 	Container grafana ready: true, restart count 0
Mar  2 13:16:08.451: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar  2 13:16:08.451: INFO: opensearch-master-0 from opensearch-system started at 2023-02-28 08:16:01 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.451: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 13:16:08.451: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-j5shm from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.451: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 13:16:08.451: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 13:16:08.451: INFO: thanos-query-query-69fc6f554b-d2q5v from thanos started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.451: INFO: 	Container query ready: true, restart count 0
Mar  2 13:16:08.451: INFO: thanos-receiver-receive-2 from thanos started at 2023-02-28 08:16:05 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.451: INFO: 	Container receive ready: true, restart count 0
Mar  2 13:16:08.451: INFO: thanos-receiver-ruler-1 from thanos started at 2023-02-28 08:15:57 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.451: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 13:16:08.451: INFO: 	Container ruler ready: true, restart count 0
Mar  2 13:16:08.451: INFO: restic-zvhnj from velero started at 2023-02-27 13:13:38 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.451: INFO: 	Container restic ready: true, restart count 2
Mar  2 13:16:08.451: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv2 before test
Mar  2 13:16:08.520: INFO: falco-4c5wt from falco started at 2023-02-27 15:51:55 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container falco ready: true, restart count 6
Mar  2 13:16:08.520: INFO: falco-exporter-cvpnp from falco started at 2023-02-27 15:51:06 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container falco-exporter ready: true, restart count 8
Mar  2 13:16:08.520: INFO: aarnq-sc-logs-logs-compaction-27961830-qfpkw from fluentd-system started at 2023-03-01 22:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container compaction ready: false, restart count 0
Mar  2 13:16:08.520: INFO: aarnq-sc-logs-logs-retention-27961890-98gmj from fluentd-system started at 2023-03-01 23:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container retention ready: false, restart count 0
Mar  2 13:16:08.520: INFO: fluentd-forwarder-b54pr from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container fluentd-forwarder ready: true, restart count 3
Mar  2 13:16:08.520: INFO: harbor-backup-cronjob-27961920-n2gpm from harbor started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container run ready: false, restart count 0
Mar  2 13:16:08.520: INFO: ingress-nginx-controller-lbvdv from ingress-nginx started at 2023-02-27 13:51:24 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container controller ready: true, restart count 3
Mar  2 13:16:08.520: INFO: calico-accountant-sb26b from kube-system started at 2023-02-27 13:50:50 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container calico-accountant ready: true, restart count 3
Mar  2 13:16:08.520: INFO: calico-node-9ps2k from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container calico-node ready: true, restart count 3
Mar  2 13:16:08.520: INFO: csi-cinder-nodeplugin-8qsk5 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container cinder-csi-plugin ready: true, restart count 9
Mar  2 13:16:08.520: INFO: 	Container liveness-probe ready: true, restart count 3
Mar  2 13:16:08.520: INFO: 	Container node-driver-registrar ready: true, restart count 3
Mar  2 13:16:08.520: INFO: kube-proxy-nrj68 from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container kube-proxy ready: true, restart count 3
Mar  2 13:16:08.520: INFO: nginx-proxy-aarnq-sc-k8s-node-srv2 from kube-system started at 2023-02-28 07:06:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container nginx-proxy ready: true, restart count 3
Mar  2 13:16:08.520: INFO: node-local-dns-pwwsn from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container node-cache ready: true, restart count 3
Mar  2 13:16:08.520: INFO: kured-fmhzj from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container kured ready: true, restart count 4
Mar  2 13:16:08.520: INFO: ciskubebench-exporter-68bcb66c46-rjnj9 from monitoring started at 2023-03-02 11:51:13 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container metrics-collector ready: true, restart count 0
Mar  2 13:16:08.520: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 13:16:08.520: INFO: kube-prometheus-stack-prometheus-node-exporter-rk7sg from monitoring started at 2023-02-27 13:49:53 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container node-exporter ready: true, restart count 3
Mar  2 13:16:08.520: INFO: scan-vulnerabilityreport-d75f9bf59-zfjgt from monitoring started at 2023-03-02 13:15:57 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container svc-latency-rc ready: false, restart count 0
Mar  2 13:16:08.520: INFO: starboard-operator-7f84bbf756-grncj from monitoring started at 2023-03-02 11:50:59 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container starboard-operator ready: true, restart count 0
Mar  2 13:16:08.520: INFO: vulnerability-exporter-8485469578-jvppp from monitoring started at 2023-03-02 11:51:29 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container metrics-collector ready: true, restart count 0
Mar  2 13:16:08.520: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 13:16:08.520: INFO: opensearch-curator-27962715-gzm9k from opensearch-system started at 2023-03-02 13:15:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container opensearch-curator ready: false, restart count 0
Mar  2 13:16:08.520: INFO: sonobuoy from sonobuoy started at 2023-03-02 12:35:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 13:16:08.520: INFO: sonobuoy-e2e-job-eae18696d9844ddc from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container e2e ready: true, restart count 0
Mar  2 13:16:08.520: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 13:16:08.520: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-zf5bk from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 13:16:08.520: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 13:16:08.520: INFO: svc-latency-rc-lcr8t from svc-latency-443 started at 2023-03-02 13:15:57 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container svc-latency-rc ready: true, restart count 0
Mar  2 13:16:08.520: INFO: restic-wcgdp from velero started at 2023-02-27 13:41:53 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.520: INFO: 	Container restic ready: true, restart count 3
Mar  2 13:16:08.520: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv3 before test
Mar  2 13:16:08.590: INFO: cert-manager-cainjector-655cfbc4d-vc586 from cert-manager started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.590: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
Mar  2 13:16:08.590: INFO: dex-58d8c68494-gnrr5 from dex started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.590: INFO: 	Container dex ready: true, restart count 0
Mar  2 13:16:08.590: INFO: falco-9v9b5 from falco started at 2023-02-27 15:52:43 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.591: INFO: 	Container falco ready: true, restart count 1
Mar  2 13:16:08.591: INFO: falco-exporter-457cd from falco started at 2023-02-27 15:51:13 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.591: INFO: 	Container falco-exporter ready: true, restart count 3
Mar  2 13:16:08.591: INFO: falco-falcosidekick-5d5c7d4db-hddwc from falco started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.591: INFO: 	Container falcosidekick ready: true, restart count 0
Mar  2 13:16:08.591: INFO: aarnq-sc-logs-logs-compaction-27960390-dkhl6 from fluentd-system started at 2023-02-28 22:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.591: INFO: 	Container compaction ready: false, restart count 0
Mar  2 13:16:08.591: INFO: aarnq-sc-logs-logs-retention-27960450-xntms from fluentd-system started at 2023-02-28 23:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.592: INFO: 	Container retention ready: false, restart count 0
Mar  2 13:16:08.592: INFO: fluentd-forwarder-9smtw from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.592: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 13:16:08.592: INFO: harbor-backup-cronjob-27960480-trvs5 from harbor started at 2023-03-01 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.592: INFO: 	Container run ready: false, restart count 0
Mar  2 13:16:08.592: INFO: ingress-nginx-controller-4bgc8 from ingress-nginx started at 2023-02-27 13:54:29 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.593: INFO: 	Container controller ready: true, restart count 2
Mar  2 13:16:08.593: INFO: ingress-nginx-default-backend-64599cb78d-t9m7m from ingress-nginx started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.593: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Mar  2 13:16:08.593: INFO: calico-accountant-wgpwj from kube-system started at 2023-02-27 13:50:52 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.593: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 13:16:08.593: INFO: calico-node-7vgvf from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.593: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 13:16:08.593: INFO: csi-cinder-controllerplugin-6fdb685467-qppqd from kube-system started at 2023-03-01 07:31:56 +0000 UTC (6 container statuses recorded)
Mar  2 13:16:08.594: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  2 13:16:08.594: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  2 13:16:08.594: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  2 13:16:08.594: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  2 13:16:08.594: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar  2 13:16:08.594: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 13:16:08.594: INFO: csi-cinder-nodeplugin-wn26q from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
Mar  2 13:16:08.594: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
Mar  2 13:16:08.594: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 13:16:08.595: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 13:16:08.595: INFO: kube-proxy-t9sqm from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.595: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 13:16:08.595: INFO: nginx-proxy-aarnq-sc-k8s-node-srv3 from kube-system started at 2023-02-27 13:14:14 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.595: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 13:16:08.595: INFO: node-local-dns-jf9nv from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.595: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 13:16:08.595: INFO: kured-g9qpk from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.595: INFO: 	Container kured ready: true, restart count 3
Mar  2 13:16:08.596: INFO: grafana-label-enforcer-ff6966584-d9872 from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.596: INFO: 	Container prom-label-enforcer ready: true, restart count 0
Mar  2 13:16:08.596: INFO: kube-prometheus-stack-kube-state-metrics-5584579f7d-jmrqj from monitoring started at 2023-03-01 07:31:57 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.596: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 13:16:08.596: INFO: kube-prometheus-stack-operator-6bd84664f-wxlxc from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.596: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Mar  2 13:16:08.596: INFO: kube-prometheus-stack-prometheus-node-exporter-9v6v5 from monitoring started at 2023-02-27 13:50:02 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.596: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 13:16:08.596: INFO: prometheus-kube-prometheus-stack-prometheus-0 from monitoring started at 2023-03-01 07:32:02 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.597: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 13:16:08.597: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 13:16:08.597: INFO: opensearch-master-1 from opensearch-system started at 2023-03-01 07:32:10 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.598: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 13:16:08.598: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-m5t49 from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 13:16:08.598: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 13:16:08.598: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 13:16:08.598: INFO: thanos-query-query-frontend-6c58dbdc6-6g7jx from thanos started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.599: INFO: 	Container query-frontend ready: true, restart count 0
Mar  2 13:16:08.599: INFO: thanos-receiver-receive-1 from thanos started at 2023-03-01 07:32:11 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.599: INFO: 	Container receive ready: true, restart count 0
Mar  2 13:16:08.599: INFO: thanos-receiver-storegateway-0 from thanos started at 2023-03-01 07:32:06 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.599: INFO: 	Container storegateway ready: true, restart count 0
Mar  2 13:16:08.599: INFO: restic-hwpfm from velero started at 2023-02-27 13:13:47 +0000 UTC (1 container statuses recorded)
Mar  2 13:16:08.599: INFO: 	Container restic ready: true, restart count 3
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 13:16:08.599
Mar  2 13:16:08.642: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8153" to be "running"
Mar  2 13:16:08.666: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 24.394306ms
Mar  2 13:16:10.694: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.05183398s
Mar  2 13:16:10.694: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 13:16:10.71
STEP: Trying to apply a random label on the found node. 03/02/23 13:16:10.734
STEP: verifying the node has the label kubernetes.io/e2e-850ef1c6-0836-43ca-84a4-4079b45847b9 95 03/02/23 13:16:10.752
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/02/23 13:16:10.755
Mar  2 13:16:10.761: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8153" to be "not pending"
Mar  2 13:16:10.764: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06434ms
Mar  2 13:16:12.783: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.021338081s
Mar  2 13:16:12.783: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.16.0.192 on the node which pod4 resides and expect not scheduled 03/02/23 13:16:12.783
Mar  2 13:16:12.798: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8153" to be "not pending"
Mar  2 13:16:12.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004037ms
Mar  2 13:16:14.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02035119s
Mar  2 13:16:16.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021561754s
Mar  2 13:16:18.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018985852s
Mar  2 13:16:20.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019436465s
Mar  2 13:16:22.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021052759s
Mar  2 13:16:24.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019027091s
Mar  2 13:16:26.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015197483s
Mar  2 13:16:28.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017700343s
Mar  2 13:16:30.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017451476s
Mar  2 13:16:32.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.028550818s
Mar  2 13:16:34.825: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.026776146s
Mar  2 13:16:36.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.016402847s
Mar  2 13:16:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020919097s
Mar  2 13:16:40.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.016849193s
Mar  2 13:16:42.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017294448s
Mar  2 13:16:44.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.022746721s
Mar  2 13:16:46.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.018446221s
Mar  2 13:16:48.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.023124117s
Mar  2 13:16:50.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.022895429s
Mar  2 13:16:52.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.016355695s
Mar  2 13:16:54.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.020574913s
Mar  2 13:16:56.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.02278335s
Mar  2 13:16:58.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016211475s
Mar  2 13:17:00.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.022016387s
Mar  2 13:17:02.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.022027945s
Mar  2 13:17:04.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019833375s
Mar  2 13:17:06.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.019394611s
Mar  2 13:17:08.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01698482s
Mar  2 13:17:10.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.022474607s
Mar  2 13:17:12.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.020185753s
Mar  2 13:17:14.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.019388466s
Mar  2 13:17:16.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.022684444s
Mar  2 13:17:18.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.020569462s
Mar  2 13:17:20.825: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.027422209s
Mar  2 13:17:22.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.019208418s
Mar  2 13:17:24.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.021620009s
Mar  2 13:17:26.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.019946413s
Mar  2 13:17:28.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01730816s
Mar  2 13:17:30.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.022011149s
Mar  2 13:17:32.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.015684695s
Mar  2 13:17:34.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.021108838s
Mar  2 13:17:36.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.021414026s
Mar  2 13:17:38.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018163649s
Mar  2 13:17:40.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.018964014s
Mar  2 13:17:42.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.018397455s
Mar  2 13:17:44.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.021697014s
Mar  2 13:17:46.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.020334575s
Mar  2 13:17:48.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.018391439s
Mar  2 13:17:50.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.018352742s
Mar  2 13:17:52.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.018346461s
Mar  2 13:17:54.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019493496s
Mar  2 13:17:56.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.019702434s
Mar  2 13:17:58.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.017422898s
Mar  2 13:18:00.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.019493942s
Mar  2 13:18:02.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.017301257s
Mar  2 13:18:04.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.020389094s
Mar  2 13:18:06.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.021158328s
Mar  2 13:18:08.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.020581759s
Mar  2 13:18:10.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.016389851s
Mar  2 13:18:12.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.021748635s
Mar  2 13:18:14.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.020163647s
Mar  2 13:18:16.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.018660704s
Mar  2 13:18:18.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.016065224s
Mar  2 13:18:20.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.019933343s
Mar  2 13:18:22.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.02078544s
Mar  2 13:18:24.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.018706065s
Mar  2 13:18:26.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.022793124s
Mar  2 13:18:28.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.01725301s
Mar  2 13:18:30.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.017146526s
Mar  2 13:18:32.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.02038634s
Mar  2 13:18:34.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.020003s
Mar  2 13:18:36.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.019532508s
Mar  2 13:18:38.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.017735611s
Mar  2 13:18:40.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.023139826s
Mar  2 13:18:42.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.024508354s
Mar  2 13:18:44.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.017769596s
Mar  2 13:18:46.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.019395658s
Mar  2 13:18:48.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.016620642s
Mar  2 13:18:50.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.023452798s
Mar  2 13:18:52.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.02031516s
Mar  2 13:18:54.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.02482022s
Mar  2 13:18:56.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.019246939s
Mar  2 13:18:58.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.020291461s
Mar  2 13:19:00.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.0229786s
Mar  2 13:19:02.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.02374517s
Mar  2 13:19:04.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.020269977s
Mar  2 13:19:06.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.020166522s
Mar  2 13:19:08.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.018671393s
Mar  2 13:19:10.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.023938789s
Mar  2 13:19:12.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.017942409s
Mar  2 13:19:14.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.022595524s
Mar  2 13:19:16.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.01922071s
Mar  2 13:19:18.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.017005649s
Mar  2 13:19:20.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.022000821s
Mar  2 13:19:22.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.017584448s
Mar  2 13:19:24.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.020828683s
Mar  2 13:19:26.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.02504779s
Mar  2 13:19:28.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.018925251s
Mar  2 13:19:30.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.019504393s
Mar  2 13:19:32.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.017374076s
Mar  2 13:19:34.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.0193592s
Mar  2 13:19:36.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.019030242s
Mar  2 13:19:38.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.016926919s
Mar  2 13:19:40.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.020396141s
Mar  2 13:19:42.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.018906992s
Mar  2 13:19:44.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.017892063s
Mar  2 13:19:46.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.018618425s
Mar  2 13:19:48.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.018477577s
Mar  2 13:19:50.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.019160842s
Mar  2 13:19:52.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.016667064s
Mar  2 13:19:54.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.021538503s
Mar  2 13:19:56.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.018851898s
Mar  2 13:19:58.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.016402304s
Mar  2 13:20:00.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.015427304s
Mar  2 13:20:02.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.018803541s
Mar  2 13:20:04.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.019560494s
Mar  2 13:20:06.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.020969728s
Mar  2 13:20:08.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.021650401s
Mar  2 13:20:10.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.026706543s
Mar  2 13:20:12.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.016723259s
Mar  2 13:20:14.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.01993139s
Mar  2 13:20:16.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.017701809s
Mar  2 13:20:18.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.016854683s
Mar  2 13:20:20.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.023460637s
Mar  2 13:20:22.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.015237368s
Mar  2 13:20:24.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.016330925s
Mar  2 13:20:26.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.019769792s
Mar  2 13:20:28.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.017108108s
Mar  2 13:20:30.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.019275661s
Mar  2 13:20:32.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.016715626s
Mar  2 13:20:34.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.01934437s
Mar  2 13:20:36.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.021718874s
Mar  2 13:20:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.021704208s
Mar  2 13:20:40.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.020467489s
Mar  2 13:20:42.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.016218826s
Mar  2 13:20:44.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.019539252s
Mar  2 13:20:46.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.021596181s
Mar  2 13:20:48.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.016565624s
Mar  2 13:20:50.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.021477642s
Mar  2 13:20:52.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.017110544s
Mar  2 13:20:54.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.01763891s
Mar  2 13:20:56.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.019915208s
Mar  2 13:20:58.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.016844116s
Mar  2 13:21:00.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.022474081s
Mar  2 13:21:02.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.016757566s
Mar  2 13:21:04.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.018609703s
Mar  2 13:21:06.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.020646528s
Mar  2 13:21:08.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.017233741s
Mar  2 13:21:10.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.018584115s
Mar  2 13:21:12.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01649096s
Mar  2 13:21:12.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.020080868s
STEP: removing the label kubernetes.io/e2e-850ef1c6-0836-43ca-84a4-4079b45847b9 off the node aarnq-sc-k8s-node-srv2 03/02/23 13:21:12.818
STEP: verifying the node doesn't have the label kubernetes.io/e2e-850ef1c6-0836-43ca-84a4-4079b45847b9 03/02/23 13:21:12.842
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  2 13:21:12.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8153" for this suite. 03/02/23 13:21:12.856
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":130,"skipped":2445,"failed":0}
------------------------------
â€¢ [SLOW TEST] [304.558 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:16:08.321
    Mar  2 13:16:08.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sched-pred 03/02/23 13:16:08.323
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:16:08.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:16:08.341
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  2 13:16:08.344: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  2 13:16:08.353: INFO: Waiting for terminating namespaces to be deleted...
    Mar  2 13:16:08.356: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv0 before test
    Mar  2 13:16:08.392: INFO: falco-exporter-r2zfx from falco started at 2023-02-27 15:51:02 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.392: INFO: 	Container falco-exporter ready: true, restart count 4
    Mar  2 13:16:08.392: INFO: falco-falcosidekick-5d5c7d4db-r6952 from falco started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.392: INFO: 	Container falcosidekick ready: true, restart count 0
    Mar  2 13:16:08.392: INFO: falco-trjnh from falco started at 2023-02-27 15:53:31 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.392: INFO: 	Container falco ready: true, restart count 2
    Mar  2 13:16:08.393: INFO: fluentd-forwarder-qxbtj from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.393: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 13:16:08.393: INFO: ingress-nginx-controller-qprg2 from ingress-nginx started at 2023-02-27 13:57:01 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.393: INFO: 	Container controller ready: true, restart count 2
    Mar  2 13:16:08.393: INFO: calico-accountant-k6t4p from kube-system started at 2023-02-27 13:50:57 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.393: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 13:16:08.393: INFO: calico-node-dz84l from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.394: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 13:16:08.394: INFO: coredns-588bb58b94-k6j76 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.394: INFO: 	Container coredns ready: true, restart count 0
    Mar  2 13:16:08.394: INFO: csi-cinder-nodeplugin-9nct9 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
    Mar  2 13:16:08.394: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
    Mar  2 13:16:08.394: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 13:16:08.394: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 13:16:08.395: INFO: kube-proxy-7bm9z from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.395: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 13:16:08.395: INFO: nginx-proxy-aarnq-sc-k8s-node-srv0 from kube-system started at 2023-02-27 14:10:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.395: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 13:16:08.395: INFO: node-local-dns-dk8hd from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.395: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 13:16:08.395: INFO: snapshot-controller-7d445c66c9-6w4w5 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.395: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 13:16:08.395: INFO: kured-hdkrw from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.395: INFO: 	Container kured ready: true, restart count 3
    Mar  2 13:16:08.396: INFO: alertmanager-kube-prometheus-stack-alertmanager-0 from monitoring started at 2023-03-01 07:32:10 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.396: INFO: 	Container alertmanager ready: true, restart count 0
    Mar  2 13:16:08.396: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 13:16:08.396: INFO: kube-prometheus-stack-grafana-84f79f467b-sr7kl from monitoring started at 2023-02-28 08:03:49 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.396: INFO: 	Container grafana ready: true, restart count 0
    Mar  2 13:16:08.396: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Mar  2 13:16:08.396: INFO: kube-prometheus-stack-prometheus-node-exporter-nl9pw from monitoring started at 2023-02-27 13:49:55 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.396: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 13:16:08.397: INFO: prometheus-blackbox-exporter-677b579798-7xm9d from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.397: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar  2 13:16:08.397: INFO: s3-exporter-867c5b9457-lfcsf from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.397: INFO: 	Container s3-exporter ready: true, restart count 0
    Mar  2 13:16:08.397: INFO: opensearch-dashboards-58c8d95f7b-9spcv from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.397: INFO: 	Container dashboards ready: true, restart count 0
    Mar  2 13:16:08.397: INFO: opensearch-master-2 from opensearch-system started at 2023-02-28 08:03:54 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.397: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 13:16:08.397: INFO: prometheus-opensearch-exporter-5688c84dcd-95vjh from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.397: INFO: 	Container exporter ready: true, restart count 0
    Mar  2 13:16:08.397: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-qv9vz from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.398: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 13:16:08.398: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 13:16:08.398: INFO: thanos-query-query-69fc6f554b-db7w4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.398: INFO: 	Container query ready: true, restart count 0
    Mar  2 13:16:08.398: INFO: thanos-receiver-bucketweb-b4955fcf8-8w2xg from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.398: INFO: 	Container bucketweb ready: true, restart count 0
    Mar  2 13:16:08.398: INFO: thanos-receiver-compactor-848df7b5d7-z2jh4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.398: INFO: 	Container compactor ready: true, restart count 0
    Mar  2 13:16:08.399: INFO: thanos-receiver-receive-0 from thanos started at 2023-02-28 08:04:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.399: INFO: 	Container receive ready: true, restart count 0
    Mar  2 13:16:08.399: INFO: thanos-receiver-receive-distributor-779c5d74d8-7hhmb from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.399: INFO: 	Container receive ready: true, restart count 0
    Mar  2 13:16:08.399: INFO: thanos-receiver-ruler-0 from thanos started at 2023-02-28 08:03:52 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.399: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 13:16:08.399: INFO: 	Container ruler ready: true, restart count 0
    Mar  2 13:16:08.399: INFO: restic-k4z4s from velero started at 2023-02-27 13:13:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.399: INFO: 	Container restic ready: true, restart count 2
    Mar  2 13:16:08.399: INFO: velero-7bbd458dfc-s8n2h from velero started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.399: INFO: 	Container velero ready: true, restart count 0
    Mar  2 13:16:08.400: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv1 before test
    Mar  2 13:16:08.449: INFO: cert-manager-754b766f8b-fvh5z from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.449: INFO: 	Container cert-manager-controller ready: true, restart count 0
    Mar  2 13:16:08.449: INFO: cert-manager-webhook-875cdf98f-lfgn7 from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.449: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    Mar  2 13:16:08.449: INFO: dex-58d8c68494-flfvv from dex started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.449: INFO: 	Container dex ready: true, restart count 0
    Mar  2 13:16:08.449: INFO: falco-8cz2x from falco started at 2023-02-27 15:54:18 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.449: INFO: 	Container falco ready: true, restart count 2
    Mar  2 13:16:08.449: INFO: falco-exporter-srbrb from falco started at 2023-02-27 15:51:10 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.449: INFO: 	Container falco-exporter ready: true, restart count 4
    Mar  2 13:16:08.449: INFO: fluentd-aggregator-0 from fluentd-system started at 2023-02-28 08:16:03 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.449: INFO: 	Container fluentd ready: true, restart count 0
    Mar  2 13:16:08.449: INFO: fluentd-forwarder-zgcds from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.449: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 13:16:08.449: INFO: harbor-chartmuseum-5c9477455d-hp9zb from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.449: INFO: 	Container chartmuseum ready: true, restart count 0
    Mar  2 13:16:08.449: INFO: harbor-core-58dc955656-2vz5k from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.449: INFO: 	Container core ready: true, restart count 1
    Mar  2 13:16:08.450: INFO: harbor-database-0 from harbor started at 2023-02-28 08:16:02 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container database ready: true, restart count 0
    Mar  2 13:16:08.450: INFO: harbor-jobservice-69c4c778fb-8qt7s from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container jobservice ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: harbor-notary-server-6cfdf66b5-sxpqw from harbor started at 2023-02-28 08:15:44 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container notary-server ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: harbor-notary-signer-5d6d45f584-rfqm7 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container notary-signer ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: harbor-portal-77d6c78fd9-p7t57 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container portal ready: true, restart count 0
    Mar  2 13:16:08.450: INFO: harbor-redis-0 from harbor started at 2023-02-28 08:16:04 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container redis ready: true, restart count 0
    Mar  2 13:16:08.450: INFO: harbor-registry-787bfb74d7-9vbht from harbor started at 2023-02-28 08:15:42 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container registry ready: true, restart count 0
    Mar  2 13:16:08.450: INFO: 	Container registryctl ready: true, restart count 0
    Mar  2 13:16:08.450: INFO: harbor-trivy-0 from harbor started at 2023-02-28 08:15:58 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container trivy ready: true, restart count 0
    Mar  2 13:16:08.450: INFO: ingress-nginx-controller-8jd6t from ingress-nginx started at 2023-02-27 13:52:07 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container controller ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: calico-accountant-sfvmv from kube-system started at 2023-02-27 13:50:54 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: calico-node-vj6gp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: csi-cinder-nodeplugin-lvpvh from kube-system started at 2023-02-27 13:28:22 +0000 UTC (3 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container cinder-csi-plugin ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: kube-proxy-nrgbs from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: metrics-server-d9dcc77d6-z4sx7 from kube-system started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container metrics-server ready: true, restart count 0
    Mar  2 13:16:08.450: INFO: nginx-proxy-aarnq-sc-k8s-node-srv1 from kube-system started at 2023-02-27 14:17:16 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: node-local-dns-b8kzp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 13:16:08.450: INFO: kured-kbmf8 from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container kured ready: true, restart count 3
    Mar  2 13:16:08.450: INFO: alertmanager-kube-prometheus-stack-alertmanager-1 from monitoring started at 2023-02-28 08:16:00 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.450: INFO: 	Container alertmanager ready: true, restart count 0
    Mar  2 13:16:08.451: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 13:16:08.451: INFO: kube-prometheus-stack-prometheus-node-exporter-jmbsj from monitoring started at 2023-02-27 13:49:59 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.451: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 13:16:08.451: INFO: user-grafana-6f7c7d589-q6clc from monitoring started at 2023-02-28 08:15:43 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.451: INFO: 	Container grafana ready: true, restart count 0
    Mar  2 13:16:08.451: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Mar  2 13:16:08.451: INFO: opensearch-master-0 from opensearch-system started at 2023-02-28 08:16:01 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.451: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 13:16:08.451: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-j5shm from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.451: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 13:16:08.451: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 13:16:08.451: INFO: thanos-query-query-69fc6f554b-d2q5v from thanos started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.451: INFO: 	Container query ready: true, restart count 0
    Mar  2 13:16:08.451: INFO: thanos-receiver-receive-2 from thanos started at 2023-02-28 08:16:05 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.451: INFO: 	Container receive ready: true, restart count 0
    Mar  2 13:16:08.451: INFO: thanos-receiver-ruler-1 from thanos started at 2023-02-28 08:15:57 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.451: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 13:16:08.451: INFO: 	Container ruler ready: true, restart count 0
    Mar  2 13:16:08.451: INFO: restic-zvhnj from velero started at 2023-02-27 13:13:38 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.451: INFO: 	Container restic ready: true, restart count 2
    Mar  2 13:16:08.451: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv2 before test
    Mar  2 13:16:08.520: INFO: falco-4c5wt from falco started at 2023-02-27 15:51:55 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container falco ready: true, restart count 6
    Mar  2 13:16:08.520: INFO: falco-exporter-cvpnp from falco started at 2023-02-27 15:51:06 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container falco-exporter ready: true, restart count 8
    Mar  2 13:16:08.520: INFO: aarnq-sc-logs-logs-compaction-27961830-qfpkw from fluentd-system started at 2023-03-01 22:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container compaction ready: false, restart count 0
    Mar  2 13:16:08.520: INFO: aarnq-sc-logs-logs-retention-27961890-98gmj from fluentd-system started at 2023-03-01 23:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container retention ready: false, restart count 0
    Mar  2 13:16:08.520: INFO: fluentd-forwarder-b54pr from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container fluentd-forwarder ready: true, restart count 3
    Mar  2 13:16:08.520: INFO: harbor-backup-cronjob-27961920-n2gpm from harbor started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container run ready: false, restart count 0
    Mar  2 13:16:08.520: INFO: ingress-nginx-controller-lbvdv from ingress-nginx started at 2023-02-27 13:51:24 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container controller ready: true, restart count 3
    Mar  2 13:16:08.520: INFO: calico-accountant-sb26b from kube-system started at 2023-02-27 13:50:50 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container calico-accountant ready: true, restart count 3
    Mar  2 13:16:08.520: INFO: calico-node-9ps2k from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container calico-node ready: true, restart count 3
    Mar  2 13:16:08.520: INFO: csi-cinder-nodeplugin-8qsk5 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container cinder-csi-plugin ready: true, restart count 9
    Mar  2 13:16:08.520: INFO: 	Container liveness-probe ready: true, restart count 3
    Mar  2 13:16:08.520: INFO: 	Container node-driver-registrar ready: true, restart count 3
    Mar  2 13:16:08.520: INFO: kube-proxy-nrj68 from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container kube-proxy ready: true, restart count 3
    Mar  2 13:16:08.520: INFO: nginx-proxy-aarnq-sc-k8s-node-srv2 from kube-system started at 2023-02-28 07:06:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container nginx-proxy ready: true, restart count 3
    Mar  2 13:16:08.520: INFO: node-local-dns-pwwsn from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container node-cache ready: true, restart count 3
    Mar  2 13:16:08.520: INFO: kured-fmhzj from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container kured ready: true, restart count 4
    Mar  2 13:16:08.520: INFO: ciskubebench-exporter-68bcb66c46-rjnj9 from monitoring started at 2023-03-02 11:51:13 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container metrics-collector ready: true, restart count 0
    Mar  2 13:16:08.520: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 13:16:08.520: INFO: kube-prometheus-stack-prometheus-node-exporter-rk7sg from monitoring started at 2023-02-27 13:49:53 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container node-exporter ready: true, restart count 3
    Mar  2 13:16:08.520: INFO: scan-vulnerabilityreport-d75f9bf59-zfjgt from monitoring started at 2023-03-02 13:15:57 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container svc-latency-rc ready: false, restart count 0
    Mar  2 13:16:08.520: INFO: starboard-operator-7f84bbf756-grncj from monitoring started at 2023-03-02 11:50:59 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container starboard-operator ready: true, restart count 0
    Mar  2 13:16:08.520: INFO: vulnerability-exporter-8485469578-jvppp from monitoring started at 2023-03-02 11:51:29 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container metrics-collector ready: true, restart count 0
    Mar  2 13:16:08.520: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 13:16:08.520: INFO: opensearch-curator-27962715-gzm9k from opensearch-system started at 2023-03-02 13:15:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container opensearch-curator ready: false, restart count 0
    Mar  2 13:16:08.520: INFO: sonobuoy from sonobuoy started at 2023-03-02 12:35:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  2 13:16:08.520: INFO: sonobuoy-e2e-job-eae18696d9844ddc from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container e2e ready: true, restart count 0
    Mar  2 13:16:08.520: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 13:16:08.520: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-zf5bk from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 13:16:08.520: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 13:16:08.520: INFO: svc-latency-rc-lcr8t from svc-latency-443 started at 2023-03-02 13:15:57 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container svc-latency-rc ready: true, restart count 0
    Mar  2 13:16:08.520: INFO: restic-wcgdp from velero started at 2023-02-27 13:41:53 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.520: INFO: 	Container restic ready: true, restart count 3
    Mar  2 13:16:08.520: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv3 before test
    Mar  2 13:16:08.590: INFO: cert-manager-cainjector-655cfbc4d-vc586 from cert-manager started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.590: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    Mar  2 13:16:08.590: INFO: dex-58d8c68494-gnrr5 from dex started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.590: INFO: 	Container dex ready: true, restart count 0
    Mar  2 13:16:08.590: INFO: falco-9v9b5 from falco started at 2023-02-27 15:52:43 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.591: INFO: 	Container falco ready: true, restart count 1
    Mar  2 13:16:08.591: INFO: falco-exporter-457cd from falco started at 2023-02-27 15:51:13 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.591: INFO: 	Container falco-exporter ready: true, restart count 3
    Mar  2 13:16:08.591: INFO: falco-falcosidekick-5d5c7d4db-hddwc from falco started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.591: INFO: 	Container falcosidekick ready: true, restart count 0
    Mar  2 13:16:08.591: INFO: aarnq-sc-logs-logs-compaction-27960390-dkhl6 from fluentd-system started at 2023-02-28 22:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.591: INFO: 	Container compaction ready: false, restart count 0
    Mar  2 13:16:08.591: INFO: aarnq-sc-logs-logs-retention-27960450-xntms from fluentd-system started at 2023-02-28 23:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.592: INFO: 	Container retention ready: false, restart count 0
    Mar  2 13:16:08.592: INFO: fluentd-forwarder-9smtw from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.592: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 13:16:08.592: INFO: harbor-backup-cronjob-27960480-trvs5 from harbor started at 2023-03-01 00:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.592: INFO: 	Container run ready: false, restart count 0
    Mar  2 13:16:08.592: INFO: ingress-nginx-controller-4bgc8 from ingress-nginx started at 2023-02-27 13:54:29 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.593: INFO: 	Container controller ready: true, restart count 2
    Mar  2 13:16:08.593: INFO: ingress-nginx-default-backend-64599cb78d-t9m7m from ingress-nginx started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.593: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
    Mar  2 13:16:08.593: INFO: calico-accountant-wgpwj from kube-system started at 2023-02-27 13:50:52 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.593: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 13:16:08.593: INFO: calico-node-7vgvf from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.593: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 13:16:08.593: INFO: csi-cinder-controllerplugin-6fdb685467-qppqd from kube-system started at 2023-03-01 07:31:56 +0000 UTC (6 container statuses recorded)
    Mar  2 13:16:08.594: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  2 13:16:08.594: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar  2 13:16:08.594: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar  2 13:16:08.594: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar  2 13:16:08.594: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar  2 13:16:08.594: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  2 13:16:08.594: INFO: csi-cinder-nodeplugin-wn26q from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
    Mar  2 13:16:08.594: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
    Mar  2 13:16:08.594: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 13:16:08.595: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 13:16:08.595: INFO: kube-proxy-t9sqm from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.595: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 13:16:08.595: INFO: nginx-proxy-aarnq-sc-k8s-node-srv3 from kube-system started at 2023-02-27 13:14:14 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.595: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 13:16:08.595: INFO: node-local-dns-jf9nv from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.595: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 13:16:08.595: INFO: kured-g9qpk from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.595: INFO: 	Container kured ready: true, restart count 3
    Mar  2 13:16:08.596: INFO: grafana-label-enforcer-ff6966584-d9872 from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.596: INFO: 	Container prom-label-enforcer ready: true, restart count 0
    Mar  2 13:16:08.596: INFO: kube-prometheus-stack-kube-state-metrics-5584579f7d-jmrqj from monitoring started at 2023-03-01 07:31:57 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.596: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Mar  2 13:16:08.596: INFO: kube-prometheus-stack-operator-6bd84664f-wxlxc from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.596: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Mar  2 13:16:08.596: INFO: kube-prometheus-stack-prometheus-node-exporter-9v6v5 from monitoring started at 2023-02-27 13:50:02 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.596: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 13:16:08.596: INFO: prometheus-kube-prometheus-stack-prometheus-0 from monitoring started at 2023-03-01 07:32:02 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.597: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 13:16:08.597: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 13:16:08.597: INFO: opensearch-master-1 from opensearch-system started at 2023-03-01 07:32:10 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.598: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 13:16:08.598: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-m5t49 from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 13:16:08.598: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 13:16:08.598: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 13:16:08.598: INFO: thanos-query-query-frontend-6c58dbdc6-6g7jx from thanos started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.599: INFO: 	Container query-frontend ready: true, restart count 0
    Mar  2 13:16:08.599: INFO: thanos-receiver-receive-1 from thanos started at 2023-03-01 07:32:11 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.599: INFO: 	Container receive ready: true, restart count 0
    Mar  2 13:16:08.599: INFO: thanos-receiver-storegateway-0 from thanos started at 2023-03-01 07:32:06 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.599: INFO: 	Container storegateway ready: true, restart count 0
    Mar  2 13:16:08.599: INFO: restic-hwpfm from velero started at 2023-02-27 13:13:47 +0000 UTC (1 container statuses recorded)
    Mar  2 13:16:08.599: INFO: 	Container restic ready: true, restart count 3
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 13:16:08.599
    Mar  2 13:16:08.642: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8153" to be "running"
    Mar  2 13:16:08.666: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 24.394306ms
    Mar  2 13:16:10.694: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.05183398s
    Mar  2 13:16:10.694: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 13:16:10.71
    STEP: Trying to apply a random label on the found node. 03/02/23 13:16:10.734
    STEP: verifying the node has the label kubernetes.io/e2e-850ef1c6-0836-43ca-84a4-4079b45847b9 95 03/02/23 13:16:10.752
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/02/23 13:16:10.755
    Mar  2 13:16:10.761: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8153" to be "not pending"
    Mar  2 13:16:10.764: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06434ms
    Mar  2 13:16:12.783: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.021338081s
    Mar  2 13:16:12.783: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.16.0.192 on the node which pod4 resides and expect not scheduled 03/02/23 13:16:12.783
    Mar  2 13:16:12.798: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8153" to be "not pending"
    Mar  2 13:16:12.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004037ms
    Mar  2 13:16:14.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02035119s
    Mar  2 13:16:16.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021561754s
    Mar  2 13:16:18.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018985852s
    Mar  2 13:16:20.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019436465s
    Mar  2 13:16:22.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021052759s
    Mar  2 13:16:24.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019027091s
    Mar  2 13:16:26.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015197483s
    Mar  2 13:16:28.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017700343s
    Mar  2 13:16:30.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017451476s
    Mar  2 13:16:32.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.028550818s
    Mar  2 13:16:34.825: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.026776146s
    Mar  2 13:16:36.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.016402847s
    Mar  2 13:16:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020919097s
    Mar  2 13:16:40.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.016849193s
    Mar  2 13:16:42.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017294448s
    Mar  2 13:16:44.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.022746721s
    Mar  2 13:16:46.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.018446221s
    Mar  2 13:16:48.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.023124117s
    Mar  2 13:16:50.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.022895429s
    Mar  2 13:16:52.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.016355695s
    Mar  2 13:16:54.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.020574913s
    Mar  2 13:16:56.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.02278335s
    Mar  2 13:16:58.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016211475s
    Mar  2 13:17:00.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.022016387s
    Mar  2 13:17:02.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.022027945s
    Mar  2 13:17:04.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019833375s
    Mar  2 13:17:06.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.019394611s
    Mar  2 13:17:08.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01698482s
    Mar  2 13:17:10.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.022474607s
    Mar  2 13:17:12.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.020185753s
    Mar  2 13:17:14.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.019388466s
    Mar  2 13:17:16.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.022684444s
    Mar  2 13:17:18.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.020569462s
    Mar  2 13:17:20.825: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.027422209s
    Mar  2 13:17:22.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.019208418s
    Mar  2 13:17:24.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.021620009s
    Mar  2 13:17:26.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.019946413s
    Mar  2 13:17:28.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01730816s
    Mar  2 13:17:30.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.022011149s
    Mar  2 13:17:32.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.015684695s
    Mar  2 13:17:34.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.021108838s
    Mar  2 13:17:36.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.021414026s
    Mar  2 13:17:38.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018163649s
    Mar  2 13:17:40.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.018964014s
    Mar  2 13:17:42.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.018397455s
    Mar  2 13:17:44.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.021697014s
    Mar  2 13:17:46.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.020334575s
    Mar  2 13:17:48.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.018391439s
    Mar  2 13:17:50.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.018352742s
    Mar  2 13:17:52.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.018346461s
    Mar  2 13:17:54.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019493496s
    Mar  2 13:17:56.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.019702434s
    Mar  2 13:17:58.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.017422898s
    Mar  2 13:18:00.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.019493942s
    Mar  2 13:18:02.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.017301257s
    Mar  2 13:18:04.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.020389094s
    Mar  2 13:18:06.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.021158328s
    Mar  2 13:18:08.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.020581759s
    Mar  2 13:18:10.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.016389851s
    Mar  2 13:18:12.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.021748635s
    Mar  2 13:18:14.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.020163647s
    Mar  2 13:18:16.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.018660704s
    Mar  2 13:18:18.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.016065224s
    Mar  2 13:18:20.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.019933343s
    Mar  2 13:18:22.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.02078544s
    Mar  2 13:18:24.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.018706065s
    Mar  2 13:18:26.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.022793124s
    Mar  2 13:18:28.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.01725301s
    Mar  2 13:18:30.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.017146526s
    Mar  2 13:18:32.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.02038634s
    Mar  2 13:18:34.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.020003s
    Mar  2 13:18:36.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.019532508s
    Mar  2 13:18:38.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.017735611s
    Mar  2 13:18:40.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.023139826s
    Mar  2 13:18:42.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.024508354s
    Mar  2 13:18:44.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.017769596s
    Mar  2 13:18:46.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.019395658s
    Mar  2 13:18:48.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.016620642s
    Mar  2 13:18:50.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.023452798s
    Mar  2 13:18:52.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.02031516s
    Mar  2 13:18:54.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.02482022s
    Mar  2 13:18:56.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.019246939s
    Mar  2 13:18:58.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.020291461s
    Mar  2 13:19:00.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.0229786s
    Mar  2 13:19:02.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.02374517s
    Mar  2 13:19:04.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.020269977s
    Mar  2 13:19:06.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.020166522s
    Mar  2 13:19:08.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.018671393s
    Mar  2 13:19:10.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.023938789s
    Mar  2 13:19:12.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.017942409s
    Mar  2 13:19:14.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.022595524s
    Mar  2 13:19:16.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.01922071s
    Mar  2 13:19:18.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.017005649s
    Mar  2 13:19:20.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.022000821s
    Mar  2 13:19:22.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.017584448s
    Mar  2 13:19:24.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.020828683s
    Mar  2 13:19:26.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.02504779s
    Mar  2 13:19:28.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.018925251s
    Mar  2 13:19:30.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.019504393s
    Mar  2 13:19:32.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.017374076s
    Mar  2 13:19:34.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.0193592s
    Mar  2 13:19:36.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.019030242s
    Mar  2 13:19:38.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.016926919s
    Mar  2 13:19:40.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.020396141s
    Mar  2 13:19:42.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.018906992s
    Mar  2 13:19:44.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.017892063s
    Mar  2 13:19:46.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.018618425s
    Mar  2 13:19:48.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.018477577s
    Mar  2 13:19:50.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.019160842s
    Mar  2 13:19:52.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.016667064s
    Mar  2 13:19:54.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.021538503s
    Mar  2 13:19:56.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.018851898s
    Mar  2 13:19:58.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.016402304s
    Mar  2 13:20:00.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.015427304s
    Mar  2 13:20:02.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.018803541s
    Mar  2 13:20:04.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.019560494s
    Mar  2 13:20:06.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.020969728s
    Mar  2 13:20:08.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.021650401s
    Mar  2 13:20:10.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.026706543s
    Mar  2 13:20:12.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.016723259s
    Mar  2 13:20:14.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.01993139s
    Mar  2 13:20:16.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.017701809s
    Mar  2 13:20:18.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.016854683s
    Mar  2 13:20:20.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.023460637s
    Mar  2 13:20:22.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.015237368s
    Mar  2 13:20:24.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.016330925s
    Mar  2 13:20:26.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.019769792s
    Mar  2 13:20:28.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.017108108s
    Mar  2 13:20:30.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.019275661s
    Mar  2 13:20:32.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.016715626s
    Mar  2 13:20:34.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.01934437s
    Mar  2 13:20:36.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.021718874s
    Mar  2 13:20:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.021704208s
    Mar  2 13:20:40.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.020467489s
    Mar  2 13:20:42.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.016218826s
    Mar  2 13:20:44.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.019539252s
    Mar  2 13:20:46.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.021596181s
    Mar  2 13:20:48.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.016565624s
    Mar  2 13:20:50.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.021477642s
    Mar  2 13:20:52.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.017110544s
    Mar  2 13:20:54.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.01763891s
    Mar  2 13:20:56.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.019915208s
    Mar  2 13:20:58.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.016844116s
    Mar  2 13:21:00.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.022474081s
    Mar  2 13:21:02.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.016757566s
    Mar  2 13:21:04.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.018609703s
    Mar  2 13:21:06.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.020646528s
    Mar  2 13:21:08.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.017233741s
    Mar  2 13:21:10.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.018584115s
    Mar  2 13:21:12.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01649096s
    Mar  2 13:21:12.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.020080868s
    STEP: removing the label kubernetes.io/e2e-850ef1c6-0836-43ca-84a4-4079b45847b9 off the node aarnq-sc-k8s-node-srv2 03/02/23 13:21:12.818
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-850ef1c6-0836-43ca-84a4-4079b45847b9 03/02/23 13:21:12.842
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 13:21:12.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-8153" for this suite. 03/02/23 13:21:12.856
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:21:12.882
Mar  2 13:21:12.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 13:21:12.886
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:12.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:12.926
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/02/23 13:21:12.931
Mar  2 13:21:12.938: INFO: Waiting up to 5m0s for pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff" in namespace "emptydir-65" to be "Succeeded or Failed"
Mar  2 13:21:12.946: INFO: Pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.766077ms
Mar  2 13:21:14.952: INFO: Pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014395954s
Mar  2 13:21:16.953: INFO: Pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015071911s
Mar  2 13:21:18.955: INFO: Pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016819909s
STEP: Saw pod success 03/02/23 13:21:18.955
Mar  2 13:21:18.955: INFO: Pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff" satisfied condition "Succeeded or Failed"
Mar  2 13:21:18.961: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-94241e16-ec8f-4b49-94ec-545a5e7482ff container test-container: <nil>
STEP: delete the pod 03/02/23 13:21:19.015
Mar  2 13:21:19.040: INFO: Waiting for pod pod-94241e16-ec8f-4b49-94ec-545a5e7482ff to disappear
Mar  2 13:21:19.048: INFO: Pod pod-94241e16-ec8f-4b49-94ec-545a5e7482ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 13:21:19.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-65" for this suite. 03/02/23 13:21:19.055
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":131,"skipped":2460,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.181 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:21:12.882
    Mar  2 13:21:12.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 13:21:12.886
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:12.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:12.926
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/02/23 13:21:12.931
    Mar  2 13:21:12.938: INFO: Waiting up to 5m0s for pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff" in namespace "emptydir-65" to be "Succeeded or Failed"
    Mar  2 13:21:12.946: INFO: Pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.766077ms
    Mar  2 13:21:14.952: INFO: Pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014395954s
    Mar  2 13:21:16.953: INFO: Pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015071911s
    Mar  2 13:21:18.955: INFO: Pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016819909s
    STEP: Saw pod success 03/02/23 13:21:18.955
    Mar  2 13:21:18.955: INFO: Pod "pod-94241e16-ec8f-4b49-94ec-545a5e7482ff" satisfied condition "Succeeded or Failed"
    Mar  2 13:21:18.961: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-94241e16-ec8f-4b49-94ec-545a5e7482ff container test-container: <nil>
    STEP: delete the pod 03/02/23 13:21:19.015
    Mar  2 13:21:19.040: INFO: Waiting for pod pod-94241e16-ec8f-4b49-94ec-545a5e7482ff to disappear
    Mar  2 13:21:19.048: INFO: Pod pod-94241e16-ec8f-4b49-94ec-545a5e7482ff no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 13:21:19.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-65" for this suite. 03/02/23 13:21:19.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:21:19.064
Mar  2 13:21:19.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:21:19.066
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:19.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:19.139
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-452c17af-5900-4cd2-9283-7b2f96c94cc0 03/02/23 13:21:19.148
STEP: Creating the pod 03/02/23 13:21:19.152
Mar  2 13:21:19.162: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4ca6704-9d6f-4610-ae63-09cebf92a676" in namespace "configmap-3882" to be "running"
Mar  2 13:21:19.168: INFO: Pod "pod-configmaps-a4ca6704-9d6f-4610-ae63-09cebf92a676": Phase="Pending", Reason="", readiness=false. Elapsed: 5.114043ms
Mar  2 13:21:21.198: INFO: Pod "pod-configmaps-a4ca6704-9d6f-4610-ae63-09cebf92a676": Phase="Running", Reason="", readiness=false. Elapsed: 2.03523113s
Mar  2 13:21:21.198: INFO: Pod "pod-configmaps-a4ca6704-9d6f-4610-ae63-09cebf92a676" satisfied condition "running"
STEP: Waiting for pod with text data 03/02/23 13:21:21.198
STEP: Waiting for pod with binary data 03/02/23 13:21:21.308
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:21:21.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3882" for this suite. 03/02/23 13:21:21.492
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":132,"skipped":2468,"failed":0}
------------------------------
â€¢ [2.440 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:21:19.064
    Mar  2 13:21:19.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:21:19.066
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:19.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:19.139
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-452c17af-5900-4cd2-9283-7b2f96c94cc0 03/02/23 13:21:19.148
    STEP: Creating the pod 03/02/23 13:21:19.152
    Mar  2 13:21:19.162: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4ca6704-9d6f-4610-ae63-09cebf92a676" in namespace "configmap-3882" to be "running"
    Mar  2 13:21:19.168: INFO: Pod "pod-configmaps-a4ca6704-9d6f-4610-ae63-09cebf92a676": Phase="Pending", Reason="", readiness=false. Elapsed: 5.114043ms
    Mar  2 13:21:21.198: INFO: Pod "pod-configmaps-a4ca6704-9d6f-4610-ae63-09cebf92a676": Phase="Running", Reason="", readiness=false. Elapsed: 2.03523113s
    Mar  2 13:21:21.198: INFO: Pod "pod-configmaps-a4ca6704-9d6f-4610-ae63-09cebf92a676" satisfied condition "running"
    STEP: Waiting for pod with text data 03/02/23 13:21:21.198
    STEP: Waiting for pod with binary data 03/02/23 13:21:21.308
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:21:21.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3882" for this suite. 03/02/23 13:21:21.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:21:21.512
Mar  2 13:21:21.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename runtimeclass 03/02/23 13:21:21.514
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:21.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:21.534
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  2 13:21:21.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-472" for this suite. 03/02/23 13:21:21.548
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":133,"skipped":2474,"failed":0}
------------------------------
â€¢ [0.041 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:21:21.512
    Mar  2 13:21:21.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename runtimeclass 03/02/23 13:21:21.514
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:21.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:21.534
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  2 13:21:21.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-472" for this suite. 03/02/23 13:21:21.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:21:21.557
Mar  2 13:21:21.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:21:21.558
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:21.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:21.579
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-f993b1e9-5310-43ba-a815-241edee89e6a 03/02/23 13:21:21.586
STEP: Creating a pod to test consume configMaps 03/02/23 13:21:21.589
Mar  2 13:21:21.596: INFO: Waiting up to 5m0s for pod "pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084" in namespace "configmap-1065" to be "Succeeded or Failed"
Mar  2 13:21:21.602: INFO: Pod "pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084": Phase="Pending", Reason="", readiness=false. Elapsed: 5.79517ms
Mar  2 13:21:23.614: INFO: Pod "pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017914721s
Mar  2 13:21:25.611: INFO: Pod "pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014733364s
STEP: Saw pod success 03/02/23 13:21:25.611
Mar  2 13:21:25.612: INFO: Pod "pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084" satisfied condition "Succeeded or Failed"
Mar  2 13:21:25.619: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084 container configmap-volume-test: <nil>
STEP: delete the pod 03/02/23 13:21:25.627
Mar  2 13:21:25.641: INFO: Waiting for pod pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084 to disappear
Mar  2 13:21:25.646: INFO: Pod pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:21:25.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1065" for this suite. 03/02/23 13:21:25.651
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":134,"skipped":2482,"failed":0}
------------------------------
â€¢ [4.100 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:21:21.557
    Mar  2 13:21:21.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:21:21.558
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:21.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:21.579
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-f993b1e9-5310-43ba-a815-241edee89e6a 03/02/23 13:21:21.586
    STEP: Creating a pod to test consume configMaps 03/02/23 13:21:21.589
    Mar  2 13:21:21.596: INFO: Waiting up to 5m0s for pod "pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084" in namespace "configmap-1065" to be "Succeeded or Failed"
    Mar  2 13:21:21.602: INFO: Pod "pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084": Phase="Pending", Reason="", readiness=false. Elapsed: 5.79517ms
    Mar  2 13:21:23.614: INFO: Pod "pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017914721s
    Mar  2 13:21:25.611: INFO: Pod "pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014733364s
    STEP: Saw pod success 03/02/23 13:21:25.611
    Mar  2 13:21:25.612: INFO: Pod "pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084" satisfied condition "Succeeded or Failed"
    Mar  2 13:21:25.619: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084 container configmap-volume-test: <nil>
    STEP: delete the pod 03/02/23 13:21:25.627
    Mar  2 13:21:25.641: INFO: Waiting for pod pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084 to disappear
    Mar  2 13:21:25.646: INFO: Pod pod-configmaps-0431fe38-eb7d-4045-9ae5-4865f8928084 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:21:25.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1065" for this suite. 03/02/23 13:21:25.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:21:25.659
Mar  2 13:21:25.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 13:21:25.66
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:25.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:25.687
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-8c4cf1ec-5a0e-43fb-97cd-b91c0904e1e0 03/02/23 13:21:25.692
STEP: Creating a pod to test consume secrets 03/02/23 13:21:25.698
Mar  2 13:21:25.714: INFO: Waiting up to 5m0s for pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53" in namespace "secrets-3595" to be "Succeeded or Failed"
Mar  2 13:21:25.737: INFO: Pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53": Phase="Pending", Reason="", readiness=false. Elapsed: 23.007698ms
Mar  2 13:21:27.743: INFO: Pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53": Phase="Running", Reason="", readiness=true. Elapsed: 2.028386853s
Mar  2 13:21:29.743: INFO: Pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53": Phase="Running", Reason="", readiness=false. Elapsed: 4.028061233s
Mar  2 13:21:31.744: INFO: Pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029668119s
STEP: Saw pod success 03/02/23 13:21:31.744
Mar  2 13:21:31.745: INFO: Pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53" satisfied condition "Succeeded or Failed"
Mar  2 13:21:31.748: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 13:21:31.757
Mar  2 13:21:31.771: INFO: Waiting for pod pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53 to disappear
Mar  2 13:21:31.781: INFO: Pod pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 13:21:31.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3595" for this suite. 03/02/23 13:21:31.787
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":135,"skipped":2489,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.138 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:21:25.659
    Mar  2 13:21:25.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 13:21:25.66
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:25.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:25.687
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-8c4cf1ec-5a0e-43fb-97cd-b91c0904e1e0 03/02/23 13:21:25.692
    STEP: Creating a pod to test consume secrets 03/02/23 13:21:25.698
    Mar  2 13:21:25.714: INFO: Waiting up to 5m0s for pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53" in namespace "secrets-3595" to be "Succeeded or Failed"
    Mar  2 13:21:25.737: INFO: Pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53": Phase="Pending", Reason="", readiness=false. Elapsed: 23.007698ms
    Mar  2 13:21:27.743: INFO: Pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53": Phase="Running", Reason="", readiness=true. Elapsed: 2.028386853s
    Mar  2 13:21:29.743: INFO: Pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53": Phase="Running", Reason="", readiness=false. Elapsed: 4.028061233s
    Mar  2 13:21:31.744: INFO: Pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029668119s
    STEP: Saw pod success 03/02/23 13:21:31.744
    Mar  2 13:21:31.745: INFO: Pod "pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53" satisfied condition "Succeeded or Failed"
    Mar  2 13:21:31.748: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 13:21:31.757
    Mar  2 13:21:31.771: INFO: Waiting for pod pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53 to disappear
    Mar  2 13:21:31.781: INFO: Pod pod-secrets-e17a48fc-4ccc-48eb-b971-e94ab1133d53 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 13:21:31.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3595" for this suite. 03/02/23 13:21:31.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:21:31.801
Mar  2 13:21:31.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:21:31.803
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:31.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:31.829
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-5316 03/02/23 13:21:31.834
STEP: creating service affinity-clusterip-transition in namespace services-5316 03/02/23 13:21:31.834
STEP: creating replication controller affinity-clusterip-transition in namespace services-5316 03/02/23 13:21:31.848
I0302 13:21:31.866978      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5316, replica count: 3
I0302 13:21:34.918507      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 13:21:34.934: INFO: Creating new exec pod
Mar  2 13:21:34.941: INFO: Waiting up to 5m0s for pod "execpod-affinitylt8qc" in namespace "services-5316" to be "running"
Mar  2 13:21:34.945: INFO: Pod "execpod-affinitylt8qc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.737212ms
Mar  2 13:21:36.949: INFO: Pod "execpod-affinitylt8qc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008320095s
Mar  2 13:21:36.950: INFO: Pod "execpod-affinitylt8qc" satisfied condition "running"
Mar  2 13:21:37.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5316 exec execpod-affinitylt8qc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar  2 13:21:38.150: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar  2 13:21:38.150: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:21:38.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5316 exec execpod-affinitylt8qc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.36.247 80'
Mar  2 13:21:38.338: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.36.247 80\nConnection to 10.233.36.247 80 port [tcp/http] succeeded!\n"
Mar  2 13:21:38.338: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:21:38.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5316 exec execpod-affinitylt8qc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.36.247:80/ ; done'
Mar  2 13:21:38.750: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n"
Mar  2 13:21:38.750: INFO: stdout: "\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-qkbg8\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-qkbg8\naffinity-clusterip-transition-qkbg8\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-qkbg8"
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-qkbg8
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-qkbg8
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-qkbg8
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-qkbg8
Mar  2 13:21:38.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5316 exec execpod-affinitylt8qc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.36.247:80/ ; done'
Mar  2 13:21:39.108: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n"
Mar  2 13:21:39.108: INFO: stdout: "\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7"
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
Mar  2 13:21:39.108: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5316, will wait for the garbage collector to delete the pods 03/02/23 13:21:39.142
Mar  2 13:21:39.220: INFO: Deleting ReplicationController affinity-clusterip-transition took: 22.867963ms
Mar  2 13:21:39.321: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.008688ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:21:41.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5316" for this suite. 03/02/23 13:21:41.947
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":136,"skipped":2503,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.156 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:21:31.801
    Mar  2 13:21:31.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:21:31.803
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:31.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:31.829
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-5316 03/02/23 13:21:31.834
    STEP: creating service affinity-clusterip-transition in namespace services-5316 03/02/23 13:21:31.834
    STEP: creating replication controller affinity-clusterip-transition in namespace services-5316 03/02/23 13:21:31.848
    I0302 13:21:31.866978      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5316, replica count: 3
    I0302 13:21:34.918507      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 13:21:34.934: INFO: Creating new exec pod
    Mar  2 13:21:34.941: INFO: Waiting up to 5m0s for pod "execpod-affinitylt8qc" in namespace "services-5316" to be "running"
    Mar  2 13:21:34.945: INFO: Pod "execpod-affinitylt8qc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.737212ms
    Mar  2 13:21:36.949: INFO: Pod "execpod-affinitylt8qc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008320095s
    Mar  2 13:21:36.950: INFO: Pod "execpod-affinitylt8qc" satisfied condition "running"
    Mar  2 13:21:37.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5316 exec execpod-affinitylt8qc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Mar  2 13:21:38.150: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar  2 13:21:38.150: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:21:38.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5316 exec execpod-affinitylt8qc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.36.247 80'
    Mar  2 13:21:38.338: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.36.247 80\nConnection to 10.233.36.247 80 port [tcp/http] succeeded!\n"
    Mar  2 13:21:38.338: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:21:38.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5316 exec execpod-affinitylt8qc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.36.247:80/ ; done'
    Mar  2 13:21:38.750: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n"
    Mar  2 13:21:38.750: INFO: stdout: "\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-qkbg8\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-qkbg8\naffinity-clusterip-transition-qkbg8\naffinity-clusterip-transition-t96dr\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-qkbg8"
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-qkbg8
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-qkbg8
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-qkbg8
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-t96dr
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:38.750: INFO: Received response from host: affinity-clusterip-transition-qkbg8
    Mar  2 13:21:38.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5316 exec execpod-affinitylt8qc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.36.247:80/ ; done'
    Mar  2 13:21:39.108: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.247:80/\n"
    Mar  2 13:21:39.108: INFO: stdout: "\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7\naffinity-clusterip-transition-ts5m7"
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Received response from host: affinity-clusterip-transition-ts5m7
    Mar  2 13:21:39.108: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5316, will wait for the garbage collector to delete the pods 03/02/23 13:21:39.142
    Mar  2 13:21:39.220: INFO: Deleting ReplicationController affinity-clusterip-transition took: 22.867963ms
    Mar  2 13:21:39.321: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.008688ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:21:41.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5316" for this suite. 03/02/23 13:21:41.947
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:21:41.969
Mar  2 13:21:41.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 13:21:41.971
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:41.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:41.997
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 13:21:42.016
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:21:43.246
STEP: Deploying the webhook pod 03/02/23 13:21:43.257
STEP: Wait for the deployment to be ready 03/02/23 13:21:43.278
Mar  2 13:21:43.289: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/02/23 13:21:45.309
STEP: Verifying the service has paired with the endpoint 03/02/23 13:21:45.321
Mar  2 13:21:46.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Mar  2 13:21:46.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/02/23 13:21:51.843
STEP: Creating a custom resource that should be denied by the webhook 03/02/23 13:21:51.883
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/02/23 13:21:53.944
STEP: Updating the custom resource with disallowed data should be denied 03/02/23 13:21:53.958
STEP: Deleting the custom resource should be denied 03/02/23 13:21:53.969
STEP: Remove the offending key and value from the custom resource data 03/02/23 13:21:53.973
STEP: Deleting the updated custom resource should be successful 03/02/23 13:21:53.982
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:21:54.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7613" for this suite. 03/02/23 13:21:54.54
STEP: Destroying namespace "webhook-7613-markers" for this suite. 03/02/23 13:21:54.546
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":137,"skipped":2520,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.665 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:21:41.969
    Mar  2 13:21:41.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 13:21:41.971
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:41.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:41.997
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 13:21:42.016
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:21:43.246
    STEP: Deploying the webhook pod 03/02/23 13:21:43.257
    STEP: Wait for the deployment to be ready 03/02/23 13:21:43.278
    Mar  2 13:21:43.289: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/02/23 13:21:45.309
    STEP: Verifying the service has paired with the endpoint 03/02/23 13:21:45.321
    Mar  2 13:21:46.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Mar  2 13:21:46.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/02/23 13:21:51.843
    STEP: Creating a custom resource that should be denied by the webhook 03/02/23 13:21:51.883
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/02/23 13:21:53.944
    STEP: Updating the custom resource with disallowed data should be denied 03/02/23 13:21:53.958
    STEP: Deleting the custom resource should be denied 03/02/23 13:21:53.969
    STEP: Remove the offending key and value from the custom resource data 03/02/23 13:21:53.973
    STEP: Deleting the updated custom resource should be successful 03/02/23 13:21:53.982
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:21:54.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7613" for this suite. 03/02/23 13:21:54.54
    STEP: Destroying namespace "webhook-7613-markers" for this suite. 03/02/23 13:21:54.546
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:21:54.634
Mar  2 13:21:54.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 13:21:54.635
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:54.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:54.742
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/02/23 13:21:54.745
Mar  2 13:21:54.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/02/23 13:22:18.123
Mar  2 13:22:18.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:22:27.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:22:50.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5324" for this suite. 03/02/23 13:22:50.544
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":138,"skipped":2523,"failed":0}
------------------------------
â€¢ [SLOW TEST] [55.914 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:21:54.634
    Mar  2 13:21:54.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 13:21:54.635
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:21:54.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:21:54.742
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/02/23 13:21:54.745
    Mar  2 13:21:54.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/02/23 13:22:18.123
    Mar  2 13:22:18.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:22:27.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:22:50.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5324" for this suite. 03/02/23 13:22:50.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:22:50.552
Mar  2 13:22:50.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename var-expansion 03/02/23 13:22:50.553
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:22:50.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:22:50.576
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 03/02/23 13:22:50.578
STEP: waiting for pod running 03/02/23 13:22:50.586
Mar  2 13:22:50.587: INFO: Waiting up to 2m0s for pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" in namespace "var-expansion-4140" to be "running"
Mar  2 13:22:50.589: INFO: Pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489597ms
Mar  2 13:22:52.594: INFO: Pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80": Phase="Running", Reason="", readiness=true. Elapsed: 2.007548709s
Mar  2 13:22:52.595: INFO: Pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" satisfied condition "running"
STEP: creating a file in subpath 03/02/23 13:22:52.595
Mar  2 13:22:52.599: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4140 PodName:var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:22:52.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:22:52.601: INFO: ExecWithOptions: Clientset creation
Mar  2 13:22:52.601: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-4140/pods/var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/02/23 13:22:52.687
Mar  2 13:22:52.691: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4140 PodName:var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:22:52.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:22:52.692: INFO: ExecWithOptions: Clientset creation
Mar  2 13:22:52.692: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-4140/pods/var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/02/23 13:22:52.786
Mar  2 13:22:53.311: INFO: Successfully updated pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80"
STEP: waiting for annotated pod running 03/02/23 13:22:53.311
Mar  2 13:22:53.311: INFO: Waiting up to 2m0s for pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" in namespace "var-expansion-4140" to be "running"
Mar  2 13:22:53.326: INFO: Pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80": Phase="Running", Reason="", readiness=true. Elapsed: 14.6756ms
Mar  2 13:22:53.326: INFO: Pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" satisfied condition "running"
STEP: deleting the pod gracefully 03/02/23 13:22:53.326
Mar  2 13:22:53.326: INFO: Deleting pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" in namespace "var-expansion-4140"
Mar  2 13:22:53.336: INFO: Wait up to 5m0s for pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 13:23:27.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4140" for this suite. 03/02/23 13:23:27.357
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":139,"skipped":2545,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.815 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:22:50.552
    Mar  2 13:22:50.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename var-expansion 03/02/23 13:22:50.553
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:22:50.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:22:50.576
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 03/02/23 13:22:50.578
    STEP: waiting for pod running 03/02/23 13:22:50.586
    Mar  2 13:22:50.587: INFO: Waiting up to 2m0s for pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" in namespace "var-expansion-4140" to be "running"
    Mar  2 13:22:50.589: INFO: Pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489597ms
    Mar  2 13:22:52.594: INFO: Pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80": Phase="Running", Reason="", readiness=true. Elapsed: 2.007548709s
    Mar  2 13:22:52.595: INFO: Pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" satisfied condition "running"
    STEP: creating a file in subpath 03/02/23 13:22:52.595
    Mar  2 13:22:52.599: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4140 PodName:var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:22:52.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:22:52.601: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:22:52.601: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-4140/pods/var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/02/23 13:22:52.687
    Mar  2 13:22:52.691: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4140 PodName:var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:22:52.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:22:52.692: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:22:52.692: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-4140/pods/var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/02/23 13:22:52.786
    Mar  2 13:22:53.311: INFO: Successfully updated pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80"
    STEP: waiting for annotated pod running 03/02/23 13:22:53.311
    Mar  2 13:22:53.311: INFO: Waiting up to 2m0s for pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" in namespace "var-expansion-4140" to be "running"
    Mar  2 13:22:53.326: INFO: Pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80": Phase="Running", Reason="", readiness=true. Elapsed: 14.6756ms
    Mar  2 13:22:53.326: INFO: Pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" satisfied condition "running"
    STEP: deleting the pod gracefully 03/02/23 13:22:53.326
    Mar  2 13:22:53.326: INFO: Deleting pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" in namespace "var-expansion-4140"
    Mar  2 13:22:53.336: INFO: Wait up to 5m0s for pod "var-expansion-7a5aa42f-3710-4623-8fb8-1d256689fa80" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 13:23:27.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4140" for this suite. 03/02/23 13:23:27.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:23:27.375
Mar  2 13:23:27.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 13:23:27.376
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:23:27.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:23:27.425
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-e204c602-382f-44c8-a95b-87b00636889f 03/02/23 13:23:27.431
STEP: Creating a pod to test consume secrets 03/02/23 13:23:27.437
Mar  2 13:23:27.451: INFO: Waiting up to 5m0s for pod "pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6" in namespace "secrets-1851" to be "Succeeded or Failed"
Mar  2 13:23:27.462: INFO: Pod "pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.601782ms
Mar  2 13:23:29.468: INFO: Pod "pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016728923s
Mar  2 13:23:31.478: INFO: Pod "pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027165861s
STEP: Saw pod success 03/02/23 13:23:31.479
Mar  2 13:23:31.480: INFO: Pod "pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6" satisfied condition "Succeeded or Failed"
Mar  2 13:23:31.486: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 13:23:31.5
Mar  2 13:23:31.516: INFO: Waiting for pod pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6 to disappear
Mar  2 13:23:31.519: INFO: Pod pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 13:23:31.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1851" for this suite. 03/02/23 13:23:31.525
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":140,"skipped":2564,"failed":0}
------------------------------
â€¢ [4.155 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:23:27.375
    Mar  2 13:23:27.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 13:23:27.376
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:23:27.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:23:27.425
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-e204c602-382f-44c8-a95b-87b00636889f 03/02/23 13:23:27.431
    STEP: Creating a pod to test consume secrets 03/02/23 13:23:27.437
    Mar  2 13:23:27.451: INFO: Waiting up to 5m0s for pod "pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6" in namespace "secrets-1851" to be "Succeeded or Failed"
    Mar  2 13:23:27.462: INFO: Pod "pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.601782ms
    Mar  2 13:23:29.468: INFO: Pod "pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016728923s
    Mar  2 13:23:31.478: INFO: Pod "pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027165861s
    STEP: Saw pod success 03/02/23 13:23:31.479
    Mar  2 13:23:31.480: INFO: Pod "pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6" satisfied condition "Succeeded or Failed"
    Mar  2 13:23:31.486: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 13:23:31.5
    Mar  2 13:23:31.516: INFO: Waiting for pod pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6 to disappear
    Mar  2 13:23:31.519: INFO: Pod pod-secrets-7a4438eb-5a2f-41f9-896e-37a9f40c5ba6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 13:23:31.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1851" for this suite. 03/02/23 13:23:31.525
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:23:31.533
Mar  2 13:23:31.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename svcaccounts 03/02/23 13:23:31.535
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:23:31.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:23:31.551
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Mar  2 13:23:31.568: INFO: created pod pod-service-account-defaultsa
Mar  2 13:23:31.568: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  2 13:23:31.580: INFO: created pod pod-service-account-mountsa
Mar  2 13:23:31.580: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  2 13:23:31.597: INFO: created pod pod-service-account-nomountsa
Mar  2 13:23:31.598: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  2 13:23:31.617: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  2 13:23:31.618: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  2 13:23:31.625: INFO: created pod pod-service-account-mountsa-mountspec
Mar  2 13:23:31.625: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  2 13:23:31.633: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  2 13:23:31.633: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  2 13:23:31.652: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  2 13:23:31.652: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  2 13:23:31.662: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  2 13:23:31.662: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  2 13:23:31.670: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  2 13:23:31.670: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 13:23:31.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-936" for this suite. 03/02/23 13:23:31.678
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":141,"skipped":2564,"failed":0}
------------------------------
â€¢ [0.156 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:23:31.533
    Mar  2 13:23:31.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 13:23:31.535
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:23:31.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:23:31.551
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Mar  2 13:23:31.568: INFO: created pod pod-service-account-defaultsa
    Mar  2 13:23:31.568: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar  2 13:23:31.580: INFO: created pod pod-service-account-mountsa
    Mar  2 13:23:31.580: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar  2 13:23:31.597: INFO: created pod pod-service-account-nomountsa
    Mar  2 13:23:31.598: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar  2 13:23:31.617: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar  2 13:23:31.618: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar  2 13:23:31.625: INFO: created pod pod-service-account-mountsa-mountspec
    Mar  2 13:23:31.625: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar  2 13:23:31.633: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar  2 13:23:31.633: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar  2 13:23:31.652: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar  2 13:23:31.652: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar  2 13:23:31.662: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar  2 13:23:31.662: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar  2 13:23:31.670: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar  2 13:23:31.670: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 13:23:31.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-936" for this suite. 03/02/23 13:23:31.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:23:31.715
Mar  2 13:23:31.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename statefulset 03/02/23 13:23:31.716
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:23:31.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:23:31.739
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8477 03/02/23 13:23:31.743
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 03/02/23 13:23:31.75
STEP: Creating pod with conflicting port in namespace statefulset-8477 03/02/23 13:23:31.758
STEP: Waiting until pod test-pod will start running in namespace statefulset-8477 03/02/23 13:23:31.764
Mar  2 13:23:31.764: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8477" to be "running"
Mar  2 13:23:31.767: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.795027ms
Mar  2 13:23:33.774: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010733381s
Mar  2 13:23:35.775: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011389924s
Mar  2 13:23:37.773: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009367692s
Mar  2 13:23:39.774: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.009958128s
Mar  2 13:23:39.774: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-8477 03/02/23 13:23:39.775
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8477 03/02/23 13:23:39.785
Mar  2 13:23:39.802: INFO: Observed stateful pod in namespace: statefulset-8477, name: ss-0, uid: 168c8b1f-c532-4ab8-ab7e-9c701aba1f9e, status phase: Pending. Waiting for statefulset controller to delete.
Mar  2 13:23:44.005: INFO: Observed stateful pod in namespace: statefulset-8477, name: ss-0, uid: 168c8b1f-c532-4ab8-ab7e-9c701aba1f9e, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 13:23:44.021: INFO: Observed stateful pod in namespace: statefulset-8477, name: ss-0, uid: 168c8b1f-c532-4ab8-ab7e-9c701aba1f9e, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 13:23:44.026: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8477
STEP: Removing pod with conflicting port in namespace statefulset-8477 03/02/23 13:23:44.026
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8477 and will be in running state 03/02/23 13:23:44.06
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 13:23:48.088: INFO: Deleting all statefulset in ns statefulset-8477
Mar  2 13:23:48.095: INFO: Scaling statefulset ss to 0
Mar  2 13:23:58.122: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 13:23:58.127: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 13:23:58.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8477" for this suite. 03/02/23 13:23:58.178
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":142,"skipped":2615,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.470 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:23:31.715
    Mar  2 13:23:31.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename statefulset 03/02/23 13:23:31.716
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:23:31.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:23:31.739
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8477 03/02/23 13:23:31.743
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 03/02/23 13:23:31.75
    STEP: Creating pod with conflicting port in namespace statefulset-8477 03/02/23 13:23:31.758
    STEP: Waiting until pod test-pod will start running in namespace statefulset-8477 03/02/23 13:23:31.764
    Mar  2 13:23:31.764: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8477" to be "running"
    Mar  2 13:23:31.767: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.795027ms
    Mar  2 13:23:33.774: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010733381s
    Mar  2 13:23:35.775: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011389924s
    Mar  2 13:23:37.773: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009367692s
    Mar  2 13:23:39.774: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.009958128s
    Mar  2 13:23:39.774: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-8477 03/02/23 13:23:39.775
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8477 03/02/23 13:23:39.785
    Mar  2 13:23:39.802: INFO: Observed stateful pod in namespace: statefulset-8477, name: ss-0, uid: 168c8b1f-c532-4ab8-ab7e-9c701aba1f9e, status phase: Pending. Waiting for statefulset controller to delete.
    Mar  2 13:23:44.005: INFO: Observed stateful pod in namespace: statefulset-8477, name: ss-0, uid: 168c8b1f-c532-4ab8-ab7e-9c701aba1f9e, status phase: Failed. Waiting for statefulset controller to delete.
    Mar  2 13:23:44.021: INFO: Observed stateful pod in namespace: statefulset-8477, name: ss-0, uid: 168c8b1f-c532-4ab8-ab7e-9c701aba1f9e, status phase: Failed. Waiting for statefulset controller to delete.
    Mar  2 13:23:44.026: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8477
    STEP: Removing pod with conflicting port in namespace statefulset-8477 03/02/23 13:23:44.026
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8477 and will be in running state 03/02/23 13:23:44.06
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 13:23:48.088: INFO: Deleting all statefulset in ns statefulset-8477
    Mar  2 13:23:48.095: INFO: Scaling statefulset ss to 0
    Mar  2 13:23:58.122: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 13:23:58.127: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 13:23:58.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8477" for this suite. 03/02/23 13:23:58.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:23:58.188
Mar  2 13:23:58.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename dns 03/02/23 13:23:58.191
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:23:58.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:23:58.212
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/02/23 13:23:58.216
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 217.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.217_udp@PTR;check="$$(dig +tcp +noall +answer +search 217.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.217_tcp@PTR;sleep 1; done
 03/02/23 13:23:58.238
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 217.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.217_udp@PTR;check="$$(dig +tcp +noall +answer +search 217.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.217_tcp@PTR;sleep 1; done
 03/02/23 13:23:58.238
STEP: creating a pod to probe DNS 03/02/23 13:23:58.238
STEP: submitting the pod to kubernetes 03/02/23 13:23:58.238
Mar  2 13:23:58.252: INFO: Waiting up to 15m0s for pod "dns-test-69c6828b-a116-4fda-8701-26d8f791f730" in namespace "dns-8100" to be "running"
Mar  2 13:23:58.258: INFO: Pod "dns-test-69c6828b-a116-4fda-8701-26d8f791f730": Phase="Pending", Reason="", readiness=false. Elapsed: 5.493432ms
Mar  2 13:24:00.266: INFO: Pod "dns-test-69c6828b-a116-4fda-8701-26d8f791f730": Phase="Running", Reason="", readiness=true. Elapsed: 2.013648968s
Mar  2 13:24:00.266: INFO: Pod "dns-test-69c6828b-a116-4fda-8701-26d8f791f730" satisfied condition "running"
STEP: retrieving the pod 03/02/23 13:24:00.266
STEP: looking for the results for each expected name from probers 03/02/23 13:24:00.271
Mar  2 13:24:00.276: INFO: Unable to read wheezy_udp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:00.280: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:00.284: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:00.287: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:00.313: INFO: Unable to read jessie_udp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:00.317: INFO: Unable to read jessie_tcp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:00.322: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:00.325: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:00.338: INFO: Lookups using dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730 failed for: [wheezy_udp@dns-test-service.dns-8100.svc.cluster.local wheezy_tcp@dns-test-service.dns-8100.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local jessie_udp@dns-test-service.dns-8100.svc.cluster.local jessie_tcp@dns-test-service.dns-8100.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local]

Mar  2 13:24:05.350: INFO: Unable to read wheezy_udp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:05.356: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:05.362: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:05.366: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
Mar  2 13:24:05.416: INFO: Lookups using dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730 failed for: [wheezy_udp@dns-test-service.dns-8100.svc.cluster.local wheezy_tcp@dns-test-service.dns-8100.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local]

Mar  2 13:24:10.448: INFO: DNS probes using dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730 succeeded

STEP: deleting the pod 03/02/23 13:24:10.449
STEP: deleting the test service 03/02/23 13:24:10.519
STEP: deleting the test headless service 03/02/23 13:24:10.579
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 13:24:10.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8100" for this suite. 03/02/23 13:24:10.626
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":143,"skipped":2623,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.450 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:23:58.188
    Mar  2 13:23:58.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename dns 03/02/23 13:23:58.191
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:23:58.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:23:58.212
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/02/23 13:23:58.216
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 217.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.217_udp@PTR;check="$$(dig +tcp +noall +answer +search 217.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.217_tcp@PTR;sleep 1; done
     03/02/23 13:23:58.238
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 217.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.217_udp@PTR;check="$$(dig +tcp +noall +answer +search 217.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.217_tcp@PTR;sleep 1; done
     03/02/23 13:23:58.238
    STEP: creating a pod to probe DNS 03/02/23 13:23:58.238
    STEP: submitting the pod to kubernetes 03/02/23 13:23:58.238
    Mar  2 13:23:58.252: INFO: Waiting up to 15m0s for pod "dns-test-69c6828b-a116-4fda-8701-26d8f791f730" in namespace "dns-8100" to be "running"
    Mar  2 13:23:58.258: INFO: Pod "dns-test-69c6828b-a116-4fda-8701-26d8f791f730": Phase="Pending", Reason="", readiness=false. Elapsed: 5.493432ms
    Mar  2 13:24:00.266: INFO: Pod "dns-test-69c6828b-a116-4fda-8701-26d8f791f730": Phase="Running", Reason="", readiness=true. Elapsed: 2.013648968s
    Mar  2 13:24:00.266: INFO: Pod "dns-test-69c6828b-a116-4fda-8701-26d8f791f730" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 13:24:00.266
    STEP: looking for the results for each expected name from probers 03/02/23 13:24:00.271
    Mar  2 13:24:00.276: INFO: Unable to read wheezy_udp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:00.280: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:00.284: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:00.287: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:00.313: INFO: Unable to read jessie_udp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:00.317: INFO: Unable to read jessie_tcp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:00.322: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:00.325: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:00.338: INFO: Lookups using dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730 failed for: [wheezy_udp@dns-test-service.dns-8100.svc.cluster.local wheezy_tcp@dns-test-service.dns-8100.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local jessie_udp@dns-test-service.dns-8100.svc.cluster.local jessie_tcp@dns-test-service.dns-8100.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local]

    Mar  2 13:24:05.350: INFO: Unable to read wheezy_udp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:05.356: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:05.362: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:05.366: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local from pod dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730: the server could not find the requested resource (get pods dns-test-69c6828b-a116-4fda-8701-26d8f791f730)
    Mar  2 13:24:05.416: INFO: Lookups using dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730 failed for: [wheezy_udp@dns-test-service.dns-8100.svc.cluster.local wheezy_tcp@dns-test-service.dns-8100.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8100.svc.cluster.local]

    Mar  2 13:24:10.448: INFO: DNS probes using dns-8100/dns-test-69c6828b-a116-4fda-8701-26d8f791f730 succeeded

    STEP: deleting the pod 03/02/23 13:24:10.449
    STEP: deleting the test service 03/02/23 13:24:10.519
    STEP: deleting the test headless service 03/02/23 13:24:10.579
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 13:24:10.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8100" for this suite. 03/02/23 13:24:10.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:24:10.64
Mar  2 13:24:10.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename daemonsets 03/02/23 13:24:10.645
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:24:10.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:24:10.685
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Mar  2 13:24:10.730: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/02/23 13:24:10.739
Mar  2 13:24:10.746: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:24:10.746: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/02/23 13:24:10.746
Mar  2 13:24:10.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:24:10.807: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
Mar  2 13:24:11.819: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:24:11.819: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
Mar  2 13:24:12.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 13:24:12.813: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/02/23 13:24:12.82
Mar  2 13:24:12.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:24:12.858: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/02/23 13:24:12.858
Mar  2 13:24:12.879: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:24:12.879: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
Mar  2 13:24:13.893: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:24:13.893: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
Mar  2 13:24:14.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:24:14.888: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
Mar  2 13:24:15.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:24:15.890: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
Mar  2 13:24:16.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:24:16.886: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
Mar  2 13:24:17.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 13:24:17.886: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 13:24:17.892
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5801, will wait for the garbage collector to delete the pods 03/02/23 13:24:17.893
Mar  2 13:24:17.959: INFO: Deleting DaemonSet.extensions daemon-set took: 12.881156ms
Mar  2 13:24:18.060: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.475225ms
Mar  2 13:24:20.867: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:24:20.867: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 13:24:20.876: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1945289"},"items":null}

Mar  2 13:24:20.880: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1945289"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 13:24:20.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5801" for this suite. 03/02/23 13:24:20.922
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":144,"skipped":2631,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.294 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:24:10.64
    Mar  2 13:24:10.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename daemonsets 03/02/23 13:24:10.645
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:24:10.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:24:10.685
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Mar  2 13:24:10.730: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/02/23 13:24:10.739
    Mar  2 13:24:10.746: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:24:10.746: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/02/23 13:24:10.746
    Mar  2 13:24:10.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:24:10.807: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
    Mar  2 13:24:11.819: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:24:11.819: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
    Mar  2 13:24:12.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  2 13:24:12.813: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/02/23 13:24:12.82
    Mar  2 13:24:12.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:24:12.858: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/02/23 13:24:12.858
    Mar  2 13:24:12.879: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:24:12.879: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
    Mar  2 13:24:13.893: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:24:13.893: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
    Mar  2 13:24:14.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:24:14.888: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
    Mar  2 13:24:15.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:24:15.890: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
    Mar  2 13:24:16.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:24:16.886: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
    Mar  2 13:24:17.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  2 13:24:17.886: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 13:24:17.892
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5801, will wait for the garbage collector to delete the pods 03/02/23 13:24:17.893
    Mar  2 13:24:17.959: INFO: Deleting DaemonSet.extensions daemon-set took: 12.881156ms
    Mar  2 13:24:18.060: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.475225ms
    Mar  2 13:24:20.867: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:24:20.867: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 13:24:20.876: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1945289"},"items":null}

    Mar  2 13:24:20.880: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1945289"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 13:24:20.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5801" for this suite. 03/02/23 13:24:20.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:24:20.941
Mar  2 13:24:20.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:24:20.942
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:24:20.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:24:20.973
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-974 03/02/23 13:24:20.981
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-974 to expose endpoints map[] 03/02/23 13:24:20.989
Mar  2 13:24:21.001: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar  2 13:24:22.014: INFO: successfully validated that service multi-endpoint-test in namespace services-974 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-974 03/02/23 13:24:22.014
Mar  2 13:24:22.026: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-974" to be "running and ready"
Mar  2 13:24:22.034: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.276893ms
Mar  2 13:24:22.034: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:24:24.051: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024734349s
Mar  2 13:24:24.051: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:24:26.041: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.014851144s
Mar  2 13:24:26.041: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  2 13:24:26.041: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-974 to expose endpoints map[pod1:[100]] 03/02/23 13:24:26.054
Mar  2 13:24:26.069: INFO: successfully validated that service multi-endpoint-test in namespace services-974 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-974 03/02/23 13:24:26.069
Mar  2 13:24:26.074: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-974" to be "running and ready"
Mar  2 13:24:26.085: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.439913ms
Mar  2 13:24:26.085: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:24:28.092: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016898311s
Mar  2 13:24:28.093: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  2 13:24:28.093: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-974 to expose endpoints map[pod1:[100] pod2:[101]] 03/02/23 13:24:28.102
Mar  2 13:24:28.117: INFO: successfully validated that service multi-endpoint-test in namespace services-974 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/02/23 13:24:28.117
Mar  2 13:24:28.117: INFO: Creating new exec pod
Mar  2 13:24:28.124: INFO: Waiting up to 5m0s for pod "execpodvxbgl" in namespace "services-974" to be "running"
Mar  2 13:24:28.132: INFO: Pod "execpodvxbgl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.679167ms
Mar  2 13:24:30.140: INFO: Pod "execpodvxbgl": Phase="Running", Reason="", readiness=true. Elapsed: 2.01491035s
Mar  2 13:24:30.140: INFO: Pod "execpodvxbgl" satisfied condition "running"
Mar  2 13:24:31.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-974 exec execpodvxbgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Mar  2 13:24:31.338: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar  2 13:24:31.338: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:24:31.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-974 exec execpodvxbgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.51 80'
Mar  2 13:24:31.518: INFO: stderr: "+ nc -v -t -w 2 10.233.7.51 80\nConnection to 10.233.7.51 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Mar  2 13:24:31.518: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:24:31.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-974 exec execpodvxbgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Mar  2 13:24:31.684: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar  2 13:24:31.684: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:24:31.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-974 exec execpodvxbgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.51 81'
Mar  2 13:24:31.855: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.7.51 81\nConnection to 10.233.7.51 81 port [tcp/*] succeeded!\n"
Mar  2 13:24:31.855: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-974 03/02/23 13:24:31.855
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-974 to expose endpoints map[pod2:[101]] 03/02/23 13:24:31.9
Mar  2 13:24:31.935: INFO: successfully validated that service multi-endpoint-test in namespace services-974 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-974 03/02/23 13:24:31.935
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-974 to expose endpoints map[] 03/02/23 13:24:31.957
Mar  2 13:24:31.966: INFO: successfully validated that service multi-endpoint-test in namespace services-974 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:24:31.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-974" for this suite. 03/02/23 13:24:32.006
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":145,"skipped":2663,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.075 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:24:20.941
    Mar  2 13:24:20.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:24:20.942
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:24:20.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:24:20.973
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-974 03/02/23 13:24:20.981
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-974 to expose endpoints map[] 03/02/23 13:24:20.989
    Mar  2 13:24:21.001: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Mar  2 13:24:22.014: INFO: successfully validated that service multi-endpoint-test in namespace services-974 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-974 03/02/23 13:24:22.014
    Mar  2 13:24:22.026: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-974" to be "running and ready"
    Mar  2 13:24:22.034: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.276893ms
    Mar  2 13:24:22.034: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:24:24.051: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024734349s
    Mar  2 13:24:24.051: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:24:26.041: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.014851144s
    Mar  2 13:24:26.041: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  2 13:24:26.041: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-974 to expose endpoints map[pod1:[100]] 03/02/23 13:24:26.054
    Mar  2 13:24:26.069: INFO: successfully validated that service multi-endpoint-test in namespace services-974 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-974 03/02/23 13:24:26.069
    Mar  2 13:24:26.074: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-974" to be "running and ready"
    Mar  2 13:24:26.085: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.439913ms
    Mar  2 13:24:26.085: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:24:28.092: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016898311s
    Mar  2 13:24:28.093: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  2 13:24:28.093: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-974 to expose endpoints map[pod1:[100] pod2:[101]] 03/02/23 13:24:28.102
    Mar  2 13:24:28.117: INFO: successfully validated that service multi-endpoint-test in namespace services-974 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/02/23 13:24:28.117
    Mar  2 13:24:28.117: INFO: Creating new exec pod
    Mar  2 13:24:28.124: INFO: Waiting up to 5m0s for pod "execpodvxbgl" in namespace "services-974" to be "running"
    Mar  2 13:24:28.132: INFO: Pod "execpodvxbgl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.679167ms
    Mar  2 13:24:30.140: INFO: Pod "execpodvxbgl": Phase="Running", Reason="", readiness=true. Elapsed: 2.01491035s
    Mar  2 13:24:30.140: INFO: Pod "execpodvxbgl" satisfied condition "running"
    Mar  2 13:24:31.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-974 exec execpodvxbgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Mar  2 13:24:31.338: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar  2 13:24:31.338: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:24:31.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-974 exec execpodvxbgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.51 80'
    Mar  2 13:24:31.518: INFO: stderr: "+ nc -v -t -w 2 10.233.7.51 80\nConnection to 10.233.7.51 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Mar  2 13:24:31.518: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:24:31.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-974 exec execpodvxbgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Mar  2 13:24:31.684: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar  2 13:24:31.684: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:24:31.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-974 exec execpodvxbgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.51 81'
    Mar  2 13:24:31.855: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.7.51 81\nConnection to 10.233.7.51 81 port [tcp/*] succeeded!\n"
    Mar  2 13:24:31.855: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-974 03/02/23 13:24:31.855
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-974 to expose endpoints map[pod2:[101]] 03/02/23 13:24:31.9
    Mar  2 13:24:31.935: INFO: successfully validated that service multi-endpoint-test in namespace services-974 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-974 03/02/23 13:24:31.935
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-974 to expose endpoints map[] 03/02/23 13:24:31.957
    Mar  2 13:24:31.966: INFO: successfully validated that service multi-endpoint-test in namespace services-974 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:24:31.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-974" for this suite. 03/02/23 13:24:32.006
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:24:32.017
Mar  2 13:24:32.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sched-pred 03/02/23 13:24:32.027
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:24:32.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:24:32.052
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 13:24:32.057: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 13:24:32.069: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 13:24:32.124: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv0 before test
Mar  2 13:24:32.157: INFO: falco-exporter-r2zfx from falco started at 2023-02-27 15:51:02 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.157: INFO: 	Container falco-exporter ready: true, restart count 4
Mar  2 13:24:32.157: INFO: falco-falcosidekick-5d5c7d4db-r6952 from falco started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.157: INFO: 	Container falcosidekick ready: true, restart count 0
Mar  2 13:24:32.157: INFO: falco-trjnh from falco started at 2023-02-27 15:53:31 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.157: INFO: 	Container falco ready: true, restart count 2
Mar  2 13:24:32.157: INFO: fluentd-forwarder-qxbtj from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.157: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 13:24:32.157: INFO: ingress-nginx-controller-qprg2 from ingress-nginx started at 2023-02-27 13:57:01 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.157: INFO: 	Container controller ready: true, restart count 2
Mar  2 13:24:32.157: INFO: calico-accountant-k6t4p from kube-system started at 2023-02-27 13:50:57 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.157: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 13:24:32.157: INFO: calico-node-dz84l from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.157: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 13:24:32.157: INFO: coredns-588bb58b94-k6j76 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.157: INFO: 	Container coredns ready: true, restart count 0
Mar  2 13:24:32.157: INFO: csi-cinder-nodeplugin-9nct9 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
Mar  2 13:24:32.157: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
Mar  2 13:24:32.157: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 13:24:32.157: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 13:24:32.158: INFO: kube-proxy-7bm9z from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.158: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 13:24:32.158: INFO: nginx-proxy-aarnq-sc-k8s-node-srv0 from kube-system started at 2023-02-27 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.158: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 13:24:32.158: INFO: node-local-dns-dk8hd from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.158: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 13:24:32.158: INFO: snapshot-controller-7d445c66c9-6w4w5 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.158: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 13:24:32.158: INFO: kured-hdkrw from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.158: INFO: 	Container kured ready: true, restart count 3
Mar  2 13:24:32.158: INFO: alertmanager-kube-prometheus-stack-alertmanager-0 from monitoring started at 2023-03-01 07:32:10 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 13:24:32.159: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 13:24:32.159: INFO: kube-prometheus-stack-grafana-84f79f467b-sr7kl from monitoring started at 2023-02-28 08:03:49 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container grafana ready: true, restart count 0
Mar  2 13:24:32.159: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar  2 13:24:32.159: INFO: kube-prometheus-stack-prometheus-node-exporter-nl9pw from monitoring started at 2023-02-27 13:49:55 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 13:24:32.159: INFO: prometheus-blackbox-exporter-677b579798-7xm9d from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar  2 13:24:32.159: INFO: s3-exporter-867c5b9457-lfcsf from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container s3-exporter ready: true, restart count 0
Mar  2 13:24:32.159: INFO: opensearch-dashboards-58c8d95f7b-9spcv from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container dashboards ready: true, restart count 0
Mar  2 13:24:32.159: INFO: opensearch-master-2 from opensearch-system started at 2023-02-28 08:03:54 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 13:24:32.159: INFO: prometheus-opensearch-exporter-5688c84dcd-95vjh from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container exporter ready: true, restart count 0
Mar  2 13:24:32.159: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-qv9vz from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 13:24:32.159: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 13:24:32.159: INFO: thanos-query-query-69fc6f554b-db7w4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container query ready: true, restart count 0
Mar  2 13:24:32.159: INFO: thanos-receiver-bucketweb-b4955fcf8-8w2xg from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container bucketweb ready: true, restart count 0
Mar  2 13:24:32.159: INFO: thanos-receiver-compactor-848df7b5d7-z2jh4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container compactor ready: true, restart count 0
Mar  2 13:24:32.159: INFO: thanos-receiver-receive-0 from thanos started at 2023-02-28 08:04:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container receive ready: true, restart count 0
Mar  2 13:24:32.159: INFO: thanos-receiver-receive-distributor-779c5d74d8-7hhmb from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container receive ready: true, restart count 0
Mar  2 13:24:32.159: INFO: thanos-receiver-ruler-0 from thanos started at 2023-02-28 08:03:52 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 13:24:32.159: INFO: 	Container ruler ready: true, restart count 0
Mar  2 13:24:32.159: INFO: restic-k4z4s from velero started at 2023-02-27 13:13:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container restic ready: true, restart count 2
Mar  2 13:24:32.159: INFO: velero-7bbd458dfc-s8n2h from velero started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.159: INFO: 	Container velero ready: true, restart count 0
Mar  2 13:24:32.159: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv1 before test
Mar  2 13:24:32.188: INFO: cert-manager-754b766f8b-fvh5z from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container cert-manager-controller ready: true, restart count 0
Mar  2 13:24:32.188: INFO: cert-manager-webhook-875cdf98f-lfgn7 from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container cert-manager-webhook ready: true, restart count 0
Mar  2 13:24:32.188: INFO: dex-58d8c68494-flfvv from dex started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container dex ready: true, restart count 0
Mar  2 13:24:32.188: INFO: falco-8cz2x from falco started at 2023-02-27 15:54:18 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container falco ready: true, restart count 2
Mar  2 13:24:32.188: INFO: falco-exporter-srbrb from falco started at 2023-02-27 15:51:10 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container falco-exporter ready: true, restart count 4
Mar  2 13:24:32.188: INFO: fluentd-aggregator-0 from fluentd-system started at 2023-02-28 08:16:03 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 13:24:32.188: INFO: fluentd-forwarder-zgcds from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 13:24:32.188: INFO: harbor-chartmuseum-5c9477455d-hp9zb from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container chartmuseum ready: true, restart count 0
Mar  2 13:24:32.188: INFO: harbor-core-58dc955656-2vz5k from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container core ready: true, restart count 1
Mar  2 13:24:32.188: INFO: harbor-database-0 from harbor started at 2023-02-28 08:16:02 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container database ready: true, restart count 0
Mar  2 13:24:32.188: INFO: harbor-jobservice-69c4c778fb-8qt7s from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container jobservice ready: true, restart count 2
Mar  2 13:24:32.188: INFO: harbor-notary-server-6cfdf66b5-sxpqw from harbor started at 2023-02-28 08:15:44 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container notary-server ready: true, restart count 2
Mar  2 13:24:32.188: INFO: harbor-notary-signer-5d6d45f584-rfqm7 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container notary-signer ready: true, restart count 2
Mar  2 13:24:32.188: INFO: harbor-portal-77d6c78fd9-p7t57 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container portal ready: true, restart count 0
Mar  2 13:24:32.188: INFO: harbor-redis-0 from harbor started at 2023-02-28 08:16:04 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container redis ready: true, restart count 0
Mar  2 13:24:32.188: INFO: harbor-registry-787bfb74d7-9vbht from harbor started at 2023-02-28 08:15:42 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container registry ready: true, restart count 0
Mar  2 13:24:32.188: INFO: 	Container registryctl ready: true, restart count 0
Mar  2 13:24:32.188: INFO: harbor-trivy-0 from harbor started at 2023-02-28 08:15:58 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container trivy ready: true, restart count 0
Mar  2 13:24:32.188: INFO: ingress-nginx-controller-8jd6t from ingress-nginx started at 2023-02-27 13:52:07 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container controller ready: true, restart count 2
Mar  2 13:24:32.188: INFO: calico-accountant-sfvmv from kube-system started at 2023-02-27 13:50:54 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 13:24:32.188: INFO: calico-node-vj6gp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 13:24:32.188: INFO: csi-cinder-nodeplugin-lvpvh from kube-system started at 2023-02-27 13:28:22 +0000 UTC (3 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container cinder-csi-plugin ready: true, restart count 2
Mar  2 13:24:32.188: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 13:24:32.188: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 13:24:32.188: INFO: kube-proxy-nrgbs from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 13:24:32.188: INFO: metrics-server-d9dcc77d6-z4sx7 from kube-system started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container metrics-server ready: true, restart count 0
Mar  2 13:24:32.188: INFO: nginx-proxy-aarnq-sc-k8s-node-srv1 from kube-system started at 2023-02-27 14:17:16 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 13:24:32.188: INFO: node-local-dns-b8kzp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 13:24:32.188: INFO: kured-kbmf8 from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container kured ready: true, restart count 3
Mar  2 13:24:32.188: INFO: alertmanager-kube-prometheus-stack-alertmanager-1 from monitoring started at 2023-02-28 08:16:00 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 13:24:32.188: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 13:24:32.188: INFO: kube-prometheus-stack-prometheus-node-exporter-jmbsj from monitoring started at 2023-02-27 13:49:59 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 13:24:32.188: INFO: user-grafana-6f7c7d589-q6clc from monitoring started at 2023-02-28 08:15:43 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container grafana ready: true, restart count 0
Mar  2 13:24:32.188: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar  2 13:24:32.188: INFO: opensearch-master-0 from opensearch-system started at 2023-02-28 08:16:01 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 13:24:32.188: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-j5shm from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 13:24:32.188: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 13:24:32.188: INFO: thanos-query-query-69fc6f554b-d2q5v from thanos started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container query ready: true, restart count 0
Mar  2 13:24:32.188: INFO: thanos-receiver-receive-2 from thanos started at 2023-02-28 08:16:05 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container receive ready: true, restart count 0
Mar  2 13:24:32.188: INFO: thanos-receiver-ruler-1 from thanos started at 2023-02-28 08:15:57 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 13:24:32.188: INFO: 	Container ruler ready: true, restart count 0
Mar  2 13:24:32.188: INFO: restic-zvhnj from velero started at 2023-02-27 13:13:38 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.188: INFO: 	Container restic ready: true, restart count 2
Mar  2 13:24:32.188: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv2 before test
Mar  2 13:24:32.242: INFO: falco-4c5wt from falco started at 2023-02-27 15:51:55 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.242: INFO: 	Container falco ready: true, restart count 7
Mar  2 13:24:32.242: INFO: falco-exporter-cvpnp from falco started at 2023-02-27 15:51:06 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.242: INFO: 	Container falco-exporter ready: true, restart count 9
Mar  2 13:24:32.242: INFO: aarnq-sc-logs-logs-compaction-27961830-qfpkw from fluentd-system started at 2023-03-01 22:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.242: INFO: 	Container compaction ready: false, restart count 0
Mar  2 13:24:32.242: INFO: aarnq-sc-logs-logs-retention-27961890-98gmj from fluentd-system started at 2023-03-01 23:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.242: INFO: 	Container retention ready: false, restart count 0
Mar  2 13:24:32.242: INFO: fluentd-forwarder-b54pr from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.242: INFO: 	Container fluentd-forwarder ready: true, restart count 3
Mar  2 13:24:32.243: INFO: harbor-backup-cronjob-27961920-n2gpm from harbor started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.243: INFO: 	Container run ready: false, restart count 0
Mar  2 13:24:32.243: INFO: ingress-nginx-controller-lbvdv from ingress-nginx started at 2023-02-27 13:51:24 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.243: INFO: 	Container controller ready: true, restart count 3
Mar  2 13:24:32.243: INFO: calico-accountant-sb26b from kube-system started at 2023-02-27 13:50:50 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.243: INFO: 	Container calico-accountant ready: true, restart count 3
Mar  2 13:24:32.243: INFO: calico-node-9ps2k from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.243: INFO: 	Container calico-node ready: true, restart count 3
Mar  2 13:24:32.243: INFO: csi-cinder-nodeplugin-8qsk5 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
Mar  2 13:24:32.243: INFO: 	Container cinder-csi-plugin ready: true, restart count 9
Mar  2 13:24:32.243: INFO: 	Container liveness-probe ready: true, restart count 3
Mar  2 13:24:32.243: INFO: 	Container node-driver-registrar ready: true, restart count 3
Mar  2 13:24:32.243: INFO: kube-proxy-nrj68 from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.243: INFO: 	Container kube-proxy ready: true, restart count 3
Mar  2 13:24:32.243: INFO: nginx-proxy-aarnq-sc-k8s-node-srv2 from kube-system started at 2023-02-28 07:06:42 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.243: INFO: 	Container nginx-proxy ready: true, restart count 3
Mar  2 13:24:32.244: INFO: node-local-dns-pwwsn from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.244: INFO: 	Container node-cache ready: true, restart count 3
Mar  2 13:24:32.244: INFO: kured-fmhzj from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.244: INFO: 	Container kured ready: true, restart count 4
Mar  2 13:24:32.244: INFO: ciskubebench-exporter-68bcb66c46-rjnj9 from monitoring started at 2023-03-02 11:51:13 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.244: INFO: 	Container metrics-collector ready: true, restart count 0
Mar  2 13:24:32.244: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 13:24:32.244: INFO: kube-prometheus-stack-prometheus-node-exporter-rk7sg from monitoring started at 2023-02-27 13:49:53 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.244: INFO: 	Container node-exporter ready: true, restart count 3
Mar  2 13:24:32.244: INFO: scan-vulnerabilityreport-6bc4bd45fb-xp4bs from monitoring started at 2023-03-02 13:21:13 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.244: INFO: 	Container test-container ready: false, restart count 0
Mar  2 13:24:32.244: INFO: starboard-operator-7f84bbf756-grncj from monitoring started at 2023-03-02 11:50:59 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.244: INFO: 	Container starboard-operator ready: true, restart count 0
Mar  2 13:24:32.244: INFO: vulnerability-exporter-8485469578-jvppp from monitoring started at 2023-03-02 11:51:29 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.244: INFO: 	Container metrics-collector ready: true, restart count 0
Mar  2 13:24:32.244: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 13:24:32.244: INFO: opensearch-curator-27962720-l88hd from opensearch-system started at 2023-03-02 13:20:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.244: INFO: 	Container opensearch-curator ready: false, restart count 0
Mar  2 13:24:32.245: INFO: execpodvxbgl from services-974 started at 2023-03-02 13:24:28 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.245: INFO: 	Container agnhost-container ready: true, restart count 0
Mar  2 13:24:32.246: INFO: sonobuoy from sonobuoy started at 2023-03-02 12:35:12 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.246: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 13:24:32.246: INFO: sonobuoy-e2e-job-eae18696d9844ddc from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.246: INFO: 	Container e2e ready: true, restart count 0
Mar  2 13:24:32.246: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 13:24:32.246: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-zf5bk from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.246: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 13:24:32.246: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 13:24:32.246: INFO: pod-service-account-defaultsa from svcaccounts-936 started at 2023-03-02 13:23:31 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.246: INFO: 	Container token-test ready: false, restart count 0
Mar  2 13:24:32.246: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-936 started at 2023-03-02 13:23:31 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.246: INFO: 	Container token-test ready: false, restart count 0
Mar  2 13:24:32.246: INFO: pod-service-account-mountsa from svcaccounts-936 started at 2023-03-02 13:23:31 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.246: INFO: 	Container token-test ready: false, restart count 0
Mar  2 13:24:32.246: INFO: pod-service-account-mountsa-mountspec from svcaccounts-936 started at 2023-03-02 13:23:31 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.246: INFO: 	Container token-test ready: false, restart count 0
Mar  2 13:24:32.246: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-936 started at 2023-03-02 13:23:31 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.246: INFO: 	Container token-test ready: false, restart count 0
Mar  2 13:24:32.246: INFO: restic-wcgdp from velero started at 2023-02-27 13:41:53 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.246: INFO: 	Container restic ready: true, restart count 3
Mar  2 13:24:32.246: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv3 before test
Mar  2 13:24:32.273: INFO: cert-manager-cainjector-655cfbc4d-vc586 from cert-manager started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.273: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
Mar  2 13:24:32.274: INFO: dex-58d8c68494-gnrr5 from dex started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.274: INFO: 	Container dex ready: true, restart count 0
Mar  2 13:24:32.274: INFO: falco-9v9b5 from falco started at 2023-02-27 15:52:43 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.274: INFO: 	Container falco ready: true, restart count 1
Mar  2 13:24:32.274: INFO: falco-exporter-457cd from falco started at 2023-02-27 15:51:13 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.274: INFO: 	Container falco-exporter ready: true, restart count 3
Mar  2 13:24:32.274: INFO: falco-falcosidekick-5d5c7d4db-hddwc from falco started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.274: INFO: 	Container falcosidekick ready: true, restart count 0
Mar  2 13:24:32.274: INFO: aarnq-sc-logs-logs-compaction-27960390-dkhl6 from fluentd-system started at 2023-02-28 22:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.275: INFO: 	Container compaction ready: false, restart count 0
Mar  2 13:24:32.275: INFO: aarnq-sc-logs-logs-retention-27960450-xntms from fluentd-system started at 2023-02-28 23:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.275: INFO: 	Container retention ready: false, restart count 0
Mar  2 13:24:32.275: INFO: fluentd-forwarder-9smtw from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.275: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 13:24:32.275: INFO: harbor-backup-cronjob-27960480-trvs5 from harbor started at 2023-03-01 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.275: INFO: 	Container run ready: false, restart count 0
Mar  2 13:24:32.275: INFO: ingress-nginx-controller-4bgc8 from ingress-nginx started at 2023-02-27 13:54:29 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.275: INFO: 	Container controller ready: true, restart count 2
Mar  2 13:24:32.275: INFO: ingress-nginx-default-backend-64599cb78d-t9m7m from ingress-nginx started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.275: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Mar  2 13:24:32.275: INFO: calico-accountant-wgpwj from kube-system started at 2023-02-27 13:50:52 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.275: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 13:24:32.275: INFO: calico-node-7vgvf from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.275: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 13:24:32.276: INFO: csi-cinder-controllerplugin-6fdb685467-qppqd from kube-system started at 2023-03-01 07:31:56 +0000 UTC (6 container statuses recorded)
Mar  2 13:24:32.276: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  2 13:24:32.276: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  2 13:24:32.276: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  2 13:24:32.276: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  2 13:24:32.276: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar  2 13:24:32.276: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 13:24:32.276: INFO: csi-cinder-nodeplugin-wn26q from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
Mar  2 13:24:32.276: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
Mar  2 13:24:32.276: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 13:24:32.276: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 13:24:32.276: INFO: kube-proxy-t9sqm from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.276: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 13:24:32.276: INFO: nginx-proxy-aarnq-sc-k8s-node-srv3 from kube-system started at 2023-02-27 13:14:14 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.276: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 13:24:32.276: INFO: node-local-dns-jf9nv from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.276: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 13:24:32.276: INFO: kured-g9qpk from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.276: INFO: 	Container kured ready: true, restart count 3
Mar  2 13:24:32.276: INFO: grafana-label-enforcer-ff6966584-d9872 from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.277: INFO: 	Container prom-label-enforcer ready: true, restart count 0
Mar  2 13:24:32.277: INFO: kube-prometheus-stack-kube-state-metrics-5584579f7d-jmrqj from monitoring started at 2023-03-01 07:31:57 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.277: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 13:24:32.277: INFO: kube-prometheus-stack-operator-6bd84664f-wxlxc from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.277: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Mar  2 13:24:32.277: INFO: kube-prometheus-stack-prometheus-node-exporter-9v6v5 from monitoring started at 2023-02-27 13:50:02 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.277: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 13:24:32.277: INFO: prometheus-kube-prometheus-stack-prometheus-0 from monitoring started at 2023-03-01 07:32:02 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.277: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 13:24:32.277: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 13:24:32.277: INFO: opensearch-master-1 from opensearch-system started at 2023-03-01 07:32:10 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.277: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 13:24:32.277: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-m5t49 from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 13:24:32.277: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 13:24:32.277: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 13:24:32.277: INFO: thanos-query-query-frontend-6c58dbdc6-6g7jx from thanos started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.277: INFO: 	Container query-frontend ready: true, restart count 0
Mar  2 13:24:32.277: INFO: thanos-receiver-receive-1 from thanos started at 2023-03-01 07:32:11 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.277: INFO: 	Container receive ready: true, restart count 0
Mar  2 13:24:32.277: INFO: thanos-receiver-storegateway-0 from thanos started at 2023-03-01 07:32:06 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.277: INFO: 	Container storegateway ready: true, restart count 0
Mar  2 13:24:32.278: INFO: restic-hwpfm from velero started at 2023-02-27 13:13:47 +0000 UTC (1 container statuses recorded)
Mar  2 13:24:32.278: INFO: 	Container restic ready: true, restart count 3
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/02/23 13:24:32.278
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17489cfd2c135388], Reason = [FailedScheduling], Message = [0/5 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.] 03/02/23 13:24:32.384
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  2 13:24:33.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4954" for this suite. 03/02/23 13:24:33.4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":146,"skipped":2674,"failed":0}
------------------------------
â€¢ [1.390 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:24:32.017
    Mar  2 13:24:32.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sched-pred 03/02/23 13:24:32.027
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:24:32.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:24:32.052
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  2 13:24:32.057: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  2 13:24:32.069: INFO: Waiting for terminating namespaces to be deleted...
    Mar  2 13:24:32.124: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv0 before test
    Mar  2 13:24:32.157: INFO: falco-exporter-r2zfx from falco started at 2023-02-27 15:51:02 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.157: INFO: 	Container falco-exporter ready: true, restart count 4
    Mar  2 13:24:32.157: INFO: falco-falcosidekick-5d5c7d4db-r6952 from falco started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.157: INFO: 	Container falcosidekick ready: true, restart count 0
    Mar  2 13:24:32.157: INFO: falco-trjnh from falco started at 2023-02-27 15:53:31 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.157: INFO: 	Container falco ready: true, restart count 2
    Mar  2 13:24:32.157: INFO: fluentd-forwarder-qxbtj from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.157: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 13:24:32.157: INFO: ingress-nginx-controller-qprg2 from ingress-nginx started at 2023-02-27 13:57:01 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.157: INFO: 	Container controller ready: true, restart count 2
    Mar  2 13:24:32.157: INFO: calico-accountant-k6t4p from kube-system started at 2023-02-27 13:50:57 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.157: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 13:24:32.157: INFO: calico-node-dz84l from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.157: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 13:24:32.157: INFO: coredns-588bb58b94-k6j76 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.157: INFO: 	Container coredns ready: true, restart count 0
    Mar  2 13:24:32.157: INFO: csi-cinder-nodeplugin-9nct9 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
    Mar  2 13:24:32.157: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
    Mar  2 13:24:32.157: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 13:24:32.157: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 13:24:32.158: INFO: kube-proxy-7bm9z from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.158: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 13:24:32.158: INFO: nginx-proxy-aarnq-sc-k8s-node-srv0 from kube-system started at 2023-02-27 14:10:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.158: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 13:24:32.158: INFO: node-local-dns-dk8hd from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.158: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 13:24:32.158: INFO: snapshot-controller-7d445c66c9-6w4w5 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.158: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 13:24:32.158: INFO: kured-hdkrw from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.158: INFO: 	Container kured ready: true, restart count 3
    Mar  2 13:24:32.158: INFO: alertmanager-kube-prometheus-stack-alertmanager-0 from monitoring started at 2023-03-01 07:32:10 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container alertmanager ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: kube-prometheus-stack-grafana-84f79f467b-sr7kl from monitoring started at 2023-02-28 08:03:49 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container grafana ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: kube-prometheus-stack-prometheus-node-exporter-nl9pw from monitoring started at 2023-02-27 13:49:55 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 13:24:32.159: INFO: prometheus-blackbox-exporter-677b579798-7xm9d from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: s3-exporter-867c5b9457-lfcsf from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container s3-exporter ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: opensearch-dashboards-58c8d95f7b-9spcv from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container dashboards ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: opensearch-master-2 from opensearch-system started at 2023-02-28 08:03:54 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: prometheus-opensearch-exporter-5688c84dcd-95vjh from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container exporter ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-qv9vz from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: thanos-query-query-69fc6f554b-db7w4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container query ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: thanos-receiver-bucketweb-b4955fcf8-8w2xg from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container bucketweb ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: thanos-receiver-compactor-848df7b5d7-z2jh4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container compactor ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: thanos-receiver-receive-0 from thanos started at 2023-02-28 08:04:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container receive ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: thanos-receiver-receive-distributor-779c5d74d8-7hhmb from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container receive ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: thanos-receiver-ruler-0 from thanos started at 2023-02-28 08:03:52 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: 	Container ruler ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: restic-k4z4s from velero started at 2023-02-27 13:13:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container restic ready: true, restart count 2
    Mar  2 13:24:32.159: INFO: velero-7bbd458dfc-s8n2h from velero started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.159: INFO: 	Container velero ready: true, restart count 0
    Mar  2 13:24:32.159: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv1 before test
    Mar  2 13:24:32.188: INFO: cert-manager-754b766f8b-fvh5z from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container cert-manager-controller ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: cert-manager-webhook-875cdf98f-lfgn7 from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: dex-58d8c68494-flfvv from dex started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container dex ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: falco-8cz2x from falco started at 2023-02-27 15:54:18 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container falco ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: falco-exporter-srbrb from falco started at 2023-02-27 15:51:10 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container falco-exporter ready: true, restart count 4
    Mar  2 13:24:32.188: INFO: fluentd-aggregator-0 from fluentd-system started at 2023-02-28 08:16:03 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container fluentd ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: fluentd-forwarder-zgcds from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 13:24:32.188: INFO: harbor-chartmuseum-5c9477455d-hp9zb from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container chartmuseum ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: harbor-core-58dc955656-2vz5k from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container core ready: true, restart count 1
    Mar  2 13:24:32.188: INFO: harbor-database-0 from harbor started at 2023-02-28 08:16:02 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container database ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: harbor-jobservice-69c4c778fb-8qt7s from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container jobservice ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: harbor-notary-server-6cfdf66b5-sxpqw from harbor started at 2023-02-28 08:15:44 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container notary-server ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: harbor-notary-signer-5d6d45f584-rfqm7 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container notary-signer ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: harbor-portal-77d6c78fd9-p7t57 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container portal ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: harbor-redis-0 from harbor started at 2023-02-28 08:16:04 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container redis ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: harbor-registry-787bfb74d7-9vbht from harbor started at 2023-02-28 08:15:42 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container registry ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: 	Container registryctl ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: harbor-trivy-0 from harbor started at 2023-02-28 08:15:58 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container trivy ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: ingress-nginx-controller-8jd6t from ingress-nginx started at 2023-02-27 13:52:07 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container controller ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: calico-accountant-sfvmv from kube-system started at 2023-02-27 13:50:54 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: calico-node-vj6gp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: csi-cinder-nodeplugin-lvpvh from kube-system started at 2023-02-27 13:28:22 +0000 UTC (3 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container cinder-csi-plugin ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: kube-proxy-nrgbs from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: metrics-server-d9dcc77d6-z4sx7 from kube-system started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container metrics-server ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: nginx-proxy-aarnq-sc-k8s-node-srv1 from kube-system started at 2023-02-27 14:17:16 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: node-local-dns-b8kzp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: kured-kbmf8 from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container kured ready: true, restart count 3
    Mar  2 13:24:32.188: INFO: alertmanager-kube-prometheus-stack-alertmanager-1 from monitoring started at 2023-02-28 08:16:00 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container alertmanager ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: kube-prometheus-stack-prometheus-node-exporter-jmbsj from monitoring started at 2023-02-27 13:49:59 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: user-grafana-6f7c7d589-q6clc from monitoring started at 2023-02-28 08:15:43 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container grafana ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: opensearch-master-0 from opensearch-system started at 2023-02-28 08:16:01 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-j5shm from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: thanos-query-query-69fc6f554b-d2q5v from thanos started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container query ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: thanos-receiver-receive-2 from thanos started at 2023-02-28 08:16:05 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container receive ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: thanos-receiver-ruler-1 from thanos started at 2023-02-28 08:15:57 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: 	Container ruler ready: true, restart count 0
    Mar  2 13:24:32.188: INFO: restic-zvhnj from velero started at 2023-02-27 13:13:38 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.188: INFO: 	Container restic ready: true, restart count 2
    Mar  2 13:24:32.188: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv2 before test
    Mar  2 13:24:32.242: INFO: falco-4c5wt from falco started at 2023-02-27 15:51:55 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.242: INFO: 	Container falco ready: true, restart count 7
    Mar  2 13:24:32.242: INFO: falco-exporter-cvpnp from falco started at 2023-02-27 15:51:06 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.242: INFO: 	Container falco-exporter ready: true, restart count 9
    Mar  2 13:24:32.242: INFO: aarnq-sc-logs-logs-compaction-27961830-qfpkw from fluentd-system started at 2023-03-01 22:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.242: INFO: 	Container compaction ready: false, restart count 0
    Mar  2 13:24:32.242: INFO: aarnq-sc-logs-logs-retention-27961890-98gmj from fluentd-system started at 2023-03-01 23:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.242: INFO: 	Container retention ready: false, restart count 0
    Mar  2 13:24:32.242: INFO: fluentd-forwarder-b54pr from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.242: INFO: 	Container fluentd-forwarder ready: true, restart count 3
    Mar  2 13:24:32.243: INFO: harbor-backup-cronjob-27961920-n2gpm from harbor started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.243: INFO: 	Container run ready: false, restart count 0
    Mar  2 13:24:32.243: INFO: ingress-nginx-controller-lbvdv from ingress-nginx started at 2023-02-27 13:51:24 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.243: INFO: 	Container controller ready: true, restart count 3
    Mar  2 13:24:32.243: INFO: calico-accountant-sb26b from kube-system started at 2023-02-27 13:50:50 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.243: INFO: 	Container calico-accountant ready: true, restart count 3
    Mar  2 13:24:32.243: INFO: calico-node-9ps2k from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.243: INFO: 	Container calico-node ready: true, restart count 3
    Mar  2 13:24:32.243: INFO: csi-cinder-nodeplugin-8qsk5 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
    Mar  2 13:24:32.243: INFO: 	Container cinder-csi-plugin ready: true, restart count 9
    Mar  2 13:24:32.243: INFO: 	Container liveness-probe ready: true, restart count 3
    Mar  2 13:24:32.243: INFO: 	Container node-driver-registrar ready: true, restart count 3
    Mar  2 13:24:32.243: INFO: kube-proxy-nrj68 from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.243: INFO: 	Container kube-proxy ready: true, restart count 3
    Mar  2 13:24:32.243: INFO: nginx-proxy-aarnq-sc-k8s-node-srv2 from kube-system started at 2023-02-28 07:06:42 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.243: INFO: 	Container nginx-proxy ready: true, restart count 3
    Mar  2 13:24:32.244: INFO: node-local-dns-pwwsn from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.244: INFO: 	Container node-cache ready: true, restart count 3
    Mar  2 13:24:32.244: INFO: kured-fmhzj from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.244: INFO: 	Container kured ready: true, restart count 4
    Mar  2 13:24:32.244: INFO: ciskubebench-exporter-68bcb66c46-rjnj9 from monitoring started at 2023-03-02 11:51:13 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.244: INFO: 	Container metrics-collector ready: true, restart count 0
    Mar  2 13:24:32.244: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 13:24:32.244: INFO: kube-prometheus-stack-prometheus-node-exporter-rk7sg from monitoring started at 2023-02-27 13:49:53 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.244: INFO: 	Container node-exporter ready: true, restart count 3
    Mar  2 13:24:32.244: INFO: scan-vulnerabilityreport-6bc4bd45fb-xp4bs from monitoring started at 2023-03-02 13:21:13 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.244: INFO: 	Container test-container ready: false, restart count 0
    Mar  2 13:24:32.244: INFO: starboard-operator-7f84bbf756-grncj from monitoring started at 2023-03-02 11:50:59 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.244: INFO: 	Container starboard-operator ready: true, restart count 0
    Mar  2 13:24:32.244: INFO: vulnerability-exporter-8485469578-jvppp from monitoring started at 2023-03-02 11:51:29 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.244: INFO: 	Container metrics-collector ready: true, restart count 0
    Mar  2 13:24:32.244: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 13:24:32.244: INFO: opensearch-curator-27962720-l88hd from opensearch-system started at 2023-03-02 13:20:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.244: INFO: 	Container opensearch-curator ready: false, restart count 0
    Mar  2 13:24:32.245: INFO: execpodvxbgl from services-974 started at 2023-03-02 13:24:28 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.245: INFO: 	Container agnhost-container ready: true, restart count 0
    Mar  2 13:24:32.246: INFO: sonobuoy from sonobuoy started at 2023-03-02 12:35:12 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.246: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  2 13:24:32.246: INFO: sonobuoy-e2e-job-eae18696d9844ddc from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.246: INFO: 	Container e2e ready: true, restart count 0
    Mar  2 13:24:32.246: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 13:24:32.246: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-zf5bk from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.246: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 13:24:32.246: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 13:24:32.246: INFO: pod-service-account-defaultsa from svcaccounts-936 started at 2023-03-02 13:23:31 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.246: INFO: 	Container token-test ready: false, restart count 0
    Mar  2 13:24:32.246: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-936 started at 2023-03-02 13:23:31 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.246: INFO: 	Container token-test ready: false, restart count 0
    Mar  2 13:24:32.246: INFO: pod-service-account-mountsa from svcaccounts-936 started at 2023-03-02 13:23:31 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.246: INFO: 	Container token-test ready: false, restart count 0
    Mar  2 13:24:32.246: INFO: pod-service-account-mountsa-mountspec from svcaccounts-936 started at 2023-03-02 13:23:31 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.246: INFO: 	Container token-test ready: false, restart count 0
    Mar  2 13:24:32.246: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-936 started at 2023-03-02 13:23:31 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.246: INFO: 	Container token-test ready: false, restart count 0
    Mar  2 13:24:32.246: INFO: restic-wcgdp from velero started at 2023-02-27 13:41:53 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.246: INFO: 	Container restic ready: true, restart count 3
    Mar  2 13:24:32.246: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv3 before test
    Mar  2 13:24:32.273: INFO: cert-manager-cainjector-655cfbc4d-vc586 from cert-manager started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.273: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    Mar  2 13:24:32.274: INFO: dex-58d8c68494-gnrr5 from dex started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.274: INFO: 	Container dex ready: true, restart count 0
    Mar  2 13:24:32.274: INFO: falco-9v9b5 from falco started at 2023-02-27 15:52:43 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.274: INFO: 	Container falco ready: true, restart count 1
    Mar  2 13:24:32.274: INFO: falco-exporter-457cd from falco started at 2023-02-27 15:51:13 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.274: INFO: 	Container falco-exporter ready: true, restart count 3
    Mar  2 13:24:32.274: INFO: falco-falcosidekick-5d5c7d4db-hddwc from falco started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.274: INFO: 	Container falcosidekick ready: true, restart count 0
    Mar  2 13:24:32.274: INFO: aarnq-sc-logs-logs-compaction-27960390-dkhl6 from fluentd-system started at 2023-02-28 22:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.275: INFO: 	Container compaction ready: false, restart count 0
    Mar  2 13:24:32.275: INFO: aarnq-sc-logs-logs-retention-27960450-xntms from fluentd-system started at 2023-02-28 23:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.275: INFO: 	Container retention ready: false, restart count 0
    Mar  2 13:24:32.275: INFO: fluentd-forwarder-9smtw from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.275: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 13:24:32.275: INFO: harbor-backup-cronjob-27960480-trvs5 from harbor started at 2023-03-01 00:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.275: INFO: 	Container run ready: false, restart count 0
    Mar  2 13:24:32.275: INFO: ingress-nginx-controller-4bgc8 from ingress-nginx started at 2023-02-27 13:54:29 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.275: INFO: 	Container controller ready: true, restart count 2
    Mar  2 13:24:32.275: INFO: ingress-nginx-default-backend-64599cb78d-t9m7m from ingress-nginx started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.275: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
    Mar  2 13:24:32.275: INFO: calico-accountant-wgpwj from kube-system started at 2023-02-27 13:50:52 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.275: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 13:24:32.275: INFO: calico-node-7vgvf from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.275: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 13:24:32.276: INFO: csi-cinder-controllerplugin-6fdb685467-qppqd from kube-system started at 2023-03-01 07:31:56 +0000 UTC (6 container statuses recorded)
    Mar  2 13:24:32.276: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  2 13:24:32.276: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar  2 13:24:32.276: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar  2 13:24:32.276: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar  2 13:24:32.276: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar  2 13:24:32.276: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  2 13:24:32.276: INFO: csi-cinder-nodeplugin-wn26q from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
    Mar  2 13:24:32.276: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
    Mar  2 13:24:32.276: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 13:24:32.276: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 13:24:32.276: INFO: kube-proxy-t9sqm from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.276: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 13:24:32.276: INFO: nginx-proxy-aarnq-sc-k8s-node-srv3 from kube-system started at 2023-02-27 13:14:14 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.276: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 13:24:32.276: INFO: node-local-dns-jf9nv from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.276: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 13:24:32.276: INFO: kured-g9qpk from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.276: INFO: 	Container kured ready: true, restart count 3
    Mar  2 13:24:32.276: INFO: grafana-label-enforcer-ff6966584-d9872 from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.277: INFO: 	Container prom-label-enforcer ready: true, restart count 0
    Mar  2 13:24:32.277: INFO: kube-prometheus-stack-kube-state-metrics-5584579f7d-jmrqj from monitoring started at 2023-03-01 07:31:57 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.277: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Mar  2 13:24:32.277: INFO: kube-prometheus-stack-operator-6bd84664f-wxlxc from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.277: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Mar  2 13:24:32.277: INFO: kube-prometheus-stack-prometheus-node-exporter-9v6v5 from monitoring started at 2023-02-27 13:50:02 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.277: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 13:24:32.277: INFO: prometheus-kube-prometheus-stack-prometheus-0 from monitoring started at 2023-03-01 07:32:02 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.277: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 13:24:32.277: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 13:24:32.277: INFO: opensearch-master-1 from opensearch-system started at 2023-03-01 07:32:10 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.277: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 13:24:32.277: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-m5t49 from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 13:24:32.277: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 13:24:32.277: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 13:24:32.277: INFO: thanos-query-query-frontend-6c58dbdc6-6g7jx from thanos started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.277: INFO: 	Container query-frontend ready: true, restart count 0
    Mar  2 13:24:32.277: INFO: thanos-receiver-receive-1 from thanos started at 2023-03-01 07:32:11 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.277: INFO: 	Container receive ready: true, restart count 0
    Mar  2 13:24:32.277: INFO: thanos-receiver-storegateway-0 from thanos started at 2023-03-01 07:32:06 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.277: INFO: 	Container storegateway ready: true, restart count 0
    Mar  2 13:24:32.278: INFO: restic-hwpfm from velero started at 2023-02-27 13:13:47 +0000 UTC (1 container statuses recorded)
    Mar  2 13:24:32.278: INFO: 	Container restic ready: true, restart count 3
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/02/23 13:24:32.278
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17489cfd2c135388], Reason = [FailedScheduling], Message = [0/5 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.] 03/02/23 13:24:32.384
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 13:24:33.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4954" for this suite. 03/02/23 13:24:33.4
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:24:33.416
Mar  2 13:24:33.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 13:24:33.417
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:24:33.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:24:33.435
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 03/02/23 13:24:33.439
Mar  2 13:24:33.450: INFO: Waiting up to 5m0s for pod "downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594" in namespace "downward-api-1249" to be "Succeeded or Failed"
Mar  2 13:24:33.461: INFO: Pod "downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594": Phase="Pending", Reason="", readiness=false. Elapsed: 10.674286ms
Mar  2 13:24:35.471: INFO: Pod "downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021130802s
Mar  2 13:24:37.469: INFO: Pod "downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018609663s
STEP: Saw pod success 03/02/23 13:24:37.469
Mar  2 13:24:37.469: INFO: Pod "downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594" satisfied condition "Succeeded or Failed"
Mar  2 13:24:37.479: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594 container dapi-container: <nil>
STEP: delete the pod 03/02/23 13:24:37.517
Mar  2 13:24:37.538: INFO: Waiting for pod downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594 to disappear
Mar  2 13:24:37.544: INFO: Pod downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  2 13:24:37.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1249" for this suite. 03/02/23 13:24:37.562
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":147,"skipped":2692,"failed":0}
------------------------------
â€¢ [4.182 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:24:33.416
    Mar  2 13:24:33.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 13:24:33.417
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:24:33.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:24:33.435
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 03/02/23 13:24:33.439
    Mar  2 13:24:33.450: INFO: Waiting up to 5m0s for pod "downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594" in namespace "downward-api-1249" to be "Succeeded or Failed"
    Mar  2 13:24:33.461: INFO: Pod "downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594": Phase="Pending", Reason="", readiness=false. Elapsed: 10.674286ms
    Mar  2 13:24:35.471: INFO: Pod "downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021130802s
    Mar  2 13:24:37.469: INFO: Pod "downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018609663s
    STEP: Saw pod success 03/02/23 13:24:37.469
    Mar  2 13:24:37.469: INFO: Pod "downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594" satisfied condition "Succeeded or Failed"
    Mar  2 13:24:37.479: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594 container dapi-container: <nil>
    STEP: delete the pod 03/02/23 13:24:37.517
    Mar  2 13:24:37.538: INFO: Waiting for pod downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594 to disappear
    Mar  2 13:24:37.544: INFO: Pod downward-api-3e2f6bc3-5b1c-4a5e-84f7-c7c44208b594 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  2 13:24:37.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1249" for this suite. 03/02/23 13:24:37.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:24:37.615
Mar  2 13:24:37.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pod-network-test 03/02/23 13:24:37.638
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:24:37.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:24:37.67
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-3604 03/02/23 13:24:37.674
STEP: creating a selector 03/02/23 13:24:37.674
STEP: Creating the service pods in kubernetes 03/02/23 13:24:37.682
Mar  2 13:24:37.682: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 13:24:37.739: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3604" to be "running and ready"
Mar  2 13:24:37.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.208386ms
Mar  2 13:24:37.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:24:39.761: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.020800227s
Mar  2 13:24:39.761: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:24:41.759: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019648814s
Mar  2 13:24:41.760: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:24:43.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018535954s
Mar  2 13:24:43.759: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:24:45.760: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020419928s
Mar  2 13:24:45.760: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:24:47.764: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024206992s
Mar  2 13:24:47.764: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:24:49.759: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.019573348s
Mar  2 13:24:49.761: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  2 13:24:49.761: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  2 13:24:49.766: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3604" to be "running and ready"
Mar  2 13:24:49.769: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.383166ms
Mar  2 13:24:49.770: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  2 13:24:49.770: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  2 13:24:49.774: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3604" to be "running and ready"
Mar  2 13:24:49.778: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.484422ms
Mar  2 13:24:49.779: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar  2 13:24:51.788: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.014340037s
Mar  2 13:24:51.788: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar  2 13:24:53.785: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.011416095s
Mar  2 13:24:53.785: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar  2 13:24:55.786: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.012512564s
Mar  2 13:24:55.786: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar  2 13:24:57.789: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.015168253s
Mar  2 13:24:57.789: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar  2 13:24:59.785: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.011224145s
Mar  2 13:24:59.785: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  2 13:24:59.785: INFO: Pod "netserver-2" satisfied condition "running and ready"
Mar  2 13:24:59.791: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-3604" to be "running and ready"
Mar  2 13:24:59.795: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.256882ms
Mar  2 13:24:59.795: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Mar  2 13:24:59.795: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 03/02/23 13:24:59.799
Mar  2 13:24:59.805: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3604" to be "running"
Mar  2 13:24:59.811: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.23514ms
Mar  2 13:25:01.823: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01803441s
Mar  2 13:25:03.822: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016869356s
Mar  2 13:25:03.822: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  2 13:25:03.825: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Mar  2 13:25:03.825: INFO: Breadth first check of 10.233.123.1 on host 172.16.0.61...
Mar  2 13:25:03.828: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.105:9080/dial?request=hostname&protocol=http&host=10.233.123.1&port=8083&tries=1'] Namespace:pod-network-test-3604 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:03.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:03.832: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:03.832: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3604/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.123.1%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 13:25:03.950: INFO: Waiting for responses: map[]
Mar  2 13:25:03.950: INFO: reached 10.233.123.1 after 0/1 tries
Mar  2 13:25:03.950: INFO: Breadth first check of 10.233.92.65 on host 172.16.0.138...
Mar  2 13:25:03.955: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.105:9080/dial?request=hostname&protocol=http&host=10.233.92.65&port=8083&tries=1'] Namespace:pod-network-test-3604 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:03.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:03.956: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:03.956: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3604/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.92.65%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 13:25:04.059: INFO: Waiting for responses: map[]
Mar  2 13:25:04.059: INFO: reached 10.233.92.65 after 0/1 tries
Mar  2 13:25:04.059: INFO: Breadth first check of 10.233.123.109 on host 172.16.0.192...
Mar  2 13:25:04.066: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.105:9080/dial?request=hostname&protocol=http&host=10.233.123.109&port=8083&tries=1'] Namespace:pod-network-test-3604 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:04.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:04.067: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:04.067: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3604/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.123.109%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 13:25:04.191: INFO: Waiting for responses: map[]
Mar  2 13:25:04.192: INFO: reached 10.233.123.109 after 0/1 tries
Mar  2 13:25:04.192: INFO: Breadth first check of 10.233.126.70 on host 172.16.0.56...
Mar  2 13:25:04.201: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.105:9080/dial?request=hostname&protocol=http&host=10.233.126.70&port=8083&tries=1'] Namespace:pod-network-test-3604 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:04.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:04.203: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:04.204: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3604/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.126.70%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 13:25:04.307: INFO: Waiting for responses: map[]
Mar  2 13:25:04.308: INFO: reached 10.233.126.70 after 0/1 tries
Mar  2 13:25:04.308: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  2 13:25:04.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3604" for this suite. 03/02/23 13:25:04.318
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":148,"skipped":2707,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.708 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:24:37.615
    Mar  2 13:24:37.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pod-network-test 03/02/23 13:24:37.638
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:24:37.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:24:37.67
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-3604 03/02/23 13:24:37.674
    STEP: creating a selector 03/02/23 13:24:37.674
    STEP: Creating the service pods in kubernetes 03/02/23 13:24:37.682
    Mar  2 13:24:37.682: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  2 13:24:37.739: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3604" to be "running and ready"
    Mar  2 13:24:37.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.208386ms
    Mar  2 13:24:37.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:24:39.761: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.020800227s
    Mar  2 13:24:39.761: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:24:41.759: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019648814s
    Mar  2 13:24:41.760: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:24:43.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018535954s
    Mar  2 13:24:43.759: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:24:45.760: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020419928s
    Mar  2 13:24:45.760: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:24:47.764: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024206992s
    Mar  2 13:24:47.764: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:24:49.759: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.019573348s
    Mar  2 13:24:49.761: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  2 13:24:49.761: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  2 13:24:49.766: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3604" to be "running and ready"
    Mar  2 13:24:49.769: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.383166ms
    Mar  2 13:24:49.770: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  2 13:24:49.770: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  2 13:24:49.774: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3604" to be "running and ready"
    Mar  2 13:24:49.778: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.484422ms
    Mar  2 13:24:49.779: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar  2 13:24:51.788: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.014340037s
    Mar  2 13:24:51.788: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar  2 13:24:53.785: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.011416095s
    Mar  2 13:24:53.785: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar  2 13:24:55.786: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.012512564s
    Mar  2 13:24:55.786: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar  2 13:24:57.789: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.015168253s
    Mar  2 13:24:57.789: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar  2 13:24:59.785: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.011224145s
    Mar  2 13:24:59.785: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  2 13:24:59.785: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Mar  2 13:24:59.791: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-3604" to be "running and ready"
    Mar  2 13:24:59.795: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.256882ms
    Mar  2 13:24:59.795: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Mar  2 13:24:59.795: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 03/02/23 13:24:59.799
    Mar  2 13:24:59.805: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3604" to be "running"
    Mar  2 13:24:59.811: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.23514ms
    Mar  2 13:25:01.823: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01803441s
    Mar  2 13:25:03.822: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016869356s
    Mar  2 13:25:03.822: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  2 13:25:03.825: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Mar  2 13:25:03.825: INFO: Breadth first check of 10.233.123.1 on host 172.16.0.61...
    Mar  2 13:25:03.828: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.105:9080/dial?request=hostname&protocol=http&host=10.233.123.1&port=8083&tries=1'] Namespace:pod-network-test-3604 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:03.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:03.832: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:03.832: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3604/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.123.1%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 13:25:03.950: INFO: Waiting for responses: map[]
    Mar  2 13:25:03.950: INFO: reached 10.233.123.1 after 0/1 tries
    Mar  2 13:25:03.950: INFO: Breadth first check of 10.233.92.65 on host 172.16.0.138...
    Mar  2 13:25:03.955: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.105:9080/dial?request=hostname&protocol=http&host=10.233.92.65&port=8083&tries=1'] Namespace:pod-network-test-3604 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:03.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:03.956: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:03.956: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3604/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.92.65%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 13:25:04.059: INFO: Waiting for responses: map[]
    Mar  2 13:25:04.059: INFO: reached 10.233.92.65 after 0/1 tries
    Mar  2 13:25:04.059: INFO: Breadth first check of 10.233.123.109 on host 172.16.0.192...
    Mar  2 13:25:04.066: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.105:9080/dial?request=hostname&protocol=http&host=10.233.123.109&port=8083&tries=1'] Namespace:pod-network-test-3604 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:04.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:04.067: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:04.067: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3604/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.123.109%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 13:25:04.191: INFO: Waiting for responses: map[]
    Mar  2 13:25:04.192: INFO: reached 10.233.123.109 after 0/1 tries
    Mar  2 13:25:04.192: INFO: Breadth first check of 10.233.126.70 on host 172.16.0.56...
    Mar  2 13:25:04.201: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.105:9080/dial?request=hostname&protocol=http&host=10.233.126.70&port=8083&tries=1'] Namespace:pod-network-test-3604 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:04.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:04.203: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:04.204: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3604/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.126.70%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 13:25:04.307: INFO: Waiting for responses: map[]
    Mar  2 13:25:04.308: INFO: reached 10.233.126.70 after 0/1 tries
    Mar  2 13:25:04.308: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  2 13:25:04.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3604" for this suite. 03/02/23 13:25:04.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:04.331
Mar  2 13:25:04.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 13:25:04.334
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:04.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:04.357
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 03/02/23 13:25:04.361
Mar  2 13:25:04.368: INFO: Waiting up to 5m0s for pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16" in namespace "emptydir-8929" to be "Succeeded or Failed"
Mar  2 13:25:04.398: INFO: Pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16": Phase="Pending", Reason="", readiness=false. Elapsed: 29.889007ms
Mar  2 13:25:06.412: INFO: Pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044039014s
Mar  2 13:25:08.403: INFO: Pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16": Phase="Running", Reason="", readiness=false. Elapsed: 4.035306711s
Mar  2 13:25:10.519: INFO: Pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.1511692s
STEP: Saw pod success 03/02/23 13:25:10.629
Mar  2 13:25:10.667: INFO: Pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16" satisfied condition "Succeeded or Failed"
Mar  2 13:25:10.734: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16 container test-container: <nil>
STEP: delete the pod 03/02/23 13:25:10.741
Mar  2 13:25:10.752: INFO: Waiting for pod pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16 to disappear
Mar  2 13:25:10.756: INFO: Pod pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 13:25:10.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8929" for this suite. 03/02/23 13:25:10.762
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":149,"skipped":2714,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.439 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:04.331
    Mar  2 13:25:04.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 13:25:04.334
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:04.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:04.357
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/02/23 13:25:04.361
    Mar  2 13:25:04.368: INFO: Waiting up to 5m0s for pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16" in namespace "emptydir-8929" to be "Succeeded or Failed"
    Mar  2 13:25:04.398: INFO: Pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16": Phase="Pending", Reason="", readiness=false. Elapsed: 29.889007ms
    Mar  2 13:25:06.412: INFO: Pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044039014s
    Mar  2 13:25:08.403: INFO: Pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16": Phase="Running", Reason="", readiness=false. Elapsed: 4.035306711s
    Mar  2 13:25:10.519: INFO: Pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.1511692s
    STEP: Saw pod success 03/02/23 13:25:10.629
    Mar  2 13:25:10.667: INFO: Pod "pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16" satisfied condition "Succeeded or Failed"
    Mar  2 13:25:10.734: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16 container test-container: <nil>
    STEP: delete the pod 03/02/23 13:25:10.741
    Mar  2 13:25:10.752: INFO: Waiting for pod pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16 to disappear
    Mar  2 13:25:10.756: INFO: Pod pod-9bd2419b-ad9c-435b-8cc5-2e71d49a1e16 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 13:25:10.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8929" for this suite. 03/02/23 13:25:10.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:10.829
Mar  2 13:25:10.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename resourcequota 03/02/23 13:25:10.833
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:10.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:10.883
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 03/02/23 13:25:10.93
STEP: Getting a ResourceQuota 03/02/23 13:25:10.94
STEP: Listing all ResourceQuotas with LabelSelector 03/02/23 13:25:10.95
STEP: Patching the ResourceQuota 03/02/23 13:25:10.952
STEP: Deleting a Collection of ResourceQuotas 03/02/23 13:25:10.96
STEP: Verifying the deleted ResourceQuota 03/02/23 13:25:10.986
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 13:25:11.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3501" for this suite. 03/02/23 13:25:11.028
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":150,"skipped":2720,"failed":0}
------------------------------
â€¢ [0.206 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:10.829
    Mar  2 13:25:10.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename resourcequota 03/02/23 13:25:10.833
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:10.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:10.883
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 03/02/23 13:25:10.93
    STEP: Getting a ResourceQuota 03/02/23 13:25:10.94
    STEP: Listing all ResourceQuotas with LabelSelector 03/02/23 13:25:10.95
    STEP: Patching the ResourceQuota 03/02/23 13:25:10.952
    STEP: Deleting a Collection of ResourceQuotas 03/02/23 13:25:10.96
    STEP: Verifying the deleted ResourceQuota 03/02/23 13:25:10.986
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 13:25:11.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3501" for this suite. 03/02/23 13:25:11.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:11.051
Mar  2 13:25:11.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename job 03/02/23 13:25:11.059
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:11.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:11.146
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 03/02/23 13:25:11.148
STEP: Ensuring job reaches completions 03/02/23 13:25:11.153
STEP: Ensuring pods with index for job exist 03/02/23 13:25:21.16
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 13:25:21.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9855" for this suite. 03/02/23 13:25:21.177
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":151,"skipped":2756,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.132 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:11.051
    Mar  2 13:25:11.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename job 03/02/23 13:25:11.059
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:11.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:11.146
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 03/02/23 13:25:11.148
    STEP: Ensuring job reaches completions 03/02/23 13:25:11.153
    STEP: Ensuring pods with index for job exist 03/02/23 13:25:21.16
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 13:25:21.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9855" for this suite. 03/02/23 13:25:21.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:21.193
Mar  2 13:25:21.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename podtemplate 03/02/23 13:25:21.194
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:21.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:21.213
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/02/23 13:25:21.216
Mar  2 13:25:21.221: INFO: created test-podtemplate-1
Mar  2 13:25:21.227: INFO: created test-podtemplate-2
Mar  2 13:25:21.232: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/02/23 13:25:21.232
STEP: delete collection of pod templates 03/02/23 13:25:21.235
Mar  2 13:25:21.235: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/02/23 13:25:21.243
Mar  2 13:25:21.243: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  2 13:25:21.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9953" for this suite. 03/02/23 13:25:21.253
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":152,"skipped":2778,"failed":0}
------------------------------
â€¢ [0.065 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:21.193
    Mar  2 13:25:21.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename podtemplate 03/02/23 13:25:21.194
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:21.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:21.213
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/02/23 13:25:21.216
    Mar  2 13:25:21.221: INFO: created test-podtemplate-1
    Mar  2 13:25:21.227: INFO: created test-podtemplate-2
    Mar  2 13:25:21.232: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/02/23 13:25:21.232
    STEP: delete collection of pod templates 03/02/23 13:25:21.235
    Mar  2 13:25:21.235: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/02/23 13:25:21.243
    Mar  2 13:25:21.243: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  2 13:25:21.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9953" for this suite. 03/02/23 13:25:21.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:21.261
Mar  2 13:25:21.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename conformance-tests 03/02/23 13:25:21.262
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:21.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:21.304
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/02/23 13:25:21.309
Mar  2 13:25:21.310: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Mar  2 13:25:21.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-3334" for this suite. 03/02/23 13:25:21.327
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":153,"skipped":2794,"failed":0}
------------------------------
â€¢ [0.072 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:21.261
    Mar  2 13:25:21.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename conformance-tests 03/02/23 13:25:21.262
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:21.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:21.304
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/02/23 13:25:21.309
    Mar  2 13:25:21.310: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Mar  2 13:25:21.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-3334" for this suite. 03/02/23 13:25:21.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:21.335
Mar  2 13:25:21.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename dns 03/02/23 13:25:21.337
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:21.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:21.36
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/02/23 13:25:21.364
Mar  2 13:25:21.426: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2703  1cf78fb0-7704-4593-a60a-2b539409048c 1945979 0 2023-03-02 13:25:21 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-02 13:25:21 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hslls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hslls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:25:21.427: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2703" to be "running and ready"
Mar  2 13:25:21.431: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.979097ms
Mar  2 13:25:21.431: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:25:23.441: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014505127s
Mar  2 13:25:23.441: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar  2 13:25:23.441: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/02/23 13:25:23.441
Mar  2 13:25:23.441: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2703 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:23.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:23.442: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:23.442: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2703/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/02/23 13:25:23.581
Mar  2 13:25:23.581: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2703 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:23.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:23.581: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:23.581: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2703/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 13:25:23.689: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 13:25:23.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2703" for this suite. 03/02/23 13:25:23.74
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":154,"skipped":2823,"failed":0}
------------------------------
â€¢ [2.416 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:21.335
    Mar  2 13:25:21.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename dns 03/02/23 13:25:21.337
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:21.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:21.36
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/02/23 13:25:21.364
    Mar  2 13:25:21.426: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2703  1cf78fb0-7704-4593-a60a-2b539409048c 1945979 0 2023-03-02 13:25:21 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-02 13:25:21 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hslls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hslls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:25:21.427: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2703" to be "running and ready"
    Mar  2 13:25:21.431: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.979097ms
    Mar  2 13:25:21.431: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:25:23.441: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014505127s
    Mar  2 13:25:23.441: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar  2 13:25:23.441: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/02/23 13:25:23.441
    Mar  2 13:25:23.441: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2703 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:23.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:23.442: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:23.442: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2703/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/02/23 13:25:23.581
    Mar  2 13:25:23.581: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2703 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:23.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:23.581: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:23.581: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2703/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 13:25:23.689: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 13:25:23.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2703" for this suite. 03/02/23 13:25:23.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:23.76
Mar  2 13:25:23.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename dns 03/02/23 13:25:23.769
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:23.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:23.81
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/02/23 13:25:23.814
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
 03/02/23 13:25:23.817
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
 03/02/23 13:25:23.818
STEP: creating a pod to probe DNS 03/02/23 13:25:23.818
STEP: submitting the pod to kubernetes 03/02/23 13:25:23.819
Mar  2 13:25:23.836: INFO: Waiting up to 15m0s for pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29" in namespace "dns-6347" to be "running"
Mar  2 13:25:23.847: INFO: Pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29": Phase="Pending", Reason="", readiness=false. Elapsed: 10.097903ms
Mar  2 13:25:25.855: INFO: Pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018880623s
Mar  2 13:25:27.854: INFO: Pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017507728s
Mar  2 13:25:29.854: INFO: Pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29": Phase="Running", Reason="", readiness=true. Elapsed: 6.017504462s
Mar  2 13:25:29.854: INFO: Pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29" satisfied condition "running"
STEP: retrieving the pod 03/02/23 13:25:29.854
STEP: looking for the results for each expected name from probers 03/02/23 13:25:29.861
Mar  2 13:25:29.870: INFO: DNS probes using dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29 succeeded

STEP: deleting the pod 03/02/23 13:25:29.87
STEP: changing the externalName to bar.example.com 03/02/23 13:25:29.913
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
 03/02/23 13:25:29.947
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
 03/02/23 13:25:29.947
STEP: creating a second pod to probe DNS 03/02/23 13:25:29.948
STEP: submitting the pod to kubernetes 03/02/23 13:25:29.948
Mar  2 13:25:29.959: INFO: Waiting up to 15m0s for pod "dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25" in namespace "dns-6347" to be "running"
Mar  2 13:25:29.966: INFO: Pod "dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25": Phase="Pending", Reason="", readiness=false. Elapsed: 7.022353ms
Mar  2 13:25:32.022: INFO: Pod "dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062751281s
Mar  2 13:25:33.981: INFO: Pod "dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25": Phase="Running", Reason="", readiness=true. Elapsed: 4.022234131s
Mar  2 13:25:33.981: INFO: Pod "dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25" satisfied condition "running"
STEP: retrieving the pod 03/02/23 13:25:33.981
STEP: looking for the results for each expected name from probers 03/02/23 13:25:33.986
Mar  2 13:25:34.036: INFO: DNS probes using dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25 succeeded

STEP: deleting the pod 03/02/23 13:25:34.037
STEP: changing the service to type=ClusterIP 03/02/23 13:25:34.054
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
 03/02/23 13:25:34.063
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
 03/02/23 13:25:34.064
STEP: creating a third pod to probe DNS 03/02/23 13:25:34.124
STEP: submitting the pod to kubernetes 03/02/23 13:25:34.148
Mar  2 13:25:34.168: INFO: Waiting up to 15m0s for pod "dns-test-4c3a2276-0102-4092-996c-35dc5090900c" in namespace "dns-6347" to be "running"
Mar  2 13:25:34.260: INFO: Pod "dns-test-4c3a2276-0102-4092-996c-35dc5090900c": Phase="Pending", Reason="", readiness=false. Elapsed: 91.660198ms
Mar  2 13:25:36.352: INFO: Pod "dns-test-4c3a2276-0102-4092-996c-35dc5090900c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1837088s
Mar  2 13:25:38.270: INFO: Pod "dns-test-4c3a2276-0102-4092-996c-35dc5090900c": Phase="Running", Reason="", readiness=true. Elapsed: 4.101492534s
Mar  2 13:25:38.271: INFO: Pod "dns-test-4c3a2276-0102-4092-996c-35dc5090900c" satisfied condition "running"
STEP: retrieving the pod 03/02/23 13:25:38.271
STEP: looking for the results for each expected name from probers 03/02/23 13:25:38.326
Mar  2 13:25:38.335: INFO: DNS probes using dns-test-4c3a2276-0102-4092-996c-35dc5090900c succeeded

STEP: deleting the pod 03/02/23 13:25:38.336
STEP: deleting the test externalName service 03/02/23 13:25:38.352
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 13:25:38.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6347" for this suite. 03/02/23 13:25:38.431
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":155,"skipped":2841,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.682 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:23.76
    Mar  2 13:25:23.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename dns 03/02/23 13:25:23.769
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:23.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:23.81
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/02/23 13:25:23.814
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
     03/02/23 13:25:23.817
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
     03/02/23 13:25:23.818
    STEP: creating a pod to probe DNS 03/02/23 13:25:23.818
    STEP: submitting the pod to kubernetes 03/02/23 13:25:23.819
    Mar  2 13:25:23.836: INFO: Waiting up to 15m0s for pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29" in namespace "dns-6347" to be "running"
    Mar  2 13:25:23.847: INFO: Pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29": Phase="Pending", Reason="", readiness=false. Elapsed: 10.097903ms
    Mar  2 13:25:25.855: INFO: Pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018880623s
    Mar  2 13:25:27.854: INFO: Pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017507728s
    Mar  2 13:25:29.854: INFO: Pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29": Phase="Running", Reason="", readiness=true. Elapsed: 6.017504462s
    Mar  2 13:25:29.854: INFO: Pod "dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 13:25:29.854
    STEP: looking for the results for each expected name from probers 03/02/23 13:25:29.861
    Mar  2 13:25:29.870: INFO: DNS probes using dns-test-ebe8d0eb-32a2-4c81-8e81-55f9b825bf29 succeeded

    STEP: deleting the pod 03/02/23 13:25:29.87
    STEP: changing the externalName to bar.example.com 03/02/23 13:25:29.913
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
     03/02/23 13:25:29.947
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
     03/02/23 13:25:29.947
    STEP: creating a second pod to probe DNS 03/02/23 13:25:29.948
    STEP: submitting the pod to kubernetes 03/02/23 13:25:29.948
    Mar  2 13:25:29.959: INFO: Waiting up to 15m0s for pod "dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25" in namespace "dns-6347" to be "running"
    Mar  2 13:25:29.966: INFO: Pod "dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25": Phase="Pending", Reason="", readiness=false. Elapsed: 7.022353ms
    Mar  2 13:25:32.022: INFO: Pod "dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062751281s
    Mar  2 13:25:33.981: INFO: Pod "dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25": Phase="Running", Reason="", readiness=true. Elapsed: 4.022234131s
    Mar  2 13:25:33.981: INFO: Pod "dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 13:25:33.981
    STEP: looking for the results for each expected name from probers 03/02/23 13:25:33.986
    Mar  2 13:25:34.036: INFO: DNS probes using dns-test-7a403aab-e98a-43f4-8be6-389ba3730b25 succeeded

    STEP: deleting the pod 03/02/23 13:25:34.037
    STEP: changing the service to type=ClusterIP 03/02/23 13:25:34.054
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
     03/02/23 13:25:34.063
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6347.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6347.svc.cluster.local; sleep 1; done
     03/02/23 13:25:34.064
    STEP: creating a third pod to probe DNS 03/02/23 13:25:34.124
    STEP: submitting the pod to kubernetes 03/02/23 13:25:34.148
    Mar  2 13:25:34.168: INFO: Waiting up to 15m0s for pod "dns-test-4c3a2276-0102-4092-996c-35dc5090900c" in namespace "dns-6347" to be "running"
    Mar  2 13:25:34.260: INFO: Pod "dns-test-4c3a2276-0102-4092-996c-35dc5090900c": Phase="Pending", Reason="", readiness=false. Elapsed: 91.660198ms
    Mar  2 13:25:36.352: INFO: Pod "dns-test-4c3a2276-0102-4092-996c-35dc5090900c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1837088s
    Mar  2 13:25:38.270: INFO: Pod "dns-test-4c3a2276-0102-4092-996c-35dc5090900c": Phase="Running", Reason="", readiness=true. Elapsed: 4.101492534s
    Mar  2 13:25:38.271: INFO: Pod "dns-test-4c3a2276-0102-4092-996c-35dc5090900c" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 13:25:38.271
    STEP: looking for the results for each expected name from probers 03/02/23 13:25:38.326
    Mar  2 13:25:38.335: INFO: DNS probes using dns-test-4c3a2276-0102-4092-996c-35dc5090900c succeeded

    STEP: deleting the pod 03/02/23 13:25:38.336
    STEP: deleting the test externalName service 03/02/23 13:25:38.352
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 13:25:38.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6347" for this suite. 03/02/23 13:25:38.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:38.449
Mar  2 13:25:38.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:25:38.451
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:38.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:38.537
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 03/02/23 13:25:38.543
Mar  2 13:25:38.543: INFO: namespace kubectl-4363
Mar  2 13:25:38.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-4363 create -f -'
Mar  2 13:25:40.114: INFO: stderr: ""
Mar  2 13:25:40.115: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/02/23 13:25:40.115
Mar  2 13:25:41.133: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:25:41.133: INFO: Found 0 / 1
Mar  2 13:25:42.120: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:25:42.120: INFO: Found 1 / 1
Mar  2 13:25:42.120: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 13:25:42.125: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:25:42.125: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 13:25:42.125: INFO: wait on agnhost-primary startup in kubectl-4363 
Mar  2 13:25:42.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-4363 logs agnhost-primary-xkx5d agnhost-primary'
Mar  2 13:25:42.282: INFO: stderr: ""
Mar  2 13:25:42.282: INFO: stdout: "Paused\n"
STEP: exposing RC 03/02/23 13:25:42.282
Mar  2 13:25:42.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-4363 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar  2 13:25:42.404: INFO: stderr: ""
Mar  2 13:25:42.404: INFO: stdout: "service/rm2 exposed\n"
Mar  2 13:25:42.410: INFO: Service rm2 in namespace kubectl-4363 found.
STEP: exposing service 03/02/23 13:25:44.422
Mar  2 13:25:44.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-4363 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar  2 13:25:44.557: INFO: stderr: ""
Mar  2 13:25:44.557: INFO: stdout: "service/rm3 exposed\n"
Mar  2 13:25:44.561: INFO: Service rm3 in namespace kubectl-4363 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:25:46.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4363" for this suite. 03/02/23 13:25:46.579
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":156,"skipped":2864,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.138 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:38.449
    Mar  2 13:25:38.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:25:38.451
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:38.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:38.537
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 03/02/23 13:25:38.543
    Mar  2 13:25:38.543: INFO: namespace kubectl-4363
    Mar  2 13:25:38.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-4363 create -f -'
    Mar  2 13:25:40.114: INFO: stderr: ""
    Mar  2 13:25:40.115: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/02/23 13:25:40.115
    Mar  2 13:25:41.133: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:25:41.133: INFO: Found 0 / 1
    Mar  2 13:25:42.120: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:25:42.120: INFO: Found 1 / 1
    Mar  2 13:25:42.120: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar  2 13:25:42.125: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:25:42.125: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  2 13:25:42.125: INFO: wait on agnhost-primary startup in kubectl-4363 
    Mar  2 13:25:42.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-4363 logs agnhost-primary-xkx5d agnhost-primary'
    Mar  2 13:25:42.282: INFO: stderr: ""
    Mar  2 13:25:42.282: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/02/23 13:25:42.282
    Mar  2 13:25:42.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-4363 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar  2 13:25:42.404: INFO: stderr: ""
    Mar  2 13:25:42.404: INFO: stdout: "service/rm2 exposed\n"
    Mar  2 13:25:42.410: INFO: Service rm2 in namespace kubectl-4363 found.
    STEP: exposing service 03/02/23 13:25:44.422
    Mar  2 13:25:44.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-4363 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar  2 13:25:44.557: INFO: stderr: ""
    Mar  2 13:25:44.557: INFO: stdout: "service/rm3 exposed\n"
    Mar  2 13:25:44.561: INFO: Service rm3 in namespace kubectl-4363 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:25:46.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4363" for this suite. 03/02/23 13:25:46.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:46.592
Mar  2 13:25:46.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename controllerrevisions 03/02/23 13:25:46.594
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:46.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:46.613
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-trjjj-daemon-set" 03/02/23 13:25:46.641
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 13:25:46.646
Mar  2 13:25:46.655: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:25:46.659: INFO: Number of nodes with available pods controlled by daemonset e2e-trjjj-daemon-set: 0
Mar  2 13:25:46.660: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 13:25:47.670: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:25:47.704: INFO: Number of nodes with available pods controlled by daemonset e2e-trjjj-daemon-set: 0
Mar  2 13:25:47.704: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 13:25:48.694: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:25:48.703: INFO: Number of nodes with available pods controlled by daemonset e2e-trjjj-daemon-set: 3
Mar  2 13:25:48.703: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 13:25:49.715: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:25:49.721: INFO: Number of nodes with available pods controlled by daemonset e2e-trjjj-daemon-set: 4
Mar  2 13:25:49.721: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-trjjj-daemon-set
STEP: Confirm DaemonSet "e2e-trjjj-daemon-set" successfully created with "daemonset-name=e2e-trjjj-daemon-set" label 03/02/23 13:25:49.724
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-trjjj-daemon-set" 03/02/23 13:25:49.731
Mar  2 13:25:49.736: INFO: Located ControllerRevision: "e2e-trjjj-daemon-set-7dc6f76c99"
STEP: Patching ControllerRevision "e2e-trjjj-daemon-set-7dc6f76c99" 03/02/23 13:25:49.744
Mar  2 13:25:49.757: INFO: e2e-trjjj-daemon-set-7dc6f76c99 has been patched
STEP: Create a new ControllerRevision 03/02/23 13:25:49.757
Mar  2 13:25:49.763: INFO: Created ControllerRevision: e2e-trjjj-daemon-set-748f554687
STEP: Confirm that there are two ControllerRevisions 03/02/23 13:25:49.763
Mar  2 13:25:49.764: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  2 13:25:49.767: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-trjjj-daemon-set-7dc6f76c99" 03/02/23 13:25:49.767
STEP: Confirm that there is only one ControllerRevision 03/02/23 13:25:49.771
Mar  2 13:25:49.772: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  2 13:25:49.802: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-trjjj-daemon-set-748f554687" 03/02/23 13:25:49.805
Mar  2 13:25:49.813: INFO: e2e-trjjj-daemon-set-748f554687 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/02/23 13:25:49.814
W0302 13:25:49.821513      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/02/23 13:25:49.822
Mar  2 13:25:49.823: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  2 13:25:50.827: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  2 13:25:50.835: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-trjjj-daemon-set-748f554687=updated" 03/02/23 13:25:50.835
STEP: Confirm that there is only one ControllerRevision 03/02/23 13:25:50.844
Mar  2 13:25:50.845: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  2 13:25:50.847: INFO: Found 1 ControllerRevisions
Mar  2 13:25:50.850: INFO: ControllerRevision "e2e-trjjj-daemon-set-7655d95486" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-trjjj-daemon-set" 03/02/23 13:25:50.855
STEP: deleting DaemonSet.extensions e2e-trjjj-daemon-set in namespace controllerrevisions-74, will wait for the garbage collector to delete the pods 03/02/23 13:25:50.855
Mar  2 13:25:50.917: INFO: Deleting DaemonSet.extensions e2e-trjjj-daemon-set took: 8.180693ms
Mar  2 13:25:51.023: INFO: Terminating DaemonSet.extensions e2e-trjjj-daemon-set pods took: 105.771591ms
Mar  2 13:25:52.427: INFO: Number of nodes with available pods controlled by daemonset e2e-trjjj-daemon-set: 0
Mar  2 13:25:52.427: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-trjjj-daemon-set
Mar  2 13:25:52.430: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1946485"},"items":null}

Mar  2 13:25:52.433: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1946485"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Mar  2 13:25:52.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-74" for this suite. 03/02/23 13:25:52.464
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":157,"skipped":2889,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.899 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:46.592
    Mar  2 13:25:46.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename controllerrevisions 03/02/23 13:25:46.594
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:46.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:46.613
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-trjjj-daemon-set" 03/02/23 13:25:46.641
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 13:25:46.646
    Mar  2 13:25:46.655: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:25:46.659: INFO: Number of nodes with available pods controlled by daemonset e2e-trjjj-daemon-set: 0
    Mar  2 13:25:46.660: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 13:25:47.670: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:25:47.704: INFO: Number of nodes with available pods controlled by daemonset e2e-trjjj-daemon-set: 0
    Mar  2 13:25:47.704: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 13:25:48.694: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:25:48.703: INFO: Number of nodes with available pods controlled by daemonset e2e-trjjj-daemon-set: 3
    Mar  2 13:25:48.703: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 13:25:49.715: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:25:49.721: INFO: Number of nodes with available pods controlled by daemonset e2e-trjjj-daemon-set: 4
    Mar  2 13:25:49.721: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-trjjj-daemon-set
    STEP: Confirm DaemonSet "e2e-trjjj-daemon-set" successfully created with "daemonset-name=e2e-trjjj-daemon-set" label 03/02/23 13:25:49.724
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-trjjj-daemon-set" 03/02/23 13:25:49.731
    Mar  2 13:25:49.736: INFO: Located ControllerRevision: "e2e-trjjj-daemon-set-7dc6f76c99"
    STEP: Patching ControllerRevision "e2e-trjjj-daemon-set-7dc6f76c99" 03/02/23 13:25:49.744
    Mar  2 13:25:49.757: INFO: e2e-trjjj-daemon-set-7dc6f76c99 has been patched
    STEP: Create a new ControllerRevision 03/02/23 13:25:49.757
    Mar  2 13:25:49.763: INFO: Created ControllerRevision: e2e-trjjj-daemon-set-748f554687
    STEP: Confirm that there are two ControllerRevisions 03/02/23 13:25:49.763
    Mar  2 13:25:49.764: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  2 13:25:49.767: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-trjjj-daemon-set-7dc6f76c99" 03/02/23 13:25:49.767
    STEP: Confirm that there is only one ControllerRevision 03/02/23 13:25:49.771
    Mar  2 13:25:49.772: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  2 13:25:49.802: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-trjjj-daemon-set-748f554687" 03/02/23 13:25:49.805
    Mar  2 13:25:49.813: INFO: e2e-trjjj-daemon-set-748f554687 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/02/23 13:25:49.814
    W0302 13:25:49.821513      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/02/23 13:25:49.822
    Mar  2 13:25:49.823: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  2 13:25:50.827: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  2 13:25:50.835: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-trjjj-daemon-set-748f554687=updated" 03/02/23 13:25:50.835
    STEP: Confirm that there is only one ControllerRevision 03/02/23 13:25:50.844
    Mar  2 13:25:50.845: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  2 13:25:50.847: INFO: Found 1 ControllerRevisions
    Mar  2 13:25:50.850: INFO: ControllerRevision "e2e-trjjj-daemon-set-7655d95486" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-trjjj-daemon-set" 03/02/23 13:25:50.855
    STEP: deleting DaemonSet.extensions e2e-trjjj-daemon-set in namespace controllerrevisions-74, will wait for the garbage collector to delete the pods 03/02/23 13:25:50.855
    Mar  2 13:25:50.917: INFO: Deleting DaemonSet.extensions e2e-trjjj-daemon-set took: 8.180693ms
    Mar  2 13:25:51.023: INFO: Terminating DaemonSet.extensions e2e-trjjj-daemon-set pods took: 105.771591ms
    Mar  2 13:25:52.427: INFO: Number of nodes with available pods controlled by daemonset e2e-trjjj-daemon-set: 0
    Mar  2 13:25:52.427: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-trjjj-daemon-set
    Mar  2 13:25:52.430: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1946485"},"items":null}

    Mar  2 13:25:52.433: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1946485"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 13:25:52.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-74" for this suite. 03/02/23 13:25:52.464
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:52.493
Mar  2 13:25:52.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:25:52.495
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:52.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:52.533
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 03/02/23 13:25:52.536
Mar  2 13:25:52.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2942 cluster-info'
Mar  2 13:25:52.722: INFO: stderr: ""
Mar  2 13:25:52.722: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:25:52.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2942" for this suite. 03/02/23 13:25:52.73
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":158,"skipped":2891,"failed":0}
------------------------------
â€¢ [0.250 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:52.493
    Mar  2 13:25:52.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:25:52.495
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:52.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:52.533
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 03/02/23 13:25:52.536
    Mar  2 13:25:52.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2942 cluster-info'
    Mar  2 13:25:52.722: INFO: stderr: ""
    Mar  2 13:25:52.722: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:25:52.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2942" for this suite. 03/02/23 13:25:52.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:52.748
Mar  2 13:25:52.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/02/23 13:25:52.75
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:52.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:52.776
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/02/23 13:25:52.781
STEP: Creating hostNetwork=false pod 03/02/23 13:25:52.782
Mar  2 13:25:52.829: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8760" to be "running and ready"
Mar  2 13:25:52.834: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.303987ms
Mar  2 13:25:52.835: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:25:54.843: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013602676s
Mar  2 13:25:54.843: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:25:56.838: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009114324s
Mar  2 13:25:56.838: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar  2 13:25:56.838: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/02/23 13:25:56.841
Mar  2 13:25:56.847: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8760" to be "running and ready"
Mar  2 13:25:56.850: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.64728ms
Mar  2 13:25:56.850: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:25:58.864: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01744447s
Mar  2 13:25:58.864: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar  2 13:25:58.864: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/02/23 13:25:58.867
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/02/23 13:25:58.867
Mar  2 13:25:58.867: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:58.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:58.868: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:58.868: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 13:25:58.957: INFO: Exec stderr: ""
Mar  2 13:25:58.957: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:58.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:58.957: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:58.957: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 13:25:59.060: INFO: Exec stderr: ""
Mar  2 13:25:59.060: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:59.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:59.061: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:59.061: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 13:25:59.165: INFO: Exec stderr: ""
Mar  2 13:25:59.166: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:59.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:59.167: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:59.167: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 13:25:59.279: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/02/23 13:25:59.28
Mar  2 13:25:59.280: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:59.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:59.283: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:59.283: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar  2 13:25:59.364: INFO: Exec stderr: ""
Mar  2 13:25:59.365: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:59.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:59.367: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:59.367: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar  2 13:25:59.459: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/02/23 13:25:59.459
Mar  2 13:25:59.459: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:59.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:59.460: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:59.460: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 13:25:59.544: INFO: Exec stderr: ""
Mar  2 13:25:59.544: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:59.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:59.546: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:59.546: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 13:25:59.645: INFO: Exec stderr: ""
Mar  2 13:25:59.645: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:59.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:59.646: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:59.646: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 13:25:59.758: INFO: Exec stderr: ""
Mar  2 13:25:59.758: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:25:59.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:25:59.764: INFO: ExecWithOptions: Clientset creation
Mar  2 13:25:59.765: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 13:25:59.937: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Mar  2 13:25:59.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8760" for this suite. 03/02/23 13:25:59.96
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":159,"skipped":2950,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.221 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:52.748
    Mar  2 13:25:52.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/02/23 13:25:52.75
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:25:52.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:25:52.776
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/02/23 13:25:52.781
    STEP: Creating hostNetwork=false pod 03/02/23 13:25:52.782
    Mar  2 13:25:52.829: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8760" to be "running and ready"
    Mar  2 13:25:52.834: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.303987ms
    Mar  2 13:25:52.835: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:25:54.843: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013602676s
    Mar  2 13:25:54.843: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:25:56.838: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009114324s
    Mar  2 13:25:56.838: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar  2 13:25:56.838: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/02/23 13:25:56.841
    Mar  2 13:25:56.847: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8760" to be "running and ready"
    Mar  2 13:25:56.850: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.64728ms
    Mar  2 13:25:56.850: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:25:58.864: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01744447s
    Mar  2 13:25:58.864: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar  2 13:25:58.864: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/02/23 13:25:58.867
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/02/23 13:25:58.867
    Mar  2 13:25:58.867: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:58.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:58.868: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:58.868: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  2 13:25:58.957: INFO: Exec stderr: ""
    Mar  2 13:25:58.957: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:58.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:58.957: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:58.957: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  2 13:25:59.060: INFO: Exec stderr: ""
    Mar  2 13:25:59.060: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:59.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:59.061: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:59.061: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  2 13:25:59.165: INFO: Exec stderr: ""
    Mar  2 13:25:59.166: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:59.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:59.167: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:59.167: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  2 13:25:59.279: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/02/23 13:25:59.28
    Mar  2 13:25:59.280: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:59.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:59.283: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:59.283: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar  2 13:25:59.364: INFO: Exec stderr: ""
    Mar  2 13:25:59.365: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:59.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:59.367: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:59.367: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar  2 13:25:59.459: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/02/23 13:25:59.459
    Mar  2 13:25:59.459: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:59.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:59.460: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:59.460: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  2 13:25:59.544: INFO: Exec stderr: ""
    Mar  2 13:25:59.544: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:59.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:59.546: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:59.546: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  2 13:25:59.645: INFO: Exec stderr: ""
    Mar  2 13:25:59.645: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:59.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:59.646: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:59.646: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  2 13:25:59.758: INFO: Exec stderr: ""
    Mar  2 13:25:59.758: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8760 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:25:59.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:25:59.764: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:25:59.765: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8760/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  2 13:25:59.937: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Mar  2 13:25:59.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-8760" for this suite. 03/02/23 13:25:59.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:25:59.972
Mar  2 13:25:59.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:25:59.973
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:26:00.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:26:00.009
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-722314bd-714e-42b9-91eb-3e4e218b58bc 03/02/23 13:26:00.034
STEP: Creating configMap with name cm-test-opt-upd-b3340794-7c88-4a4a-971a-329d09028db0 03/02/23 13:26:00.043
STEP: Creating the pod 03/02/23 13:26:00.049
Mar  2 13:26:00.059: INFO: Waiting up to 5m0s for pod "pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721" in namespace "configmap-3413" to be "running and ready"
Mar  2 13:26:00.063: INFO: Pod "pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924362ms
Mar  2 13:26:00.063: INFO: The phase of Pod pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:26:02.068: INFO: Pod "pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009128799s
Mar  2 13:26:02.068: INFO: The phase of Pod pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:26:04.068: INFO: Pod "pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721": Phase="Running", Reason="", readiness=true. Elapsed: 4.009029318s
Mar  2 13:26:04.068: INFO: The phase of Pod pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721 is Running (Ready = true)
Mar  2 13:26:04.068: INFO: Pod "pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-722314bd-714e-42b9-91eb-3e4e218b58bc 03/02/23 13:26:04.091
STEP: Updating configmap cm-test-opt-upd-b3340794-7c88-4a4a-971a-329d09028db0 03/02/23 13:26:04.096
STEP: Creating configMap with name cm-test-opt-create-0f7b801c-a8fb-4f74-bc0c-025836727050 03/02/23 13:26:04.1
STEP: waiting to observe update in volume 03/02/23 13:26:04.103
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:27:06.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3413" for this suite. 03/02/23 13:27:06.755
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":160,"skipped":3001,"failed":0}
------------------------------
â€¢ [SLOW TEST] [66.791 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:25:59.972
    Mar  2 13:25:59.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:25:59.973
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:26:00.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:26:00.009
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-722314bd-714e-42b9-91eb-3e4e218b58bc 03/02/23 13:26:00.034
    STEP: Creating configMap with name cm-test-opt-upd-b3340794-7c88-4a4a-971a-329d09028db0 03/02/23 13:26:00.043
    STEP: Creating the pod 03/02/23 13:26:00.049
    Mar  2 13:26:00.059: INFO: Waiting up to 5m0s for pod "pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721" in namespace "configmap-3413" to be "running and ready"
    Mar  2 13:26:00.063: INFO: Pod "pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924362ms
    Mar  2 13:26:00.063: INFO: The phase of Pod pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:26:02.068: INFO: Pod "pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009128799s
    Mar  2 13:26:02.068: INFO: The phase of Pod pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:26:04.068: INFO: Pod "pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721": Phase="Running", Reason="", readiness=true. Elapsed: 4.009029318s
    Mar  2 13:26:04.068: INFO: The phase of Pod pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721 is Running (Ready = true)
    Mar  2 13:26:04.068: INFO: Pod "pod-configmaps-56e77b24-1925-4116-a34c-5e7e3ef35721" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-722314bd-714e-42b9-91eb-3e4e218b58bc 03/02/23 13:26:04.091
    STEP: Updating configmap cm-test-opt-upd-b3340794-7c88-4a4a-971a-329d09028db0 03/02/23 13:26:04.096
    STEP: Creating configMap with name cm-test-opt-create-0f7b801c-a8fb-4f74-bc0c-025836727050 03/02/23 13:26:04.1
    STEP: waiting to observe update in volume 03/02/23 13:26:04.103
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:27:06.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3413" for this suite. 03/02/23 13:27:06.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:27:06.772
Mar  2 13:27:06.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:27:06.777
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:27:06.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:27:06.805
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-7954 03/02/23 13:27:06.811
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7954 to expose endpoints map[] 03/02/23 13:27:06.819
Mar  2 13:27:06.840: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Mar  2 13:27:07.856: INFO: successfully validated that service endpoint-test2 in namespace services-7954 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7954 03/02/23 13:27:07.856
Mar  2 13:27:07.893: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7954" to be "running and ready"
Mar  2 13:27:07.900: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.48627ms
Mar  2 13:27:07.900: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:27:09.916: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.022405261s
Mar  2 13:27:09.916: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  2 13:27:09.916: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7954 to expose endpoints map[pod1:[80]] 03/02/23 13:27:09.922
Mar  2 13:27:09.932: INFO: successfully validated that service endpoint-test2 in namespace services-7954 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/02/23 13:27:09.932
Mar  2 13:27:09.933: INFO: Creating new exec pod
Mar  2 13:27:09.941: INFO: Waiting up to 5m0s for pod "execpod7djt4" in namespace "services-7954" to be "running"
Mar  2 13:27:09.945: INFO: Pod "execpod7djt4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.581669ms
Mar  2 13:27:11.950: INFO: Pod "execpod7djt4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00888739s
Mar  2 13:27:11.950: INFO: Pod "execpod7djt4" satisfied condition "running"
Mar  2 13:27:12.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 13:27:13.255: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 13:27:13.255: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:27:13.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.214 80'
Mar  2 13:27:13.503: INFO: stderr: "+ nc -v -t -w 2 10.233.4.214 80\n+ echo hostName\nConnection to 10.233.4.214 80 port [tcp/http] succeeded!\n"
Mar  2 13:27:13.503: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-7954 03/02/23 13:27:13.504
Mar  2 13:27:13.519: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7954" to be "running and ready"
Mar  2 13:27:13.525: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.979993ms
Mar  2 13:27:13.525: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:27:15.532: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012857334s
Mar  2 13:27:15.532: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  2 13:27:15.532: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7954 to expose endpoints map[pod1:[80] pod2:[80]] 03/02/23 13:27:15.537
Mar  2 13:27:15.550: INFO: successfully validated that service endpoint-test2 in namespace services-7954 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/02/23 13:27:15.55
Mar  2 13:27:16.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 13:27:16.781: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 13:27:16.781: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:27:16.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.214 80'
Mar  2 13:27:16.979: INFO: stderr: "+ nc -v -t -w 2 10.233.4.214 80\n+ echo hostName\nConnection to 10.233.4.214 80 port [tcp/http] succeeded!\n"
Mar  2 13:27:16.979: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7954 03/02/23 13:27:16.979
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7954 to expose endpoints map[pod2:[80]] 03/02/23 13:27:17.004
Mar  2 13:27:18.073: INFO: successfully validated that service endpoint-test2 in namespace services-7954 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/02/23 13:27:18.073
Mar  2 13:27:19.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 13:27:19.282: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 13:27:19.282: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:27:19.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.214 80'
Mar  2 13:27:19.446: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.214 80\nConnection to 10.233.4.214 80 port [tcp/http] succeeded!\n"
Mar  2 13:27:19.446: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-7954 03/02/23 13:27:19.446
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7954 to expose endpoints map[] 03/02/23 13:27:19.467
Mar  2 13:27:19.495: INFO: successfully validated that service endpoint-test2 in namespace services-7954 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:27:19.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7954" for this suite. 03/02/23 13:27:19.526
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":161,"skipped":3009,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.761 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:27:06.772
    Mar  2 13:27:06.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:27:06.777
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:27:06.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:27:06.805
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-7954 03/02/23 13:27:06.811
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7954 to expose endpoints map[] 03/02/23 13:27:06.819
    Mar  2 13:27:06.840: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Mar  2 13:27:07.856: INFO: successfully validated that service endpoint-test2 in namespace services-7954 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7954 03/02/23 13:27:07.856
    Mar  2 13:27:07.893: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7954" to be "running and ready"
    Mar  2 13:27:07.900: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.48627ms
    Mar  2 13:27:07.900: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:27:09.916: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.022405261s
    Mar  2 13:27:09.916: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  2 13:27:09.916: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7954 to expose endpoints map[pod1:[80]] 03/02/23 13:27:09.922
    Mar  2 13:27:09.932: INFO: successfully validated that service endpoint-test2 in namespace services-7954 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/02/23 13:27:09.932
    Mar  2 13:27:09.933: INFO: Creating new exec pod
    Mar  2 13:27:09.941: INFO: Waiting up to 5m0s for pod "execpod7djt4" in namespace "services-7954" to be "running"
    Mar  2 13:27:09.945: INFO: Pod "execpod7djt4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.581669ms
    Mar  2 13:27:11.950: INFO: Pod "execpod7djt4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00888739s
    Mar  2 13:27:11.950: INFO: Pod "execpod7djt4" satisfied condition "running"
    Mar  2 13:27:12.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  2 13:27:13.255: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  2 13:27:13.255: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:27:13.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.214 80'
    Mar  2 13:27:13.503: INFO: stderr: "+ nc -v -t -w 2 10.233.4.214 80\n+ echo hostName\nConnection to 10.233.4.214 80 port [tcp/http] succeeded!\n"
    Mar  2 13:27:13.503: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-7954 03/02/23 13:27:13.504
    Mar  2 13:27:13.519: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7954" to be "running and ready"
    Mar  2 13:27:13.525: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.979993ms
    Mar  2 13:27:13.525: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:27:15.532: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012857334s
    Mar  2 13:27:15.532: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  2 13:27:15.532: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7954 to expose endpoints map[pod1:[80] pod2:[80]] 03/02/23 13:27:15.537
    Mar  2 13:27:15.550: INFO: successfully validated that service endpoint-test2 in namespace services-7954 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/02/23 13:27:15.55
    Mar  2 13:27:16.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  2 13:27:16.781: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  2 13:27:16.781: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:27:16.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.214 80'
    Mar  2 13:27:16.979: INFO: stderr: "+ nc -v -t -w 2 10.233.4.214 80\n+ echo hostName\nConnection to 10.233.4.214 80 port [tcp/http] succeeded!\n"
    Mar  2 13:27:16.979: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-7954 03/02/23 13:27:16.979
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7954 to expose endpoints map[pod2:[80]] 03/02/23 13:27:17.004
    Mar  2 13:27:18.073: INFO: successfully validated that service endpoint-test2 in namespace services-7954 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/02/23 13:27:18.073
    Mar  2 13:27:19.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  2 13:27:19.282: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  2 13:27:19.282: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:27:19.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-7954 exec execpod7djt4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.214 80'
    Mar  2 13:27:19.446: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.214 80\nConnection to 10.233.4.214 80 port [tcp/http] succeeded!\n"
    Mar  2 13:27:19.446: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-7954 03/02/23 13:27:19.446
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7954 to expose endpoints map[] 03/02/23 13:27:19.467
    Mar  2 13:27:19.495: INFO: successfully validated that service endpoint-test2 in namespace services-7954 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:27:19.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7954" for this suite. 03/02/23 13:27:19.526
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:27:19.533
Mar  2 13:27:19.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 13:27:19.535
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:27:19.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:27:19.562
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 13:27:19.602
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:27:20.298
STEP: Deploying the webhook pod 03/02/23 13:27:20.308
STEP: Wait for the deployment to be ready 03/02/23 13:27:20.33
Mar  2 13:27:20.341: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 13:27:22.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 13, 27, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 27, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 27, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 27, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 13:27:24.359
STEP: Verifying the service has paired with the endpoint 03/02/23 13:27:24.425
Mar  2 13:27:25.426: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 03/02/23 13:27:25.43
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/02/23 13:27:25.451
STEP: Creating a configMap that should not be mutated 03/02/23 13:27:25.456
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/02/23 13:27:25.469
STEP: Creating a configMap that should be mutated 03/02/23 13:27:25.522
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:27:25.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6648" for this suite. 03/02/23 13:27:25.565
STEP: Destroying namespace "webhook-6648-markers" for this suite. 03/02/23 13:27:25.59
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":162,"skipped":3014,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.234 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:27:19.533
    Mar  2 13:27:19.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 13:27:19.535
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:27:19.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:27:19.562
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 13:27:19.602
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:27:20.298
    STEP: Deploying the webhook pod 03/02/23 13:27:20.308
    STEP: Wait for the deployment to be ready 03/02/23 13:27:20.33
    Mar  2 13:27:20.341: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 13:27:22.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 13, 27, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 27, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 27, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 27, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 13:27:24.359
    STEP: Verifying the service has paired with the endpoint 03/02/23 13:27:24.425
    Mar  2 13:27:25.426: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 03/02/23 13:27:25.43
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/02/23 13:27:25.451
    STEP: Creating a configMap that should not be mutated 03/02/23 13:27:25.456
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/02/23 13:27:25.469
    STEP: Creating a configMap that should be mutated 03/02/23 13:27:25.522
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:27:25.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6648" for this suite. 03/02/23 13:27:25.565
    STEP: Destroying namespace "webhook-6648-markers" for this suite. 03/02/23 13:27:25.59
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:27:25.77
Mar  2 13:27:25.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename dns 03/02/23 13:27:25.771
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:27:25.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:27:25.868
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/02/23 13:27:25.882
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/02/23 13:27:25.882
STEP: creating a pod to probe DNS 03/02/23 13:27:25.882
STEP: submitting the pod to kubernetes 03/02/23 13:27:25.882
Mar  2 13:27:25.903: INFO: Waiting up to 15m0s for pod "dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5" in namespace "dns-6760" to be "running"
Mar  2 13:27:25.958: INFO: Pod "dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.648621ms
Mar  2 13:27:27.965: INFO: Pod "dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061463222s
Mar  2 13:27:29.964: INFO: Pod "dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5": Phase="Running", Reason="", readiness=true. Elapsed: 4.060576058s
Mar  2 13:27:29.964: INFO: Pod "dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5" satisfied condition "running"
STEP: retrieving the pod 03/02/23 13:27:29.964
STEP: looking for the results for each expected name from probers 03/02/23 13:27:29.968
Mar  2 13:27:30.030: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:30.037: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:30.042: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:30.049: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:30.049: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:27:35.087: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:35.105: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:35.122: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:35.135: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:35.135: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:27:40.057: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:40.062: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:40.066: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:40.070: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:40.071: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:27:45.056: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:45.062: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:45.071: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:45.079: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:45.082: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:27:50.066: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:50.075: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:50.096: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:27:55.059: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:55.064: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:27:55.077: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:28:00.068: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:00.075: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:00.091: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:28:05.055: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:05.059: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:05.069: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:28:10.057: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:10.062: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:10.079: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:28:15.063: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:15.068: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:15.081: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:28:20.056: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:20.062: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:20.068: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:28:25.066: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:25.071: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
Mar  2 13:28:25.079: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

Mar  2 13:28:30.083: INFO: DNS probes using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 succeeded

STEP: deleting the pod 03/02/23 13:28:30.083
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 13:28:30.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6760" for this suite. 03/02/23 13:28:30.144
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":163,"skipped":3032,"failed":0}
------------------------------
â€¢ [SLOW TEST] [64.380 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:27:25.77
    Mar  2 13:27:25.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename dns 03/02/23 13:27:25.771
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:27:25.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:27:25.868
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/02/23 13:27:25.882
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/02/23 13:27:25.882
    STEP: creating a pod to probe DNS 03/02/23 13:27:25.882
    STEP: submitting the pod to kubernetes 03/02/23 13:27:25.882
    Mar  2 13:27:25.903: INFO: Waiting up to 15m0s for pod "dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5" in namespace "dns-6760" to be "running"
    Mar  2 13:27:25.958: INFO: Pod "dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.648621ms
    Mar  2 13:27:27.965: INFO: Pod "dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061463222s
    Mar  2 13:27:29.964: INFO: Pod "dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5": Phase="Running", Reason="", readiness=true. Elapsed: 4.060576058s
    Mar  2 13:27:29.964: INFO: Pod "dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 13:27:29.964
    STEP: looking for the results for each expected name from probers 03/02/23 13:27:29.968
    Mar  2 13:27:30.030: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:30.037: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:30.042: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:30.049: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:30.049: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:27:35.087: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:35.105: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:35.122: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:35.135: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:35.135: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:27:40.057: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:40.062: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:40.066: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:40.070: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:40.071: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:27:45.056: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:45.062: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:45.071: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:45.079: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:45.082: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:27:50.066: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:50.075: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:50.096: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:27:55.059: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:55.064: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:27:55.077: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:28:00.068: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:00.075: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:00.091: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:28:05.055: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:05.059: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:05.069: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:28:10.057: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:10.062: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:10.079: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:28:15.063: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:15.068: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:15.081: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:28:20.056: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:20.062: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:20.068: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:28:25.066: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:25.071: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5: the server could not find the requested resource (get pods dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5)
    Mar  2 13:28:25.079: INFO: Lookups using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local]

    Mar  2 13:28:30.083: INFO: DNS probes using dns-6760/dns-test-75bcface-73e8-4497-b92f-4a66d95fcef5 succeeded

    STEP: deleting the pod 03/02/23 13:28:30.083
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 13:28:30.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6760" for this suite. 03/02/23 13:28:30.144
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:28:30.157
Mar  2 13:28:30.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename events 03/02/23 13:28:30.161
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:30.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:30.26
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/02/23 13:28:30.282
STEP: listing events in all namespaces 03/02/23 13:28:30.294
STEP: listing events in test namespace 03/02/23 13:28:30.327
STEP: listing events with field selection filtering on source 03/02/23 13:28:30.331
STEP: listing events with field selection filtering on reportingController 03/02/23 13:28:30.338
STEP: getting the test event 03/02/23 13:28:30.342
STEP: patching the test event 03/02/23 13:28:30.345
STEP: getting the test event 03/02/23 13:28:30.36
STEP: updating the test event 03/02/23 13:28:30.364
STEP: getting the test event 03/02/23 13:28:30.381
STEP: deleting the test event 03/02/23 13:28:30.385
STEP: listing events in all namespaces 03/02/23 13:28:30.392
STEP: listing events in test namespace 03/02/23 13:28:30.416
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar  2 13:28:30.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-970" for this suite. 03/02/23 13:28:30.433
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":164,"skipped":3035,"failed":0}
------------------------------
â€¢ [0.295 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:28:30.157
    Mar  2 13:28:30.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename events 03/02/23 13:28:30.161
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:30.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:30.26
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/02/23 13:28:30.282
    STEP: listing events in all namespaces 03/02/23 13:28:30.294
    STEP: listing events in test namespace 03/02/23 13:28:30.327
    STEP: listing events with field selection filtering on source 03/02/23 13:28:30.331
    STEP: listing events with field selection filtering on reportingController 03/02/23 13:28:30.338
    STEP: getting the test event 03/02/23 13:28:30.342
    STEP: patching the test event 03/02/23 13:28:30.345
    STEP: getting the test event 03/02/23 13:28:30.36
    STEP: updating the test event 03/02/23 13:28:30.364
    STEP: getting the test event 03/02/23 13:28:30.381
    STEP: deleting the test event 03/02/23 13:28:30.385
    STEP: listing events in all namespaces 03/02/23 13:28:30.392
    STEP: listing events in test namespace 03/02/23 13:28:30.416
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar  2 13:28:30.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-970" for this suite. 03/02/23 13:28:30.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:28:30.456
Mar  2 13:28:30.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:28:30.458
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:30.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:30.527
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-6569/configmap-test-8428998f-c5f4-44cf-a7aa-bb6231a0c919 03/02/23 13:28:30.53
STEP: Creating a pod to test consume configMaps 03/02/23 13:28:30.535
Mar  2 13:28:30.546: INFO: Waiting up to 5m0s for pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70" in namespace "configmap-6569" to be "Succeeded or Failed"
Mar  2 13:28:30.556: INFO: Pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70": Phase="Pending", Reason="", readiness=false. Elapsed: 9.991765ms
Mar  2 13:28:32.572: INFO: Pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026410762s
Mar  2 13:28:34.583: INFO: Pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037434184s
Mar  2 13:28:36.622: INFO: Pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076245457s
STEP: Saw pod success 03/02/23 13:28:36.622
Mar  2 13:28:36.623: INFO: Pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70" satisfied condition "Succeeded or Failed"
Mar  2 13:28:36.630: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70 container env-test: <nil>
STEP: delete the pod 03/02/23 13:28:36.636
Mar  2 13:28:36.651: INFO: Waiting for pod pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70 to disappear
Mar  2 13:28:36.654: INFO: Pod pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:28:36.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6569" for this suite. 03/02/23 13:28:36.66
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":165,"skipped":3050,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.213 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:28:30.456
    Mar  2 13:28:30.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:28:30.458
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:30.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:30.527
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-6569/configmap-test-8428998f-c5f4-44cf-a7aa-bb6231a0c919 03/02/23 13:28:30.53
    STEP: Creating a pod to test consume configMaps 03/02/23 13:28:30.535
    Mar  2 13:28:30.546: INFO: Waiting up to 5m0s for pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70" in namespace "configmap-6569" to be "Succeeded or Failed"
    Mar  2 13:28:30.556: INFO: Pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70": Phase="Pending", Reason="", readiness=false. Elapsed: 9.991765ms
    Mar  2 13:28:32.572: INFO: Pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026410762s
    Mar  2 13:28:34.583: INFO: Pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037434184s
    Mar  2 13:28:36.622: INFO: Pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076245457s
    STEP: Saw pod success 03/02/23 13:28:36.622
    Mar  2 13:28:36.623: INFO: Pod "pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70" satisfied condition "Succeeded or Failed"
    Mar  2 13:28:36.630: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70 container env-test: <nil>
    STEP: delete the pod 03/02/23 13:28:36.636
    Mar  2 13:28:36.651: INFO: Waiting for pod pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70 to disappear
    Mar  2 13:28:36.654: INFO: Pod pod-configmaps-6173cbf6-30f3-4e94-981c-a847471e8f70 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:28:36.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6569" for this suite. 03/02/23 13:28:36.66
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:28:36.724
Mar  2 13:28:36.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 13:28:36.725
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:36.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:36.749
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 03/02/23 13:28:36.752
Mar  2 13:28:36.761: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22" in namespace "downward-api-2090" to be "Succeeded or Failed"
Mar  2 13:28:36.764: INFO: Pod "downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22": Phase="Pending", Reason="", readiness=false. Elapsed: 3.00439ms
Mar  2 13:28:38.770: INFO: Pod "downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00903168s
Mar  2 13:28:40.802: INFO: Pod "downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040604708s
STEP: Saw pod success 03/02/23 13:28:40.802
Mar  2 13:28:40.802: INFO: Pod "downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22" satisfied condition "Succeeded or Failed"
Mar  2 13:28:40.811: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22 container client-container: <nil>
STEP: delete the pod 03/02/23 13:28:40.821
Mar  2 13:28:40.846: INFO: Waiting for pod downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22 to disappear
Mar  2 13:28:40.849: INFO: Pod downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 13:28:40.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2090" for this suite. 03/02/23 13:28:40.857
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":166,"skipped":3052,"failed":0}
------------------------------
â€¢ [4.141 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:28:36.724
    Mar  2 13:28:36.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 13:28:36.725
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:36.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:36.749
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 03/02/23 13:28:36.752
    Mar  2 13:28:36.761: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22" in namespace "downward-api-2090" to be "Succeeded or Failed"
    Mar  2 13:28:36.764: INFO: Pod "downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22": Phase="Pending", Reason="", readiness=false. Elapsed: 3.00439ms
    Mar  2 13:28:38.770: INFO: Pod "downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00903168s
    Mar  2 13:28:40.802: INFO: Pod "downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040604708s
    STEP: Saw pod success 03/02/23 13:28:40.802
    Mar  2 13:28:40.802: INFO: Pod "downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22" satisfied condition "Succeeded or Failed"
    Mar  2 13:28:40.811: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22 container client-container: <nil>
    STEP: delete the pod 03/02/23 13:28:40.821
    Mar  2 13:28:40.846: INFO: Waiting for pod downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22 to disappear
    Mar  2 13:28:40.849: INFO: Pod downwardapi-volume-24dba461-5b97-4276-84f4-d899acf96d22 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 13:28:40.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2090" for this suite. 03/02/23 13:28:40.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:28:40.868
Mar  2 13:28:40.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 13:28:40.869
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:40.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:40.907
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar  2 13:28:40.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:28:49.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9862" for this suite. 03/02/23 13:28:49.115
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":167,"skipped":3082,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.259 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:28:40.868
    Mar  2 13:28:40.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 13:28:40.869
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:40.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:40.907
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar  2 13:28:40.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:28:49.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9862" for this suite. 03/02/23 13:28:49.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:28:49.131
Mar  2 13:28:49.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 13:28:49.132
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:49.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:49.164
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-010db39c-2fea-4077-8f26-0883d9d8f0e8 03/02/23 13:28:49.168
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  2 13:28:49.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1530" for this suite. 03/02/23 13:28:49.175
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":168,"skipped":3096,"failed":0}
------------------------------
â€¢ [0.107 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:28:49.131
    Mar  2 13:28:49.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 13:28:49.132
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:49.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:49.164
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-010db39c-2fea-4077-8f26-0883d9d8f0e8 03/02/23 13:28:49.168
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 13:28:49.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1530" for this suite. 03/02/23 13:28:49.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:28:49.248
Mar  2 13:28:49.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename disruption 03/02/23 13:28:49.249
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:49.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:49.369
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 03/02/23 13:28:49.38
STEP: Updating PodDisruptionBudget status 03/02/23 13:28:49.389
STEP: Waiting for all pods to be running 03/02/23 13:28:49.403
Mar  2 13:28:49.415: INFO: running pods: 0 < 1
STEP: locating a running pod 03/02/23 13:28:51.421
STEP: Waiting for the pdb to be processed 03/02/23 13:28:51.435
STEP: Patching PodDisruptionBudget status 03/02/23 13:28:51.446
STEP: Waiting for the pdb to be processed 03/02/23 13:28:51.47
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  2 13:28:51.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3477" for this suite. 03/02/23 13:28:51.478
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":169,"skipped":3105,"failed":0}
------------------------------
â€¢ [2.278 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:28:49.248
    Mar  2 13:28:49.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename disruption 03/02/23 13:28:49.249
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:49.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:49.369
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 03/02/23 13:28:49.38
    STEP: Updating PodDisruptionBudget status 03/02/23 13:28:49.389
    STEP: Waiting for all pods to be running 03/02/23 13:28:49.403
    Mar  2 13:28:49.415: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/02/23 13:28:51.421
    STEP: Waiting for the pdb to be processed 03/02/23 13:28:51.435
    STEP: Patching PodDisruptionBudget status 03/02/23 13:28:51.446
    STEP: Waiting for the pdb to be processed 03/02/23 13:28:51.47
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  2 13:28:51.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3477" for this suite. 03/02/23 13:28:51.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:28:51.536
Mar  2 13:28:51.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename daemonsets 03/02/23 13:28:51.537
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:51.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:51.565
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 03/02/23 13:28:51.617
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 13:28:51.621
Mar  2 13:28:51.627: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:28:51.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:28:51.630: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 13:28:52.643: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:28:52.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 13:28:52.647: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 13:28:53.638: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:28:53.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 13:28:53.646: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 13:28:54.641: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 13:28:54.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Mar  2 13:28:54.650: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: listing all DeamonSets 03/02/23 13:28:54.656
STEP: DeleteCollection of the DaemonSets 03/02/23 13:28:54.661
STEP: Verify that ReplicaSets have been deleted 03/02/23 13:28:54.666
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Mar  2 13:28:54.740: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1947817"},"items":null}

Mar  2 13:28:54.744: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1947817"},"items":[{"metadata":{"name":"daemon-set-84kz7","generateName":"daemon-set-","namespace":"daemonsets-3139","uid":"aa55edda-e008-4822-aa6f-bfc341f8914d","resourceVersion":"1947815","creationTimestamp":"2023-03-02T13:28:51Z","deletionTimestamp":"2023-03-02T13:29:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"105088d5e96c8babef034c002c22f83055d942570db486c3582d4cf311b10993","cni.projectcalico.org/podIP":"10.233.92.84/32","cni.projectcalico.org/podIPs":"10.233.92.84/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0ec2cf79-e06e-4335-97a7-dc7eb8308904","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ec2cf79-e06e-4335-97a7-dc7eb8308904\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.92.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-d45g8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-d45g8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aarnq-sc-k8s-node-srv1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aarnq-sc-k8s-node-srv1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:52Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:52Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"}],"hostIP":"172.16.0.138","podIP":"10.233.92.84","podIPs":[{"ip":"10.233.92.84"}],"startTime":"2023-03-02T13:28:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T13:28:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://42f7be715c10b97dcc1b3ce1a97d7073c264db7f8b7c3b75717c0606709c757c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-cwm9n","generateName":"daemon-set-","namespace":"daemonsets-3139","uid":"ead2018d-7a61-48bc-861b-07bde9dd8e86","resourceVersion":"1947816","creationTimestamp":"2023-03-02T13:28:51Z","deletionTimestamp":"2023-03-02T13:29:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d1a77c4c5286bd14d58b126c24ee5f8d0b20211ff9b3188501b718c50b594ba3","cni.projectcalico.org/podIP":"10.233.123.29/32","cni.projectcalico.org/podIPs":"10.233.123.29/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0ec2cf79-e06e-4335-97a7-dc7eb8308904","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ec2cf79-e06e-4335-97a7-dc7eb8308904\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-czm96","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-czm96","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aarnq-sc-k8s-node-srv0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aarnq-sc-k8s-node-srv0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"}],"hostIP":"172.16.0.61","podIP":"10.233.123.29","podIPs":[{"ip":"10.233.123.29"}],"startTime":"2023-03-02T13:28:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T13:28:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://1633b157b8f443abf806d26febd9a8b00394ff12461abec0ec49189e0e46439a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jf4hl","generateName":"daemon-set-","namespace":"daemonsets-3139","uid":"2bc66627-2fd1-443e-9349-b09775abd45f","resourceVersion":"1947814","creationTimestamp":"2023-03-02T13:28:51Z","deletionTimestamp":"2023-03-02T13:29:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"e7349a57bdc8bc60054087b10acd3e55e11e3362e40fa20e3f1bf69d3cb186ca","cni.projectcalico.org/podIP":"10.233.126.120/32","cni.projectcalico.org/podIPs":"10.233.126.120/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0ec2cf79-e06e-4335-97a7-dc7eb8308904","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ec2cf79-e06e-4335-97a7-dc7eb8308904\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.126.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ffj6t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ffj6t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aarnq-sc-k8s-node-srv3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aarnq-sc-k8s-node-srv3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"}],"hostIP":"172.16.0.56","podIP":"10.233.126.120","podIPs":[{"ip":"10.233.126.120"}],"startTime":"2023-03-02T13:28:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T13:28:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://b71a528a297fa6271b9e5a5b7bf14169242266747cf9a929d7466c71662b2817","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rzncm","generateName":"daemon-set-","namespace":"daemonsets-3139","uid":"9c221e5c-06f3-4bf1-88a4-4508ca11f343","resourceVersion":"1947817","creationTimestamp":"2023-03-02T13:28:51Z","deletionTimestamp":"2023-03-02T13:29:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3c08a7472389d91061f9f2f161ff20c9a50ba0a9b249238f3ed7ba639cc0c645","cni.projectcalico.org/podIP":"10.233.123.123/32","cni.projectcalico.org/podIPs":"10.233.123.123/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0ec2cf79-e06e-4335-97a7-dc7eb8308904","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ec2cf79-e06e-4335-97a7-dc7eb8308904\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kpwdn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kpwdn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aarnq-sc-k8s-node-srv2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aarnq-sc-k8s-node-srv2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"}],"hostIP":"172.16.0.192","podIP":"10.233.123.123","podIPs":[{"ip":"10.233.123.123"}],"startTime":"2023-03-02T13:28:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T13:28:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://96dca8d9b3737a9bcd8125fa47c1ebb4f6946e0b3b94162682092d475667463e","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 13:28:54.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3139" for this suite. 03/02/23 13:28:54.767
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":170,"skipped":3138,"failed":0}
------------------------------
â€¢ [3.281 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:28:51.536
    Mar  2 13:28:51.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename daemonsets 03/02/23 13:28:51.537
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:51.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:51.565
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 03/02/23 13:28:51.617
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 13:28:51.621
    Mar  2 13:28:51.627: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:28:51.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:28:51.630: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 13:28:52.643: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:28:52.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 13:28:52.647: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 13:28:53.638: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:28:53.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 13:28:53.646: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 13:28:54.641: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 13:28:54.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Mar  2 13:28:54.650: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: listing all DeamonSets 03/02/23 13:28:54.656
    STEP: DeleteCollection of the DaemonSets 03/02/23 13:28:54.661
    STEP: Verify that ReplicaSets have been deleted 03/02/23 13:28:54.666
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Mar  2 13:28:54.740: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1947817"},"items":null}

    Mar  2 13:28:54.744: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1947817"},"items":[{"metadata":{"name":"daemon-set-84kz7","generateName":"daemon-set-","namespace":"daemonsets-3139","uid":"aa55edda-e008-4822-aa6f-bfc341f8914d","resourceVersion":"1947815","creationTimestamp":"2023-03-02T13:28:51Z","deletionTimestamp":"2023-03-02T13:29:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"105088d5e96c8babef034c002c22f83055d942570db486c3582d4cf311b10993","cni.projectcalico.org/podIP":"10.233.92.84/32","cni.projectcalico.org/podIPs":"10.233.92.84/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0ec2cf79-e06e-4335-97a7-dc7eb8308904","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ec2cf79-e06e-4335-97a7-dc7eb8308904\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.92.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-d45g8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-d45g8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aarnq-sc-k8s-node-srv1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aarnq-sc-k8s-node-srv1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:52Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:52Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"}],"hostIP":"172.16.0.138","podIP":"10.233.92.84","podIPs":[{"ip":"10.233.92.84"}],"startTime":"2023-03-02T13:28:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T13:28:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://42f7be715c10b97dcc1b3ce1a97d7073c264db7f8b7c3b75717c0606709c757c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-cwm9n","generateName":"daemon-set-","namespace":"daemonsets-3139","uid":"ead2018d-7a61-48bc-861b-07bde9dd8e86","resourceVersion":"1947816","creationTimestamp":"2023-03-02T13:28:51Z","deletionTimestamp":"2023-03-02T13:29:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d1a77c4c5286bd14d58b126c24ee5f8d0b20211ff9b3188501b718c50b594ba3","cni.projectcalico.org/podIP":"10.233.123.29/32","cni.projectcalico.org/podIPs":"10.233.123.29/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0ec2cf79-e06e-4335-97a7-dc7eb8308904","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ec2cf79-e06e-4335-97a7-dc7eb8308904\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-czm96","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-czm96","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aarnq-sc-k8s-node-srv0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aarnq-sc-k8s-node-srv0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"}],"hostIP":"172.16.0.61","podIP":"10.233.123.29","podIPs":[{"ip":"10.233.123.29"}],"startTime":"2023-03-02T13:28:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T13:28:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://1633b157b8f443abf806d26febd9a8b00394ff12461abec0ec49189e0e46439a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jf4hl","generateName":"daemon-set-","namespace":"daemonsets-3139","uid":"2bc66627-2fd1-443e-9349-b09775abd45f","resourceVersion":"1947814","creationTimestamp":"2023-03-02T13:28:51Z","deletionTimestamp":"2023-03-02T13:29:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"e7349a57bdc8bc60054087b10acd3e55e11e3362e40fa20e3f1bf69d3cb186ca","cni.projectcalico.org/podIP":"10.233.126.120/32","cni.projectcalico.org/podIPs":"10.233.126.120/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0ec2cf79-e06e-4335-97a7-dc7eb8308904","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ec2cf79-e06e-4335-97a7-dc7eb8308904\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.126.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ffj6t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ffj6t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aarnq-sc-k8s-node-srv3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aarnq-sc-k8s-node-srv3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"}],"hostIP":"172.16.0.56","podIP":"10.233.126.120","podIPs":[{"ip":"10.233.126.120"}],"startTime":"2023-03-02T13:28:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T13:28:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://b71a528a297fa6271b9e5a5b7bf14169242266747cf9a929d7466c71662b2817","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rzncm","generateName":"daemon-set-","namespace":"daemonsets-3139","uid":"9c221e5c-06f3-4bf1-88a4-4508ca11f343","resourceVersion":"1947817","creationTimestamp":"2023-03-02T13:28:51Z","deletionTimestamp":"2023-03-02T13:29:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3c08a7472389d91061f9f2f161ff20c9a50ba0a9b249238f3ed7ba639cc0c645","cni.projectcalico.org/podIP":"10.233.123.123/32","cni.projectcalico.org/podIPs":"10.233.123.123/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0ec2cf79-e06e-4335-97a7-dc7eb8308904","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ec2cf79-e06e-4335-97a7-dc7eb8308904\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T13:28:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kpwdn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kpwdn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aarnq-sc-k8s-node-srv2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aarnq-sc-k8s-node-srv2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T13:28:51Z"}],"hostIP":"172.16.0.192","podIP":"10.233.123.123","podIPs":[{"ip":"10.233.123.123"}],"startTime":"2023-03-02T13:28:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T13:28:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://96dca8d9b3737a9bcd8125fa47c1ebb4f6946e0b3b94162682092d475667463e","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 13:28:54.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3139" for this suite. 03/02/23 13:28:54.767
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:28:54.823
Mar  2 13:28:54.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:28:54.824
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:54.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:54.847
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 03/02/23 13:28:54.85
Mar  2 13:28:54.850: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar  2 13:28:54.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
Mar  2 13:28:56.860: INFO: stderr: ""
Mar  2 13:28:56.860: INFO: stdout: "service/agnhost-replica created\n"
Mar  2 13:28:56.860: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar  2 13:28:56.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
Mar  2 13:28:57.420: INFO: stderr: ""
Mar  2 13:28:57.420: INFO: stdout: "service/agnhost-primary created\n"
Mar  2 13:28:57.420: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  2 13:28:57.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
Mar  2 13:28:58.156: INFO: stderr: ""
Mar  2 13:28:58.156: INFO: stdout: "service/frontend created\n"
Mar  2 13:28:58.156: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  2 13:28:58.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
Mar  2 13:28:58.834: INFO: stderr: ""
Mar  2 13:28:58.834: INFO: stdout: "deployment.apps/frontend created\n"
Mar  2 13:28:58.834: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 13:28:58.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
Mar  2 13:28:59.557: INFO: stderr: ""
Mar  2 13:28:59.557: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar  2 13:28:59.557: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 13:28:59.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
Mar  2 13:29:00.249: INFO: stderr: ""
Mar  2 13:29:00.249: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/02/23 13:29:00.249
Mar  2 13:29:00.250: INFO: Waiting for all frontend pods to be Running.
Mar  2 13:29:05.314: INFO: Waiting for frontend to serve content.
Mar  2 13:29:05.347: INFO: Trying to add a new entry to the guestbook.
Mar  2 13:29:05.393: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/02/23 13:29:05.404
Mar  2 13:29:05.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
Mar  2 13:29:05.555: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 13:29:05.555: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/02/23 13:29:05.555
Mar  2 13:29:05.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
Mar  2 13:29:05.757: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 13:29:05.757: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/02/23 13:29:05.757
Mar  2 13:29:05.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
Mar  2 13:29:06.014: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 13:29:06.014: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/02/23 13:29:06.014
Mar  2 13:29:06.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
Mar  2 13:29:06.111: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 13:29:06.111: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/02/23 13:29:06.112
Mar  2 13:29:06.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
Mar  2 13:29:06.253: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 13:29:06.253: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/02/23 13:29:06.253
Mar  2 13:29:06.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
Mar  2 13:29:06.526: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 13:29:06.526: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:29:06.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9809" for this suite. 03/02/23 13:29:06.537
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":171,"skipped":3140,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.735 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:28:54.823
    Mar  2 13:28:54.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:28:54.824
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:28:54.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:28:54.847
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 03/02/23 13:28:54.85
    Mar  2 13:28:54.850: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar  2 13:28:54.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
    Mar  2 13:28:56.860: INFO: stderr: ""
    Mar  2 13:28:56.860: INFO: stdout: "service/agnhost-replica created\n"
    Mar  2 13:28:56.860: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar  2 13:28:56.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
    Mar  2 13:28:57.420: INFO: stderr: ""
    Mar  2 13:28:57.420: INFO: stdout: "service/agnhost-primary created\n"
    Mar  2 13:28:57.420: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar  2 13:28:57.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
    Mar  2 13:28:58.156: INFO: stderr: ""
    Mar  2 13:28:58.156: INFO: stdout: "service/frontend created\n"
    Mar  2 13:28:58.156: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar  2 13:28:58.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
    Mar  2 13:28:58.834: INFO: stderr: ""
    Mar  2 13:28:58.834: INFO: stdout: "deployment.apps/frontend created\n"
    Mar  2 13:28:58.834: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar  2 13:28:58.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
    Mar  2 13:28:59.557: INFO: stderr: ""
    Mar  2 13:28:59.557: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar  2 13:28:59.557: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar  2 13:28:59.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 create -f -'
    Mar  2 13:29:00.249: INFO: stderr: ""
    Mar  2 13:29:00.249: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/02/23 13:29:00.249
    Mar  2 13:29:00.250: INFO: Waiting for all frontend pods to be Running.
    Mar  2 13:29:05.314: INFO: Waiting for frontend to serve content.
    Mar  2 13:29:05.347: INFO: Trying to add a new entry to the guestbook.
    Mar  2 13:29:05.393: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/02/23 13:29:05.404
    Mar  2 13:29:05.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
    Mar  2 13:29:05.555: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 13:29:05.555: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/02/23 13:29:05.555
    Mar  2 13:29:05.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
    Mar  2 13:29:05.757: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 13:29:05.757: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/02/23 13:29:05.757
    Mar  2 13:29:05.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
    Mar  2 13:29:06.014: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 13:29:06.014: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/02/23 13:29:06.014
    Mar  2 13:29:06.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
    Mar  2 13:29:06.111: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 13:29:06.111: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/02/23 13:29:06.112
    Mar  2 13:29:06.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
    Mar  2 13:29:06.253: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 13:29:06.253: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/02/23 13:29:06.253
    Mar  2 13:29:06.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9809 delete --grace-period=0 --force -f -'
    Mar  2 13:29:06.526: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 13:29:06.526: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:29:06.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9809" for this suite. 03/02/23 13:29:06.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:29:06.597
Mar  2 13:29:06.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename csistoragecapacity 03/02/23 13:29:06.598
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:06.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:06.637
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/02/23 13:29:06.64
STEP: getting /apis/storage.k8s.io 03/02/23 13:29:06.645
STEP: getting /apis/storage.k8s.io/v1 03/02/23 13:29:06.654
STEP: creating 03/02/23 13:29:06.656
STEP: watching 03/02/23 13:29:06.669
Mar  2 13:29:06.669: INFO: starting watch
STEP: getting 03/02/23 13:29:06.695
STEP: listing in namespace 03/02/23 13:29:06.721
STEP: listing across namespaces 03/02/23 13:29:06.729
STEP: patching 03/02/23 13:29:06.742
STEP: updating 03/02/23 13:29:06.753
Mar  2 13:29:06.758: INFO: waiting for watch events with expected annotations in namespace
Mar  2 13:29:06.758: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/02/23 13:29:06.758
STEP: deleting a collection 03/02/23 13:29:06.798
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Mar  2 13:29:06.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-7571" for this suite. 03/02/23 13:29:06.852
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":172,"skipped":3154,"failed":0}
------------------------------
â€¢ [0.267 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:29:06.597
    Mar  2 13:29:06.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename csistoragecapacity 03/02/23 13:29:06.598
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:06.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:06.637
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/02/23 13:29:06.64
    STEP: getting /apis/storage.k8s.io 03/02/23 13:29:06.645
    STEP: getting /apis/storage.k8s.io/v1 03/02/23 13:29:06.654
    STEP: creating 03/02/23 13:29:06.656
    STEP: watching 03/02/23 13:29:06.669
    Mar  2 13:29:06.669: INFO: starting watch
    STEP: getting 03/02/23 13:29:06.695
    STEP: listing in namespace 03/02/23 13:29:06.721
    STEP: listing across namespaces 03/02/23 13:29:06.729
    STEP: patching 03/02/23 13:29:06.742
    STEP: updating 03/02/23 13:29:06.753
    Mar  2 13:29:06.758: INFO: waiting for watch events with expected annotations in namespace
    Mar  2 13:29:06.758: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/02/23 13:29:06.758
    STEP: deleting a collection 03/02/23 13:29:06.798
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Mar  2 13:29:06.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-7571" for this suite. 03/02/23 13:29:06.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:29:06.881
Mar  2 13:29:06.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:29:06.883
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:06.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:06.935
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-865ced61-7c2e-48b2-83c6-65471f5b000a 03/02/23 13:29:06.949
STEP: Creating secret with name secret-projected-all-test-volume-fde2e0a8-31f4-4db2-b8c3-9975b907e09d 03/02/23 13:29:06.957
STEP: Creating a pod to test Check all projections for projected volume plugin 03/02/23 13:29:07.006
Mar  2 13:29:07.020: INFO: Waiting up to 5m0s for pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e" in namespace "projected-1478" to be "Succeeded or Failed"
Mar  2 13:29:07.030: INFO: Pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.651845ms
Mar  2 13:29:09.053: INFO: Pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032748281s
Mar  2 13:29:11.042: INFO: Pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021721984s
Mar  2 13:29:13.035: INFO: Pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01474906s
STEP: Saw pod success 03/02/23 13:29:13.036
Mar  2 13:29:13.036: INFO: Pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e" satisfied condition "Succeeded or Failed"
Mar  2 13:29:13.040: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e container projected-all-volume-test: <nil>
STEP: delete the pod 03/02/23 13:29:13.049
Mar  2 13:29:13.064: INFO: Waiting for pod projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e to disappear
Mar  2 13:29:13.067: INFO: Pod projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Mar  2 13:29:13.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1478" for this suite. 03/02/23 13:29:13.124
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":173,"skipped":3165,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.252 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:29:06.881
    Mar  2 13:29:06.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:29:06.883
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:06.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:06.935
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-865ced61-7c2e-48b2-83c6-65471f5b000a 03/02/23 13:29:06.949
    STEP: Creating secret with name secret-projected-all-test-volume-fde2e0a8-31f4-4db2-b8c3-9975b907e09d 03/02/23 13:29:06.957
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/02/23 13:29:07.006
    Mar  2 13:29:07.020: INFO: Waiting up to 5m0s for pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e" in namespace "projected-1478" to be "Succeeded or Failed"
    Mar  2 13:29:07.030: INFO: Pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.651845ms
    Mar  2 13:29:09.053: INFO: Pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032748281s
    Mar  2 13:29:11.042: INFO: Pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021721984s
    Mar  2 13:29:13.035: INFO: Pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01474906s
    STEP: Saw pod success 03/02/23 13:29:13.036
    Mar  2 13:29:13.036: INFO: Pod "projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e" satisfied condition "Succeeded or Failed"
    Mar  2 13:29:13.040: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e container projected-all-volume-test: <nil>
    STEP: delete the pod 03/02/23 13:29:13.049
    Mar  2 13:29:13.064: INFO: Waiting for pod projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e to disappear
    Mar  2 13:29:13.067: INFO: Pod projected-volume-862d5b8f-6159-4bac-9c18-5220ea3c666e no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Mar  2 13:29:13.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1478" for this suite. 03/02/23 13:29:13.124
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:29:13.142
Mar  2 13:29:13.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:29:13.143
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:13.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:13.16
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 03/02/23 13:29:13.163
Mar  2 13:29:13.171: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1" in namespace "projected-1068" to be "Succeeded or Failed"
Mar  2 13:29:13.243: INFO: Pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 72.265719ms
Mar  2 13:29:15.251: INFO: Pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079898259s
Mar  2 13:29:17.250: INFO: Pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078905767s
Mar  2 13:29:19.251: INFO: Pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079798529s
STEP: Saw pod success 03/02/23 13:29:19.251
Mar  2 13:29:19.251: INFO: Pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1" satisfied condition "Succeeded or Failed"
Mar  2 13:29:19.255: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1 container client-container: <nil>
STEP: delete the pod 03/02/23 13:29:19.263
Mar  2 13:29:19.280: INFO: Waiting for pod downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1 to disappear
Mar  2 13:29:19.298: INFO: Pod downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 13:29:19.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1068" for this suite. 03/02/23 13:29:19.304
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":174,"skipped":3168,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.169 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:29:13.142
    Mar  2 13:29:13.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:29:13.143
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:13.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:13.16
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 03/02/23 13:29:13.163
    Mar  2 13:29:13.171: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1" in namespace "projected-1068" to be "Succeeded or Failed"
    Mar  2 13:29:13.243: INFO: Pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 72.265719ms
    Mar  2 13:29:15.251: INFO: Pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079898259s
    Mar  2 13:29:17.250: INFO: Pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078905767s
    Mar  2 13:29:19.251: INFO: Pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079798529s
    STEP: Saw pod success 03/02/23 13:29:19.251
    Mar  2 13:29:19.251: INFO: Pod "downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1" satisfied condition "Succeeded or Failed"
    Mar  2 13:29:19.255: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1 container client-container: <nil>
    STEP: delete the pod 03/02/23 13:29:19.263
    Mar  2 13:29:19.280: INFO: Waiting for pod downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1 to disappear
    Mar  2 13:29:19.298: INFO: Pod downwardapi-volume-51d1578d-5efc-4e7a-8985-9764995b1fc1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 13:29:19.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1068" for this suite. 03/02/23 13:29:19.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:29:19.315
Mar  2 13:29:19.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:29:19.32
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:19.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:19.345
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 03/02/23 13:29:19.352
STEP: waiting for available Endpoint 03/02/23 13:29:19.356
STEP: listing all Endpoints 03/02/23 13:29:19.358
STEP: updating the Endpoint 03/02/23 13:29:19.362
STEP: fetching the Endpoint 03/02/23 13:29:19.368
STEP: patching the Endpoint 03/02/23 13:29:19.37
STEP: fetching the Endpoint 03/02/23 13:29:19.389
STEP: deleting the Endpoint by Collection 03/02/23 13:29:19.392
STEP: waiting for Endpoint deletion 03/02/23 13:29:19.399
STEP: fetching the Endpoint 03/02/23 13:29:19.401
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:29:19.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1910" for this suite. 03/02/23 13:29:19.407
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":175,"skipped":3178,"failed":0}
------------------------------
â€¢ [0.096 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:29:19.315
    Mar  2 13:29:19.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:29:19.32
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:19.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:19.345
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 03/02/23 13:29:19.352
    STEP: waiting for available Endpoint 03/02/23 13:29:19.356
    STEP: listing all Endpoints 03/02/23 13:29:19.358
    STEP: updating the Endpoint 03/02/23 13:29:19.362
    STEP: fetching the Endpoint 03/02/23 13:29:19.368
    STEP: patching the Endpoint 03/02/23 13:29:19.37
    STEP: fetching the Endpoint 03/02/23 13:29:19.389
    STEP: deleting the Endpoint by Collection 03/02/23 13:29:19.392
    STEP: waiting for Endpoint deletion 03/02/23 13:29:19.399
    STEP: fetching the Endpoint 03/02/23 13:29:19.401
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:29:19.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1910" for this suite. 03/02/23 13:29:19.407
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:29:19.426
Mar  2 13:29:19.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:29:19.427
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:19.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:19.45
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 13:29:19.454
Mar  2 13:29:19.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-7476 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  2 13:29:19.598: INFO: stderr: ""
Mar  2 13:29:19.598: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/02/23 13:29:19.598
STEP: verifying the pod e2e-test-httpd-pod was created 03/02/23 13:29:24.662
Mar  2 13:29:24.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-7476 get pod e2e-test-httpd-pod -o json'
Mar  2 13:29:25.000: INFO: stderr: ""
Mar  2 13:29:25.000: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"fa201b0a8d7c17cf8ca59b57a0de5d6d6d2de519a267c2c38c880440ce2b7c90\",\n            \"cni.projectcalico.org/podIP\": \"10.233.123.122/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.123.122/32\"\n        },\n        \"creationTimestamp\": \"2023-03-02T13:29:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7476\",\n        \"resourceVersion\": \"1948387\",\n        \"uid\": \"7791d136-9201-4f96-854e-d41c01398ef7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-rb5zk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"aarnq-sc-k8s-node-srv2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-rb5zk\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T13:29:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T13:29:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T13:29:21Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T13:29:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c75b386ebde3514c48581dea45eab51594ba44cd305771beb6c300807ca22450\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-02T13:29:20Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.0.192\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.123.122\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.123.122\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-02T13:29:19Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/02/23 13:29:25.001
Mar  2 13:29:25.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-7476 replace -f -'
Mar  2 13:29:25.567: INFO: stderr: ""
Mar  2 13:29:25.567: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/02/23 13:29:25.567
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Mar  2 13:29:25.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-7476 delete pods e2e-test-httpd-pod'
Mar  2 13:29:28.702: INFO: stderr: ""
Mar  2 13:29:28.702: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:29:28.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7476" for this suite. 03/02/23 13:29:28.735
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":176,"skipped":3248,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.333 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:29:19.426
    Mar  2 13:29:19.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:29:19.427
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:19.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:19.45
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 13:29:19.454
    Mar  2 13:29:19.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-7476 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar  2 13:29:19.598: INFO: stderr: ""
    Mar  2 13:29:19.598: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/02/23 13:29:19.598
    STEP: verifying the pod e2e-test-httpd-pod was created 03/02/23 13:29:24.662
    Mar  2 13:29:24.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-7476 get pod e2e-test-httpd-pod -o json'
    Mar  2 13:29:25.000: INFO: stderr: ""
    Mar  2 13:29:25.000: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"fa201b0a8d7c17cf8ca59b57a0de5d6d6d2de519a267c2c38c880440ce2b7c90\",\n            \"cni.projectcalico.org/podIP\": \"10.233.123.122/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.123.122/32\"\n        },\n        \"creationTimestamp\": \"2023-03-02T13:29:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7476\",\n        \"resourceVersion\": \"1948387\",\n        \"uid\": \"7791d136-9201-4f96-854e-d41c01398ef7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-rb5zk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"aarnq-sc-k8s-node-srv2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-rb5zk\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T13:29:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T13:29:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T13:29:21Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T13:29:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c75b386ebde3514c48581dea45eab51594ba44cd305771beb6c300807ca22450\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-02T13:29:20Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.0.192\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.123.122\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.123.122\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-02T13:29:19Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/02/23 13:29:25.001
    Mar  2 13:29:25.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-7476 replace -f -'
    Mar  2 13:29:25.567: INFO: stderr: ""
    Mar  2 13:29:25.567: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/02/23 13:29:25.567
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Mar  2 13:29:25.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-7476 delete pods e2e-test-httpd-pod'
    Mar  2 13:29:28.702: INFO: stderr: ""
    Mar  2 13:29:28.702: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:29:28.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7476" for this suite. 03/02/23 13:29:28.735
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:29:28.763
Mar  2 13:29:28.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename runtimeclass 03/02/23 13:29:28.765
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:28.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:28.869
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-6973-delete-me 03/02/23 13:29:28.934
STEP: Waiting for the RuntimeClass to disappear 03/02/23 13:29:28.953
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  2 13:29:28.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6973" for this suite. 03/02/23 13:29:29.013
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":177,"skipped":3249,"failed":0}
------------------------------
â€¢ [0.267 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:29:28.763
    Mar  2 13:29:28.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename runtimeclass 03/02/23 13:29:28.765
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:28.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:28.869
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-6973-delete-me 03/02/23 13:29:28.934
    STEP: Waiting for the RuntimeClass to disappear 03/02/23 13:29:28.953
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  2 13:29:28.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6973" for this suite. 03/02/23 13:29:29.013
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:29:29.03
Mar  2 13:29:29.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename resourcequota 03/02/23 13:29:29.031
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:29.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:29.055
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 03/02/23 13:29:29.061
STEP: Counting existing ResourceQuota 03/02/23 13:29:34.066
STEP: Creating a ResourceQuota 03/02/23 13:29:39.072
STEP: Ensuring resource quota status is calculated 03/02/23 13:29:39.081
STEP: Creating a Secret 03/02/23 13:29:41.086
STEP: Ensuring resource quota status captures secret creation 03/02/23 13:29:41.099
STEP: Deleting a secret 03/02/23 13:29:43.104
STEP: Ensuring resource quota status released usage 03/02/23 13:29:43.11
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 13:29:45.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8288" for this suite. 03/02/23 13:29:45.126
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":178,"skipped":3249,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.106 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:29:29.03
    Mar  2 13:29:29.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename resourcequota 03/02/23 13:29:29.031
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:29.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:29.055
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 03/02/23 13:29:29.061
    STEP: Counting existing ResourceQuota 03/02/23 13:29:34.066
    STEP: Creating a ResourceQuota 03/02/23 13:29:39.072
    STEP: Ensuring resource quota status is calculated 03/02/23 13:29:39.081
    STEP: Creating a Secret 03/02/23 13:29:41.086
    STEP: Ensuring resource quota status captures secret creation 03/02/23 13:29:41.099
    STEP: Deleting a secret 03/02/23 13:29:43.104
    STEP: Ensuring resource quota status released usage 03/02/23 13:29:43.11
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 13:29:45.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8288" for this suite. 03/02/23 13:29:45.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:29:45.144
Mar  2 13:29:45.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename watch 03/02/23 13:29:45.145
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:45.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:45.166
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/02/23 13:29:45.169
STEP: creating a watch on configmaps with label B 03/02/23 13:29:45.17
STEP: creating a watch on configmaps with label A or B 03/02/23 13:29:45.171
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/02/23 13:29:45.173
Mar  2 13:29:45.177: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948563 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:29:45.177: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948563 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/02/23 13:29:45.177
Mar  2 13:29:45.183: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948564 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:29:45.183: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948564 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/02/23 13:29:45.184
Mar  2 13:29:45.190: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948565 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:29:45.190: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948565 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/02/23 13:29:45.19
Mar  2 13:29:45.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948566 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:29:45.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948566 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/02/23 13:29:45.195
Mar  2 13:29:45.198: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1307  4c75390d-dd68-4dde-8c5b-8935a5b1f250 1948567 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:29:45.198: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1307  4c75390d-dd68-4dde-8c5b-8935a5b1f250 1948567 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/02/23 13:29:55.199
Mar  2 13:29:55.210: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1307  4c75390d-dd68-4dde-8c5b-8935a5b1f250 1948608 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:29:55.210: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1307  4c75390d-dd68-4dde-8c5b-8935a5b1f250 1948608 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  2 13:30:05.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1307" for this suite. 03/02/23 13:30:05.217
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":179,"skipped":3266,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.080 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:29:45.144
    Mar  2 13:29:45.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename watch 03/02/23 13:29:45.145
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:29:45.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:29:45.166
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/02/23 13:29:45.169
    STEP: creating a watch on configmaps with label B 03/02/23 13:29:45.17
    STEP: creating a watch on configmaps with label A or B 03/02/23 13:29:45.171
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/02/23 13:29:45.173
    Mar  2 13:29:45.177: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948563 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:29:45.177: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948563 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/02/23 13:29:45.177
    Mar  2 13:29:45.183: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948564 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:29:45.183: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948564 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/02/23 13:29:45.184
    Mar  2 13:29:45.190: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948565 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:29:45.190: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948565 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/02/23 13:29:45.19
    Mar  2 13:29:45.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948566 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:29:45.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1307  6fa140fc-25d4-426d-8d7c-9b1fbf3d226b 1948566 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/02/23 13:29:45.195
    Mar  2 13:29:45.198: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1307  4c75390d-dd68-4dde-8c5b-8935a5b1f250 1948567 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:29:45.198: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1307  4c75390d-dd68-4dde-8c5b-8935a5b1f250 1948567 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/02/23 13:29:55.199
    Mar  2 13:29:55.210: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1307  4c75390d-dd68-4dde-8c5b-8935a5b1f250 1948608 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:29:55.210: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1307  4c75390d-dd68-4dde-8c5b-8935a5b1f250 1948608 0 2023-03-02 13:29:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 13:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  2 13:30:05.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1307" for this suite. 03/02/23 13:30:05.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:30:05.227
Mar  2 13:30:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:30:05.229
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:05.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:05.25
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 03/02/23 13:30:05.259
Mar  2 13:30:05.260: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2541 proxy --unix-socket=/tmp/kubectl-proxy-unix490983617/test'
STEP: retrieving proxy /api/ output 03/02/23 13:30:05.356
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:30:05.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2541" for this suite. 03/02/23 13:30:05.367
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":180,"skipped":3278,"failed":0}
------------------------------
â€¢ [0.149 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:30:05.227
    Mar  2 13:30:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:30:05.229
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:05.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:05.25
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 03/02/23 13:30:05.259
    Mar  2 13:30:05.260: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2541 proxy --unix-socket=/tmp/kubectl-proxy-unix490983617/test'
    STEP: retrieving proxy /api/ output 03/02/23 13:30:05.356
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:30:05.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2541" for this suite. 03/02/23 13:30:05.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:30:05.383
Mar  2 13:30:05.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 13:30:05.384
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:05.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:05.401
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 13:30:05.414
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:30:06.025
STEP: Deploying the webhook pod 03/02/23 13:30:06.034
STEP: Wait for the deployment to be ready 03/02/23 13:30:06.049
Mar  2 13:30:06.061: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/02/23 13:30:08.089
STEP: Verifying the service has paired with the endpoint 03/02/23 13:30:08.103
Mar  2 13:30:09.108: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 03/02/23 13:30:09.112
STEP: create a pod 03/02/23 13:30:09.132
Mar  2 13:30:09.140: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8799" to be "running"
Mar  2 13:30:09.145: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.714134ms
Mar  2 13:30:11.152: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012383709s
Mar  2 13:30:11.153: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/02/23 13:30:11.153
Mar  2 13:30:11.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=webhook-8799 attach --namespace=webhook-8799 to-be-attached-pod -i -c=container1'
Mar  2 13:30:11.457: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:30:11.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8799" for this suite. 03/02/23 13:30:11.468
STEP: Destroying namespace "webhook-8799-markers" for this suite. 03/02/23 13:30:11.482
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":181,"skipped":3320,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.364 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:30:05.383
    Mar  2 13:30:05.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 13:30:05.384
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:05.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:05.401
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 13:30:05.414
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:30:06.025
    STEP: Deploying the webhook pod 03/02/23 13:30:06.034
    STEP: Wait for the deployment to be ready 03/02/23 13:30:06.049
    Mar  2 13:30:06.061: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/02/23 13:30:08.089
    STEP: Verifying the service has paired with the endpoint 03/02/23 13:30:08.103
    Mar  2 13:30:09.108: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 03/02/23 13:30:09.112
    STEP: create a pod 03/02/23 13:30:09.132
    Mar  2 13:30:09.140: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8799" to be "running"
    Mar  2 13:30:09.145: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.714134ms
    Mar  2 13:30:11.152: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012383709s
    Mar  2 13:30:11.153: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/02/23 13:30:11.153
    Mar  2 13:30:11.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=webhook-8799 attach --namespace=webhook-8799 to-be-attached-pod -i -c=container1'
    Mar  2 13:30:11.457: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:30:11.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8799" for this suite. 03/02/23 13:30:11.468
    STEP: Destroying namespace "webhook-8799-markers" for this suite. 03/02/23 13:30:11.482
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:30:11.747
Mar  2 13:30:11.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:30:11.748
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:12.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:12.033
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-1911 03/02/23 13:30:12.048
STEP: creating service affinity-clusterip in namespace services-1911 03/02/23 13:30:12.048
STEP: creating replication controller affinity-clusterip in namespace services-1911 03/02/23 13:30:12.064
I0302 13:30:12.071880      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1911, replica count: 3
I0302 13:30:15.166088      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 13:30:18.166435      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 13:30:18.182: INFO: Creating new exec pod
Mar  2 13:30:18.192: INFO: Waiting up to 5m0s for pod "execpod-affinityfxkrh" in namespace "services-1911" to be "running"
Mar  2 13:30:18.199: INFO: Pod "execpod-affinityfxkrh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.298409ms
Mar  2 13:30:20.210: INFO: Pod "execpod-affinityfxkrh": Phase="Running", Reason="", readiness=true. Elapsed: 2.017946964s
Mar  2 13:30:20.211: INFO: Pod "execpod-affinityfxkrh" satisfied condition "running"
Mar  2 13:30:21.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-1911 exec execpod-affinityfxkrh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar  2 13:30:21.421: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar  2 13:30:21.421: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:30:21.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-1911 exec execpod-affinityfxkrh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.21.81 80'
Mar  2 13:30:21.618: INFO: stderr: "+ nc -v -t -w 2 10.233.21.81 80\n+ echo hostName\nConnection to 10.233.21.81 80 port [tcp/http] succeeded!\n"
Mar  2 13:30:21.618: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:30:21.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-1911 exec execpod-affinityfxkrh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.21.81:80/ ; done'
Mar  2 13:30:21.988: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n"
Mar  2 13:30:21.988: INFO: stdout: "\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k"
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
Mar  2 13:30:21.988: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1911, will wait for the garbage collector to delete the pods 03/02/23 13:30:22.028
Mar  2 13:30:22.110: INFO: Deleting ReplicationController affinity-clusterip took: 25.442282ms
Mar  2 13:30:22.214: INFO: Terminating ReplicationController affinity-clusterip pods took: 104.443672ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:30:24.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1911" for this suite. 03/02/23 13:30:24.245
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":182,"skipped":3320,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.505 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:30:11.747
    Mar  2 13:30:11.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:30:11.748
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:12.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:12.033
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-1911 03/02/23 13:30:12.048
    STEP: creating service affinity-clusterip in namespace services-1911 03/02/23 13:30:12.048
    STEP: creating replication controller affinity-clusterip in namespace services-1911 03/02/23 13:30:12.064
    I0302 13:30:12.071880      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1911, replica count: 3
    I0302 13:30:15.166088      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 13:30:18.166435      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 13:30:18.182: INFO: Creating new exec pod
    Mar  2 13:30:18.192: INFO: Waiting up to 5m0s for pod "execpod-affinityfxkrh" in namespace "services-1911" to be "running"
    Mar  2 13:30:18.199: INFO: Pod "execpod-affinityfxkrh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.298409ms
    Mar  2 13:30:20.210: INFO: Pod "execpod-affinityfxkrh": Phase="Running", Reason="", readiness=true. Elapsed: 2.017946964s
    Mar  2 13:30:20.211: INFO: Pod "execpod-affinityfxkrh" satisfied condition "running"
    Mar  2 13:30:21.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-1911 exec execpod-affinityfxkrh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Mar  2 13:30:21.421: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar  2 13:30:21.421: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:30:21.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-1911 exec execpod-affinityfxkrh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.21.81 80'
    Mar  2 13:30:21.618: INFO: stderr: "+ nc -v -t -w 2 10.233.21.81 80\n+ echo hostName\nConnection to 10.233.21.81 80 port [tcp/http] succeeded!\n"
    Mar  2 13:30:21.618: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:30:21.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-1911 exec execpod-affinityfxkrh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.21.81:80/ ; done'
    Mar  2 13:30:21.988: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.81:80/\n"
    Mar  2 13:30:21.988: INFO: stdout: "\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k\naffinity-clusterip-c8t5k"
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Received response from host: affinity-clusterip-c8t5k
    Mar  2 13:30:21.988: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-1911, will wait for the garbage collector to delete the pods 03/02/23 13:30:22.028
    Mar  2 13:30:22.110: INFO: Deleting ReplicationController affinity-clusterip took: 25.442282ms
    Mar  2 13:30:22.214: INFO: Terminating ReplicationController affinity-clusterip pods took: 104.443672ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:30:24.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1911" for this suite. 03/02/23 13:30:24.245
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:30:24.255
Mar  2 13:30:24.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:30:24.256
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:24.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:24.278
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:30:24.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7991" for this suite. 03/02/23 13:30:24.305
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":183,"skipped":3322,"failed":0}
------------------------------
â€¢ [0.060 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:30:24.255
    Mar  2 13:30:24.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:30:24.256
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:24.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:24.278
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:30:24.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7991" for this suite. 03/02/23 13:30:24.305
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:30:24.317
Mar  2 13:30:24.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename gc 03/02/23 13:30:24.318
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:24.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:24.347
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/02/23 13:30:24.35
STEP: Wait for the Deployment to create new ReplicaSet 03/02/23 13:30:24.356
STEP: delete the deployment 03/02/23 13:30:24.361
STEP: wait for all rs to be garbage collected 03/02/23 13:30:24.37
STEP: expected 0 pods, got 2 pods 03/02/23 13:30:24.415
STEP: Gathering metrics 03/02/23 13:30:24.929
Mar  2 13:30:24.965: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
Mar  2 13:30:24.970: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 4.728351ms
Mar  2 13:30:24.970: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
Mar  2 13:30:24.970: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
Mar  2 13:30:25.138: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 13:30:25.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-773" for this suite. 03/02/23 13:30:25.147
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":184,"skipped":3322,"failed":0}
------------------------------
â€¢ [0.837 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:30:24.317
    Mar  2 13:30:24.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename gc 03/02/23 13:30:24.318
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:24.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:24.347
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/02/23 13:30:24.35
    STEP: Wait for the Deployment to create new ReplicaSet 03/02/23 13:30:24.356
    STEP: delete the deployment 03/02/23 13:30:24.361
    STEP: wait for all rs to be garbage collected 03/02/23 13:30:24.37
    STEP: expected 0 pods, got 2 pods 03/02/23 13:30:24.415
    STEP: Gathering metrics 03/02/23 13:30:24.929
    Mar  2 13:30:24.965: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
    Mar  2 13:30:24.970: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 4.728351ms
    Mar  2 13:30:24.970: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
    Mar  2 13:30:24.970: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
    Mar  2 13:30:25.138: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 13:30:25.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-773" for this suite. 03/02/23 13:30:25.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:30:25.156
Mar  2 13:30:25.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 13:30:25.158
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:25.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:25.22
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 03/02/23 13:30:25.23
Mar  2 13:30:25.242: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d" in namespace "downward-api-8151" to be "Succeeded or Failed"
Mar  2 13:30:25.254: INFO: Pod "downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.279097ms
Mar  2 13:30:27.267: INFO: Pod "downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024898082s
Mar  2 13:30:29.264: INFO: Pod "downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02191807s
STEP: Saw pod success 03/02/23 13:30:29.264
Mar  2 13:30:29.264: INFO: Pod "downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d" satisfied condition "Succeeded or Failed"
Mar  2 13:30:29.307: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d container client-container: <nil>
STEP: delete the pod 03/02/23 13:30:29.323
Mar  2 13:30:29.346: INFO: Waiting for pod downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d to disappear
Mar  2 13:30:29.351: INFO: Pod downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 13:30:29.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8151" for this suite. 03/02/23 13:30:29.357
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":185,"skipped":3332,"failed":0}
------------------------------
â€¢ [4.208 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:30:25.156
    Mar  2 13:30:25.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 13:30:25.158
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:25.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:25.22
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 03/02/23 13:30:25.23
    Mar  2 13:30:25.242: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d" in namespace "downward-api-8151" to be "Succeeded or Failed"
    Mar  2 13:30:25.254: INFO: Pod "downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.279097ms
    Mar  2 13:30:27.267: INFO: Pod "downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024898082s
    Mar  2 13:30:29.264: INFO: Pod "downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02191807s
    STEP: Saw pod success 03/02/23 13:30:29.264
    Mar  2 13:30:29.264: INFO: Pod "downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d" satisfied condition "Succeeded or Failed"
    Mar  2 13:30:29.307: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d container client-container: <nil>
    STEP: delete the pod 03/02/23 13:30:29.323
    Mar  2 13:30:29.346: INFO: Waiting for pod downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d to disappear
    Mar  2 13:30:29.351: INFO: Pod downwardapi-volume-c63187f0-8424-4029-ad30-68273e2a1f5d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 13:30:29.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8151" for this suite. 03/02/23 13:30:29.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:30:29.368
Mar  2 13:30:29.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename deployment 03/02/23 13:30:29.369
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:29.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:29.406
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar  2 13:30:29.445: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  2 13:30:34.453: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 13:30:34.453
Mar  2 13:30:34.453: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/02/23 13:30:34.464
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 13:30:34.482: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9158  49a55d92-3a2f-4d30-808a-6e6388bdb93d 1949144 1 2023-03-02 13:30:34 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-02 13:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0081ad8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 13:30:34.489: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-9158  f77e63f6-592a-43d1-aae8-36ab947b1956 1949146 1 2023-03-02 13:30:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 49a55d92-3a2f-4d30-808a-6e6388bdb93d 0xc001eb71a7 0xc001eb71a8}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49a55d92-3a2f-4d30-808a-6e6388bdb93d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001eb7238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 13:30:34.489: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  2 13:30:34.490: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9158  c80e47c1-1754-43cb-8fbb-052c924dc597 1949145 1 2023-03-02 13:30:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 49a55d92-3a2f-4d30-808a-6e6388bdb93d 0xc001eb7077 0xc001eb7078}] [] [{e2e.test Update apps/v1 2023-03-02 13:30:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:30:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 13:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"49a55d92-3a2f-4d30-808a-6e6388bdb93d\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001eb7138 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 13:30:34.510: INFO: Pod "test-cleanup-controller-5jvsb" is available:
&Pod{ObjectMeta:{test-cleanup-controller-5jvsb test-cleanup-controller- deployment-9158  250ea002-ec7f-42f8-9d90-d755b0f2e81b 1949130 0 2023-03-02 13:30:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:bf723e495beee4340330c94f68c1255eba705d7b65486619225883e5e91d5b4d cni.projectcalico.org/podIP:10.233.123.121/32 cni.projectcalico.org/podIPs:10.233.123.121/32] [{apps/v1 ReplicaSet test-cleanup-controller c80e47c1-1754-43cb-8fbb-052c924dc597 0xc001eb7647 0xc001eb7648}] [] [{kube-controller-manager Update v1 2023-03-02 13:30:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c80e47c1-1754-43cb-8fbb-052c924dc597\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:30:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:30:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-krbfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krbfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:30:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:30:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:30:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:30:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.121,StartTime:2023-03-02 13:30:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:30:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9c6fa2478a7b6020e7f71ada1dac89979eab4fce95ac3625d014d3008f8c8063,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 13:30:34.511: INFO: Pod "test-cleanup-deployment-69cb9c5497-m88bx" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-m88bx test-cleanup-deployment-69cb9c5497- deployment-9158  d8a1e631-0e67-46c0-a3fa-2824dd988499 1949148 0 2023-03-02 13:30:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 f77e63f6-592a-43d1-aae8-36ab947b1956 0xc001eb7857 0xc001eb7858}] [] [{kube-controller-manager Update v1 2023-03-02 13:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f77e63f6-592a-43d1-aae8-36ab947b1956\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-llddc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llddc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 13:30:34.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9158" for this suite. 03/02/23 13:30:34.528
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":186,"skipped":3357,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.183 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:30:29.368
    Mar  2 13:30:29.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename deployment 03/02/23 13:30:29.369
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:29.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:29.406
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar  2 13:30:29.445: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Mar  2 13:30:34.453: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 13:30:34.453
    Mar  2 13:30:34.453: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/02/23 13:30:34.464
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 13:30:34.482: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9158  49a55d92-3a2f-4d30-808a-6e6388bdb93d 1949144 1 2023-03-02 13:30:34 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-02 13:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0081ad8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Mar  2 13:30:34.489: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-9158  f77e63f6-592a-43d1-aae8-36ab947b1956 1949146 1 2023-03-02 13:30:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 49a55d92-3a2f-4d30-808a-6e6388bdb93d 0xc001eb71a7 0xc001eb71a8}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49a55d92-3a2f-4d30-808a-6e6388bdb93d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001eb7238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 13:30:34.489: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Mar  2 13:30:34.490: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9158  c80e47c1-1754-43cb-8fbb-052c924dc597 1949145 1 2023-03-02 13:30:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 49a55d92-3a2f-4d30-808a-6e6388bdb93d 0xc001eb7077 0xc001eb7078}] [] [{e2e.test Update apps/v1 2023-03-02 13:30:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:30:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 13:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"49a55d92-3a2f-4d30-808a-6e6388bdb93d\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001eb7138 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 13:30:34.510: INFO: Pod "test-cleanup-controller-5jvsb" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-5jvsb test-cleanup-controller- deployment-9158  250ea002-ec7f-42f8-9d90-d755b0f2e81b 1949130 0 2023-03-02 13:30:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:bf723e495beee4340330c94f68c1255eba705d7b65486619225883e5e91d5b4d cni.projectcalico.org/podIP:10.233.123.121/32 cni.projectcalico.org/podIPs:10.233.123.121/32] [{apps/v1 ReplicaSet test-cleanup-controller c80e47c1-1754-43cb-8fbb-052c924dc597 0xc001eb7647 0xc001eb7648}] [] [{kube-controller-manager Update v1 2023-03-02 13:30:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c80e47c1-1754-43cb-8fbb-052c924dc597\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:30:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:30:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-krbfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krbfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:30:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:30:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:30:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:30:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.121,StartTime:2023-03-02 13:30:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:30:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9c6fa2478a7b6020e7f71ada1dac89979eab4fce95ac3625d014d3008f8c8063,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 13:30:34.511: INFO: Pod "test-cleanup-deployment-69cb9c5497-m88bx" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-m88bx test-cleanup-deployment-69cb9c5497- deployment-9158  d8a1e631-0e67-46c0-a3fa-2824dd988499 1949148 0 2023-03-02 13:30:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 f77e63f6-592a-43d1-aae8-36ab947b1956 0xc001eb7857 0xc001eb7858}] [] [{kube-controller-manager Update v1 2023-03-02 13:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f77e63f6-592a-43d1-aae8-36ab947b1956\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-llddc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llddc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 13:30:34.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9158" for this suite. 03/02/23 13:30:34.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:30:34.552
Mar  2 13:30:34.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename gc 03/02/23 13:30:34.555
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:34.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:34.614
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/02/23 13:30:34.623
STEP: Wait for the Deployment to create new ReplicaSet 03/02/23 13:30:34.634
STEP: delete the deployment 03/02/23 13:30:35.157
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/02/23 13:30:35.163
STEP: Gathering metrics 03/02/23 13:30:35.751
Mar  2 13:30:35.939: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
Mar  2 13:30:35.958: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 19.06528ms
Mar  2 13:30:35.958: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
Mar  2 13:30:35.958: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
Mar  2 13:30:36.329: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 13:30:36.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7094" for this suite. 03/02/23 13:30:36.335
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":187,"skipped":3366,"failed":0}
------------------------------
â€¢ [1.788 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:30:34.552
    Mar  2 13:30:34.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename gc 03/02/23 13:30:34.555
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:34.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:34.614
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/02/23 13:30:34.623
    STEP: Wait for the Deployment to create new ReplicaSet 03/02/23 13:30:34.634
    STEP: delete the deployment 03/02/23 13:30:35.157
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/02/23 13:30:35.163
    STEP: Gathering metrics 03/02/23 13:30:35.751
    Mar  2 13:30:35.939: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
    Mar  2 13:30:35.958: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 19.06528ms
    Mar  2 13:30:35.958: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
    Mar  2 13:30:35.958: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
    Mar  2 13:30:36.329: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 13:30:36.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7094" for this suite. 03/02/23 13:30:36.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:30:36.34
Mar  2 13:30:36.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 13:30:36.341
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:36.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:36.362
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-edfdc69b-2334-4101-9482-b6a8ead7f8f5 03/02/23 13:30:36.427
STEP: Creating a pod to test consume secrets 03/02/23 13:30:36.431
Mar  2 13:30:36.443: INFO: Waiting up to 5m0s for pod "pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173" in namespace "secrets-1842" to be "Succeeded or Failed"
Mar  2 13:30:36.470: INFO: Pod "pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173": Phase="Pending", Reason="", readiness=false. Elapsed: 26.773245ms
Mar  2 13:30:38.490: INFO: Pod "pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047238033s
Mar  2 13:30:40.522: INFO: Pod "pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079102035s
STEP: Saw pod success 03/02/23 13:30:40.522
Mar  2 13:30:40.522: INFO: Pod "pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173" satisfied condition "Succeeded or Failed"
Mar  2 13:30:40.527: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 13:30:40.539
Mar  2 13:30:40.552: INFO: Waiting for pod pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173 to disappear
Mar  2 13:30:40.557: INFO: Pod pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 13:30:40.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1842" for this suite. 03/02/23 13:30:40.563
STEP: Destroying namespace "secret-namespace-2331" for this suite. 03/02/23 13:30:40.57
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":188,"skipped":3386,"failed":0}
------------------------------
â€¢ [4.237 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:30:36.34
    Mar  2 13:30:36.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 13:30:36.341
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:36.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:36.362
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-edfdc69b-2334-4101-9482-b6a8ead7f8f5 03/02/23 13:30:36.427
    STEP: Creating a pod to test consume secrets 03/02/23 13:30:36.431
    Mar  2 13:30:36.443: INFO: Waiting up to 5m0s for pod "pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173" in namespace "secrets-1842" to be "Succeeded or Failed"
    Mar  2 13:30:36.470: INFO: Pod "pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173": Phase="Pending", Reason="", readiness=false. Elapsed: 26.773245ms
    Mar  2 13:30:38.490: INFO: Pod "pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047238033s
    Mar  2 13:30:40.522: INFO: Pod "pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079102035s
    STEP: Saw pod success 03/02/23 13:30:40.522
    Mar  2 13:30:40.522: INFO: Pod "pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173" satisfied condition "Succeeded or Failed"
    Mar  2 13:30:40.527: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 13:30:40.539
    Mar  2 13:30:40.552: INFO: Waiting for pod pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173 to disappear
    Mar  2 13:30:40.557: INFO: Pod pod-secrets-c6077a58-ed3f-4d88-a936-3b65baedd173 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 13:30:40.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1842" for this suite. 03/02/23 13:30:40.563
    STEP: Destroying namespace "secret-namespace-2331" for this suite. 03/02/23 13:30:40.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:30:40.578
Mar  2 13:30:40.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:30:40.579
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:40.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:40.647
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 03/02/23 13:30:40.65
Mar  2 13:30:40.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-1006 create -f -'
Mar  2 13:30:41.172: INFO: stderr: ""
Mar  2 13:30:41.172: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/02/23 13:30:41.172
Mar  2 13:30:41.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-1006 diff -f -'
Mar  2 13:30:41.846: INFO: rc: 1
Mar  2 13:30:41.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-1006 delete -f -'
Mar  2 13:30:42.021: INFO: stderr: ""
Mar  2 13:30:42.022: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:30:42.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1006" for this suite. 03/02/23 13:30:42.034
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":189,"skipped":3392,"failed":0}
------------------------------
â€¢ [1.468 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:30:40.578
    Mar  2 13:30:40.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:30:40.579
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:40.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:40.647
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 03/02/23 13:30:40.65
    Mar  2 13:30:40.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-1006 create -f -'
    Mar  2 13:30:41.172: INFO: stderr: ""
    Mar  2 13:30:41.172: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/02/23 13:30:41.172
    Mar  2 13:30:41.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-1006 diff -f -'
    Mar  2 13:30:41.846: INFO: rc: 1
    Mar  2 13:30:41.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-1006 delete -f -'
    Mar  2 13:30:42.021: INFO: stderr: ""
    Mar  2 13:30:42.022: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:30:42.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1006" for this suite. 03/02/23 13:30:42.034
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:30:42.046
Mar  2 13:30:42.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:30:42.048
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:42.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:42.079
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-4857 03/02/23 13:30:42.123
Mar  2 13:30:42.130: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4857" to be "running and ready"
Mar  2 13:30:42.136: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 6.177486ms
Mar  2 13:30:42.136: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:30:44.153: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.022976826s
Mar  2 13:30:44.153: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  2 13:30:44.153: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar  2 13:30:44.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 13:30:44.352: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  2 13:30:44.352: INFO: stdout: "iptables"
Mar  2 13:30:44.352: INFO: proxyMode: iptables
Mar  2 13:30:44.367: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 13:30:44.398: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-4857 03/02/23 13:30:44.398
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4857 03/02/23 13:30:44.412
I0302 13:30:44.435072      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4857, replica count: 3
I0302 13:30:47.526079      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 13:30:47.545: INFO: Creating new exec pod
Mar  2 13:30:47.565: INFO: Waiting up to 5m0s for pod "execpod-affinityb6mrm" in namespace "services-4857" to be "running"
Mar  2 13:30:47.580: INFO: Pod "execpod-affinityb6mrm": Phase="Pending", Reason="", readiness=false. Elapsed: 14.66567ms
Mar  2 13:30:49.593: INFO: Pod "execpod-affinityb6mrm": Phase="Running", Reason="", readiness=true. Elapsed: 2.027734147s
Mar  2 13:30:49.594: INFO: Pod "execpod-affinityb6mrm" satisfied condition "running"
Mar  2 13:30:50.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar  2 13:30:50.802: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-timeout 80\n+ echo hostName\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 13:30:50.802: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:30:50.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.44.67 80'
Mar  2 13:30:50.967: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.44.67 80\nConnection to 10.233.44.67 80 port [tcp/http] succeeded!\n"
Mar  2 13:30:50.967: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:30:50.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.61 31677'
Mar  2 13:30:51.159: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.0.61 31677\nConnection to 172.16.0.61 31677 port [tcp/*] succeeded!\n"
Mar  2 13:30:51.159: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:30:51.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.192 31677'
Mar  2 13:30:51.338: INFO: stderr: "+ nc -v -t -w 2 172.16.0.192 31677\nConnection to 172.16.0.192 31677 port [tcp/*] succeeded!\n+ echo hostName\n"
Mar  2 13:30:51.338: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:30:51.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.61:31677/ ; done'
Mar  2 13:30:51.735: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n"
Mar  2 13:30:51.735: INFO: stdout: "\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf"
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
Mar  2 13:30:51.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.0.61:31677/'
Mar  2 13:30:51.901: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n"
Mar  2 13:30:51.901: INFO: stdout: "affinity-nodeport-timeout-vglkf"
Mar  2 13:31:11.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.0.61:31677/'
Mar  2 13:31:12.080: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n"
Mar  2 13:31:12.080: INFO: stdout: "affinity-nodeport-timeout-5qrlf"
Mar  2 13:31:12.080: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4857, will wait for the garbage collector to delete the pods 03/02/23 13:31:12.11
Mar  2 13:31:12.175: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.220745ms
Mar  2 13:31:12.282: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 107.429501ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:31:14.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4857" for this suite. 03/02/23 13:31:14.721
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":190,"skipped":3396,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.685 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:30:42.046
    Mar  2 13:30:42.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:30:42.048
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:30:42.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:30:42.079
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-4857 03/02/23 13:30:42.123
    Mar  2 13:30:42.130: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4857" to be "running and ready"
    Mar  2 13:30:42.136: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 6.177486ms
    Mar  2 13:30:42.136: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:30:44.153: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.022976826s
    Mar  2 13:30:44.153: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar  2 13:30:44.153: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar  2 13:30:44.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar  2 13:30:44.352: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar  2 13:30:44.352: INFO: stdout: "iptables"
    Mar  2 13:30:44.352: INFO: proxyMode: iptables
    Mar  2 13:30:44.367: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar  2 13:30:44.398: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-4857 03/02/23 13:30:44.398
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-4857 03/02/23 13:30:44.412
    I0302 13:30:44.435072      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4857, replica count: 3
    I0302 13:30:47.526079      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 13:30:47.545: INFO: Creating new exec pod
    Mar  2 13:30:47.565: INFO: Waiting up to 5m0s for pod "execpod-affinityb6mrm" in namespace "services-4857" to be "running"
    Mar  2 13:30:47.580: INFO: Pod "execpod-affinityb6mrm": Phase="Pending", Reason="", readiness=false. Elapsed: 14.66567ms
    Mar  2 13:30:49.593: INFO: Pod "execpod-affinityb6mrm": Phase="Running", Reason="", readiness=true. Elapsed: 2.027734147s
    Mar  2 13:30:49.594: INFO: Pod "execpod-affinityb6mrm" satisfied condition "running"
    Mar  2 13:30:50.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Mar  2 13:30:50.802: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-timeout 80\n+ echo hostName\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Mar  2 13:30:50.802: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:30:50.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.44.67 80'
    Mar  2 13:30:50.967: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.44.67 80\nConnection to 10.233.44.67 80 port [tcp/http] succeeded!\n"
    Mar  2 13:30:50.967: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:30:50.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.61 31677'
    Mar  2 13:30:51.159: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.0.61 31677\nConnection to 172.16.0.61 31677 port [tcp/*] succeeded!\n"
    Mar  2 13:30:51.159: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:30:51.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.192 31677'
    Mar  2 13:30:51.338: INFO: stderr: "+ nc -v -t -w 2 172.16.0.192 31677\nConnection to 172.16.0.192 31677 port [tcp/*] succeeded!\n+ echo hostName\n"
    Mar  2 13:30:51.338: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:30:51.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.61:31677/ ; done'
    Mar  2 13:30:51.735: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n"
    Mar  2 13:30:51.735: INFO: stdout: "\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf\naffinity-nodeport-timeout-vglkf"
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Received response from host: affinity-nodeport-timeout-vglkf
    Mar  2 13:30:51.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.0.61:31677/'
    Mar  2 13:30:51.901: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n"
    Mar  2 13:30:51.901: INFO: stdout: "affinity-nodeport-timeout-vglkf"
    Mar  2 13:31:11.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4857 exec execpod-affinityb6mrm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.0.61:31677/'
    Mar  2 13:31:12.080: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.0.61:31677/\n"
    Mar  2 13:31:12.080: INFO: stdout: "affinity-nodeport-timeout-5qrlf"
    Mar  2 13:31:12.080: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4857, will wait for the garbage collector to delete the pods 03/02/23 13:31:12.11
    Mar  2 13:31:12.175: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.220745ms
    Mar  2 13:31:12.282: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 107.429501ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:31:14.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4857" for this suite. 03/02/23 13:31:14.721
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:31:14.735
Mar  2 13:31:14.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pods 03/02/23 13:31:14.736
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:31:14.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:31:14.763
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 03/02/23 13:31:14.767
Mar  2 13:31:14.777: INFO: Waiting up to 5m0s for pod "pod-bkrmb" in namespace "pods-3751" to be "running"
Mar  2 13:31:14.805: INFO: Pod "pod-bkrmb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.63546ms
Mar  2 13:31:16.816: INFO: Pod "pod-bkrmb": Phase="Running", Reason="", readiness=true. Elapsed: 2.018846866s
Mar  2 13:31:16.816: INFO: Pod "pod-bkrmb" satisfied condition "running"
STEP: patching /status 03/02/23 13:31:16.816
Mar  2 13:31:16.834: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 13:31:16.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3751" for this suite. 03/02/23 13:31:16.842
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":191,"skipped":3400,"failed":0}
------------------------------
â€¢ [2.118 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:31:14.735
    Mar  2 13:31:14.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pods 03/02/23 13:31:14.736
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:31:14.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:31:14.763
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 03/02/23 13:31:14.767
    Mar  2 13:31:14.777: INFO: Waiting up to 5m0s for pod "pod-bkrmb" in namespace "pods-3751" to be "running"
    Mar  2 13:31:14.805: INFO: Pod "pod-bkrmb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.63546ms
    Mar  2 13:31:16.816: INFO: Pod "pod-bkrmb": Phase="Running", Reason="", readiness=true. Elapsed: 2.018846866s
    Mar  2 13:31:16.816: INFO: Pod "pod-bkrmb" satisfied condition "running"
    STEP: patching /status 03/02/23 13:31:16.816
    Mar  2 13:31:16.834: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 13:31:16.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3751" for this suite. 03/02/23 13:31:16.842
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:31:16.858
Mar  2 13:31:16.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 13:31:16.859
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:31:16.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:31:16.881
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-1666c2d4-0018-4f9a-a6ab-df69af5c2258 03/02/23 13:31:16.887
STEP: Creating a pod to test consume secrets 03/02/23 13:31:16.891
Mar  2 13:31:16.902: INFO: Waiting up to 5m0s for pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841" in namespace "secrets-4762" to be "Succeeded or Failed"
Mar  2 13:31:16.910: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841": Phase="Pending", Reason="", readiness=false. Elapsed: 8.28021ms
Mar  2 13:31:18.919: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841": Phase="Running", Reason="", readiness=true. Elapsed: 2.016675152s
Mar  2 13:31:20.920: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841": Phase="Running", Reason="", readiness=false. Elapsed: 4.018396149s
Mar  2 13:31:22.963: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841": Phase="Running", Reason="", readiness=false. Elapsed: 6.061102702s
Mar  2 13:31:24.919: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016593246s
STEP: Saw pod success 03/02/23 13:31:24.919
Mar  2 13:31:24.919: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841" satisfied condition "Succeeded or Failed"
Mar  2 13:31:24.927: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 13:31:24.941
Mar  2 13:31:24.958: INFO: Waiting for pod pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841 to disappear
Mar  2 13:31:24.960: INFO: Pod pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 13:31:24.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4762" for this suite. 03/02/23 13:31:24.966
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":192,"skipped":3404,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.115 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:31:16.858
    Mar  2 13:31:16.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 13:31:16.859
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:31:16.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:31:16.881
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-1666c2d4-0018-4f9a-a6ab-df69af5c2258 03/02/23 13:31:16.887
    STEP: Creating a pod to test consume secrets 03/02/23 13:31:16.891
    Mar  2 13:31:16.902: INFO: Waiting up to 5m0s for pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841" in namespace "secrets-4762" to be "Succeeded or Failed"
    Mar  2 13:31:16.910: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841": Phase="Pending", Reason="", readiness=false. Elapsed: 8.28021ms
    Mar  2 13:31:18.919: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841": Phase="Running", Reason="", readiness=true. Elapsed: 2.016675152s
    Mar  2 13:31:20.920: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841": Phase="Running", Reason="", readiness=false. Elapsed: 4.018396149s
    Mar  2 13:31:22.963: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841": Phase="Running", Reason="", readiness=false. Elapsed: 6.061102702s
    Mar  2 13:31:24.919: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016593246s
    STEP: Saw pod success 03/02/23 13:31:24.919
    Mar  2 13:31:24.919: INFO: Pod "pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841" satisfied condition "Succeeded or Failed"
    Mar  2 13:31:24.927: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 13:31:24.941
    Mar  2 13:31:24.958: INFO: Waiting for pod pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841 to disappear
    Mar  2 13:31:24.960: INFO: Pod pod-secrets-ac0c70e4-3e22-4dba-a24f-8f008f470841 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 13:31:24.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4762" for this suite. 03/02/23 13:31:24.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:31:24.977
Mar  2 13:31:24.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename taint-single-pod 03/02/23 13:31:24.979
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:31:25.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:31:25.009
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Mar  2 13:31:25.013: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 13:32:25.187: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Mar  2 13:32:25.192: INFO: Starting informer...
STEP: Starting pod... 03/02/23 13:32:25.192
Mar  2 13:32:25.428: INFO: Pod is running on aarnq-sc-k8s-node-srv2. Tainting Node
STEP: Trying to apply a taint on the Node 03/02/23 13:32:25.428
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 13:32:25.448
STEP: Waiting short time to make sure Pod is queued for deletion 03/02/23 13:32:25.462
Mar  2 13:32:25.462: INFO: Pod wasn't evicted. Proceeding
Mar  2 13:32:25.462: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 13:32:25.509
STEP: Waiting some time to make sure that toleration time passed. 03/02/23 13:32:25.562
Mar  2 13:33:40.562: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Mar  2 13:33:40.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1307" for this suite. 03/02/23 13:33:40.592
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":193,"skipped":3430,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.623 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:31:24.977
    Mar  2 13:31:24.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename taint-single-pod 03/02/23 13:31:24.979
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:31:25.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:31:25.009
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Mar  2 13:31:25.013: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 13:32:25.187: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Mar  2 13:32:25.192: INFO: Starting informer...
    STEP: Starting pod... 03/02/23 13:32:25.192
    Mar  2 13:32:25.428: INFO: Pod is running on aarnq-sc-k8s-node-srv2. Tainting Node
    STEP: Trying to apply a taint on the Node 03/02/23 13:32:25.428
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 13:32:25.448
    STEP: Waiting short time to make sure Pod is queued for deletion 03/02/23 13:32:25.462
    Mar  2 13:32:25.462: INFO: Pod wasn't evicted. Proceeding
    Mar  2 13:32:25.462: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 13:32:25.509
    STEP: Waiting some time to make sure that toleration time passed. 03/02/23 13:32:25.562
    Mar  2 13:33:40.562: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 13:33:40.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-1307" for this suite. 03/02/23 13:33:40.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:33:40.607
Mar  2 13:33:40.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:33:40.611
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:33:40.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:33:40.631
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-0b908966-8455-49cd-bdbb-0e158eb8ef2c 03/02/23 13:33:40.639
STEP: Creating the pod 03/02/23 13:33:40.645
Mar  2 13:33:40.654: INFO: Waiting up to 5m0s for pod "pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2" in namespace "configmap-6078" to be "running and ready"
Mar  2 13:33:40.661: INFO: Pod "pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042827ms
Mar  2 13:33:40.661: INFO: The phase of Pod pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:33:42.673: INFO: Pod "pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018767298s
Mar  2 13:33:42.673: INFO: The phase of Pod pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:33:44.668: INFO: Pod "pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.013986937s
Mar  2 13:33:44.669: INFO: The phase of Pod pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2 is Running (Ready = true)
Mar  2 13:33:44.669: INFO: Pod "pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-0b908966-8455-49cd-bdbb-0e158eb8ef2c 03/02/23 13:33:44.738
STEP: waiting to observe update in volume 03/02/23 13:33:44.75
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:35:15.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6078" for this suite. 03/02/23 13:35:15.428
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":194,"skipped":3439,"failed":0}
------------------------------
â€¢ [SLOW TEST] [94.832 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:33:40.607
    Mar  2 13:33:40.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:33:40.611
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:33:40.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:33:40.631
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-0b908966-8455-49cd-bdbb-0e158eb8ef2c 03/02/23 13:33:40.639
    STEP: Creating the pod 03/02/23 13:33:40.645
    Mar  2 13:33:40.654: INFO: Waiting up to 5m0s for pod "pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2" in namespace "configmap-6078" to be "running and ready"
    Mar  2 13:33:40.661: INFO: Pod "pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042827ms
    Mar  2 13:33:40.661: INFO: The phase of Pod pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:33:42.673: INFO: Pod "pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018767298s
    Mar  2 13:33:42.673: INFO: The phase of Pod pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:33:44.668: INFO: Pod "pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.013986937s
    Mar  2 13:33:44.669: INFO: The phase of Pod pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2 is Running (Ready = true)
    Mar  2 13:33:44.669: INFO: Pod "pod-configmaps-053cb0c0-7b60-4d3c-a3d4-4ff37a5fc1e2" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-0b908966-8455-49cd-bdbb-0e158eb8ef2c 03/02/23 13:33:44.738
    STEP: waiting to observe update in volume 03/02/23 13:33:44.75
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:35:15.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6078" for this suite. 03/02/23 13:35:15.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:35:15.446
Mar  2 13:35:15.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:35:15.45
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:15.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:15.485
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-8647 03/02/23 13:35:15.488
STEP: creating service affinity-nodeport in namespace services-8647 03/02/23 13:35:15.489
STEP: creating replication controller affinity-nodeport in namespace services-8647 03/02/23 13:35:15.505
I0302 13:35:15.518380      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8647, replica count: 3
I0302 13:35:18.611377      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 13:35:18.632: INFO: Creating new exec pod
Mar  2 13:35:18.639: INFO: Waiting up to 5m0s for pod "execpod-affinityqsnnz" in namespace "services-8647" to be "running"
Mar  2 13:35:18.650: INFO: Pod "execpod-affinityqsnnz": Phase="Pending", Reason="", readiness=false. Elapsed: 11.808351ms
Mar  2 13:35:20.656: INFO: Pod "execpod-affinityqsnnz": Phase="Running", Reason="", readiness=true. Elapsed: 2.016941032s
Mar  2 13:35:20.656: INFO: Pod "execpod-affinityqsnnz" satisfied condition "running"
Mar  2 13:35:21.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8647 exec execpod-affinityqsnnz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar  2 13:35:22.171: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar  2 13:35:22.176: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:35:22.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8647 exec execpod-affinityqsnnz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.51.202 80'
Mar  2 13:35:22.391: INFO: stderr: "+ nc -v -t -w 2 10.233.51.202 80\n+ echo hostName\nConnection to 10.233.51.202 80 port [tcp/http] succeeded!\n"
Mar  2 13:35:22.391: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:35:22.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8647 exec execpod-affinityqsnnz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.192 31632'
Mar  2 13:35:22.585: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.0.192 31632\nConnection to 172.16.0.192 31632 port [tcp/*] succeeded!\n"
Mar  2 13:35:22.585: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:35:22.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8647 exec execpod-affinityqsnnz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.56 31632'
Mar  2 13:35:22.808: INFO: stderr: "+ nc -v -t -w 2 172.16.0.56 31632\n+ echo hostName\nConnection to 172.16.0.56 31632 port [tcp/*] succeeded!\n"
Mar  2 13:35:22.808: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:35:22.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8647 exec execpod-affinityqsnnz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.61:31632/ ; done'
Mar  2 13:35:23.210: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n"
Mar  2 13:35:23.211: INFO: stdout: "\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf"
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
Mar  2 13:35:23.211: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8647, will wait for the garbage collector to delete the pods 03/02/23 13:35:23.235
Mar  2 13:35:23.309: INFO: Deleting ReplicationController affinity-nodeport took: 15.760829ms
Mar  2 13:35:23.414: INFO: Terminating ReplicationController affinity-nodeport pods took: 104.756342ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:35:25.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8647" for this suite. 03/02/23 13:35:25.879
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":195,"skipped":3451,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.441 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:35:15.446
    Mar  2 13:35:15.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:35:15.45
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:15.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:15.485
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-8647 03/02/23 13:35:15.488
    STEP: creating service affinity-nodeport in namespace services-8647 03/02/23 13:35:15.489
    STEP: creating replication controller affinity-nodeport in namespace services-8647 03/02/23 13:35:15.505
    I0302 13:35:15.518380      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8647, replica count: 3
    I0302 13:35:18.611377      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 13:35:18.632: INFO: Creating new exec pod
    Mar  2 13:35:18.639: INFO: Waiting up to 5m0s for pod "execpod-affinityqsnnz" in namespace "services-8647" to be "running"
    Mar  2 13:35:18.650: INFO: Pod "execpod-affinityqsnnz": Phase="Pending", Reason="", readiness=false. Elapsed: 11.808351ms
    Mar  2 13:35:20.656: INFO: Pod "execpod-affinityqsnnz": Phase="Running", Reason="", readiness=true. Elapsed: 2.016941032s
    Mar  2 13:35:20.656: INFO: Pod "execpod-affinityqsnnz" satisfied condition "running"
    Mar  2 13:35:21.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8647 exec execpod-affinityqsnnz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Mar  2 13:35:22.171: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar  2 13:35:22.176: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:35:22.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8647 exec execpod-affinityqsnnz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.51.202 80'
    Mar  2 13:35:22.391: INFO: stderr: "+ nc -v -t -w 2 10.233.51.202 80\n+ echo hostName\nConnection to 10.233.51.202 80 port [tcp/http] succeeded!\n"
    Mar  2 13:35:22.391: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:35:22.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8647 exec execpod-affinityqsnnz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.192 31632'
    Mar  2 13:35:22.585: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.0.192 31632\nConnection to 172.16.0.192 31632 port [tcp/*] succeeded!\n"
    Mar  2 13:35:22.585: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:35:22.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8647 exec execpod-affinityqsnnz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.56 31632'
    Mar  2 13:35:22.808: INFO: stderr: "+ nc -v -t -w 2 172.16.0.56 31632\n+ echo hostName\nConnection to 172.16.0.56 31632 port [tcp/*] succeeded!\n"
    Mar  2 13:35:22.808: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:35:22.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8647 exec execpod-affinityqsnnz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.61:31632/ ; done'
    Mar  2 13:35:23.210: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:31632/\n"
    Mar  2 13:35:23.211: INFO: stdout: "\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf\naffinity-nodeport-qtsvf"
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Received response from host: affinity-nodeport-qtsvf
    Mar  2 13:35:23.211: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-8647, will wait for the garbage collector to delete the pods 03/02/23 13:35:23.235
    Mar  2 13:35:23.309: INFO: Deleting ReplicationController affinity-nodeport took: 15.760829ms
    Mar  2 13:35:23.414: INFO: Terminating ReplicationController affinity-nodeport pods took: 104.756342ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:35:25.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8647" for this suite. 03/02/23 13:35:25.879
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:35:25.898
Mar  2 13:35:25.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:35:25.899
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:25.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:25.926
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-5856 03/02/23 13:35:25.929
STEP: creating service affinity-nodeport-transition in namespace services-5856 03/02/23 13:35:25.93
STEP: creating replication controller affinity-nodeport-transition in namespace services-5856 03/02/23 13:35:25.94
I0302 13:35:25.952315      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5856, replica count: 3
I0302 13:35:29.071053      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 13:35:29.087: INFO: Creating new exec pod
Mar  2 13:35:29.096: INFO: Waiting up to 5m0s for pod "execpod-affinity9hglr" in namespace "services-5856" to be "running"
Mar  2 13:35:29.102: INFO: Pod "execpod-affinity9hglr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.790408ms
Mar  2 13:35:31.107: INFO: Pod "execpod-affinity9hglr": Phase="Running", Reason="", readiness=true. Elapsed: 2.011063306s
Mar  2 13:35:31.108: INFO: Pod "execpod-affinity9hglr" satisfied condition "running"
Mar  2 13:35:32.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar  2 13:35:32.309: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar  2 13:35:32.310: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:35:32.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.26.79 80'
Mar  2 13:35:32.519: INFO: stderr: "+ nc -v -t -w 2 10.233.26.79 80\n+ echo hostName\nConnection to 10.233.26.79 80 port [tcp/http] succeeded!\n"
Mar  2 13:35:32.519: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:35:32.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.61 32267'
Mar  2 13:35:32.672: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.0.61 32267\nConnection to 172.16.0.61 32267 port [tcp/*] succeeded!\n"
Mar  2 13:35:32.672: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:35:32.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.56 32267'
Mar  2 13:35:32.848: INFO: stderr: "+ nc -v -t -w 2 172.16.0.56 32267\n+ echo hostName\nConnection to 172.16.0.56 32267 port [tcp/*] succeeded!\n"
Mar  2 13:35:32.848: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 13:35:32.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.61:32267/ ; done'
Mar  2 13:35:33.221: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n"
Mar  2 13:35:33.221: INFO: stdout: "\naffinity-nodeport-transition-flmkx\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-flmkx\naffinity-nodeport-transition-flmkx\naffinity-nodeport-transition-flmkx\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-flmkx\naffinity-nodeport-transition-kblqz"
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-flmkx
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-flmkx
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-flmkx
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-flmkx
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-flmkx
Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
Mar  2 13:35:33.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.61:32267/ ; done'
Mar  2 13:35:33.507: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n"
Mar  2 13:35:33.507: INFO: stdout: "\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24"
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
Mar  2 13:35:33.507: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5856, will wait for the garbage collector to delete the pods 03/02/23 13:35:33.53
Mar  2 13:35:33.606: INFO: Deleting ReplicationController affinity-nodeport-transition took: 16.274087ms
Mar  2 13:35:33.707: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.899781ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:35:35.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5856" for this suite. 03/02/23 13:35:35.943
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":196,"skipped":3494,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.052 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:35:25.898
    Mar  2 13:35:25.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:35:25.899
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:25.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:25.926
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-5856 03/02/23 13:35:25.929
    STEP: creating service affinity-nodeport-transition in namespace services-5856 03/02/23 13:35:25.93
    STEP: creating replication controller affinity-nodeport-transition in namespace services-5856 03/02/23 13:35:25.94
    I0302 13:35:25.952315      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5856, replica count: 3
    I0302 13:35:29.071053      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 13:35:29.087: INFO: Creating new exec pod
    Mar  2 13:35:29.096: INFO: Waiting up to 5m0s for pod "execpod-affinity9hglr" in namespace "services-5856" to be "running"
    Mar  2 13:35:29.102: INFO: Pod "execpod-affinity9hglr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.790408ms
    Mar  2 13:35:31.107: INFO: Pod "execpod-affinity9hglr": Phase="Running", Reason="", readiness=true. Elapsed: 2.011063306s
    Mar  2 13:35:31.108: INFO: Pod "execpod-affinity9hglr" satisfied condition "running"
    Mar  2 13:35:32.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Mar  2 13:35:32.309: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar  2 13:35:32.310: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:35:32.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.26.79 80'
    Mar  2 13:35:32.519: INFO: stderr: "+ nc -v -t -w 2 10.233.26.79 80\n+ echo hostName\nConnection to 10.233.26.79 80 port [tcp/http] succeeded!\n"
    Mar  2 13:35:32.519: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:35:32.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.61 32267'
    Mar  2 13:35:32.672: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.0.61 32267\nConnection to 172.16.0.61 32267 port [tcp/*] succeeded!\n"
    Mar  2 13:35:32.672: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:35:32.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.56 32267'
    Mar  2 13:35:32.848: INFO: stderr: "+ nc -v -t -w 2 172.16.0.56 32267\n+ echo hostName\nConnection to 172.16.0.56 32267 port [tcp/*] succeeded!\n"
    Mar  2 13:35:32.848: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 13:35:32.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.61:32267/ ; done'
    Mar  2 13:35:33.221: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n"
    Mar  2 13:35:33.221: INFO: stdout: "\naffinity-nodeport-transition-flmkx\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-flmkx\naffinity-nodeport-transition-flmkx\naffinity-nodeport-transition-flmkx\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-kblqz\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-flmkx\naffinity-nodeport-transition-kblqz"
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-flmkx
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-flmkx
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-flmkx
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-flmkx
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-flmkx
    Mar  2 13:35:33.221: INFO: Received response from host: affinity-nodeport-transition-kblqz
    Mar  2 13:35:33.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-5856 exec execpod-affinity9hglr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.61:32267/ ; done'
    Mar  2 13:35:33.507: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.61:32267/\n"
    Mar  2 13:35:33.507: INFO: stdout: "\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24\naffinity-nodeport-transition-g7x24"
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Received response from host: affinity-nodeport-transition-g7x24
    Mar  2 13:35:33.507: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5856, will wait for the garbage collector to delete the pods 03/02/23 13:35:33.53
    Mar  2 13:35:33.606: INFO: Deleting ReplicationController affinity-nodeport-transition took: 16.274087ms
    Mar  2 13:35:33.707: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.899781ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:35:35.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5856" for this suite. 03/02/23 13:35:35.943
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:35:35.956
Mar  2 13:35:35.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:35:35.959
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:36.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:36.033
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-cb469555-c545-4716-a078-56c2b384ec1b 03/02/23 13:35:36.039
STEP: Creating a pod to test consume configMaps 03/02/23 13:35:36.045
Mar  2 13:35:36.058: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4" in namespace "projected-4257" to be "Succeeded or Failed"
Mar  2 13:35:36.068: INFO: Pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.610562ms
Mar  2 13:35:38.082: INFO: Pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022900765s
Mar  2 13:35:40.075: INFO: Pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016047097s
Mar  2 13:35:42.082: INFO: Pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022924731s
STEP: Saw pod success 03/02/23 13:35:42.082
Mar  2 13:35:42.083: INFO: Pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4" satisfied condition "Succeeded or Failed"
Mar  2 13:35:42.087: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 13:35:42.096
Mar  2 13:35:42.135: INFO: Waiting for pod pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4 to disappear
Mar  2 13:35:42.141: INFO: Pod pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 13:35:42.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4257" for this suite. 03/02/23 13:35:42.147
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":197,"skipped":3508,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.200 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:35:35.956
    Mar  2 13:35:35.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:35:35.959
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:36.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:36.033
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-cb469555-c545-4716-a078-56c2b384ec1b 03/02/23 13:35:36.039
    STEP: Creating a pod to test consume configMaps 03/02/23 13:35:36.045
    Mar  2 13:35:36.058: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4" in namespace "projected-4257" to be "Succeeded or Failed"
    Mar  2 13:35:36.068: INFO: Pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.610562ms
    Mar  2 13:35:38.082: INFO: Pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022900765s
    Mar  2 13:35:40.075: INFO: Pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016047097s
    Mar  2 13:35:42.082: INFO: Pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022924731s
    STEP: Saw pod success 03/02/23 13:35:42.082
    Mar  2 13:35:42.083: INFO: Pod "pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4" satisfied condition "Succeeded or Failed"
    Mar  2 13:35:42.087: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 13:35:42.096
    Mar  2 13:35:42.135: INFO: Waiting for pod pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4 to disappear
    Mar  2 13:35:42.141: INFO: Pod pod-projected-configmaps-6055ff7a-ab3c-44c6-90a1-a67d880b75b4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 13:35:42.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4257" for this suite. 03/02/23 13:35:42.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:35:42.158
Mar  2 13:35:42.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:35:42.159
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:42.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:42.232
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-4217/configmap-test-8f13430b-ea8b-48b6-9cbf-183ea49811bf 03/02/23 13:35:42.256
STEP: Creating a pod to test consume configMaps 03/02/23 13:35:42.269
Mar  2 13:35:42.307: INFO: Waiting up to 5m0s for pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7" in namespace "configmap-4217" to be "Succeeded or Failed"
Mar  2 13:35:42.317: INFO: Pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.563231ms
Mar  2 13:35:44.324: INFO: Pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014356186s
Mar  2 13:35:46.325: INFO: Pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015283641s
Mar  2 13:35:48.323: INFO: Pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013310592s
STEP: Saw pod success 03/02/23 13:35:48.323
Mar  2 13:35:48.323: INFO: Pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7" satisfied condition "Succeeded or Failed"
Mar  2 13:35:48.328: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7 container env-test: <nil>
STEP: delete the pod 03/02/23 13:35:48.335
Mar  2 13:35:48.354: INFO: Waiting for pod pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7 to disappear
Mar  2 13:35:48.357: INFO: Pod pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:35:48.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4217" for this suite. 03/02/23 13:35:48.366
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":198,"skipped":3550,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.224 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:35:42.158
    Mar  2 13:35:42.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:35:42.159
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:42.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:42.232
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-4217/configmap-test-8f13430b-ea8b-48b6-9cbf-183ea49811bf 03/02/23 13:35:42.256
    STEP: Creating a pod to test consume configMaps 03/02/23 13:35:42.269
    Mar  2 13:35:42.307: INFO: Waiting up to 5m0s for pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7" in namespace "configmap-4217" to be "Succeeded or Failed"
    Mar  2 13:35:42.317: INFO: Pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.563231ms
    Mar  2 13:35:44.324: INFO: Pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014356186s
    Mar  2 13:35:46.325: INFO: Pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015283641s
    Mar  2 13:35:48.323: INFO: Pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013310592s
    STEP: Saw pod success 03/02/23 13:35:48.323
    Mar  2 13:35:48.323: INFO: Pod "pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7" satisfied condition "Succeeded or Failed"
    Mar  2 13:35:48.328: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7 container env-test: <nil>
    STEP: delete the pod 03/02/23 13:35:48.335
    Mar  2 13:35:48.354: INFO: Waiting for pod pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7 to disappear
    Mar  2 13:35:48.357: INFO: Pod pod-configmaps-97069074-3b36-4871-b312-c079fdea12c7 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:35:48.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4217" for this suite. 03/02/23 13:35:48.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:35:48.383
Mar  2 13:35:48.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:35:48.384
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:48.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:48.412
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-8105 03/02/23 13:35:48.416
STEP: creating replication controller nodeport-test in namespace services-8105 03/02/23 13:35:48.428
I0302 13:35:48.444627      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8105, replica count: 2
I0302 13:35:51.521700      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 13:35:51.522: INFO: Creating new exec pod
Mar  2 13:35:51.533: INFO: Waiting up to 5m0s for pod "execpod5n247" in namespace "services-8105" to be "running"
Mar  2 13:35:51.539: INFO: Pod "execpod5n247": Phase="Pending", Reason="", readiness=false. Elapsed: 4.796575ms
Mar  2 13:35:53.543: INFO: Pod "execpod5n247": Phase="Running", Reason="", readiness=true. Elapsed: 2.00864538s
Mar  2 13:35:53.543: INFO: Pod "execpod5n247" satisfied condition "running"
Mar  2 13:35:54.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar  2 13:35:54.850: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  2 13:35:54.850: INFO: stdout: "nodeport-test-ctnw4"
Mar  2 13:35:54.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.134 80'
Mar  2 13:35:55.071: INFO: stderr: "+ nc -v -t -w 2 10.233.5.134 80\nConnection to 10.233.5.134 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Mar  2 13:35:55.071: INFO: stdout: ""
Mar  2 13:35:56.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.134 80'
Mar  2 13:35:56.260: INFO: stderr: "+ nc -v -t -w 2 10.233.5.134 80\n+ echo hostName\nConnection to 10.233.5.134 80 port [tcp/http] succeeded!\n"
Mar  2 13:35:56.260: INFO: stdout: ""
Mar  2 13:35:57.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.134 80'
Mar  2 13:35:57.246: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.134 80\nConnection to 10.233.5.134 80 port [tcp/http] succeeded!\n"
Mar  2 13:35:57.246: INFO: stdout: "nodeport-test-ctnw4"
Mar  2 13:35:57.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.192 32766'
Mar  2 13:35:57.415: INFO: stderr: "+ nc -v -t -w 2 172.16.0.192 32766\n+ echo hostName\nConnection to 172.16.0.192 32766 port [tcp/*] succeeded!\n"
Mar  2 13:35:57.415: INFO: stdout: "nodeport-test-ctnw4"
Mar  2 13:35:57.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.61 32766'
Mar  2 13:35:57.575: INFO: stderr: "+ nc -v -t -w 2 172.16.0.61 32766\n+ echo hostName\nConnection to 172.16.0.61 32766 port [tcp/*] succeeded!\n"
Mar  2 13:35:57.575: INFO: stdout: "nodeport-test-fjch9"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:35:57.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8105" for this suite. 03/02/23 13:35:57.584
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":199,"skipped":3564,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.207 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:35:48.383
    Mar  2 13:35:48.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:35:48.384
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:48.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:48.412
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-8105 03/02/23 13:35:48.416
    STEP: creating replication controller nodeport-test in namespace services-8105 03/02/23 13:35:48.428
    I0302 13:35:48.444627      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8105, replica count: 2
    I0302 13:35:51.521700      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 13:35:51.522: INFO: Creating new exec pod
    Mar  2 13:35:51.533: INFO: Waiting up to 5m0s for pod "execpod5n247" in namespace "services-8105" to be "running"
    Mar  2 13:35:51.539: INFO: Pod "execpod5n247": Phase="Pending", Reason="", readiness=false. Elapsed: 4.796575ms
    Mar  2 13:35:53.543: INFO: Pod "execpod5n247": Phase="Running", Reason="", readiness=true. Elapsed: 2.00864538s
    Mar  2 13:35:53.543: INFO: Pod "execpod5n247" satisfied condition "running"
    Mar  2 13:35:54.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Mar  2 13:35:54.850: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar  2 13:35:54.850: INFO: stdout: "nodeport-test-ctnw4"
    Mar  2 13:35:54.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.134 80'
    Mar  2 13:35:55.071: INFO: stderr: "+ nc -v -t -w 2 10.233.5.134 80\nConnection to 10.233.5.134 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Mar  2 13:35:55.071: INFO: stdout: ""
    Mar  2 13:35:56.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.134 80'
    Mar  2 13:35:56.260: INFO: stderr: "+ nc -v -t -w 2 10.233.5.134 80\n+ echo hostName\nConnection to 10.233.5.134 80 port [tcp/http] succeeded!\n"
    Mar  2 13:35:56.260: INFO: stdout: ""
    Mar  2 13:35:57.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.134 80'
    Mar  2 13:35:57.246: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.134 80\nConnection to 10.233.5.134 80 port [tcp/http] succeeded!\n"
    Mar  2 13:35:57.246: INFO: stdout: "nodeport-test-ctnw4"
    Mar  2 13:35:57.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.192 32766'
    Mar  2 13:35:57.415: INFO: stderr: "+ nc -v -t -w 2 172.16.0.192 32766\n+ echo hostName\nConnection to 172.16.0.192 32766 port [tcp/*] succeeded!\n"
    Mar  2 13:35:57.415: INFO: stdout: "nodeport-test-ctnw4"
    Mar  2 13:35:57.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-8105 exec execpod5n247 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.61 32766'
    Mar  2 13:35:57.575: INFO: stderr: "+ nc -v -t -w 2 172.16.0.61 32766\n+ echo hostName\nConnection to 172.16.0.61 32766 port [tcp/*] succeeded!\n"
    Mar  2 13:35:57.575: INFO: stdout: "nodeport-test-fjch9"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:35:57.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8105" for this suite. 03/02/23 13:35:57.584
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:35:57.592
Mar  2 13:35:57.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:35:57.593
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:57.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:57.614
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 03/02/23 13:35:57.617
Mar  2 13:35:57.618: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-7117 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/02/23 13:35:57.67
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:35:57.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7117" for this suite. 03/02/23 13:35:57.696
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":200,"skipped":3572,"failed":0}
------------------------------
â€¢ [0.112 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:35:57.592
    Mar  2 13:35:57.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:35:57.593
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:57.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:57.614
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 03/02/23 13:35:57.617
    Mar  2 13:35:57.618: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-7117 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/02/23 13:35:57.67
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:35:57.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7117" for this suite. 03/02/23 13:35:57.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:35:57.707
Mar  2 13:35:57.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename resourcequota 03/02/23 13:35:57.71
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:57.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:57.724
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 03/02/23 13:35:57.728
STEP: Creating a ResourceQuota 03/02/23 13:36:02.732
STEP: Ensuring resource quota status is calculated 03/02/23 13:36:02.738
STEP: Creating a ReplicationController 03/02/23 13:36:04.743
STEP: Ensuring resource quota status captures replication controller creation 03/02/23 13:36:04.755
STEP: Deleting a ReplicationController 03/02/23 13:36:06.764
STEP: Ensuring resource quota status released usage 03/02/23 13:36:06.77
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 13:36:08.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7863" for this suite. 03/02/23 13:36:08.784
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":201,"skipped":3601,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:35:57.707
    Mar  2 13:35:57.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename resourcequota 03/02/23 13:35:57.71
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:35:57.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:35:57.724
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 03/02/23 13:35:57.728
    STEP: Creating a ResourceQuota 03/02/23 13:36:02.732
    STEP: Ensuring resource quota status is calculated 03/02/23 13:36:02.738
    STEP: Creating a ReplicationController 03/02/23 13:36:04.743
    STEP: Ensuring resource quota status captures replication controller creation 03/02/23 13:36:04.755
    STEP: Deleting a ReplicationController 03/02/23 13:36:06.764
    STEP: Ensuring resource quota status released usage 03/02/23 13:36:06.77
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 13:36:08.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7863" for this suite. 03/02/23 13:36:08.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:36:08.799
Mar  2 13:36:08.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename cronjob 03/02/23 13:36:08.8
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:36:08.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:36:08.822
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/02/23 13:36:08.827
STEP: Ensuring a job is scheduled 03/02/23 13:36:08.831
STEP: Ensuring exactly one is scheduled 03/02/23 13:37:00.842
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/02/23 13:37:00.853
STEP: Ensuring no more jobs are scheduled 03/02/23 13:37:00.874
STEP: Removing cronjob 03/02/23 13:42:00.884
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  2 13:42:00.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6743" for this suite. 03/02/23 13:42:00.902
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":202,"skipped":3619,"failed":0}
------------------------------
â€¢ [SLOW TEST] [352.114 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:36:08.799
    Mar  2 13:36:08.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename cronjob 03/02/23 13:36:08.8
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:36:08.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:36:08.822
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/02/23 13:36:08.827
    STEP: Ensuring a job is scheduled 03/02/23 13:36:08.831
    STEP: Ensuring exactly one is scheduled 03/02/23 13:37:00.842
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/02/23 13:37:00.853
    STEP: Ensuring no more jobs are scheduled 03/02/23 13:37:00.874
    STEP: Removing cronjob 03/02/23 13:42:00.884
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  2 13:42:00.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6743" for this suite. 03/02/23 13:42:00.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:42:00.927
Mar  2 13:42:00.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-runtime 03/02/23 13:42:00.93
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:42:00.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:42:00.977
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 03/02/23 13:42:00.982
STEP: wait for the container to reach Succeeded 03/02/23 13:42:00.991
STEP: get the container status 03/02/23 13:42:06.047
STEP: the container should be terminated 03/02/23 13:42:06.05
STEP: the termination message should be set 03/02/23 13:42:06.051
Mar  2 13:42:06.051: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/02/23 13:42:06.051
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  2 13:42:06.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2216" for this suite. 03/02/23 13:42:06.081
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":203,"skipped":3652,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.159 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:42:00.927
    Mar  2 13:42:00.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-runtime 03/02/23 13:42:00.93
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:42:00.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:42:00.977
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 03/02/23 13:42:00.982
    STEP: wait for the container to reach Succeeded 03/02/23 13:42:00.991
    STEP: get the container status 03/02/23 13:42:06.047
    STEP: the container should be terminated 03/02/23 13:42:06.05
    STEP: the termination message should be set 03/02/23 13:42:06.051
    Mar  2 13:42:06.051: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/02/23 13:42:06.051
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  2 13:42:06.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2216" for this suite. 03/02/23 13:42:06.081
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:42:06.091
Mar  2 13:42:06.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 13:42:06.092
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:42:06.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:42:06.161
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/02/23 13:42:06.165
Mar  2 13:42:06.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:42:16.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:42:38.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2983" for this suite. 03/02/23 13:42:38.656
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":204,"skipped":3653,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.569 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:42:06.091
    Mar  2 13:42:06.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 13:42:06.092
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:42:06.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:42:06.161
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/02/23 13:42:06.165
    Mar  2 13:42:06.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:42:16.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:42:38.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2983" for this suite. 03/02/23 13:42:38.656
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:42:38.673
Mar  2 13:42:38.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename deployment 03/02/23 13:42:38.676
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:42:38.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:42:38.693
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar  2 13:42:38.706: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  2 13:42:43.719: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 13:42:43.719
Mar  2 13:42:43.719: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  2 13:42:45.725: INFO: Creating deployment "test-rollover-deployment"
Mar  2 13:42:45.740: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  2 13:42:47.757: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  2 13:42:47.764: INFO: Ensure that both replica sets have 1 created replica
Mar  2 13:42:47.770: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  2 13:42:47.787: INFO: Updating deployment test-rollover-deployment
Mar  2 13:42:47.787: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  2 13:42:49.814: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  2 13:42:49.820: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  2 13:42:49.824: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 13:42:49.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 13:42:51.836: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 13:42:51.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 13:42:53.833: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 13:42:53.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 13:42:55.838: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 13:42:55.838: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 13:42:57.830: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 13:42:57.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 13:42:59.832: INFO: 
Mar  2 13:42:59.833: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 13:42:59.843: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4225  8026ee5a-e610-4e52-9398-a85cda83e118 1953706 2 2023-03-02 13:42:45 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 13:42:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:42:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e45958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 13:42:45 +0000 UTC,LastTransitionTime:2023-03-02 13:42:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-02 13:42:59 +0000 UTC,LastTransitionTime:2023-03-02 13:42:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 13:42:59.846: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-4225  54128c03-6998-4488-ae83-7cde5c4b2d72 1953689 2 2023-03-02 13:42:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 8026ee5a-e610-4e52-9398-a85cda83e118 0xc003fb6457 0xc003fb6458}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:42:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8026ee5a-e610-4e52-9398-a85cda83e118\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:42:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fb6538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 13:42:59.846: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  2 13:42:59.846: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4225  9bb76c6c-b808-47b4-9495-e3427bb6b71a 1953704 2 2023-03-02 13:42:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 8026ee5a-e610-4e52-9398-a85cda83e118 0xc003fb6127 0xc003fb6128}] [] [{e2e.test Update apps/v1 2023-03-02 13:42:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:42:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8026ee5a-e610-4e52-9398-a85cda83e118\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:42:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003fb6228 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 13:42:59.846: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4225  56bf4ca4-f4ec-412d-9343-05afdae0f3b8 1953614 2 2023-03-02 13:42:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 8026ee5a-e610-4e52-9398-a85cda83e118 0xc003fb62a7 0xc003fb62a8}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:42:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8026ee5a-e610-4e52-9398-a85cda83e118\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:42:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fb6358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 13:42:59.849: INFO: Pod "test-rollover-deployment-6d45fd857b-6f5bz" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-6f5bz test-rollover-deployment-6d45fd857b- deployment-4225  14f77bd7-0670-4715-9d72-4f4451cdbc11 1953638 0 2023-03-02 13:42:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:ca8236092e20b466896299b66bf18bcc290d0e75f1028e23f894009bc7c8f8eb cni.projectcalico.org/podIP:10.233.123.101/32 cni.projectcalico.org/podIPs:10.233.123.101/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 54128c03-6998-4488-ae83-7cde5c4b2d72 0xc003e45ec7 0xc003e45ec8}] [] [{kube-controller-manager Update v1 2023-03-02 13:42:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54128c03-6998-4488-ae83-7cde5c4b2d72\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:42:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:42:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2whv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2whv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:42:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:42:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.101,StartTime:2023-03-02 13:42:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:42:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://ced3f6768aff6f5028b645919817f60f60156acc50fea3fd4dff26eef8a81b60,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 13:42:59.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4225" for this suite. 03/02/23 13:42:59.854
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":205,"skipped":3656,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.189 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:42:38.673
    Mar  2 13:42:38.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename deployment 03/02/23 13:42:38.676
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:42:38.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:42:38.693
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar  2 13:42:38.706: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Mar  2 13:42:43.719: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 13:42:43.719
    Mar  2 13:42:43.719: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar  2 13:42:45.725: INFO: Creating deployment "test-rollover-deployment"
    Mar  2 13:42:45.740: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar  2 13:42:47.757: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar  2 13:42:47.764: INFO: Ensure that both replica sets have 1 created replica
    Mar  2 13:42:47.770: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar  2 13:42:47.787: INFO: Updating deployment test-rollover-deployment
    Mar  2 13:42:47.787: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar  2 13:42:49.814: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar  2 13:42:49.820: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar  2 13:42:49.824: INFO: all replica sets need to contain the pod-template-hash label
    Mar  2 13:42:49.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 13:42:51.836: INFO: all replica sets need to contain the pod-template-hash label
    Mar  2 13:42:51.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 13:42:53.833: INFO: all replica sets need to contain the pod-template-hash label
    Mar  2 13:42:53.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 13:42:55.838: INFO: all replica sets need to contain the pod-template-hash label
    Mar  2 13:42:55.838: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 13:42:57.830: INFO: all replica sets need to contain the pod-template-hash label
    Mar  2 13:42:57.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 42, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 42, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 13:42:59.832: INFO: 
    Mar  2 13:42:59.833: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 13:42:59.843: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4225  8026ee5a-e610-4e52-9398-a85cda83e118 1953706 2 2023-03-02 13:42:45 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 13:42:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:42:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e45958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 13:42:45 +0000 UTC,LastTransitionTime:2023-03-02 13:42:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-02 13:42:59 +0000 UTC,LastTransitionTime:2023-03-02 13:42:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  2 13:42:59.846: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-4225  54128c03-6998-4488-ae83-7cde5c4b2d72 1953689 2 2023-03-02 13:42:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 8026ee5a-e610-4e52-9398-a85cda83e118 0xc003fb6457 0xc003fb6458}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:42:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8026ee5a-e610-4e52-9398-a85cda83e118\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:42:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fb6538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 13:42:59.846: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar  2 13:42:59.846: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4225  9bb76c6c-b808-47b4-9495-e3427bb6b71a 1953704 2 2023-03-02 13:42:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 8026ee5a-e610-4e52-9398-a85cda83e118 0xc003fb6127 0xc003fb6128}] [] [{e2e.test Update apps/v1 2023-03-02 13:42:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:42:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8026ee5a-e610-4e52-9398-a85cda83e118\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:42:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003fb6228 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 13:42:59.846: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4225  56bf4ca4-f4ec-412d-9343-05afdae0f3b8 1953614 2 2023-03-02 13:42:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 8026ee5a-e610-4e52-9398-a85cda83e118 0xc003fb62a7 0xc003fb62a8}] [] [{kube-controller-manager Update apps/v1 2023-03-02 13:42:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8026ee5a-e610-4e52-9398-a85cda83e118\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 13:42:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fb6358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 13:42:59.849: INFO: Pod "test-rollover-deployment-6d45fd857b-6f5bz" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-6f5bz test-rollover-deployment-6d45fd857b- deployment-4225  14f77bd7-0670-4715-9d72-4f4451cdbc11 1953638 0 2023-03-02 13:42:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:ca8236092e20b466896299b66bf18bcc290d0e75f1028e23f894009bc7c8f8eb cni.projectcalico.org/podIP:10.233.123.101/32 cni.projectcalico.org/podIPs:10.233.123.101/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 54128c03-6998-4488-ae83-7cde5c4b2d72 0xc003e45ec7 0xc003e45ec8}] [] [{kube-controller-manager Update v1 2023-03-02 13:42:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54128c03-6998-4488-ae83-7cde5c4b2d72\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 13:42:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 13:42:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2whv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2whv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:42:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 13:42:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.101,StartTime:2023-03-02 13:42:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 13:42:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://ced3f6768aff6f5028b645919817f60f60156acc50fea3fd4dff26eef8a81b60,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 13:42:59.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4225" for this suite. 03/02/23 13:42:59.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:42:59.864
Mar  2 13:42:59.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename var-expansion 03/02/23 13:42:59.865
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:42:59.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:42:59.908
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 03/02/23 13:42:59.913
Mar  2 13:42:59.925: INFO: Waiting up to 5m0s for pod "var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0" in namespace "var-expansion-5440" to be "Succeeded or Failed"
Mar  2 13:42:59.932: INFO: Pod "var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.256635ms
Mar  2 13:43:01.936: INFO: Pod "var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01136263s
Mar  2 13:43:03.938: INFO: Pod "var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013403316s
STEP: Saw pod success 03/02/23 13:43:03.938
Mar  2 13:43:03.939: INFO: Pod "var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0" satisfied condition "Succeeded or Failed"
Mar  2 13:43:03.943: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0 container dapi-container: <nil>
STEP: delete the pod 03/02/23 13:43:03.948
Mar  2 13:43:03.963: INFO: Waiting for pod var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0 to disappear
Mar  2 13:43:03.966: INFO: Pod var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 13:43:03.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5440" for this suite. 03/02/23 13:43:03.971
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":206,"skipped":3693,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:42:59.864
    Mar  2 13:42:59.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename var-expansion 03/02/23 13:42:59.865
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:42:59.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:42:59.908
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 03/02/23 13:42:59.913
    Mar  2 13:42:59.925: INFO: Waiting up to 5m0s for pod "var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0" in namespace "var-expansion-5440" to be "Succeeded or Failed"
    Mar  2 13:42:59.932: INFO: Pod "var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.256635ms
    Mar  2 13:43:01.936: INFO: Pod "var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01136263s
    Mar  2 13:43:03.938: INFO: Pod "var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013403316s
    STEP: Saw pod success 03/02/23 13:43:03.938
    Mar  2 13:43:03.939: INFO: Pod "var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0" satisfied condition "Succeeded or Failed"
    Mar  2 13:43:03.943: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0 container dapi-container: <nil>
    STEP: delete the pod 03/02/23 13:43:03.948
    Mar  2 13:43:03.963: INFO: Waiting for pod var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0 to disappear
    Mar  2 13:43:03.966: INFO: Pod var-expansion-9637b879-3a37-4226-b666-6cbcdf0f7eb0 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 13:43:03.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5440" for this suite. 03/02/23 13:43:03.971
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:43:03.979
Mar  2 13:43:03.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 13:43:03.98
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:04.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:04.042
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/02/23 13:43:04.053
Mar  2 13:43:04.167: INFO: Waiting up to 5m0s for pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82" in namespace "emptydir-9649" to be "Succeeded or Failed"
Mar  2 13:43:04.226: INFO: Pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82": Phase="Pending", Reason="", readiness=false. Elapsed: 58.41614ms
Mar  2 13:43:06.230: INFO: Pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82": Phase="Running", Reason="", readiness=true. Elapsed: 2.062873861s
Mar  2 13:43:08.233: INFO: Pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82": Phase="Running", Reason="", readiness=false. Elapsed: 4.065428535s
Mar  2 13:43:10.232: INFO: Pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065091763s
STEP: Saw pod success 03/02/23 13:43:10.233
Mar  2 13:43:10.233: INFO: Pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82" satisfied condition "Succeeded or Failed"
Mar  2 13:43:10.238: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82 container test-container: <nil>
STEP: delete the pod 03/02/23 13:43:10.245
Mar  2 13:43:10.256: INFO: Waiting for pod pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82 to disappear
Mar  2 13:43:10.261: INFO: Pod pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 13:43:10.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9649" for this suite. 03/02/23 13:43:10.265
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":207,"skipped":3694,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.291 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:43:03.979
    Mar  2 13:43:03.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 13:43:03.98
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:04.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:04.042
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/02/23 13:43:04.053
    Mar  2 13:43:04.167: INFO: Waiting up to 5m0s for pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82" in namespace "emptydir-9649" to be "Succeeded or Failed"
    Mar  2 13:43:04.226: INFO: Pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82": Phase="Pending", Reason="", readiness=false. Elapsed: 58.41614ms
    Mar  2 13:43:06.230: INFO: Pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82": Phase="Running", Reason="", readiness=true. Elapsed: 2.062873861s
    Mar  2 13:43:08.233: INFO: Pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82": Phase="Running", Reason="", readiness=false. Elapsed: 4.065428535s
    Mar  2 13:43:10.232: INFO: Pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065091763s
    STEP: Saw pod success 03/02/23 13:43:10.233
    Mar  2 13:43:10.233: INFO: Pod "pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82" satisfied condition "Succeeded or Failed"
    Mar  2 13:43:10.238: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82 container test-container: <nil>
    STEP: delete the pod 03/02/23 13:43:10.245
    Mar  2 13:43:10.256: INFO: Waiting for pod pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82 to disappear
    Mar  2 13:43:10.261: INFO: Pod pod-24f0f025-b3ae-4b9c-bdb9-2884133c5b82 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 13:43:10.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9649" for this suite. 03/02/23 13:43:10.265
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:43:10.303
Mar  2 13:43:10.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:43:10.304
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:10.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:10.327
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-879374c9-50b4-4715-a839-487916352b70 03/02/23 13:43:10.33
STEP: Creating a pod to test consume configMaps 03/02/23 13:43:10.335
Mar  2 13:43:10.343: INFO: Waiting up to 5m0s for pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91" in namespace "configmap-3927" to be "Succeeded or Failed"
Mar  2 13:43:10.348: INFO: Pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91": Phase="Pending", Reason="", readiness=false. Elapsed: 5.577693ms
Mar  2 13:43:12.355: INFO: Pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91": Phase="Running", Reason="", readiness=true. Elapsed: 2.012000595s
Mar  2 13:43:14.355: INFO: Pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91": Phase="Running", Reason="", readiness=false. Elapsed: 4.011675589s
Mar  2 13:43:16.359: INFO: Pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015651421s
STEP: Saw pod success 03/02/23 13:43:16.359
Mar  2 13:43:16.359: INFO: Pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91" satisfied condition "Succeeded or Failed"
Mar  2 13:43:16.364: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 13:43:16.372
Mar  2 13:43:16.383: INFO: Waiting for pod pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91 to disappear
Mar  2 13:43:16.384: INFO: Pod pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:43:16.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3927" for this suite. 03/02/23 13:43:16.39
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":208,"skipped":3697,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.093 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:43:10.303
    Mar  2 13:43:10.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:43:10.304
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:10.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:10.327
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-879374c9-50b4-4715-a839-487916352b70 03/02/23 13:43:10.33
    STEP: Creating a pod to test consume configMaps 03/02/23 13:43:10.335
    Mar  2 13:43:10.343: INFO: Waiting up to 5m0s for pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91" in namespace "configmap-3927" to be "Succeeded or Failed"
    Mar  2 13:43:10.348: INFO: Pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91": Phase="Pending", Reason="", readiness=false. Elapsed: 5.577693ms
    Mar  2 13:43:12.355: INFO: Pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91": Phase="Running", Reason="", readiness=true. Elapsed: 2.012000595s
    Mar  2 13:43:14.355: INFO: Pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91": Phase="Running", Reason="", readiness=false. Elapsed: 4.011675589s
    Mar  2 13:43:16.359: INFO: Pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015651421s
    STEP: Saw pod success 03/02/23 13:43:16.359
    Mar  2 13:43:16.359: INFO: Pod "pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91" satisfied condition "Succeeded or Failed"
    Mar  2 13:43:16.364: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 13:43:16.372
    Mar  2 13:43:16.383: INFO: Waiting for pod pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91 to disappear
    Mar  2 13:43:16.384: INFO: Pod pod-configmaps-1fa89d90-f4cc-4b7c-abca-d75e6dbbeb91 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:43:16.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3927" for this suite. 03/02/23 13:43:16.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:43:16.405
Mar  2 13:43:16.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:43:16.407
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:16.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:16.43
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 03/02/23 13:43:16.436
STEP: watching for the Service to be added 03/02/23 13:43:16.443
Mar  2 13:43:16.445: INFO: Found Service test-service-878w2 in namespace services-8767 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar  2 13:43:16.446: INFO: Service test-service-878w2 created
STEP: Getting /status 03/02/23 13:43:16.447
Mar  2 13:43:16.452: INFO: Service test-service-878w2 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/02/23 13:43:16.452
STEP: watching for the Service to be patched 03/02/23 13:43:16.462
Mar  2 13:43:16.463: INFO: observed Service test-service-878w2 in namespace services-8767 with annotations: map[] & LoadBalancer: {[]}
Mar  2 13:43:16.464: INFO: Found Service test-service-878w2 in namespace services-8767 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar  2 13:43:16.464: INFO: Service test-service-878w2 has service status patched
STEP: updating the ServiceStatus 03/02/23 13:43:16.464
Mar  2 13:43:16.473: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/02/23 13:43:16.473
Mar  2 13:43:16.475: INFO: Observed Service test-service-878w2 in namespace services-8767 with annotations: map[] & Conditions: {[]}
Mar  2 13:43:16.475: INFO: Observed event: &Service{ObjectMeta:{test-service-878w2  services-8767  de10da92-ac02-4214-b4b8-7c283fce60e2 1953923 0 2023-03-02 13:43:16 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-02 13:43:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-02 13:43:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.30.75,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.30.75],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar  2 13:43:16.475: INFO: Found Service test-service-878w2 in namespace services-8767 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 13:43:16.475: INFO: Service test-service-878w2 has service status updated
STEP: patching the service 03/02/23 13:43:16.475
STEP: watching for the Service to be patched 03/02/23 13:43:16.482
Mar  2 13:43:16.485: INFO: observed Service test-service-878w2 in namespace services-8767 with labels: map[test-service-static:true]
Mar  2 13:43:16.485: INFO: observed Service test-service-878w2 in namespace services-8767 with labels: map[test-service-static:true]
Mar  2 13:43:16.485: INFO: observed Service test-service-878w2 in namespace services-8767 with labels: map[test-service-static:true]
Mar  2 13:43:16.486: INFO: Found Service test-service-878w2 in namespace services-8767 with labels: map[test-service:patched test-service-static:true]
Mar  2 13:43:16.486: INFO: Service test-service-878w2 patched
STEP: deleting the service 03/02/23 13:43:16.486
STEP: watching for the Service to be deleted 03/02/23 13:43:16.496
Mar  2 13:43:16.497: INFO: Observed event: ADDED
Mar  2 13:43:16.498: INFO: Observed event: MODIFIED
Mar  2 13:43:16.498: INFO: Observed event: MODIFIED
Mar  2 13:43:16.498: INFO: Observed event: MODIFIED
Mar  2 13:43:16.499: INFO: Found Service test-service-878w2 in namespace services-8767 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar  2 13:43:16.499: INFO: Service test-service-878w2 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:43:16.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8767" for this suite. 03/02/23 13:43:16.506
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":209,"skipped":3708,"failed":0}
------------------------------
â€¢ [0.107 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:43:16.405
    Mar  2 13:43:16.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:43:16.407
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:16.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:16.43
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 03/02/23 13:43:16.436
    STEP: watching for the Service to be added 03/02/23 13:43:16.443
    Mar  2 13:43:16.445: INFO: Found Service test-service-878w2 in namespace services-8767 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar  2 13:43:16.446: INFO: Service test-service-878w2 created
    STEP: Getting /status 03/02/23 13:43:16.447
    Mar  2 13:43:16.452: INFO: Service test-service-878w2 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/02/23 13:43:16.452
    STEP: watching for the Service to be patched 03/02/23 13:43:16.462
    Mar  2 13:43:16.463: INFO: observed Service test-service-878w2 in namespace services-8767 with annotations: map[] & LoadBalancer: {[]}
    Mar  2 13:43:16.464: INFO: Found Service test-service-878w2 in namespace services-8767 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar  2 13:43:16.464: INFO: Service test-service-878w2 has service status patched
    STEP: updating the ServiceStatus 03/02/23 13:43:16.464
    Mar  2 13:43:16.473: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/02/23 13:43:16.473
    Mar  2 13:43:16.475: INFO: Observed Service test-service-878w2 in namespace services-8767 with annotations: map[] & Conditions: {[]}
    Mar  2 13:43:16.475: INFO: Observed event: &Service{ObjectMeta:{test-service-878w2  services-8767  de10da92-ac02-4214-b4b8-7c283fce60e2 1953923 0 2023-03-02 13:43:16 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-02 13:43:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-02 13:43:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.30.75,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.30.75],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar  2 13:43:16.475: INFO: Found Service test-service-878w2 in namespace services-8767 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  2 13:43:16.475: INFO: Service test-service-878w2 has service status updated
    STEP: patching the service 03/02/23 13:43:16.475
    STEP: watching for the Service to be patched 03/02/23 13:43:16.482
    Mar  2 13:43:16.485: INFO: observed Service test-service-878w2 in namespace services-8767 with labels: map[test-service-static:true]
    Mar  2 13:43:16.485: INFO: observed Service test-service-878w2 in namespace services-8767 with labels: map[test-service-static:true]
    Mar  2 13:43:16.485: INFO: observed Service test-service-878w2 in namespace services-8767 with labels: map[test-service-static:true]
    Mar  2 13:43:16.486: INFO: Found Service test-service-878w2 in namespace services-8767 with labels: map[test-service:patched test-service-static:true]
    Mar  2 13:43:16.486: INFO: Service test-service-878w2 patched
    STEP: deleting the service 03/02/23 13:43:16.486
    STEP: watching for the Service to be deleted 03/02/23 13:43:16.496
    Mar  2 13:43:16.497: INFO: Observed event: ADDED
    Mar  2 13:43:16.498: INFO: Observed event: MODIFIED
    Mar  2 13:43:16.498: INFO: Observed event: MODIFIED
    Mar  2 13:43:16.498: INFO: Observed event: MODIFIED
    Mar  2 13:43:16.499: INFO: Found Service test-service-878w2 in namespace services-8767 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar  2 13:43:16.499: INFO: Service test-service-878w2 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:43:16.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8767" for this suite. 03/02/23 13:43:16.506
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:43:16.522
Mar  2 13:43:16.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir-wrapper 03/02/23 13:43:16.523
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:16.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:16.543
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar  2 13:43:16.565: INFO: Waiting up to 5m0s for pod "pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7" in namespace "emptydir-wrapper-8199" to be "running and ready"
Mar  2 13:43:16.577: INFO: Pod "pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.266359ms
Mar  2 13:43:16.577: INFO: The phase of Pod pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:43:18.585: INFO: Pod "pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016409224s
Mar  2 13:43:18.585: INFO: The phase of Pod pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:43:20.588: INFO: Pod "pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7": Phase="Running", Reason="", readiness=true. Elapsed: 4.019414999s
Mar  2 13:43:20.588: INFO: The phase of Pod pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7 is Running (Ready = true)
Mar  2 13:43:20.588: INFO: Pod "pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/02/23 13:43:20.592
STEP: Cleaning up the configmap 03/02/23 13:43:20.6
STEP: Cleaning up the pod 03/02/23 13:43:20.62
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar  2 13:43:20.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8199" for this suite. 03/02/23 13:43:20.749
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":210,"skipped":3717,"failed":0}
------------------------------
â€¢ [4.235 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:43:16.522
    Mar  2 13:43:16.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir-wrapper 03/02/23 13:43:16.523
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:16.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:16.543
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar  2 13:43:16.565: INFO: Waiting up to 5m0s for pod "pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7" in namespace "emptydir-wrapper-8199" to be "running and ready"
    Mar  2 13:43:16.577: INFO: Pod "pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.266359ms
    Mar  2 13:43:16.577: INFO: The phase of Pod pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:43:18.585: INFO: Pod "pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016409224s
    Mar  2 13:43:18.585: INFO: The phase of Pod pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:43:20.588: INFO: Pod "pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7": Phase="Running", Reason="", readiness=true. Elapsed: 4.019414999s
    Mar  2 13:43:20.588: INFO: The phase of Pod pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7 is Running (Ready = true)
    Mar  2 13:43:20.588: INFO: Pod "pod-secrets-f8394062-8b1a-47b9-aec7-f40ad2449ff7" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/02/23 13:43:20.592
    STEP: Cleaning up the configmap 03/02/23 13:43:20.6
    STEP: Cleaning up the pod 03/02/23 13:43:20.62
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar  2 13:43:20.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-8199" for this suite. 03/02/23 13:43:20.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:43:20.763
Mar  2 13:43:20.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename job 03/02/23 13:43:20.764
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:20.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:20.817
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 03/02/23 13:43:20.821
STEP: Ensuring job reaches completions 03/02/23 13:43:20.827
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 13:43:36.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3569" for this suite. 03/02/23 13:43:36.842
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":211,"skipped":3761,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.086 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:43:20.763
    Mar  2 13:43:20.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename job 03/02/23 13:43:20.764
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:20.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:20.817
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 03/02/23 13:43:20.821
    STEP: Ensuring job reaches completions 03/02/23 13:43:20.827
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 13:43:36.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3569" for this suite. 03/02/23 13:43:36.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:43:36.85
Mar  2 13:43:36.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename resourcequota 03/02/23 13:43:36.852
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:36.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:36.878
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 03/02/23 13:43:36.883
STEP: Creating a ResourceQuota 03/02/23 13:43:41.886
STEP: Ensuring resource quota status is calculated 03/02/23 13:43:41.894
STEP: Creating a ReplicaSet 03/02/23 13:43:43.9
STEP: Ensuring resource quota status captures replicaset creation 03/02/23 13:43:43.911
STEP: Deleting a ReplicaSet 03/02/23 13:43:45.919
STEP: Ensuring resource quota status released usage 03/02/23 13:43:45.929
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 13:43:47.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2132" for this suite. 03/02/23 13:43:47.959
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":212,"skipped":3772,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.114 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:43:36.85
    Mar  2 13:43:36.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename resourcequota 03/02/23 13:43:36.852
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:36.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:36.878
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 03/02/23 13:43:36.883
    STEP: Creating a ResourceQuota 03/02/23 13:43:41.886
    STEP: Ensuring resource quota status is calculated 03/02/23 13:43:41.894
    STEP: Creating a ReplicaSet 03/02/23 13:43:43.9
    STEP: Ensuring resource quota status captures replicaset creation 03/02/23 13:43:43.911
    STEP: Deleting a ReplicaSet 03/02/23 13:43:45.919
    STEP: Ensuring resource quota status released usage 03/02/23 13:43:45.929
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 13:43:47.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2132" for this suite. 03/02/23 13:43:47.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:43:47.966
Mar  2 13:43:47.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename init-container 03/02/23 13:43:47.97
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:47.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:47.989
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 03/02/23 13:43:48
Mar  2 13:43:48.000: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 13:43:56.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5595" for this suite. 03/02/23 13:43:56.273
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":213,"skipped":3787,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.312 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:43:47.966
    Mar  2 13:43:47.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename init-container 03/02/23 13:43:47.97
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:47.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:47.989
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 03/02/23 13:43:48
    Mar  2 13:43:48.000: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 13:43:56.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5595" for this suite. 03/02/23 13:43:56.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:43:56.279
Mar  2 13:43:56.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:43:56.29
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:56.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:56.308
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 03/02/23 13:43:56.311
Mar  2 13:43:56.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  2 13:43:56.417: INFO: stderr: ""
Mar  2 13:43:56.417: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 03/02/23 13:43:56.417
Mar  2 13:43:56.418: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  2 13:43:56.418: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2683" to be "running and ready, or succeeded"
Mar  2 13:43:56.425: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.146796ms
Mar  2 13:43:56.425: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'aarnq-sc-k8s-node-srv2' to be 'Running' but was 'Pending'
Mar  2 13:43:58.437: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.019487791s
Mar  2 13:43:58.437: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  2 13:43:58.437: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/02/23 13:43:58.437
Mar  2 13:43:58.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator'
Mar  2 13:43:58.562: INFO: stderr: ""
Mar  2 13:43:58.562: INFO: stdout: "I0302 13:43:57.451584       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/p59 320\nI0302 13:43:57.651991       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nsz 227\nI0302 13:43:57.852424       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/7glt 429\nI0302 13:43:58.051797       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/7qqq 542\nI0302 13:43:58.252510       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/bbv 393\nI0302 13:43:58.452515       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/sfc 329\n"
STEP: limiting log lines 03/02/23 13:43:58.562
Mar  2 13:43:58.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator --tail=1'
Mar  2 13:43:58.671: INFO: stderr: ""
Mar  2 13:43:58.671: INFO: stdout: "I0302 13:43:58.651824       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/jmql 466\n"
Mar  2 13:43:58.671: INFO: got output "I0302 13:43:58.651824       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/jmql 466\n"
STEP: limiting log bytes 03/02/23 13:43:58.671
Mar  2 13:43:58.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator --limit-bytes=1'
Mar  2 13:43:58.763: INFO: stderr: ""
Mar  2 13:43:58.763: INFO: stdout: "I"
Mar  2 13:43:58.763: INFO: got output "I"
STEP: exposing timestamps 03/02/23 13:43:58.763
Mar  2 13:43:58.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator --tail=1 --timestamps'
Mar  2 13:43:58.864: INFO: stderr: ""
Mar  2 13:43:58.864: INFO: stdout: "2023-03-02T13:43:58.852793767Z I0302 13:43:58.852560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6fkq 477\n"
Mar  2 13:43:58.864: INFO: got output "2023-03-02T13:43:58.852793767Z I0302 13:43:58.852560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6fkq 477\n"
STEP: restricting to a time range 03/02/23 13:43:58.864
Mar  2 13:44:01.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator --since=1s'
Mar  2 13:44:01.466: INFO: stderr: ""
Mar  2 13:44:01.466: INFO: stdout: "I0302 13:44:00.651747       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/x42 236\nI0302 13:44:00.852134       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/x49 580\nI0302 13:44:01.052556       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/djr 508\nI0302 13:44:01.251824       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/f22 512\nI0302 13:44:01.452200       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/7f69 418\n"
Mar  2 13:44:01.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator --since=24h'
Mar  2 13:44:01.549: INFO: stderr: ""
Mar  2 13:44:01.549: INFO: stdout: "I0302 13:43:57.451584       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/p59 320\nI0302 13:43:57.651991       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nsz 227\nI0302 13:43:57.852424       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/7glt 429\nI0302 13:43:58.051797       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/7qqq 542\nI0302 13:43:58.252510       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/bbv 393\nI0302 13:43:58.452515       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/sfc 329\nI0302 13:43:58.651824       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/jmql 466\nI0302 13:43:58.852560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6fkq 477\nI0302 13:43:59.051966       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/vlt8 321\nI0302 13:43:59.252376       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/mpn 210\nI0302 13:43:59.452922       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/dkz6 375\nI0302 13:43:59.651635       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/6vdf 444\nI0302 13:43:59.852440       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/zx6 579\nI0302 13:44:00.052678       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/jtqt 470\nI0302 13:44:00.252083       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/gq5 552\nI0302 13:44:00.452382       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/dnq6 379\nI0302 13:44:00.651747       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/x42 236\nI0302 13:44:00.852134       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/x49 580\nI0302 13:44:01.052556       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/djr 508\nI0302 13:44:01.251824       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/f22 512\nI0302 13:44:01.452200       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/7f69 418\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Mar  2 13:44:01.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 delete pod logs-generator'
Mar  2 13:44:02.309: INFO: stderr: ""
Mar  2 13:44:02.310: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:44:02.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2683" for this suite. 03/02/23 13:44:02.315
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":214,"skipped":3794,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.046 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:43:56.279
    Mar  2 13:43:56.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:43:56.29
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:43:56.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:43:56.308
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 03/02/23 13:43:56.311
    Mar  2 13:43:56.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar  2 13:43:56.417: INFO: stderr: ""
    Mar  2 13:43:56.417: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 03/02/23 13:43:56.417
    Mar  2 13:43:56.418: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar  2 13:43:56.418: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2683" to be "running and ready, or succeeded"
    Mar  2 13:43:56.425: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.146796ms
    Mar  2 13:43:56.425: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'aarnq-sc-k8s-node-srv2' to be 'Running' but was 'Pending'
    Mar  2 13:43:58.437: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.019487791s
    Mar  2 13:43:58.437: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar  2 13:43:58.437: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/02/23 13:43:58.437
    Mar  2 13:43:58.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator'
    Mar  2 13:43:58.562: INFO: stderr: ""
    Mar  2 13:43:58.562: INFO: stdout: "I0302 13:43:57.451584       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/p59 320\nI0302 13:43:57.651991       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nsz 227\nI0302 13:43:57.852424       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/7glt 429\nI0302 13:43:58.051797       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/7qqq 542\nI0302 13:43:58.252510       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/bbv 393\nI0302 13:43:58.452515       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/sfc 329\n"
    STEP: limiting log lines 03/02/23 13:43:58.562
    Mar  2 13:43:58.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator --tail=1'
    Mar  2 13:43:58.671: INFO: stderr: ""
    Mar  2 13:43:58.671: INFO: stdout: "I0302 13:43:58.651824       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/jmql 466\n"
    Mar  2 13:43:58.671: INFO: got output "I0302 13:43:58.651824       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/jmql 466\n"
    STEP: limiting log bytes 03/02/23 13:43:58.671
    Mar  2 13:43:58.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator --limit-bytes=1'
    Mar  2 13:43:58.763: INFO: stderr: ""
    Mar  2 13:43:58.763: INFO: stdout: "I"
    Mar  2 13:43:58.763: INFO: got output "I"
    STEP: exposing timestamps 03/02/23 13:43:58.763
    Mar  2 13:43:58.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar  2 13:43:58.864: INFO: stderr: ""
    Mar  2 13:43:58.864: INFO: stdout: "2023-03-02T13:43:58.852793767Z I0302 13:43:58.852560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6fkq 477\n"
    Mar  2 13:43:58.864: INFO: got output "2023-03-02T13:43:58.852793767Z I0302 13:43:58.852560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6fkq 477\n"
    STEP: restricting to a time range 03/02/23 13:43:58.864
    Mar  2 13:44:01.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator --since=1s'
    Mar  2 13:44:01.466: INFO: stderr: ""
    Mar  2 13:44:01.466: INFO: stdout: "I0302 13:44:00.651747       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/x42 236\nI0302 13:44:00.852134       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/x49 580\nI0302 13:44:01.052556       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/djr 508\nI0302 13:44:01.251824       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/f22 512\nI0302 13:44:01.452200       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/7f69 418\n"
    Mar  2 13:44:01.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 logs logs-generator logs-generator --since=24h'
    Mar  2 13:44:01.549: INFO: stderr: ""
    Mar  2 13:44:01.549: INFO: stdout: "I0302 13:43:57.451584       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/p59 320\nI0302 13:43:57.651991       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nsz 227\nI0302 13:43:57.852424       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/7glt 429\nI0302 13:43:58.051797       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/7qqq 542\nI0302 13:43:58.252510       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/bbv 393\nI0302 13:43:58.452515       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/sfc 329\nI0302 13:43:58.651824       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/jmql 466\nI0302 13:43:58.852560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6fkq 477\nI0302 13:43:59.051966       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/vlt8 321\nI0302 13:43:59.252376       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/mpn 210\nI0302 13:43:59.452922       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/dkz6 375\nI0302 13:43:59.651635       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/6vdf 444\nI0302 13:43:59.852440       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/zx6 579\nI0302 13:44:00.052678       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/jtqt 470\nI0302 13:44:00.252083       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/gq5 552\nI0302 13:44:00.452382       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/dnq6 379\nI0302 13:44:00.651747       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/x42 236\nI0302 13:44:00.852134       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/x49 580\nI0302 13:44:01.052556       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/djr 508\nI0302 13:44:01.251824       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/f22 512\nI0302 13:44:01.452200       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/7f69 418\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Mar  2 13:44:01.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2683 delete pod logs-generator'
    Mar  2 13:44:02.309: INFO: stderr: ""
    Mar  2 13:44:02.310: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:44:02.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2683" for this suite. 03/02/23 13:44:02.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:44:02.332
Mar  2 13:44:02.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename ingress 03/02/23 13:44:02.334
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:44:02.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:44:02.35
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/02/23 13:44:02.353
STEP: getting /apis/networking.k8s.io 03/02/23 13:44:02.359
STEP: getting /apis/networking.k8s.iov1 03/02/23 13:44:02.361
STEP: creating 03/02/23 13:44:02.362
STEP: getting 03/02/23 13:44:04.053
STEP: listing 03/02/23 13:44:04.061
STEP: watching 03/02/23 13:44:04.064
Mar  2 13:44:04.064: INFO: starting watch
STEP: cluster-wide listing 03/02/23 13:44:04.065
STEP: cluster-wide watching 03/02/23 13:44:04.068
Mar  2 13:44:04.068: INFO: starting watch
STEP: patching 03/02/23 13:44:04.07
STEP: updating 03/02/23 13:44:05.064
Mar  2 13:44:05.561: INFO: waiting for watch events with expected annotations
Mar  2 13:44:05.561: INFO: saw patched and updated annotations
STEP: patching /status 03/02/23 13:44:05.562
STEP: updating /status 03/02/23 13:44:05.582
STEP: get /status 03/02/23 13:44:05.598
STEP: deleting 03/02/23 13:44:05.606
STEP: deleting a collection 03/02/23 13:44:05.618
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Mar  2 13:44:05.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9492" for this suite. 03/02/23 13:44:05.631
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":215,"skipped":3824,"failed":0}
------------------------------
â€¢ [3.304 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:44:02.332
    Mar  2 13:44:02.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename ingress 03/02/23 13:44:02.334
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:44:02.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:44:02.35
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/02/23 13:44:02.353
    STEP: getting /apis/networking.k8s.io 03/02/23 13:44:02.359
    STEP: getting /apis/networking.k8s.iov1 03/02/23 13:44:02.361
    STEP: creating 03/02/23 13:44:02.362
    STEP: getting 03/02/23 13:44:04.053
    STEP: listing 03/02/23 13:44:04.061
    STEP: watching 03/02/23 13:44:04.064
    Mar  2 13:44:04.064: INFO: starting watch
    STEP: cluster-wide listing 03/02/23 13:44:04.065
    STEP: cluster-wide watching 03/02/23 13:44:04.068
    Mar  2 13:44:04.068: INFO: starting watch
    STEP: patching 03/02/23 13:44:04.07
    STEP: updating 03/02/23 13:44:05.064
    Mar  2 13:44:05.561: INFO: waiting for watch events with expected annotations
    Mar  2 13:44:05.561: INFO: saw patched and updated annotations
    STEP: patching /status 03/02/23 13:44:05.562
    STEP: updating /status 03/02/23 13:44:05.582
    STEP: get /status 03/02/23 13:44:05.598
    STEP: deleting 03/02/23 13:44:05.606
    STEP: deleting a collection 03/02/23 13:44:05.618
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Mar  2 13:44:05.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-9492" for this suite. 03/02/23 13:44:05.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:44:05.64
Mar  2 13:44:05.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename endpointslicemirroring 03/02/23 13:44:05.641
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:44:05.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:44:05.659
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/02/23 13:44:05.678
Mar  2 13:44:05.687: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 03/02/23 13:44:07.694
STEP: mirroring deletion of a custom Endpoint 03/02/23 13:44:07.71
Mar  2 13:44:07.724: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Mar  2 13:44:09.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9410" for this suite. 03/02/23 13:44:09.74
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":216,"skipped":3835,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:44:05.64
    Mar  2 13:44:05.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename endpointslicemirroring 03/02/23 13:44:05.641
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:44:05.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:44:05.659
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/02/23 13:44:05.678
    Mar  2 13:44:05.687: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 03/02/23 13:44:07.694
    STEP: mirroring deletion of a custom Endpoint 03/02/23 13:44:07.71
    Mar  2 13:44:07.724: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Mar  2 13:44:09.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-9410" for this suite. 03/02/23 13:44:09.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:44:09.759
Mar  2 13:44:09.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-probe 03/02/23 13:44:09.76
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:44:09.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:44:09.779
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-1310048c-de50-455a-ae94-e4bbbc83afee in namespace container-probe-2767 03/02/23 13:44:09.782
Mar  2 13:44:09.790: INFO: Waiting up to 5m0s for pod "busybox-1310048c-de50-455a-ae94-e4bbbc83afee" in namespace "container-probe-2767" to be "not pending"
Mar  2 13:44:09.794: INFO: Pod "busybox-1310048c-de50-455a-ae94-e4bbbc83afee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155989ms
Mar  2 13:44:11.803: INFO: Pod "busybox-1310048c-de50-455a-ae94-e4bbbc83afee": Phase="Running", Reason="", readiness=true. Elapsed: 2.013300164s
Mar  2 13:44:11.803: INFO: Pod "busybox-1310048c-de50-455a-ae94-e4bbbc83afee" satisfied condition "not pending"
Mar  2 13:44:11.803: INFO: Started pod busybox-1310048c-de50-455a-ae94-e4bbbc83afee in namespace container-probe-2767
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 13:44:11.803
Mar  2 13:44:11.807: INFO: Initial restart count of pod busybox-1310048c-de50-455a-ae94-e4bbbc83afee is 0
STEP: deleting the pod 03/02/23 13:48:12.824
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 13:48:12.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2767" for this suite. 03/02/23 13:48:12.898
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":217,"skipped":3863,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.147 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:44:09.759
    Mar  2 13:44:09.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-probe 03/02/23 13:44:09.76
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:44:09.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:44:09.779
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-1310048c-de50-455a-ae94-e4bbbc83afee in namespace container-probe-2767 03/02/23 13:44:09.782
    Mar  2 13:44:09.790: INFO: Waiting up to 5m0s for pod "busybox-1310048c-de50-455a-ae94-e4bbbc83afee" in namespace "container-probe-2767" to be "not pending"
    Mar  2 13:44:09.794: INFO: Pod "busybox-1310048c-de50-455a-ae94-e4bbbc83afee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155989ms
    Mar  2 13:44:11.803: INFO: Pod "busybox-1310048c-de50-455a-ae94-e4bbbc83afee": Phase="Running", Reason="", readiness=true. Elapsed: 2.013300164s
    Mar  2 13:44:11.803: INFO: Pod "busybox-1310048c-de50-455a-ae94-e4bbbc83afee" satisfied condition "not pending"
    Mar  2 13:44:11.803: INFO: Started pod busybox-1310048c-de50-455a-ae94-e4bbbc83afee in namespace container-probe-2767
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 13:44:11.803
    Mar  2 13:44:11.807: INFO: Initial restart count of pod busybox-1310048c-de50-455a-ae94-e4bbbc83afee is 0
    STEP: deleting the pod 03/02/23 13:48:12.824
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 13:48:12.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2767" for this suite. 03/02/23 13:48:12.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:48:12.907
Mar  2 13:48:12.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:48:12.912
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:48:12.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:48:12.935
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-2b18eb2a-99ec-4024-999a-c94d099780dc 03/02/23 13:48:13.003
STEP: Creating a pod to test consume secrets 03/02/23 13:48:13.012
Mar  2 13:48:13.024: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e" in namespace "projected-6486" to be "Succeeded or Failed"
Mar  2 13:48:13.040: INFO: Pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.027592ms
Mar  2 13:48:15.048: INFO: Pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024111328s
Mar  2 13:48:17.049: INFO: Pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025363632s
Mar  2 13:48:19.048: INFO: Pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024386094s
STEP: Saw pod success 03/02/23 13:48:19.049
Mar  2 13:48:19.049: INFO: Pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e" satisfied condition "Succeeded or Failed"
Mar  2 13:48:19.053: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e container projected-secret-volume-test: <nil>
STEP: delete the pod 03/02/23 13:48:19.077
Mar  2 13:48:19.087: INFO: Waiting for pod pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e to disappear
Mar  2 13:48:19.092: INFO: Pod pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 13:48:19.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6486" for this suite. 03/02/23 13:48:19.097
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":218,"skipped":3877,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.196 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:48:12.907
    Mar  2 13:48:12.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:48:12.912
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:48:12.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:48:12.935
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-2b18eb2a-99ec-4024-999a-c94d099780dc 03/02/23 13:48:13.003
    STEP: Creating a pod to test consume secrets 03/02/23 13:48:13.012
    Mar  2 13:48:13.024: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e" in namespace "projected-6486" to be "Succeeded or Failed"
    Mar  2 13:48:13.040: INFO: Pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.027592ms
    Mar  2 13:48:15.048: INFO: Pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024111328s
    Mar  2 13:48:17.049: INFO: Pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025363632s
    Mar  2 13:48:19.048: INFO: Pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024386094s
    STEP: Saw pod success 03/02/23 13:48:19.049
    Mar  2 13:48:19.049: INFO: Pod "pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e" satisfied condition "Succeeded or Failed"
    Mar  2 13:48:19.053: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 13:48:19.077
    Mar  2 13:48:19.087: INFO: Waiting for pod pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e to disappear
    Mar  2 13:48:19.092: INFO: Pod pod-projected-secrets-e7188769-c467-47ab-8e0b-54a65903c88e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 13:48:19.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6486" for this suite. 03/02/23 13:48:19.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:48:19.106
Mar  2 13:48:19.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename cronjob 03/02/23 13:48:19.11
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:48:19.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:48:19.132
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/02/23 13:48:19.136
STEP: Ensuring more than one job is running at a time 03/02/23 13:48:19.141
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/02/23 13:50:01.146
STEP: Removing cronjob 03/02/23 13:50:01.151
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  2 13:50:01.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7583" for this suite. 03/02/23 13:50:01.165
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":219,"skipped":3893,"failed":0}
------------------------------
â€¢ [SLOW TEST] [102.092 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:48:19.106
    Mar  2 13:48:19.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename cronjob 03/02/23 13:48:19.11
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:48:19.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:48:19.132
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/02/23 13:48:19.136
    STEP: Ensuring more than one job is running at a time 03/02/23 13:48:19.141
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/02/23 13:50:01.146
    STEP: Removing cronjob 03/02/23 13:50:01.151
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  2 13:50:01.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7583" for this suite. 03/02/23 13:50:01.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:50:01.204
Mar  2 13:50:01.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename resourcequota 03/02/23 13:50:01.211
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:50:01.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:50:01.251
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 03/02/23 13:50:01.257
STEP: Creating a ResourceQuota 03/02/23 13:50:06.26
STEP: Ensuring resource quota status is calculated 03/02/23 13:50:06.265
STEP: Creating a Pod that fits quota 03/02/23 13:50:08.294
STEP: Ensuring ResourceQuota status captures the pod usage 03/02/23 13:50:08.317
STEP: Not allowing a pod to be created that exceeds remaining quota 03/02/23 13:50:10.324
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/02/23 13:50:10.33
STEP: Ensuring a pod cannot update its resource requirements 03/02/23 13:50:10.334
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/02/23 13:50:10.339
STEP: Deleting the pod 03/02/23 13:50:12.348
STEP: Ensuring resource quota status released the pod usage 03/02/23 13:50:12.387
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 13:50:14.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9719" for this suite. 03/02/23 13:50:14.44
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":220,"skipped":3908,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.244 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:50:01.204
    Mar  2 13:50:01.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename resourcequota 03/02/23 13:50:01.211
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:50:01.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:50:01.251
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 03/02/23 13:50:01.257
    STEP: Creating a ResourceQuota 03/02/23 13:50:06.26
    STEP: Ensuring resource quota status is calculated 03/02/23 13:50:06.265
    STEP: Creating a Pod that fits quota 03/02/23 13:50:08.294
    STEP: Ensuring ResourceQuota status captures the pod usage 03/02/23 13:50:08.317
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/02/23 13:50:10.324
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/02/23 13:50:10.33
    STEP: Ensuring a pod cannot update its resource requirements 03/02/23 13:50:10.334
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/02/23 13:50:10.339
    STEP: Deleting the pod 03/02/23 13:50:12.348
    STEP: Ensuring resource quota status released the pod usage 03/02/23 13:50:12.387
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 13:50:14.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9719" for this suite. 03/02/23 13:50:14.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:50:14.454
Mar  2 13:50:14.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename var-expansion 03/02/23 13:50:14.456
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:50:14.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:50:14.527
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 03/02/23 13:50:14.53
Mar  2 13:50:14.540: INFO: Waiting up to 5m0s for pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda" in namespace "var-expansion-6895" to be "Succeeded or Failed"
Mar  2 13:50:14.544: INFO: Pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda": Phase="Pending", Reason="", readiness=false. Elapsed: 3.469979ms
Mar  2 13:50:16.548: INFO: Pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008097814s
Mar  2 13:50:18.551: INFO: Pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010766006s
Mar  2 13:50:20.550: INFO: Pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009662982s
STEP: Saw pod success 03/02/23 13:50:20.55
Mar  2 13:50:20.550: INFO: Pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda" satisfied condition "Succeeded or Failed"
Mar  2 13:50:20.554: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda container dapi-container: <nil>
STEP: delete the pod 03/02/23 13:50:20.585
Mar  2 13:50:20.600: INFO: Waiting for pod var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda to disappear
Mar  2 13:50:20.607: INFO: Pod var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 13:50:20.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6895" for this suite. 03/02/23 13:50:20.63
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":221,"skipped":3930,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.191 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:50:14.454
    Mar  2 13:50:14.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename var-expansion 03/02/23 13:50:14.456
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:50:14.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:50:14.527
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 03/02/23 13:50:14.53
    Mar  2 13:50:14.540: INFO: Waiting up to 5m0s for pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda" in namespace "var-expansion-6895" to be "Succeeded or Failed"
    Mar  2 13:50:14.544: INFO: Pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda": Phase="Pending", Reason="", readiness=false. Elapsed: 3.469979ms
    Mar  2 13:50:16.548: INFO: Pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008097814s
    Mar  2 13:50:18.551: INFO: Pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010766006s
    Mar  2 13:50:20.550: INFO: Pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009662982s
    STEP: Saw pod success 03/02/23 13:50:20.55
    Mar  2 13:50:20.550: INFO: Pod "var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda" satisfied condition "Succeeded or Failed"
    Mar  2 13:50:20.554: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda container dapi-container: <nil>
    STEP: delete the pod 03/02/23 13:50:20.585
    Mar  2 13:50:20.600: INFO: Waiting for pod var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda to disappear
    Mar  2 13:50:20.607: INFO: Pod var-expansion-a1376999-0e32-4873-bb19-d0d7574c9bda no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 13:50:20.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6895" for this suite. 03/02/23 13:50:20.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:50:20.648
Mar  2 13:50:20.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename security-context-test 03/02/23 13:50:20.65
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:50:20.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:50:20.67
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Mar  2 13:50:20.701: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c" in namespace "security-context-test-9006" to be "Succeeded or Failed"
Mar  2 13:50:20.705: INFO: Pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259793ms
Mar  2 13:50:22.716: INFO: Pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c": Phase="Running", Reason="", readiness=true. Elapsed: 2.014826468s
Mar  2 13:50:24.718: INFO: Pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c": Phase="Running", Reason="", readiness=false. Elapsed: 4.017618859s
Mar  2 13:50:26.718: INFO: Pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017074941s
Mar  2 13:50:26.718: INFO: Pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 13:50:26.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9006" for this suite. 03/02/23 13:50:26.726
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":222,"skipped":3952,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.084 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:50:20.648
    Mar  2 13:50:20.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename security-context-test 03/02/23 13:50:20.65
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:50:20.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:50:20.67
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Mar  2 13:50:20.701: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c" in namespace "security-context-test-9006" to be "Succeeded or Failed"
    Mar  2 13:50:20.705: INFO: Pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259793ms
    Mar  2 13:50:22.716: INFO: Pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c": Phase="Running", Reason="", readiness=true. Elapsed: 2.014826468s
    Mar  2 13:50:24.718: INFO: Pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c": Phase="Running", Reason="", readiness=false. Elapsed: 4.017618859s
    Mar  2 13:50:26.718: INFO: Pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017074941s
    Mar  2 13:50:26.718: INFO: Pod "busybox-readonly-false-30ccb9ce-b596-40ab-ae4a-2caa99cf161c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 13:50:26.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9006" for this suite. 03/02/23 13:50:26.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:50:26.736
Mar  2 13:50:26.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sched-preemption 03/02/23 13:50:26.737
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:50:26.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:50:26.764
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 13:50:26.837: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 13:51:27.085: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:51:27.093
Mar  2 13:51:27.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sched-preemption-path 03/02/23 13:51:27.096
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:51:27.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:51:27.122
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Mar  2 13:51:27.140: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar  2 13:51:27.142: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Mar  2 13:51:27.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4858" for this suite. 03/02/23 13:51:27.159
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  2 13:51:27.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4463" for this suite. 03/02/23 13:51:27.184
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":223,"skipped":3965,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.551 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:50:26.736
    Mar  2 13:50:26.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sched-preemption 03/02/23 13:50:26.737
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:50:26.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:50:26.764
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  2 13:50:26.837: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 13:51:27.085: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:51:27.093
    Mar  2 13:51:27.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sched-preemption-path 03/02/23 13:51:27.096
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:51:27.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:51:27.122
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Mar  2 13:51:27.140: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar  2 13:51:27.142: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Mar  2 13:51:27.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-4858" for this suite. 03/02/23 13:51:27.159
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 13:51:27.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4463" for this suite. 03/02/23 13:51:27.184
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:51:27.29
Mar  2 13:51:27.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename taint-multiple-pods 03/02/23 13:51:27.291
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:51:27.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:51:27.318
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Mar  2 13:51:27.324: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 13:52:27.490: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Mar  2 13:52:27.495: INFO: Starting informer...
STEP: Starting pods... 03/02/23 13:52:27.495
Mar  2 13:52:27.534: INFO: Pod1 is running on aarnq-sc-k8s-node-srv2. Tainting Node
Mar  2 13:52:27.775: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3455" to be "running"
Mar  2 13:52:27.779: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052988ms
Mar  2 13:52:29.794: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019844683s
Mar  2 13:52:29.795: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar  2 13:52:29.795: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3455" to be "running"
Mar  2 13:52:29.801: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.979113ms
Mar  2 13:52:29.801: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar  2 13:52:29.801: INFO: Pod2 is running on aarnq-sc-k8s-node-srv2. Tainting Node
STEP: Trying to apply a taint on the Node 03/02/23 13:52:29.801
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 13:52:29.844
STEP: Waiting for Pod1 and Pod2 to be deleted 03/02/23 13:52:29.862
Mar  2 13:52:40.536: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  2 13:52:56.182: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 13:52:56.234
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Mar  2 13:52:56.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3455" for this suite. 03/02/23 13:52:56.26
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":224,"skipped":3968,"failed":0}
------------------------------
â€¢ [SLOW TEST] [89.003 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:51:27.29
    Mar  2 13:51:27.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename taint-multiple-pods 03/02/23 13:51:27.291
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:51:27.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:51:27.318
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Mar  2 13:51:27.324: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 13:52:27.490: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Mar  2 13:52:27.495: INFO: Starting informer...
    STEP: Starting pods... 03/02/23 13:52:27.495
    Mar  2 13:52:27.534: INFO: Pod1 is running on aarnq-sc-k8s-node-srv2. Tainting Node
    Mar  2 13:52:27.775: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3455" to be "running"
    Mar  2 13:52:27.779: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052988ms
    Mar  2 13:52:29.794: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019844683s
    Mar  2 13:52:29.795: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar  2 13:52:29.795: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3455" to be "running"
    Mar  2 13:52:29.801: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.979113ms
    Mar  2 13:52:29.801: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar  2 13:52:29.801: INFO: Pod2 is running on aarnq-sc-k8s-node-srv2. Tainting Node
    STEP: Trying to apply a taint on the Node 03/02/23 13:52:29.801
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 13:52:29.844
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/02/23 13:52:29.862
    Mar  2 13:52:40.536: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar  2 13:52:56.182: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 13:52:56.234
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 13:52:56.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-3455" for this suite. 03/02/23 13:52:56.26
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:52:56.297
Mar  2 13:52:56.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 13:52:56.302
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:52:56.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:52:56.382
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/02/23 13:52:56.389
Mar  2 13:52:56.442: INFO: Waiting up to 5m0s for pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30" in namespace "emptydir-7696" to be "Succeeded or Failed"
Mar  2 13:52:56.452: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 9.7383ms
Mar  2 13:52:58.456: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013521859s
Mar  2 13:53:00.464: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021895521s
Mar  2 13:53:02.461: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018716637s
Mar  2 13:53:04.457: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014705221s
Mar  2 13:53:06.463: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020490165s
Mar  2 13:53:08.457: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.014628501s
STEP: Saw pod success 03/02/23 13:53:08.457
Mar  2 13:53:08.457: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30" satisfied condition "Succeeded or Failed"
Mar  2 13:53:08.461: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-3f7f110e-fea1-4865-88d4-297302f81a30 container test-container: <nil>
STEP: delete the pod 03/02/23 13:53:08.486
Mar  2 13:53:08.503: INFO: Waiting for pod pod-3f7f110e-fea1-4865-88d4-297302f81a30 to disappear
Mar  2 13:53:08.508: INFO: Pod pod-3f7f110e-fea1-4865-88d4-297302f81a30 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 13:53:08.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7696" for this suite. 03/02/23 13:53:08.514
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":225,"skipped":3972,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.228 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:52:56.297
    Mar  2 13:52:56.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 13:52:56.302
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:52:56.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:52:56.382
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/02/23 13:52:56.389
    Mar  2 13:52:56.442: INFO: Waiting up to 5m0s for pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30" in namespace "emptydir-7696" to be "Succeeded or Failed"
    Mar  2 13:52:56.452: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 9.7383ms
    Mar  2 13:52:58.456: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013521859s
    Mar  2 13:53:00.464: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021895521s
    Mar  2 13:53:02.461: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018716637s
    Mar  2 13:53:04.457: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014705221s
    Mar  2 13:53:06.463: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020490165s
    Mar  2 13:53:08.457: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.014628501s
    STEP: Saw pod success 03/02/23 13:53:08.457
    Mar  2 13:53:08.457: INFO: Pod "pod-3f7f110e-fea1-4865-88d4-297302f81a30" satisfied condition "Succeeded or Failed"
    Mar  2 13:53:08.461: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-3f7f110e-fea1-4865-88d4-297302f81a30 container test-container: <nil>
    STEP: delete the pod 03/02/23 13:53:08.486
    Mar  2 13:53:08.503: INFO: Waiting for pod pod-3f7f110e-fea1-4865-88d4-297302f81a30 to disappear
    Mar  2 13:53:08.508: INFO: Pod pod-3f7f110e-fea1-4865-88d4-297302f81a30 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 13:53:08.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7696" for this suite. 03/02/23 13:53:08.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:53:08.528
Mar  2 13:53:08.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename resourcequota 03/02/23 13:53:08.533
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:08.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:08.553
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 03/02/23 13:53:08.556
STEP: Creating a ResourceQuota 03/02/23 13:53:13.559
STEP: Ensuring resource quota status is calculated 03/02/23 13:53:13.569
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 13:53:15.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2336" for this suite. 03/02/23 13:53:15.581
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":226,"skipped":3980,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.059 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:53:08.528
    Mar  2 13:53:08.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename resourcequota 03/02/23 13:53:08.533
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:08.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:08.553
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 03/02/23 13:53:08.556
    STEP: Creating a ResourceQuota 03/02/23 13:53:13.559
    STEP: Ensuring resource quota status is calculated 03/02/23 13:53:13.569
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 13:53:15.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2336" for this suite. 03/02/23 13:53:15.581
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:53:15.59
Mar  2 13:53:15.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 13:53:15.591
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:15.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:15.608
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-cdc7b695-9b50-4395-b167-857c5dfaac1e 03/02/23 13:53:15.611
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 13:53:15.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9120" for this suite. 03/02/23 13:53:15.617
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":227,"skipped":3983,"failed":0}
------------------------------
â€¢ [0.034 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:53:15.59
    Mar  2 13:53:15.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 13:53:15.591
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:15.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:15.608
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-cdc7b695-9b50-4395-b167-857c5dfaac1e 03/02/23 13:53:15.611
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 13:53:15.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9120" for this suite. 03/02/23 13:53:15.617
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:53:15.625
Mar  2 13:53:15.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:53:15.627
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:15.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:15.651
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-370aff4a-5183-419b-9f91-b7830c125cd3 03/02/23 13:53:15.655
STEP: Creating a pod to test consume secrets 03/02/23 13:53:15.664
Mar  2 13:53:15.676: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7" in namespace "projected-388" to be "Succeeded or Failed"
Mar  2 13:53:15.681: INFO: Pod "pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295783ms
Mar  2 13:53:17.689: INFO: Pod "pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012919735s
Mar  2 13:53:19.714: INFO: Pod "pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03833591s
STEP: Saw pod success 03/02/23 13:53:19.715
Mar  2 13:53:19.715: INFO: Pod "pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7" satisfied condition "Succeeded or Failed"
Mar  2 13:53:19.719: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/02/23 13:53:19.729
Mar  2 13:53:19.746: INFO: Waiting for pod pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7 to disappear
Mar  2 13:53:19.750: INFO: Pod pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 13:53:19.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-388" for this suite. 03/02/23 13:53:19.755
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":228,"skipped":3987,"failed":0}
------------------------------
â€¢ [4.135 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:53:15.625
    Mar  2 13:53:15.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:53:15.627
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:15.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:15.651
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-370aff4a-5183-419b-9f91-b7830c125cd3 03/02/23 13:53:15.655
    STEP: Creating a pod to test consume secrets 03/02/23 13:53:15.664
    Mar  2 13:53:15.676: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7" in namespace "projected-388" to be "Succeeded or Failed"
    Mar  2 13:53:15.681: INFO: Pod "pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295783ms
    Mar  2 13:53:17.689: INFO: Pod "pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012919735s
    Mar  2 13:53:19.714: INFO: Pod "pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03833591s
    STEP: Saw pod success 03/02/23 13:53:19.715
    Mar  2 13:53:19.715: INFO: Pod "pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7" satisfied condition "Succeeded or Failed"
    Mar  2 13:53:19.719: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 13:53:19.729
    Mar  2 13:53:19.746: INFO: Waiting for pod pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7 to disappear
    Mar  2 13:53:19.750: INFO: Pod pod-projected-secrets-b1aa09af-7f47-4b3d-951a-6a7b6723c0f7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 13:53:19.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-388" for this suite. 03/02/23 13:53:19.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:53:19.769
Mar  2 13:53:19.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename events 03/02/23 13:53:19.77
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:19.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:19.799
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/02/23 13:53:19.802
Mar  2 13:53:19.805: INFO: created test-event-1
Mar  2 13:53:19.809: INFO: created test-event-2
Mar  2 13:53:19.814: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/02/23 13:53:19.814
STEP: delete collection of events 03/02/23 13:53:19.816
Mar  2 13:53:19.816: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/02/23 13:53:19.839
Mar  2 13:53:19.840: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar  2 13:53:19.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9362" for this suite. 03/02/23 13:53:19.848
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":229,"skipped":4011,"failed":0}
------------------------------
â€¢ [0.084 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:53:19.769
    Mar  2 13:53:19.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename events 03/02/23 13:53:19.77
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:19.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:19.799
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/02/23 13:53:19.802
    Mar  2 13:53:19.805: INFO: created test-event-1
    Mar  2 13:53:19.809: INFO: created test-event-2
    Mar  2 13:53:19.814: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/02/23 13:53:19.814
    STEP: delete collection of events 03/02/23 13:53:19.816
    Mar  2 13:53:19.816: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/02/23 13:53:19.839
    Mar  2 13:53:19.840: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar  2 13:53:19.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9362" for this suite. 03/02/23 13:53:19.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:53:19.855
Mar  2 13:53:19.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:53:19.857
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:19.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:19.888
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 03/02/23 13:53:19.895
Mar  2 13:53:19.896: INFO: Creating e2e-svc-a-glfdm
Mar  2 13:53:19.911: INFO: Creating e2e-svc-b-glwx2
Mar  2 13:53:19.921: INFO: Creating e2e-svc-c-qbcdz
STEP: deleting service collection 03/02/23 13:53:19.94
Mar  2 13:53:19.961: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:53:19.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5906" for this suite. 03/02/23 13:53:19.968
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":230,"skipped":4033,"failed":0}
------------------------------
â€¢ [0.118 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:53:19.855
    Mar  2 13:53:19.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:53:19.857
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:19.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:19.888
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 03/02/23 13:53:19.895
    Mar  2 13:53:19.896: INFO: Creating e2e-svc-a-glfdm
    Mar  2 13:53:19.911: INFO: Creating e2e-svc-b-glwx2
    Mar  2 13:53:19.921: INFO: Creating e2e-svc-c-qbcdz
    STEP: deleting service collection 03/02/23 13:53:19.94
    Mar  2 13:53:19.961: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:53:19.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5906" for this suite. 03/02/23 13:53:19.968
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:53:19.98
Mar  2 13:53:19.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-runtime 03/02/23 13:53:19.981
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:19.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:20.002
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 03/02/23 13:53:20.009
STEP: wait for the container to reach Succeeded 03/02/23 13:53:20.016
STEP: get the container status 03/02/23 13:53:23.046
STEP: the container should be terminated 03/02/23 13:53:23.051
STEP: the termination message should be set 03/02/23 13:53:23.051
Mar  2 13:53:23.052: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/02/23 13:53:23.052
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  2 13:53:23.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2301" for this suite. 03/02/23 13:53:23.086
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":231,"skipped":4043,"failed":0}
------------------------------
â€¢ [3.113 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:53:19.98
    Mar  2 13:53:19.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-runtime 03/02/23 13:53:19.981
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:19.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:20.002
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 03/02/23 13:53:20.009
    STEP: wait for the container to reach Succeeded 03/02/23 13:53:20.016
    STEP: get the container status 03/02/23 13:53:23.046
    STEP: the container should be terminated 03/02/23 13:53:23.051
    STEP: the termination message should be set 03/02/23 13:53:23.051
    Mar  2 13:53:23.052: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/02/23 13:53:23.052
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  2 13:53:23.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2301" for this suite. 03/02/23 13:53:23.086
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:53:23.098
Mar  2 13:53:23.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:53:23.1
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:23.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:23.125
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-3c2414d6-1ab1-44c1-be90-612ecff4affc 03/02/23 13:53:23.129
STEP: Creating a pod to test consume secrets 03/02/23 13:53:23.134
Mar  2 13:53:23.141: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d" in namespace "projected-9703" to be "Succeeded or Failed"
Mar  2 13:53:23.144: INFO: Pod "pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174172ms
Mar  2 13:53:25.151: INFO: Pod "pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010635029s
Mar  2 13:53:27.150: INFO: Pod "pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009620967s
STEP: Saw pod success 03/02/23 13:53:27.151
Mar  2 13:53:27.152: INFO: Pod "pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d" satisfied condition "Succeeded or Failed"
Mar  2 13:53:27.157: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d container projected-secret-volume-test: <nil>
STEP: delete the pod 03/02/23 13:53:27.166
Mar  2 13:53:27.187: INFO: Waiting for pod pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d to disappear
Mar  2 13:53:27.191: INFO: Pod pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 13:53:27.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9703" for this suite. 03/02/23 13:53:27.196
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":232,"skipped":4047,"failed":0}
------------------------------
â€¢ [4.106 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:53:23.098
    Mar  2 13:53:23.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:53:23.1
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:23.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:23.125
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-3c2414d6-1ab1-44c1-be90-612ecff4affc 03/02/23 13:53:23.129
    STEP: Creating a pod to test consume secrets 03/02/23 13:53:23.134
    Mar  2 13:53:23.141: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d" in namespace "projected-9703" to be "Succeeded or Failed"
    Mar  2 13:53:23.144: INFO: Pod "pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174172ms
    Mar  2 13:53:25.151: INFO: Pod "pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010635029s
    Mar  2 13:53:27.150: INFO: Pod "pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009620967s
    STEP: Saw pod success 03/02/23 13:53:27.151
    Mar  2 13:53:27.152: INFO: Pod "pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d" satisfied condition "Succeeded or Failed"
    Mar  2 13:53:27.157: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 13:53:27.166
    Mar  2 13:53:27.187: INFO: Waiting for pod pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d to disappear
    Mar  2 13:53:27.191: INFO: Pod pod-projected-secrets-38af61ed-a527-4d25-a209-57dd7c9d342d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 13:53:27.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9703" for this suite. 03/02/23 13:53:27.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:53:27.214
Mar  2 13:53:27.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-watch 03/02/23 13:53:27.215
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:27.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:27.238
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar  2 13:53:27.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Creating first CR  03/02/23 13:53:34.849
Mar  2 13:53:34.854: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:53:34Z]] name:name1 resourceVersion:1957656 uid:62b3807d-00b8-497d-8c8f-49abfa54bd64] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/02/23 13:53:44.855
Mar  2 13:53:44.865: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:53:44Z]] name:name2 resourceVersion:1957690 uid:46064961-f6b6-42b6-8628-27ba1aaff0e4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/02/23 13:53:54.866
Mar  2 13:53:54.876: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:53:54Z]] name:name1 resourceVersion:1957723 uid:62b3807d-00b8-497d-8c8f-49abfa54bd64] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/02/23 13:54:04.879
Mar  2 13:54:04.919: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:54:04Z]] name:name2 resourceVersion:1957755 uid:46064961-f6b6-42b6-8628-27ba1aaff0e4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/02/23 13:54:14.922
Mar  2 13:54:14.935: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:53:54Z]] name:name1 resourceVersion:1957803 uid:62b3807d-00b8-497d-8c8f-49abfa54bd64] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/02/23 13:54:24.935
Mar  2 13:54:24.947: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:54:04Z]] name:name2 resourceVersion:1957889 uid:46064961-f6b6-42b6-8628-27ba1aaff0e4] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:54:35.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6045" for this suite. 03/02/23 13:54:35.483
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":233,"skipped":4057,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.277 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:53:27.214
    Mar  2 13:53:27.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-watch 03/02/23 13:53:27.215
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:53:27.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:53:27.238
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar  2 13:53:27.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Creating first CR  03/02/23 13:53:34.849
    Mar  2 13:53:34.854: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:53:34Z]] name:name1 resourceVersion:1957656 uid:62b3807d-00b8-497d-8c8f-49abfa54bd64] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/02/23 13:53:44.855
    Mar  2 13:53:44.865: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:53:44Z]] name:name2 resourceVersion:1957690 uid:46064961-f6b6-42b6-8628-27ba1aaff0e4] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/02/23 13:53:54.866
    Mar  2 13:53:54.876: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:53:54Z]] name:name1 resourceVersion:1957723 uid:62b3807d-00b8-497d-8c8f-49abfa54bd64] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/02/23 13:54:04.879
    Mar  2 13:54:04.919: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:54:04Z]] name:name2 resourceVersion:1957755 uid:46064961-f6b6-42b6-8628-27ba1aaff0e4] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/02/23 13:54:14.922
    Mar  2 13:54:14.935: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:53:54Z]] name:name1 resourceVersion:1957803 uid:62b3807d-00b8-497d-8c8f-49abfa54bd64] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/02/23 13:54:24.935
    Mar  2 13:54:24.947: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T13:53:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T13:54:04Z]] name:name2 resourceVersion:1957889 uid:46064961-f6b6-42b6-8628-27ba1aaff0e4] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:54:35.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-6045" for this suite. 03/02/23 13:54:35.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:54:35.494
Mar  2 13:54:35.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename watch 03/02/23 13:54:35.495
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:54:35.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:54:35.517
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/02/23 13:54:35.521
STEP: creating a new configmap 03/02/23 13:54:35.522
STEP: modifying the configmap once 03/02/23 13:54:35.526
STEP: changing the label value of the configmap 03/02/23 13:54:35.533
STEP: Expecting to observe a delete notification for the watched object 03/02/23 13:54:35.542
Mar  2 13:54:35.542: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957938 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:54:35.543: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957939 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:54:35.543: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957940 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/02/23 13:54:35.543
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/02/23 13:54:35.551
STEP: changing the label value of the configmap back 03/02/23 13:54:45.551
STEP: modifying the configmap a third time 03/02/23 13:54:45.6
STEP: deleting the configmap 03/02/23 13:54:45.609
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/02/23 13:54:45.614
Mar  2 13:54:45.614: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957978 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:54:45.615: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957979 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:54:45.615: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957980 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  2 13:54:45.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6980" for this suite. 03/02/23 13:54:45.622
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":234,"skipped":4078,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.133 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:54:35.494
    Mar  2 13:54:35.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename watch 03/02/23 13:54:35.495
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:54:35.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:54:35.517
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/02/23 13:54:35.521
    STEP: creating a new configmap 03/02/23 13:54:35.522
    STEP: modifying the configmap once 03/02/23 13:54:35.526
    STEP: changing the label value of the configmap 03/02/23 13:54:35.533
    STEP: Expecting to observe a delete notification for the watched object 03/02/23 13:54:35.542
    Mar  2 13:54:35.542: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957938 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:54:35.543: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957939 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:54:35.543: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957940 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/02/23 13:54:35.543
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/02/23 13:54:35.551
    STEP: changing the label value of the configmap back 03/02/23 13:54:45.551
    STEP: modifying the configmap a third time 03/02/23 13:54:45.6
    STEP: deleting the configmap 03/02/23 13:54:45.609
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/02/23 13:54:45.614
    Mar  2 13:54:45.614: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957978 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:54:45.615: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957979 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:54:45.615: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6980  adf2feb8-4978-439c-9087-a93ade87bf32 1957980 0 2023-03-02 13:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 13:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  2 13:54:45.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6980" for this suite. 03/02/23 13:54:45.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:54:45.645
Mar  2 13:54:45.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:54:45.647
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:54:45.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:54:45.675
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 03/02/23 13:54:45.679
Mar  2 13:54:45.687: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa" in namespace "projected-8100" to be "Succeeded or Failed"
Mar  2 13:54:45.690: INFO: Pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489282ms
Mar  2 13:54:47.720: INFO: Pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033310555s
Mar  2 13:54:49.703: INFO: Pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015490719s
Mar  2 13:54:51.721: INFO: Pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033501371s
STEP: Saw pod success 03/02/23 13:54:51.721
Mar  2 13:54:51.722: INFO: Pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa" satisfied condition "Succeeded or Failed"
Mar  2 13:54:51.725: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa container client-container: <nil>
STEP: delete the pod 03/02/23 13:54:51.732
Mar  2 13:54:51.766: INFO: Waiting for pod downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa to disappear
Mar  2 13:54:51.802: INFO: Pod downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 13:54:51.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8100" for this suite. 03/02/23 13:54:51.81
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":235,"skipped":4136,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.173 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:54:45.645
    Mar  2 13:54:45.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:54:45.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:54:45.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:54:45.675
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 03/02/23 13:54:45.679
    Mar  2 13:54:45.687: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa" in namespace "projected-8100" to be "Succeeded or Failed"
    Mar  2 13:54:45.690: INFO: Pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489282ms
    Mar  2 13:54:47.720: INFO: Pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033310555s
    Mar  2 13:54:49.703: INFO: Pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015490719s
    Mar  2 13:54:51.721: INFO: Pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033501371s
    STEP: Saw pod success 03/02/23 13:54:51.721
    Mar  2 13:54:51.722: INFO: Pod "downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa" satisfied condition "Succeeded or Failed"
    Mar  2 13:54:51.725: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa container client-container: <nil>
    STEP: delete the pod 03/02/23 13:54:51.732
    Mar  2 13:54:51.766: INFO: Waiting for pod downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa to disappear
    Mar  2 13:54:51.802: INFO: Pod downwardapi-volume-7a31db05-8602-4efb-9777-7caf22daf1fa no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 13:54:51.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8100" for this suite. 03/02/23 13:54:51.81
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:54:51.823
Mar  2 13:54:51.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename var-expansion 03/02/23 13:54:51.824
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:54:51.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:54:51.85
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 03/02/23 13:54:51.859
Mar  2 13:54:51.868: INFO: Waiting up to 2m0s for pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f" in namespace "var-expansion-9280" to be "running"
Mar  2 13:54:51.884: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.613395ms
Mar  2 13:54:53.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022427901s
Mar  2 13:54:55.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020015436s
Mar  2 13:54:57.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022477516s
Mar  2 13:54:59.897: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028035466s
Mar  2 13:55:01.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021273165s
Mar  2 13:55:03.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02139045s
Mar  2 13:55:05.892: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.022905991s
Mar  2 13:55:07.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.021571449s
Mar  2 13:55:09.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.021552419s
Mar  2 13:55:11.895: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.026746735s
Mar  2 13:55:13.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.020633928s
Mar  2 13:55:15.928: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.059735249s
Mar  2 13:55:17.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020211068s
Mar  2 13:55:19.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.021781183s
Mar  2 13:55:21.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.020287844s
Mar  2 13:55:23.898: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 32.029541595s
Mar  2 13:55:25.914: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.044981175s
Mar  2 13:55:27.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.019828013s
Mar  2 13:55:29.898: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.029357985s
Mar  2 13:55:31.904: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 40.035207962s
Mar  2 13:55:33.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021558363s
Mar  2 13:55:35.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.020129116s
Mar  2 13:55:37.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 46.021594064s
Mar  2 13:55:39.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 48.021750005s
Mar  2 13:55:41.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 50.021870633s
Mar  2 13:55:43.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021078227s
Mar  2 13:55:45.913: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 54.04424023s
Mar  2 13:55:47.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 56.021168397s
Mar  2 13:55:49.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 58.021585771s
Mar  2 13:55:51.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.022497964s
Mar  2 13:55:53.895: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.02617518s
Mar  2 13:55:55.902: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.033383811s
Mar  2 13:55:57.898: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.029662241s
Mar  2 13:55:59.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.022634614s
Mar  2 13:56:01.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.021607514s
Mar  2 13:56:03.893: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.0240724s
Mar  2 13:56:05.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021182822s
Mar  2 13:56:07.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.020483664s
Mar  2 13:56:09.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.021550656s
Mar  2 13:56:11.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.022165902s
Mar  2 13:56:13.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.021995589s
Mar  2 13:56:15.910: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.04132493s
Mar  2 13:56:17.910: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.041095387s
Mar  2 13:56:19.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.019792844s
Mar  2 13:56:21.913: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.043789248s
Mar  2 13:56:23.896: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.027565454s
Mar  2 13:56:25.908: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.038808462s
Mar  2 13:56:27.906: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.037144087s
Mar  2 13:56:29.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.020895082s
Mar  2 13:56:31.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.020725048s
Mar  2 13:56:33.892: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023001811s
Mar  2 13:56:35.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.0217158s
Mar  2 13:56:37.888: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.019235421s
Mar  2 13:56:39.892: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.022913552s
Mar  2 13:56:41.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.02035005s
Mar  2 13:56:43.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.020272534s
Mar  2 13:56:45.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.02137696s
Mar  2 13:56:47.932: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.063077349s
Mar  2 13:56:49.905: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.036045945s
Mar  2 13:56:51.904: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.03496138s
Mar  2 13:56:51.909: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.040265453s
STEP: updating the pod 03/02/23 13:56:51.91
Mar  2 13:56:52.441: INFO: Successfully updated pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f"
STEP: waiting for pod running 03/02/23 13:56:52.441
Mar  2 13:56:52.441: INFO: Waiting up to 2m0s for pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f" in namespace "var-expansion-9280" to be "running"
Mar  2 13:56:52.447: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061154ms
Mar  2 13:56:54.455: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Running", Reason="", readiness=true. Elapsed: 2.014424091s
Mar  2 13:56:54.456: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f" satisfied condition "running"
STEP: deleting the pod gracefully 03/02/23 13:56:54.456
Mar  2 13:56:54.457: INFO: Deleting pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f" in namespace "var-expansion-9280"
Mar  2 13:56:54.469: INFO: Wait up to 5m0s for pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 13:57:28.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9280" for this suite. 03/02/23 13:57:28.534
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":236,"skipped":4139,"failed":0}
------------------------------
â€¢ [SLOW TEST] [156.719 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:54:51.823
    Mar  2 13:54:51.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename var-expansion 03/02/23 13:54:51.824
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:54:51.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:54:51.85
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 03/02/23 13:54:51.859
    Mar  2 13:54:51.868: INFO: Waiting up to 2m0s for pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f" in namespace "var-expansion-9280" to be "running"
    Mar  2 13:54:51.884: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.613395ms
    Mar  2 13:54:53.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022427901s
    Mar  2 13:54:55.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020015436s
    Mar  2 13:54:57.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022477516s
    Mar  2 13:54:59.897: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028035466s
    Mar  2 13:55:01.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021273165s
    Mar  2 13:55:03.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02139045s
    Mar  2 13:55:05.892: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.022905991s
    Mar  2 13:55:07.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.021571449s
    Mar  2 13:55:09.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.021552419s
    Mar  2 13:55:11.895: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.026746735s
    Mar  2 13:55:13.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.020633928s
    Mar  2 13:55:15.928: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.059735249s
    Mar  2 13:55:17.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020211068s
    Mar  2 13:55:19.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.021781183s
    Mar  2 13:55:21.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.020287844s
    Mar  2 13:55:23.898: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 32.029541595s
    Mar  2 13:55:25.914: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.044981175s
    Mar  2 13:55:27.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.019828013s
    Mar  2 13:55:29.898: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.029357985s
    Mar  2 13:55:31.904: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 40.035207962s
    Mar  2 13:55:33.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021558363s
    Mar  2 13:55:35.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.020129116s
    Mar  2 13:55:37.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 46.021594064s
    Mar  2 13:55:39.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 48.021750005s
    Mar  2 13:55:41.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 50.021870633s
    Mar  2 13:55:43.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021078227s
    Mar  2 13:55:45.913: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 54.04424023s
    Mar  2 13:55:47.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 56.021168397s
    Mar  2 13:55:49.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 58.021585771s
    Mar  2 13:55:51.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.022497964s
    Mar  2 13:55:53.895: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.02617518s
    Mar  2 13:55:55.902: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.033383811s
    Mar  2 13:55:57.898: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.029662241s
    Mar  2 13:55:59.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.022634614s
    Mar  2 13:56:01.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.021607514s
    Mar  2 13:56:03.893: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.0240724s
    Mar  2 13:56:05.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021182822s
    Mar  2 13:56:07.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.020483664s
    Mar  2 13:56:09.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.021550656s
    Mar  2 13:56:11.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.022165902s
    Mar  2 13:56:13.891: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.021995589s
    Mar  2 13:56:15.910: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.04132493s
    Mar  2 13:56:17.910: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.041095387s
    Mar  2 13:56:19.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.019792844s
    Mar  2 13:56:21.913: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.043789248s
    Mar  2 13:56:23.896: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.027565454s
    Mar  2 13:56:25.908: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.038808462s
    Mar  2 13:56:27.906: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.037144087s
    Mar  2 13:56:29.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.020895082s
    Mar  2 13:56:31.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.020725048s
    Mar  2 13:56:33.892: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023001811s
    Mar  2 13:56:35.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.0217158s
    Mar  2 13:56:37.888: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.019235421s
    Mar  2 13:56:39.892: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.022913552s
    Mar  2 13:56:41.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.02035005s
    Mar  2 13:56:43.889: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.020272534s
    Mar  2 13:56:45.890: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.02137696s
    Mar  2 13:56:47.932: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.063077349s
    Mar  2 13:56:49.905: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.036045945s
    Mar  2 13:56:51.904: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.03496138s
    Mar  2 13:56:51.909: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.040265453s
    STEP: updating the pod 03/02/23 13:56:51.91
    Mar  2 13:56:52.441: INFO: Successfully updated pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f"
    STEP: waiting for pod running 03/02/23 13:56:52.441
    Mar  2 13:56:52.441: INFO: Waiting up to 2m0s for pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f" in namespace "var-expansion-9280" to be "running"
    Mar  2 13:56:52.447: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061154ms
    Mar  2 13:56:54.455: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f": Phase="Running", Reason="", readiness=true. Elapsed: 2.014424091s
    Mar  2 13:56:54.456: INFO: Pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f" satisfied condition "running"
    STEP: deleting the pod gracefully 03/02/23 13:56:54.456
    Mar  2 13:56:54.457: INFO: Deleting pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f" in namespace "var-expansion-9280"
    Mar  2 13:56:54.469: INFO: Wait up to 5m0s for pod "var-expansion-b0a76790-3dec-4ef4-9c8e-9535bf80245f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 13:57:28.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9280" for this suite. 03/02/23 13:57:28.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:57:28.559
Mar  2 13:57:28.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 13:57:28.563
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:28.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:28.585
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 03/02/23 13:57:28.592
Mar  2 13:57:28.604: INFO: Waiting up to 5m0s for pod "pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7" in namespace "emptydir-1370" to be "Succeeded or Failed"
Mar  2 13:57:28.611: INFO: Pod "pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.053823ms
Mar  2 13:57:30.618: INFO: Pod "pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014327379s
Mar  2 13:57:32.632: INFO: Pod "pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028454197s
STEP: Saw pod success 03/02/23 13:57:32.632
Mar  2 13:57:32.632: INFO: Pod "pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7" satisfied condition "Succeeded or Failed"
Mar  2 13:57:32.637: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7 container test-container: <nil>
STEP: delete the pod 03/02/23 13:57:32.644
Mar  2 13:57:32.661: INFO: Waiting for pod pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7 to disappear
Mar  2 13:57:32.666: INFO: Pod pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 13:57:32.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1370" for this suite. 03/02/23 13:57:32.703
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":237,"skipped":4183,"failed":0}
------------------------------
â€¢ [4.166 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:57:28.559
    Mar  2 13:57:28.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 13:57:28.563
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:28.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:28.585
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 03/02/23 13:57:28.592
    Mar  2 13:57:28.604: INFO: Waiting up to 5m0s for pod "pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7" in namespace "emptydir-1370" to be "Succeeded or Failed"
    Mar  2 13:57:28.611: INFO: Pod "pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.053823ms
    Mar  2 13:57:30.618: INFO: Pod "pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014327379s
    Mar  2 13:57:32.632: INFO: Pod "pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028454197s
    STEP: Saw pod success 03/02/23 13:57:32.632
    Mar  2 13:57:32.632: INFO: Pod "pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7" satisfied condition "Succeeded or Failed"
    Mar  2 13:57:32.637: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7 container test-container: <nil>
    STEP: delete the pod 03/02/23 13:57:32.644
    Mar  2 13:57:32.661: INFO: Waiting for pod pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7 to disappear
    Mar  2 13:57:32.666: INFO: Pod pod-d8070ce2-ea5a-4f38-900e-d4845f0dfcc7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 13:57:32.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1370" for this suite. 03/02/23 13:57:32.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:57:32.73
Mar  2 13:57:32.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 13:57:32.731
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:32.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:32.754
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2884 03/02/23 13:57:32.758
STEP: changing the ExternalName service to type=NodePort 03/02/23 13:57:32.783
STEP: creating replication controller externalname-service in namespace services-2884 03/02/23 13:57:32.824
I0302 13:57:32.831431      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2884, replica count: 2
I0302 13:57:35.910043      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 13:57:35.910: INFO: Creating new exec pod
Mar  2 13:57:35.918: INFO: Waiting up to 5m0s for pod "execpodjf2lq" in namespace "services-2884" to be "running"
Mar  2 13:57:35.929: INFO: Pod "execpodjf2lq": Phase="Pending", Reason="", readiness=false. Elapsed: 10.689409ms
Mar  2 13:57:37.933: INFO: Pod "execpodjf2lq": Phase="Running", Reason="", readiness=true. Elapsed: 2.014825348s
Mar  2 13:57:37.934: INFO: Pod "execpodjf2lq" satisfied condition "running"
Mar  2 13:57:38.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-2884 exec execpodjf2lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 13:57:39.138: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 13:57:39.138: INFO: stdout: "externalname-service-j79jt"
Mar  2 13:57:39.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-2884 exec execpodjf2lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.46.78 80'
Mar  2 13:57:39.325: INFO: stderr: "+ nc -v -t -w 2 10.233.46.78 80\n+ echo hostName\nConnection to 10.233.46.78 80 port [tcp/http] succeeded!\n"
Mar  2 13:57:39.325: INFO: stdout: "externalname-service-j79jt"
Mar  2 13:57:39.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-2884 exec execpodjf2lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.61 31986'
Mar  2 13:57:39.502: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.0.61 31986\nConnection to 172.16.0.61 31986 port [tcp/*] succeeded!\n"
Mar  2 13:57:39.502: INFO: stdout: ""
Mar  2 13:57:40.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-2884 exec execpodjf2lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.61 31986'
Mar  2 13:57:40.688: INFO: stderr: "+ nc -v -t -w 2 172.16.0.61 31986\n+ echo hostName\nConnection to 172.16.0.61 31986 port [tcp/*] succeeded!\n"
Mar  2 13:57:40.688: INFO: stdout: "externalname-service-pp8gg"
Mar  2 13:57:40.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-2884 exec execpodjf2lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.56 31986'
Mar  2 13:57:40.845: INFO: stderr: "+ nc -v -t -w 2 172.16.0.56 31986\n+ echo hostName\nConnection to 172.16.0.56 31986 port [tcp/*] succeeded!\n"
Mar  2 13:57:40.845: INFO: stdout: "externalname-service-j79jt"
Mar  2 13:57:40.845: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 13:57:40.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2884" for this suite. 03/02/23 13:57:40.886
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":238,"skipped":4226,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.164 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:57:32.73
    Mar  2 13:57:32.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 13:57:32.731
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:32.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:32.754
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2884 03/02/23 13:57:32.758
    STEP: changing the ExternalName service to type=NodePort 03/02/23 13:57:32.783
    STEP: creating replication controller externalname-service in namespace services-2884 03/02/23 13:57:32.824
    I0302 13:57:32.831431      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2884, replica count: 2
    I0302 13:57:35.910043      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 13:57:35.910: INFO: Creating new exec pod
    Mar  2 13:57:35.918: INFO: Waiting up to 5m0s for pod "execpodjf2lq" in namespace "services-2884" to be "running"
    Mar  2 13:57:35.929: INFO: Pod "execpodjf2lq": Phase="Pending", Reason="", readiness=false. Elapsed: 10.689409ms
    Mar  2 13:57:37.933: INFO: Pod "execpodjf2lq": Phase="Running", Reason="", readiness=true. Elapsed: 2.014825348s
    Mar  2 13:57:37.934: INFO: Pod "execpodjf2lq" satisfied condition "running"
    Mar  2 13:57:38.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-2884 exec execpodjf2lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  2 13:57:39.138: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  2 13:57:39.138: INFO: stdout: "externalname-service-j79jt"
    Mar  2 13:57:39.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-2884 exec execpodjf2lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.46.78 80'
    Mar  2 13:57:39.325: INFO: stderr: "+ nc -v -t -w 2 10.233.46.78 80\n+ echo hostName\nConnection to 10.233.46.78 80 port [tcp/http] succeeded!\n"
    Mar  2 13:57:39.325: INFO: stdout: "externalname-service-j79jt"
    Mar  2 13:57:39.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-2884 exec execpodjf2lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.61 31986'
    Mar  2 13:57:39.502: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.0.61 31986\nConnection to 172.16.0.61 31986 port [tcp/*] succeeded!\n"
    Mar  2 13:57:39.502: INFO: stdout: ""
    Mar  2 13:57:40.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-2884 exec execpodjf2lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.61 31986'
    Mar  2 13:57:40.688: INFO: stderr: "+ nc -v -t -w 2 172.16.0.61 31986\n+ echo hostName\nConnection to 172.16.0.61 31986 port [tcp/*] succeeded!\n"
    Mar  2 13:57:40.688: INFO: stdout: "externalname-service-pp8gg"
    Mar  2 13:57:40.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-2884 exec execpodjf2lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.0.56 31986'
    Mar  2 13:57:40.845: INFO: stderr: "+ nc -v -t -w 2 172.16.0.56 31986\n+ echo hostName\nConnection to 172.16.0.56 31986 port [tcp/*] succeeded!\n"
    Mar  2 13:57:40.845: INFO: stdout: "externalname-service-j79jt"
    Mar  2 13:57:40.845: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 13:57:40.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2884" for this suite. 03/02/23 13:57:40.886
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:57:40.906
Mar  2 13:57:40.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:57:40.909
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:40.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:40.93
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 03/02/23 13:57:40.935
Mar  2 13:57:40.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9576 create -f -'
Mar  2 13:57:42.425: INFO: stderr: ""
Mar  2 13:57:42.425: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/02/23 13:57:42.425
Mar  2 13:57:43.431: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:57:43.431: INFO: Found 0 / 1
Mar  2 13:57:44.431: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:57:44.431: INFO: Found 0 / 1
Mar  2 13:57:45.434: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:57:45.434: INFO: Found 0 / 1
Mar  2 13:57:46.443: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:57:46.443: INFO: Found 0 / 1
Mar  2 13:57:47.431: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:57:47.431: INFO: Found 0 / 1
Mar  2 13:57:48.446: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:57:48.446: INFO: Found 0 / 1
Mar  2 13:57:49.432: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:57:49.432: INFO: Found 0 / 1
Mar  2 13:57:50.434: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:57:50.435: INFO: Found 1 / 1
Mar  2 13:57:50.435: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/02/23 13:57:50.435
Mar  2 13:57:50.441: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:57:50.442: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 13:57:50.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9576 patch pod agnhost-primary-7kq25 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  2 13:57:50.627: INFO: stderr: ""
Mar  2 13:57:50.627: INFO: stdout: "pod/agnhost-primary-7kq25 patched\n"
STEP: checking annotations 03/02/23 13:57:50.627
Mar  2 13:57:50.630: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 13:57:50.630: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:57:50.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9576" for this suite. 03/02/23 13:57:50.639
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":239,"skipped":4245,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.737 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:57:40.906
    Mar  2 13:57:40.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:57:40.909
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:40.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:40.93
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 03/02/23 13:57:40.935
    Mar  2 13:57:40.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9576 create -f -'
    Mar  2 13:57:42.425: INFO: stderr: ""
    Mar  2 13:57:42.425: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/02/23 13:57:42.425
    Mar  2 13:57:43.431: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:57:43.431: INFO: Found 0 / 1
    Mar  2 13:57:44.431: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:57:44.431: INFO: Found 0 / 1
    Mar  2 13:57:45.434: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:57:45.434: INFO: Found 0 / 1
    Mar  2 13:57:46.443: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:57:46.443: INFO: Found 0 / 1
    Mar  2 13:57:47.431: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:57:47.431: INFO: Found 0 / 1
    Mar  2 13:57:48.446: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:57:48.446: INFO: Found 0 / 1
    Mar  2 13:57:49.432: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:57:49.432: INFO: Found 0 / 1
    Mar  2 13:57:50.434: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:57:50.435: INFO: Found 1 / 1
    Mar  2 13:57:50.435: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/02/23 13:57:50.435
    Mar  2 13:57:50.441: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:57:50.442: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  2 13:57:50.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9576 patch pod agnhost-primary-7kq25 -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar  2 13:57:50.627: INFO: stderr: ""
    Mar  2 13:57:50.627: INFO: stdout: "pod/agnhost-primary-7kq25 patched\n"
    STEP: checking annotations 03/02/23 13:57:50.627
    Mar  2 13:57:50.630: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 13:57:50.630: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:57:50.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9576" for this suite. 03/02/23 13:57:50.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:57:50.644
Mar  2 13:57:50.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename runtimeclass 03/02/23 13:57:50.646
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:50.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:50.666
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar  2 13:57:50.742: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6754 to be scheduled
Mar  2 13:57:50.773: INFO: 1 pods are not scheduled: [runtimeclass-6754/test-runtimeclass-runtimeclass-6754-preconfigured-handler-7cpgc(039c1ed3-c219-4a39-be33-a4601614dfb7)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  2 13:57:52.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6754" for this suite. 03/02/23 13:57:52.797
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":240,"skipped":4256,"failed":0}
------------------------------
â€¢ [2.167 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:57:50.644
    Mar  2 13:57:50.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename runtimeclass 03/02/23 13:57:50.646
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:50.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:50.666
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar  2 13:57:50.742: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6754 to be scheduled
    Mar  2 13:57:50.773: INFO: 1 pods are not scheduled: [runtimeclass-6754/test-runtimeclass-runtimeclass-6754-preconfigured-handler-7cpgc(039c1ed3-c219-4a39-be33-a4601614dfb7)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  2 13:57:52.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6754" for this suite. 03/02/23 13:57:52.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:57:52.815
Mar  2 13:57:52.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename watch 03/02/23 13:57:52.817
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:52.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:52.843
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/02/23 13:57:52.846
STEP: creating a new configmap 03/02/23 13:57:52.847
STEP: modifying the configmap once 03/02/23 13:57:52.852
STEP: closing the watch once it receives two notifications 03/02/23 13:57:52.859
Mar  2 13:57:52.860: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335  8367080c-407d-405e-8a79-dca058869793 1959092 0 2023-03-02 13:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 13:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:57:52.861: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335  8367080c-407d-405e-8a79-dca058869793 1959093 0 2023-03-02 13:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 13:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/02/23 13:57:52.861
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/02/23 13:57:52.871
STEP: deleting the configmap 03/02/23 13:57:52.874
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/02/23 13:57:52.884
Mar  2 13:57:52.884: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335  8367080c-407d-405e-8a79-dca058869793 1959094 0 2023-03-02 13:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 13:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 13:57:52.885: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335  8367080c-407d-405e-8a79-dca058869793 1959095 0 2023-03-02 13:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 13:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  2 13:57:52.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8335" for this suite. 03/02/23 13:57:52.891
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":241,"skipped":4264,"failed":0}
------------------------------
â€¢ [0.084 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:57:52.815
    Mar  2 13:57:52.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename watch 03/02/23 13:57:52.817
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:52.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:52.843
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/02/23 13:57:52.846
    STEP: creating a new configmap 03/02/23 13:57:52.847
    STEP: modifying the configmap once 03/02/23 13:57:52.852
    STEP: closing the watch once it receives two notifications 03/02/23 13:57:52.859
    Mar  2 13:57:52.860: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335  8367080c-407d-405e-8a79-dca058869793 1959092 0 2023-03-02 13:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 13:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:57:52.861: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335  8367080c-407d-405e-8a79-dca058869793 1959093 0 2023-03-02 13:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 13:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/02/23 13:57:52.861
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/02/23 13:57:52.871
    STEP: deleting the configmap 03/02/23 13:57:52.874
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/02/23 13:57:52.884
    Mar  2 13:57:52.884: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335  8367080c-407d-405e-8a79-dca058869793 1959094 0 2023-03-02 13:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 13:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 13:57:52.885: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335  8367080c-407d-405e-8a79-dca058869793 1959095 0 2023-03-02 13:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 13:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  2 13:57:52.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8335" for this suite. 03/02/23 13:57:52.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:57:52.912
Mar  2 13:57:52.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename containers 03/02/23 13:57:52.913
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:52.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:52.931
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Mar  2 13:57:52.943: INFO: Waiting up to 5m0s for pod "client-containers-dabc8328-6be3-4416-b7cb-0be20eefbfc4" in namespace "containers-3863" to be "running"
Mar  2 13:57:52.949: INFO: Pod "client-containers-dabc8328-6be3-4416-b7cb-0be20eefbfc4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.900691ms
Mar  2 13:57:54.955: INFO: Pod "client-containers-dabc8328-6be3-4416-b7cb-0be20eefbfc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011958101s
Mar  2 13:57:56.957: INFO: Pod "client-containers-dabc8328-6be3-4416-b7cb-0be20eefbfc4": Phase="Running", Reason="", readiness=true. Elapsed: 4.013750003s
Mar  2 13:57:56.957: INFO: Pod "client-containers-dabc8328-6be3-4416-b7cb-0be20eefbfc4" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  2 13:57:56.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3863" for this suite. 03/02/23 13:57:56.969
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":242,"skipped":4279,"failed":0}
------------------------------
â€¢ [4.069 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:57:52.912
    Mar  2 13:57:52.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename containers 03/02/23 13:57:52.913
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:52.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:52.931
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Mar  2 13:57:52.943: INFO: Waiting up to 5m0s for pod "client-containers-dabc8328-6be3-4416-b7cb-0be20eefbfc4" in namespace "containers-3863" to be "running"
    Mar  2 13:57:52.949: INFO: Pod "client-containers-dabc8328-6be3-4416-b7cb-0be20eefbfc4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.900691ms
    Mar  2 13:57:54.955: INFO: Pod "client-containers-dabc8328-6be3-4416-b7cb-0be20eefbfc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011958101s
    Mar  2 13:57:56.957: INFO: Pod "client-containers-dabc8328-6be3-4416-b7cb-0be20eefbfc4": Phase="Running", Reason="", readiness=true. Elapsed: 4.013750003s
    Mar  2 13:57:56.957: INFO: Pod "client-containers-dabc8328-6be3-4416-b7cb-0be20eefbfc4" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  2 13:57:56.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3863" for this suite. 03/02/23 13:57:56.969
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:57:56.987
Mar  2 13:57:56.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename resourcequota 03/02/23 13:57:56.989
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:57.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:57.009
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 03/02/23 13:58:14.022
STEP: Creating a ResourceQuota 03/02/23 13:58:19.031
STEP: Ensuring resource quota status is calculated 03/02/23 13:58:19.039
STEP: Creating a ConfigMap 03/02/23 13:58:21.047
STEP: Ensuring resource quota status captures configMap creation 03/02/23 13:58:21.065
STEP: Deleting a ConfigMap 03/02/23 13:58:23.072
STEP: Ensuring resource quota status released usage 03/02/23 13:58:23.084
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 13:58:25.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1255" for this suite. 03/02/23 13:58:25.096
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":243,"skipped":4283,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.116 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:57:56.987
    Mar  2 13:57:56.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename resourcequota 03/02/23 13:57:56.989
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:57:57.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:57:57.009
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 03/02/23 13:58:14.022
    STEP: Creating a ResourceQuota 03/02/23 13:58:19.031
    STEP: Ensuring resource quota status is calculated 03/02/23 13:58:19.039
    STEP: Creating a ConfigMap 03/02/23 13:58:21.047
    STEP: Ensuring resource quota status captures configMap creation 03/02/23 13:58:21.065
    STEP: Deleting a ConfigMap 03/02/23 13:58:23.072
    STEP: Ensuring resource quota status released usage 03/02/23 13:58:23.084
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 13:58:25.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1255" for this suite. 03/02/23 13:58:25.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:58:25.107
Mar  2 13:58:25.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:58:25.113
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:58:25.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:58:25.141
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 03/02/23 13:58:25.143
Mar  2 13:58:25.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2961 api-versions'
Mar  2 13:58:25.228: INFO: stderr: ""
Mar  2 13:58:25.228: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\naquasecurity.github.io/v1alpha1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndex.coreos.com/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvelero.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:58:25.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2961" for this suite. 03/02/23 13:58:25.235
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":244,"skipped":4289,"failed":0}
------------------------------
â€¢ [0.156 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:58:25.107
    Mar  2 13:58:25.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:58:25.113
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:58:25.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:58:25.141
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 03/02/23 13:58:25.143
    Mar  2 13:58:25.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-2961 api-versions'
    Mar  2 13:58:25.228: INFO: stderr: ""
    Mar  2 13:58:25.228: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\naquasecurity.github.io/v1alpha1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndex.coreos.com/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvelero.io/v1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:58:25.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2961" for this suite. 03/02/23 13:58:25.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:58:25.266
Mar  2 13:58:25.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 13:58:25.269
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:58:25.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:58:25.301
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 03/02/23 13:58:25.305
Mar  2 13:58:25.320: INFO: Waiting up to 5m0s for pod "labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f" in namespace "downward-api-1094" to be "running and ready"
Mar  2 13:58:25.325: INFO: Pod "labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377864ms
Mar  2 13:58:25.325: INFO: The phase of Pod labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:58:27.331: INFO: Pod "labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f": Phase="Running", Reason="", readiness=true. Elapsed: 2.01133692s
Mar  2 13:58:27.331: INFO: The phase of Pod labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f is Running (Ready = true)
Mar  2 13:58:27.331: INFO: Pod "labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f" satisfied condition "running and ready"
Mar  2 13:58:27.868: INFO: Successfully updated pod "labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 13:58:29.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1094" for this suite. 03/02/23 13:58:29.906
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":245,"skipped":4309,"failed":0}
------------------------------
â€¢ [4.652 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:58:25.266
    Mar  2 13:58:25.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 13:58:25.269
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:58:25.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:58:25.301
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 03/02/23 13:58:25.305
    Mar  2 13:58:25.320: INFO: Waiting up to 5m0s for pod "labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f" in namespace "downward-api-1094" to be "running and ready"
    Mar  2 13:58:25.325: INFO: Pod "labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377864ms
    Mar  2 13:58:25.325: INFO: The phase of Pod labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:58:27.331: INFO: Pod "labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f": Phase="Running", Reason="", readiness=true. Elapsed: 2.01133692s
    Mar  2 13:58:27.331: INFO: The phase of Pod labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f is Running (Ready = true)
    Mar  2 13:58:27.331: INFO: Pod "labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f" satisfied condition "running and ready"
    Mar  2 13:58:27.868: INFO: Successfully updated pod "labelsupdated5f0dbbd-d714-4fba-96cd-c8f4ca15718f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 13:58:29.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1094" for this suite. 03/02/23 13:58:29.906
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:58:29.923
Mar  2 13:58:29.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename subpath 03/02/23 13:58:29.925
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:58:29.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:58:29.955
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/02/23 13:58:29.96
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-d6s6 03/02/23 13:58:29.969
STEP: Creating a pod to test atomic-volume-subpath 03/02/23 13:58:29.969
Mar  2 13:58:29.978: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d6s6" in namespace "subpath-4535" to be "Succeeded or Failed"
Mar  2 13:58:30.014: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.833814ms
Mar  2 13:58:32.030: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 2.031840994s
Mar  2 13:58:34.027: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 4.029089452s
Mar  2 13:58:36.031: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 6.033279367s
Mar  2 13:58:38.029: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 8.031102707s
Mar  2 13:58:40.030: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 10.031912623s
Mar  2 13:58:42.029: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 12.031087023s
Mar  2 13:58:44.029: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 14.031270968s
Mar  2 13:58:46.028: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 16.030534627s
Mar  2 13:58:48.033: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 18.034796471s
Mar  2 13:58:50.028: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 20.030408733s
Mar  2 13:58:52.030: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 22.032294369s
Mar  2 13:58:54.030: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=false. Elapsed: 24.032454877s
Mar  2 13:58:56.029: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.031145503s
STEP: Saw pod success 03/02/23 13:58:56.029
Mar  2 13:58:56.029: INFO: Pod "pod-subpath-test-configmap-d6s6" satisfied condition "Succeeded or Failed"
Mar  2 13:58:56.036: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-subpath-test-configmap-d6s6 container test-container-subpath-configmap-d6s6: <nil>
STEP: delete the pod 03/02/23 13:58:56.044
Mar  2 13:58:56.060: INFO: Waiting for pod pod-subpath-test-configmap-d6s6 to disappear
Mar  2 13:58:56.064: INFO: Pod pod-subpath-test-configmap-d6s6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d6s6 03/02/23 13:58:56.064
Mar  2 13:58:56.065: INFO: Deleting pod "pod-subpath-test-configmap-d6s6" in namespace "subpath-4535"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  2 13:58:56.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4535" for this suite. 03/02/23 13:58:56.074
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":246,"skipped":4311,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.158 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:58:29.923
    Mar  2 13:58:29.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename subpath 03/02/23 13:58:29.925
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:58:29.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:58:29.955
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/02/23 13:58:29.96
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-d6s6 03/02/23 13:58:29.969
    STEP: Creating a pod to test atomic-volume-subpath 03/02/23 13:58:29.969
    Mar  2 13:58:29.978: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d6s6" in namespace "subpath-4535" to be "Succeeded or Failed"
    Mar  2 13:58:30.014: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.833814ms
    Mar  2 13:58:32.030: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 2.031840994s
    Mar  2 13:58:34.027: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 4.029089452s
    Mar  2 13:58:36.031: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 6.033279367s
    Mar  2 13:58:38.029: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 8.031102707s
    Mar  2 13:58:40.030: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 10.031912623s
    Mar  2 13:58:42.029: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 12.031087023s
    Mar  2 13:58:44.029: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 14.031270968s
    Mar  2 13:58:46.028: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 16.030534627s
    Mar  2 13:58:48.033: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 18.034796471s
    Mar  2 13:58:50.028: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 20.030408733s
    Mar  2 13:58:52.030: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=true. Elapsed: 22.032294369s
    Mar  2 13:58:54.030: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Running", Reason="", readiness=false. Elapsed: 24.032454877s
    Mar  2 13:58:56.029: INFO: Pod "pod-subpath-test-configmap-d6s6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.031145503s
    STEP: Saw pod success 03/02/23 13:58:56.029
    Mar  2 13:58:56.029: INFO: Pod "pod-subpath-test-configmap-d6s6" satisfied condition "Succeeded or Failed"
    Mar  2 13:58:56.036: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-subpath-test-configmap-d6s6 container test-container-subpath-configmap-d6s6: <nil>
    STEP: delete the pod 03/02/23 13:58:56.044
    Mar  2 13:58:56.060: INFO: Waiting for pod pod-subpath-test-configmap-d6s6 to disappear
    Mar  2 13:58:56.064: INFO: Pod pod-subpath-test-configmap-d6s6 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-d6s6 03/02/23 13:58:56.064
    Mar  2 13:58:56.065: INFO: Deleting pod "pod-subpath-test-configmap-d6s6" in namespace "subpath-4535"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  2 13:58:56.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4535" for this suite. 03/02/23 13:58:56.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:58:56.084
Mar  2 13:58:56.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename replicaset 03/02/23 13:58:56.086
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:58:56.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:58:56.112
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/02/23 13:58:56.122
STEP: Verify that the required pods have come up. 03/02/23 13:58:56.13
Mar  2 13:58:56.136: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 13:58:56.136
Mar  2 13:58:56.136: INFO: Waiting up to 5m0s for pod "test-rs-lxm9g" in namespace "replicaset-722" to be "running"
Mar  2 13:58:56.152: INFO: Pod "test-rs-lxm9g": Phase="Pending", Reason="", readiness=false. Elapsed: 15.481082ms
Mar  2 13:58:58.174: INFO: Pod "test-rs-lxm9g": Phase="Running", Reason="", readiness=true. Elapsed: 2.037496636s
Mar  2 13:58:58.175: INFO: Pod "test-rs-lxm9g" satisfied condition "running"
STEP: Getting /status 03/02/23 13:58:58.175
Mar  2 13:58:58.179: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/02/23 13:58:58.179
Mar  2 13:58:58.188: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/02/23 13:58:58.188
Mar  2 13:58:58.192: INFO: Observed &ReplicaSet event: ADDED
Mar  2 13:58:58.193: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 13:58:58.193: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 13:58:58.194: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 13:58:58.194: INFO: Found replicaset test-rs in namespace replicaset-722 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 13:58:58.194: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/02/23 13:58:58.194
Mar  2 13:58:58.195: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 13:58:58.201: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/02/23 13:58:58.201
Mar  2 13:58:58.203: INFO: Observed &ReplicaSet event: ADDED
Mar  2 13:58:58.204: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 13:58:58.204: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 13:58:58.204: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 13:58:58.204: INFO: Observed replicaset test-rs in namespace replicaset-722 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 13:58:58.204: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 13:58:58.204: INFO: Found replicaset test-rs in namespace replicaset-722 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar  2 13:58:58.204: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 13:58:58.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-722" for this suite. 03/02/23 13:58:58.211
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":247,"skipped":4344,"failed":0}
------------------------------
â€¢ [2.134 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:58:56.084
    Mar  2 13:58:56.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename replicaset 03/02/23 13:58:56.086
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:58:56.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:58:56.112
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/02/23 13:58:56.122
    STEP: Verify that the required pods have come up. 03/02/23 13:58:56.13
    Mar  2 13:58:56.136: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 13:58:56.136
    Mar  2 13:58:56.136: INFO: Waiting up to 5m0s for pod "test-rs-lxm9g" in namespace "replicaset-722" to be "running"
    Mar  2 13:58:56.152: INFO: Pod "test-rs-lxm9g": Phase="Pending", Reason="", readiness=false. Elapsed: 15.481082ms
    Mar  2 13:58:58.174: INFO: Pod "test-rs-lxm9g": Phase="Running", Reason="", readiness=true. Elapsed: 2.037496636s
    Mar  2 13:58:58.175: INFO: Pod "test-rs-lxm9g" satisfied condition "running"
    STEP: Getting /status 03/02/23 13:58:58.175
    Mar  2 13:58:58.179: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/02/23 13:58:58.179
    Mar  2 13:58:58.188: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/02/23 13:58:58.188
    Mar  2 13:58:58.192: INFO: Observed &ReplicaSet event: ADDED
    Mar  2 13:58:58.193: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 13:58:58.193: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 13:58:58.194: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 13:58:58.194: INFO: Found replicaset test-rs in namespace replicaset-722 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  2 13:58:58.194: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/02/23 13:58:58.194
    Mar  2 13:58:58.195: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  2 13:58:58.201: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/02/23 13:58:58.201
    Mar  2 13:58:58.203: INFO: Observed &ReplicaSet event: ADDED
    Mar  2 13:58:58.204: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 13:58:58.204: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 13:58:58.204: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 13:58:58.204: INFO: Observed replicaset test-rs in namespace replicaset-722 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  2 13:58:58.204: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 13:58:58.204: INFO: Found replicaset test-rs in namespace replicaset-722 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar  2 13:58:58.204: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 13:58:58.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-722" for this suite. 03/02/23 13:58:58.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:58:58.228
Mar  2 13:58:58.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubelet-test 03/02/23 13:58:58.229
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:58:58.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:58:58.253
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  2 13:59:02.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6009" for this suite. 03/02/23 13:59:02.332
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":248,"skipped":4361,"failed":0}
------------------------------
â€¢ [4.113 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:58:58.228
    Mar  2 13:58:58.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubelet-test 03/02/23 13:58:58.229
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:58:58.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:58:58.253
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  2 13:59:02.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6009" for this suite. 03/02/23 13:59:02.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:59:02.347
Mar  2 13:59:02.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 13:59:02.348
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:02.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:02.439
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 13:59:02.443
Mar  2 13:59:02.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5197 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  2 13:59:02.666: INFO: stderr: ""
Mar  2 13:59:02.666: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/02/23 13:59:02.666
Mar  2 13:59:02.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5197 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Mar  2 13:59:03.962: INFO: stderr: ""
Mar  2 13:59:03.962: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 13:59:03.962
Mar  2 13:59:03.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5197 delete pods e2e-test-httpd-pod'
Mar  2 13:59:08.276: INFO: stderr: ""
Mar  2 13:59:08.276: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 13:59:08.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5197" for this suite. 03/02/23 13:59:08.287
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":249,"skipped":4381,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.953 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:59:02.347
    Mar  2 13:59:02.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 13:59:02.348
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:02.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:02.439
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 13:59:02.443
    Mar  2 13:59:02.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5197 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar  2 13:59:02.666: INFO: stderr: ""
    Mar  2 13:59:02.666: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/02/23 13:59:02.666
    Mar  2 13:59:02.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5197 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Mar  2 13:59:03.962: INFO: stderr: ""
    Mar  2 13:59:03.962: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 13:59:03.962
    Mar  2 13:59:03.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-5197 delete pods e2e-test-httpd-pod'
    Mar  2 13:59:08.276: INFO: stderr: ""
    Mar  2 13:59:08.276: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 13:59:08.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5197" for this suite. 03/02/23 13:59:08.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:59:08.31
Mar  2 13:59:08.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 13:59:08.319
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:08.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:08.362
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 13:59:08.4
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:59:08.96
STEP: Deploying the webhook pod 03/02/23 13:59:08.99
STEP: Wait for the deployment to be ready 03/02/23 13:59:09.025
Mar  2 13:59:09.036: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 13:59:11.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 13, 59, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 59, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 59, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 59, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 13:59:13.056
STEP: Verifying the service has paired with the endpoint 03/02/23 13:59:13.069
Mar  2 13:59:14.071: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 03/02/23 13:59:14.079
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/02/23 13:59:14.085
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/02/23 13:59:14.086
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/02/23 13:59:14.086
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/02/23 13:59:14.088
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/02/23 13:59:14.088
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/02/23 13:59:14.091
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:59:14.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1697" for this suite. 03/02/23 13:59:14.1
STEP: Destroying namespace "webhook-1697-markers" for this suite. 03/02/23 13:59:14.111
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":250,"skipped":4409,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.904 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:59:08.31
    Mar  2 13:59:08.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 13:59:08.319
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:08.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:08.362
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 13:59:08.4
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:59:08.96
    STEP: Deploying the webhook pod 03/02/23 13:59:08.99
    STEP: Wait for the deployment to be ready 03/02/23 13:59:09.025
    Mar  2 13:59:09.036: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 13:59:11.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 13, 59, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 59, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 59, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 59, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 13:59:13.056
    STEP: Verifying the service has paired with the endpoint 03/02/23 13:59:13.069
    Mar  2 13:59:14.071: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 03/02/23 13:59:14.079
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/02/23 13:59:14.085
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/02/23 13:59:14.086
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/02/23 13:59:14.086
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/02/23 13:59:14.088
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/02/23 13:59:14.088
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/02/23 13:59:14.091
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:59:14.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1697" for this suite. 03/02/23 13:59:14.1
    STEP: Destroying namespace "webhook-1697-markers" for this suite. 03/02/23 13:59:14.111
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:59:14.22
Mar  2 13:59:14.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 13:59:14.225
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:14.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:14.254
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 03/02/23 13:59:14.261
Mar  2 13:59:14.304: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356" in namespace "projected-2656" to be "Succeeded or Failed"
Mar  2 13:59:14.330: INFO: Pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356": Phase="Pending", Reason="", readiness=false. Elapsed: 26.160432ms
Mar  2 13:59:16.337: INFO: Pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03255742s
Mar  2 13:59:18.342: INFO: Pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038073527s
Mar  2 13:59:20.339: INFO: Pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035329107s
STEP: Saw pod success 03/02/23 13:59:20.34
Mar  2 13:59:20.340: INFO: Pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356" satisfied condition "Succeeded or Failed"
Mar  2 13:59:20.354: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356 container client-container: <nil>
STEP: delete the pod 03/02/23 13:59:20.362
Mar  2 13:59:20.396: INFO: Waiting for pod downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356 to disappear
Mar  2 13:59:20.401: INFO: Pod downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 13:59:20.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2656" for this suite. 03/02/23 13:59:20.411
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":251,"skipped":4428,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.221 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:59:14.22
    Mar  2 13:59:14.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 13:59:14.225
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:14.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:14.254
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 03/02/23 13:59:14.261
    Mar  2 13:59:14.304: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356" in namespace "projected-2656" to be "Succeeded or Failed"
    Mar  2 13:59:14.330: INFO: Pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356": Phase="Pending", Reason="", readiness=false. Elapsed: 26.160432ms
    Mar  2 13:59:16.337: INFO: Pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03255742s
    Mar  2 13:59:18.342: INFO: Pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038073527s
    Mar  2 13:59:20.339: INFO: Pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035329107s
    STEP: Saw pod success 03/02/23 13:59:20.34
    Mar  2 13:59:20.340: INFO: Pod "downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356" satisfied condition "Succeeded or Failed"
    Mar  2 13:59:20.354: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356 container client-container: <nil>
    STEP: delete the pod 03/02/23 13:59:20.362
    Mar  2 13:59:20.396: INFO: Waiting for pod downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356 to disappear
    Mar  2 13:59:20.401: INFO: Pod downwardapi-volume-93cafa3d-0f16-4855-afe5-c8fcb21d0356 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 13:59:20.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2656" for this suite. 03/02/23 13:59:20.411
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:59:20.441
Mar  2 13:59:20.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename containers 03/02/23 13:59:20.466
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:20.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:20.546
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 03/02/23 13:59:20.553
Mar  2 13:59:20.563: INFO: Waiting up to 5m0s for pod "client-containers-41b9ac70-02dc-4647-8a36-dd258174e771" in namespace "containers-7019" to be "Succeeded or Failed"
Mar  2 13:59:20.566: INFO: Pod "client-containers-41b9ac70-02dc-4647-8a36-dd258174e771": Phase="Pending", Reason="", readiness=false. Elapsed: 3.291203ms
Mar  2 13:59:22.602: INFO: Pod "client-containers-41b9ac70-02dc-4647-8a36-dd258174e771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039122004s
Mar  2 13:59:24.586: INFO: Pod "client-containers-41b9ac70-02dc-4647-8a36-dd258174e771": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023162453s
STEP: Saw pod success 03/02/23 13:59:24.594
Mar  2 13:59:24.594: INFO: Pod "client-containers-41b9ac70-02dc-4647-8a36-dd258174e771" satisfied condition "Succeeded or Failed"
Mar  2 13:59:24.598: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod client-containers-41b9ac70-02dc-4647-8a36-dd258174e771 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 13:59:24.605
Mar  2 13:59:24.618: INFO: Waiting for pod client-containers-41b9ac70-02dc-4647-8a36-dd258174e771 to disappear
Mar  2 13:59:24.622: INFO: Pod client-containers-41b9ac70-02dc-4647-8a36-dd258174e771 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  2 13:59:24.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7019" for this suite. 03/02/23 13:59:24.63
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":252,"skipped":4428,"failed":0}
------------------------------
â€¢ [4.195 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:59:20.441
    Mar  2 13:59:20.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename containers 03/02/23 13:59:20.466
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:20.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:20.546
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 03/02/23 13:59:20.553
    Mar  2 13:59:20.563: INFO: Waiting up to 5m0s for pod "client-containers-41b9ac70-02dc-4647-8a36-dd258174e771" in namespace "containers-7019" to be "Succeeded or Failed"
    Mar  2 13:59:20.566: INFO: Pod "client-containers-41b9ac70-02dc-4647-8a36-dd258174e771": Phase="Pending", Reason="", readiness=false. Elapsed: 3.291203ms
    Mar  2 13:59:22.602: INFO: Pod "client-containers-41b9ac70-02dc-4647-8a36-dd258174e771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039122004s
    Mar  2 13:59:24.586: INFO: Pod "client-containers-41b9ac70-02dc-4647-8a36-dd258174e771": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023162453s
    STEP: Saw pod success 03/02/23 13:59:24.594
    Mar  2 13:59:24.594: INFO: Pod "client-containers-41b9ac70-02dc-4647-8a36-dd258174e771" satisfied condition "Succeeded or Failed"
    Mar  2 13:59:24.598: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod client-containers-41b9ac70-02dc-4647-8a36-dd258174e771 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 13:59:24.605
    Mar  2 13:59:24.618: INFO: Waiting for pod client-containers-41b9ac70-02dc-4647-8a36-dd258174e771 to disappear
    Mar  2 13:59:24.622: INFO: Pod client-containers-41b9ac70-02dc-4647-8a36-dd258174e771 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  2 13:59:24.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7019" for this suite. 03/02/23 13:59:24.63
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:59:24.637
Mar  2 13:59:24.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 13:59:24.639
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:24.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:24.664
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 13:59:24.707
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:59:25.033
STEP: Deploying the webhook pod 03/02/23 13:59:25.044
STEP: Wait for the deployment to be ready 03/02/23 13:59:25.059
Mar  2 13:59:25.068: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  2 13:59:27.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 13, 59, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 59, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 59, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 59, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 13:59:29.092
STEP: Verifying the service has paired with the endpoint 03/02/23 13:59:29.108
Mar  2 13:59:30.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/02/23 13:59:30.119
STEP: create a namespace for the webhook 03/02/23 13:59:30.154
STEP: create a configmap should be unconditionally rejected by the webhook 03/02/23 13:59:30.186
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 13:59:30.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9595" for this suite. 03/02/23 13:59:30.226
STEP: Destroying namespace "webhook-9595-markers" for this suite. 03/02/23 13:59:30.234
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":253,"skipped":4428,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.678 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:59:24.637
    Mar  2 13:59:24.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 13:59:24.639
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:24.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:24.664
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 13:59:24.707
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 13:59:25.033
    STEP: Deploying the webhook pod 03/02/23 13:59:25.044
    STEP: Wait for the deployment to be ready 03/02/23 13:59:25.059
    Mar  2 13:59:25.068: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Mar  2 13:59:27.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 13, 59, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 59, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 13, 59, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 13, 59, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 13:59:29.092
    STEP: Verifying the service has paired with the endpoint 03/02/23 13:59:29.108
    Mar  2 13:59:30.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/02/23 13:59:30.119
    STEP: create a namespace for the webhook 03/02/23 13:59:30.154
    STEP: create a configmap should be unconditionally rejected by the webhook 03/02/23 13:59:30.186
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 13:59:30.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9595" for this suite. 03/02/23 13:59:30.226
    STEP: Destroying namespace "webhook-9595-markers" for this suite. 03/02/23 13:59:30.234
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:59:30.316
Mar  2 13:59:30.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename endpointslice 03/02/23 13:59:30.324
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:30.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:30.417
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 03/02/23 13:59:30.46
STEP: getting /apis/discovery.k8s.io 03/02/23 13:59:30.47
STEP: getting /apis/discovery.k8s.iov1 03/02/23 13:59:30.49
STEP: creating 03/02/23 13:59:30.508
STEP: getting 03/02/23 13:59:30.639
STEP: listing 03/02/23 13:59:30.662
STEP: watching 03/02/23 13:59:30.684
Mar  2 13:59:30.684: INFO: starting watch
STEP: cluster-wide listing 03/02/23 13:59:30.714
STEP: cluster-wide watching 03/02/23 13:59:30.73
Mar  2 13:59:30.730: INFO: starting watch
STEP: patching 03/02/23 13:59:30.732
STEP: updating 03/02/23 13:59:30.749
Mar  2 13:59:30.774: INFO: waiting for watch events with expected annotations
Mar  2 13:59:30.774: INFO: saw patched and updated annotations
STEP: deleting 03/02/23 13:59:30.775
STEP: deleting a collection 03/02/23 13:59:30.817
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  2 13:59:30.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6536" for this suite. 03/02/23 13:59:30.87
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":254,"skipped":4437,"failed":0}
------------------------------
â€¢ [0.598 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:59:30.316
    Mar  2 13:59:30.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename endpointslice 03/02/23 13:59:30.324
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:30.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:30.417
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 03/02/23 13:59:30.46
    STEP: getting /apis/discovery.k8s.io 03/02/23 13:59:30.47
    STEP: getting /apis/discovery.k8s.iov1 03/02/23 13:59:30.49
    STEP: creating 03/02/23 13:59:30.508
    STEP: getting 03/02/23 13:59:30.639
    STEP: listing 03/02/23 13:59:30.662
    STEP: watching 03/02/23 13:59:30.684
    Mar  2 13:59:30.684: INFO: starting watch
    STEP: cluster-wide listing 03/02/23 13:59:30.714
    STEP: cluster-wide watching 03/02/23 13:59:30.73
    Mar  2 13:59:30.730: INFO: starting watch
    STEP: patching 03/02/23 13:59:30.732
    STEP: updating 03/02/23 13:59:30.749
    Mar  2 13:59:30.774: INFO: waiting for watch events with expected annotations
    Mar  2 13:59:30.774: INFO: saw patched and updated annotations
    STEP: deleting 03/02/23 13:59:30.775
    STEP: deleting a collection 03/02/23 13:59:30.817
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  2 13:59:30.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6536" for this suite. 03/02/23 13:59:30.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 13:59:30.918
Mar  2 13:59:30.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pod-network-test 03/02/23 13:59:30.919
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:30.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:31.027
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5655 03/02/23 13:59:31.03
STEP: creating a selector 03/02/23 13:59:31.03
STEP: Creating the service pods in kubernetes 03/02/23 13:59:31.031
Mar  2 13:59:31.031: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 13:59:31.162: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5655" to be "running and ready"
Mar  2 13:59:31.174: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.25784ms
Mar  2 13:59:31.174: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 13:59:33.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.021590361s
Mar  2 13:59:33.184: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:59:35.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020871289s
Mar  2 13:59:35.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:59:37.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.02007064s
Mar  2 13:59:37.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:59:39.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020348291s
Mar  2 13:59:39.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:59:41.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.036590702s
Mar  2 13:59:41.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:59:43.184: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022133784s
Mar  2 13:59:43.184: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:59:45.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.021401921s
Mar  2 13:59:45.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:59:47.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019807278s
Mar  2 13:59:47.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:59:49.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020985671s
Mar  2 13:59:49.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:59:51.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.020739097s
Mar  2 13:59:51.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 13:59:53.186: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.024679589s
Mar  2 13:59:53.187: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  2 13:59:53.187: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  2 13:59:53.192: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5655" to be "running and ready"
Mar  2 13:59:53.195: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.339089ms
Mar  2 13:59:53.195: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  2 13:59:53.196: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  2 13:59:53.199: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5655" to be "running and ready"
Mar  2 13:59:53.202: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.888002ms
Mar  2 13:59:53.202: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  2 13:59:53.202: INFO: Pod "netserver-2" satisfied condition "running and ready"
Mar  2 13:59:53.205: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5655" to be "running and ready"
Mar  2 13:59:53.208: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.646895ms
Mar  2 13:59:53.208: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Mar  2 13:59:53.208: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 03/02/23 13:59:53.211
Mar  2 13:59:53.231: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5655" to be "running"
Mar  2 13:59:53.239: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.322097ms
Mar  2 13:59:55.244: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012827094s
Mar  2 13:59:57.248: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016777994s
Mar  2 13:59:57.248: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  2 13:59:57.254: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5655" to be "running"
Mar  2 13:59:57.258: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.36631ms
Mar  2 13:59:57.258: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar  2 13:59:57.263: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Mar  2 13:59:57.263: INFO: Going to poll 10.233.123.58 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Mar  2 13:59:57.274: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.123.58 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5655 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:59:57.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:59:57.276: INFO: ExecWithOptions: Clientset creation
Mar  2 13:59:57.276: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5655/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.123.58+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 13:59:58.373: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 13:59:58.373: INFO: Going to poll 10.233.92.115 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Mar  2 13:59:58.384: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.92.115 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5655 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 13:59:58.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 13:59:58.385: INFO: ExecWithOptions: Clientset creation
Mar  2 13:59:58.386: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5655/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.92.115+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 14:00:00.875: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 14:00:00.875: INFO: Going to poll 10.233.123.71 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Mar  2 14:00:00.896: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.123.71 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5655 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:00:00.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 14:00:00.897: INFO: ExecWithOptions: Clientset creation
Mar  2 14:00:00.897: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5655/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.123.71+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 14:00:02.010: INFO: Found all 1 expected endpoints: [netserver-2]
Mar  2 14:00:02.010: INFO: Going to poll 10.233.126.113 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Mar  2 14:00:02.033: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.126.113 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5655 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:00:02.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 14:00:02.035: INFO: ExecWithOptions: Clientset creation
Mar  2 14:00:02.035: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5655/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.126.113+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 14:00:03.164: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  2 14:00:03.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5655" for this suite. 03/02/23 14:00:03.177
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":255,"skipped":4445,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.270 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 13:59:30.918
    Mar  2 13:59:30.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pod-network-test 03/02/23 13:59:30.919
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 13:59:30.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 13:59:31.027
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5655 03/02/23 13:59:31.03
    STEP: creating a selector 03/02/23 13:59:31.03
    STEP: Creating the service pods in kubernetes 03/02/23 13:59:31.031
    Mar  2 13:59:31.031: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  2 13:59:31.162: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5655" to be "running and ready"
    Mar  2 13:59:31.174: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.25784ms
    Mar  2 13:59:31.174: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 13:59:33.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.021590361s
    Mar  2 13:59:33.184: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:59:35.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020871289s
    Mar  2 13:59:35.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:59:37.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.02007064s
    Mar  2 13:59:37.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:59:39.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020348291s
    Mar  2 13:59:39.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:59:41.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.036590702s
    Mar  2 13:59:41.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:59:43.184: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022133784s
    Mar  2 13:59:43.184: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:59:45.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.021401921s
    Mar  2 13:59:45.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:59:47.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019807278s
    Mar  2 13:59:47.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:59:49.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020985671s
    Mar  2 13:59:49.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:59:51.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.020739097s
    Mar  2 13:59:51.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 13:59:53.186: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.024679589s
    Mar  2 13:59:53.187: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  2 13:59:53.187: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  2 13:59:53.192: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5655" to be "running and ready"
    Mar  2 13:59:53.195: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.339089ms
    Mar  2 13:59:53.195: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  2 13:59:53.196: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  2 13:59:53.199: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5655" to be "running and ready"
    Mar  2 13:59:53.202: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.888002ms
    Mar  2 13:59:53.202: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  2 13:59:53.202: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Mar  2 13:59:53.205: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5655" to be "running and ready"
    Mar  2 13:59:53.208: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.646895ms
    Mar  2 13:59:53.208: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Mar  2 13:59:53.208: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 03/02/23 13:59:53.211
    Mar  2 13:59:53.231: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5655" to be "running"
    Mar  2 13:59:53.239: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.322097ms
    Mar  2 13:59:55.244: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012827094s
    Mar  2 13:59:57.248: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016777994s
    Mar  2 13:59:57.248: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  2 13:59:57.254: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5655" to be "running"
    Mar  2 13:59:57.258: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.36631ms
    Mar  2 13:59:57.258: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar  2 13:59:57.263: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Mar  2 13:59:57.263: INFO: Going to poll 10.233.123.58 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Mar  2 13:59:57.274: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.123.58 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5655 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:59:57.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:59:57.276: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:59:57.276: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5655/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.123.58+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 13:59:58.373: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar  2 13:59:58.373: INFO: Going to poll 10.233.92.115 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Mar  2 13:59:58.384: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.92.115 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5655 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 13:59:58.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 13:59:58.385: INFO: ExecWithOptions: Clientset creation
    Mar  2 13:59:58.386: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5655/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.92.115+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 14:00:00.875: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar  2 14:00:00.875: INFO: Going to poll 10.233.123.71 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Mar  2 14:00:00.896: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.123.71 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5655 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 14:00:00.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 14:00:00.897: INFO: ExecWithOptions: Clientset creation
    Mar  2 14:00:00.897: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5655/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.123.71+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 14:00:02.010: INFO: Found all 1 expected endpoints: [netserver-2]
    Mar  2 14:00:02.010: INFO: Going to poll 10.233.126.113 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Mar  2 14:00:02.033: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.126.113 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5655 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 14:00:02.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 14:00:02.035: INFO: ExecWithOptions: Clientset creation
    Mar  2 14:00:02.035: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5655/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.126.113+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 14:00:03.164: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  2 14:00:03.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5655" for this suite. 03/02/23 14:00:03.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:00:03.195
Mar  2 14:00:03.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 14:00:03.196
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:00:03.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:00:03.225
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4686 03/02/23 14:00:03.229
STEP: changing the ExternalName service to type=ClusterIP 03/02/23 14:00:03.235
STEP: creating replication controller externalname-service in namespace services-4686 03/02/23 14:00:03.255
I0302 14:00:03.264124      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4686, replica count: 2
I0302 14:00:06.329950      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 14:00:06.330: INFO: Creating new exec pod
Mar  2 14:00:06.338: INFO: Waiting up to 5m0s for pod "execpod48l4c" in namespace "services-4686" to be "running"
Mar  2 14:00:06.345: INFO: Pod "execpod48l4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.889522ms
Mar  2 14:00:08.463: INFO: Pod "execpod48l4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.125302907s
Mar  2 14:00:10.350: INFO: Pod "execpod48l4c": Phase="Running", Reason="", readiness=true. Elapsed: 4.012355024s
Mar  2 14:00:10.350: INFO: Pod "execpod48l4c" satisfied condition "running"
Mar  2 14:00:11.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4686 exec execpod48l4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 14:00:11.752: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 14:00:11.752: INFO: stdout: ""
Mar  2 14:00:12.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4686 exec execpod48l4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 14:00:12.953: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 14:00:12.953: INFO: stdout: "externalname-service-rjcr7"
Mar  2 14:00:12.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4686 exec execpod48l4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.11.43 80'
Mar  2 14:00:13.149: INFO: stderr: "+ nc -v -t -w 2 10.233.11.43 80\n+ echo hostName\nConnection to 10.233.11.43 80 port [tcp/http] succeeded!\n"
Mar  2 14:00:13.149: INFO: stdout: "externalname-service-8lfcg"
Mar  2 14:00:13.149: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 14:00:13.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4686" for this suite. 03/02/23 14:00:13.201
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":256,"skipped":4466,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.017 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:00:03.195
    Mar  2 14:00:03.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 14:00:03.196
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:00:03.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:00:03.225
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4686 03/02/23 14:00:03.229
    STEP: changing the ExternalName service to type=ClusterIP 03/02/23 14:00:03.235
    STEP: creating replication controller externalname-service in namespace services-4686 03/02/23 14:00:03.255
    I0302 14:00:03.264124      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4686, replica count: 2
    I0302 14:00:06.329950      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 14:00:06.330: INFO: Creating new exec pod
    Mar  2 14:00:06.338: INFO: Waiting up to 5m0s for pod "execpod48l4c" in namespace "services-4686" to be "running"
    Mar  2 14:00:06.345: INFO: Pod "execpod48l4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.889522ms
    Mar  2 14:00:08.463: INFO: Pod "execpod48l4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.125302907s
    Mar  2 14:00:10.350: INFO: Pod "execpod48l4c": Phase="Running", Reason="", readiness=true. Elapsed: 4.012355024s
    Mar  2 14:00:10.350: INFO: Pod "execpod48l4c" satisfied condition "running"
    Mar  2 14:00:11.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4686 exec execpod48l4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  2 14:00:11.752: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  2 14:00:11.752: INFO: stdout: ""
    Mar  2 14:00:12.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4686 exec execpod48l4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  2 14:00:12.953: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  2 14:00:12.953: INFO: stdout: "externalname-service-rjcr7"
    Mar  2 14:00:12.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-4686 exec execpod48l4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.11.43 80'
    Mar  2 14:00:13.149: INFO: stderr: "+ nc -v -t -w 2 10.233.11.43 80\n+ echo hostName\nConnection to 10.233.11.43 80 port [tcp/http] succeeded!\n"
    Mar  2 14:00:13.149: INFO: stdout: "externalname-service-8lfcg"
    Mar  2 14:00:13.149: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 14:00:13.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4686" for this suite. 03/02/23 14:00:13.201
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:00:13.214
Mar  2 14:00:13.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 14:00:13.216
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:00:13.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:00:13.246
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 03/02/23 14:00:13.251
Mar  2 14:00:13.258: INFO: Waiting up to 5m0s for pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca" in namespace "emptydir-7108" to be "Succeeded or Failed"
Mar  2 14:00:13.261: INFO: Pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.848836ms
Mar  2 14:00:15.270: INFO: Pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011107702s
Mar  2 14:00:17.270: INFO: Pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011768463s
Mar  2 14:00:19.270: INFO: Pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011579386s
STEP: Saw pod success 03/02/23 14:00:19.27
Mar  2 14:00:19.270: INFO: Pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca" satisfied condition "Succeeded or Failed"
Mar  2 14:00:19.286: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-66c279c8-6ea5-40dc-bd29-009f47d00bca container test-container: <nil>
STEP: delete the pod 03/02/23 14:00:19.314
Mar  2 14:00:19.353: INFO: Waiting for pod pod-66c279c8-6ea5-40dc-bd29-009f47d00bca to disappear
Mar  2 14:00:19.357: INFO: Pod pod-66c279c8-6ea5-40dc-bd29-009f47d00bca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 14:00:19.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7108" for this suite. 03/02/23 14:00:19.365
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":257,"skipped":4469,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.164 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:00:13.214
    Mar  2 14:00:13.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 14:00:13.216
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:00:13.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:00:13.246
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/02/23 14:00:13.251
    Mar  2 14:00:13.258: INFO: Waiting up to 5m0s for pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca" in namespace "emptydir-7108" to be "Succeeded or Failed"
    Mar  2 14:00:13.261: INFO: Pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.848836ms
    Mar  2 14:00:15.270: INFO: Pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011107702s
    Mar  2 14:00:17.270: INFO: Pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011768463s
    Mar  2 14:00:19.270: INFO: Pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011579386s
    STEP: Saw pod success 03/02/23 14:00:19.27
    Mar  2 14:00:19.270: INFO: Pod "pod-66c279c8-6ea5-40dc-bd29-009f47d00bca" satisfied condition "Succeeded or Failed"
    Mar  2 14:00:19.286: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-66c279c8-6ea5-40dc-bd29-009f47d00bca container test-container: <nil>
    STEP: delete the pod 03/02/23 14:00:19.314
    Mar  2 14:00:19.353: INFO: Waiting for pod pod-66c279c8-6ea5-40dc-bd29-009f47d00bca to disappear
    Mar  2 14:00:19.357: INFO: Pod pod-66c279c8-6ea5-40dc-bd29-009f47d00bca no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 14:00:19.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7108" for this suite. 03/02/23 14:00:19.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:00:19.417
Mar  2 14:00:19.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename daemonsets 03/02/23 14:00:19.419
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:00:19.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:00:19.463
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Mar  2 14:00:19.837: INFO: Create a RollingUpdate DaemonSet
Mar  2 14:00:19.843: INFO: Check that daemon pods launch on every node of the cluster
Mar  2 14:00:19.858: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:00:19.862: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:00:19.862: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 14:00:20.878: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:00:20.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:00:20.884: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 14:00:21.874: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:00:21.895: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:00:21.895: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 14:00:22.877: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:00:22.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Mar  2 14:00:22.884: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
Mar  2 14:00:22.884: INFO: Update the DaemonSet to trigger a rollout
Mar  2 14:00:22.904: INFO: Updating DaemonSet daemon-set
Mar  2 14:00:26.046: INFO: Roll back the DaemonSet before rollout is complete
Mar  2 14:00:26.063: INFO: Updating DaemonSet daemon-set
Mar  2 14:00:26.063: INFO: Make sure DaemonSet rollback is complete
Mar  2 14:00:26.071: INFO: Wrong image for pod: daemon-set-4tqxt. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Mar  2 14:00:26.071: INFO: Pod daemon-set-4tqxt is not available
Mar  2 14:00:26.132: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:00:27.149: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:00:28.199: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:00:29.167: INFO: Pod daemon-set-hdr59 is not available
Mar  2 14:00:29.195: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 14:00:29.236
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9933, will wait for the garbage collector to delete the pods 03/02/23 14:00:29.236
Mar  2 14:00:29.318: INFO: Deleting DaemonSet.extensions daemon-set took: 9.414558ms
Mar  2 14:00:29.419: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.867371ms
Mar  2 14:00:32.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:00:32.022: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 14:00:32.024: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1960764"},"items":null}

Mar  2 14:00:32.029: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1960764"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 14:00:32.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9933" for this suite. 03/02/23 14:00:32.053
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":258,"skipped":4487,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.642 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:00:19.417
    Mar  2 14:00:19.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename daemonsets 03/02/23 14:00:19.419
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:00:19.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:00:19.463
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Mar  2 14:00:19.837: INFO: Create a RollingUpdate DaemonSet
    Mar  2 14:00:19.843: INFO: Check that daemon pods launch on every node of the cluster
    Mar  2 14:00:19.858: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:00:19.862: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:00:19.862: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 14:00:20.878: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:00:20.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:00:20.884: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 14:00:21.874: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:00:21.895: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:00:21.895: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 14:00:22.877: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:00:22.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Mar  2 14:00:22.884: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    Mar  2 14:00:22.884: INFO: Update the DaemonSet to trigger a rollout
    Mar  2 14:00:22.904: INFO: Updating DaemonSet daemon-set
    Mar  2 14:00:26.046: INFO: Roll back the DaemonSet before rollout is complete
    Mar  2 14:00:26.063: INFO: Updating DaemonSet daemon-set
    Mar  2 14:00:26.063: INFO: Make sure DaemonSet rollback is complete
    Mar  2 14:00:26.071: INFO: Wrong image for pod: daemon-set-4tqxt. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Mar  2 14:00:26.071: INFO: Pod daemon-set-4tqxt is not available
    Mar  2 14:00:26.132: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:00:27.149: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:00:28.199: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:00:29.167: INFO: Pod daemon-set-hdr59 is not available
    Mar  2 14:00:29.195: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 14:00:29.236
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9933, will wait for the garbage collector to delete the pods 03/02/23 14:00:29.236
    Mar  2 14:00:29.318: INFO: Deleting DaemonSet.extensions daemon-set took: 9.414558ms
    Mar  2 14:00:29.419: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.867371ms
    Mar  2 14:00:32.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:00:32.022: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 14:00:32.024: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1960764"},"items":null}

    Mar  2 14:00:32.029: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1960764"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 14:00:32.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9933" for this suite. 03/02/23 14:00:32.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:00:32.06
Mar  2 14:00:32.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename server-version 03/02/23 14:00:32.061
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:00:32.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:00:32.082
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/02/23 14:00:32.086
STEP: Confirm major version 03/02/23 14:00:32.087
Mar  2 14:00:32.087: INFO: Major version: 1
STEP: Confirm minor version 03/02/23 14:00:32.087
Mar  2 14:00:32.088: INFO: cleanMinorVersion: 25
Mar  2 14:00:32.088: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Mar  2 14:00:32.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5795" for this suite. 03/02/23 14:00:32.094
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":259,"skipped":4504,"failed":0}
------------------------------
â€¢ [0.039 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:00:32.06
    Mar  2 14:00:32.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename server-version 03/02/23 14:00:32.061
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:00:32.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:00:32.082
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/02/23 14:00:32.086
    STEP: Confirm major version 03/02/23 14:00:32.087
    Mar  2 14:00:32.087: INFO: Major version: 1
    STEP: Confirm minor version 03/02/23 14:00:32.087
    Mar  2 14:00:32.088: INFO: cleanMinorVersion: 25
    Mar  2 14:00:32.088: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Mar  2 14:00:32.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-5795" for this suite. 03/02/23 14:00:32.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:00:32.112
Mar  2 14:00:32.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-runtime 03/02/23 14:00:32.113
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:00:32.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:00:32.137
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/02/23 14:00:32.183
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/02/23 14:00:53.518
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/02/23 14:00:53.521
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/02/23 14:00:53.535
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/02/23 14:00:53.536
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/02/23 14:00:53.568
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/02/23 14:01:00.705
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/02/23 14:01:02.723
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/02/23 14:01:02.741
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/02/23 14:01:02.742
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/02/23 14:01:02.802
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/02/23 14:01:03.826
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/02/23 14:01:08.862
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/02/23 14:01:08.869
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/02/23 14:01:08.869
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  2 14:01:08.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-709" for this suite. 03/02/23 14:01:08.921
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":260,"skipped":4535,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.814 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:00:32.112
    Mar  2 14:00:32.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-runtime 03/02/23 14:00:32.113
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:00:32.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:00:32.137
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/02/23 14:00:32.183
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/02/23 14:00:53.518
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/02/23 14:00:53.521
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/02/23 14:00:53.535
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/02/23 14:00:53.536
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/02/23 14:00:53.568
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/02/23 14:01:00.705
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/02/23 14:01:02.723
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/02/23 14:01:02.741
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/02/23 14:01:02.742
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/02/23 14:01:02.802
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/02/23 14:01:03.826
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/02/23 14:01:08.862
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/02/23 14:01:08.869
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/02/23 14:01:08.869
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  2 14:01:08.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-709" for this suite. 03/02/23 14:01:08.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:01:08.931
Mar  2 14:01:08.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 14:01:08.932
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:08.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:08.955
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-b85a27ae-7b96-4e28-b95a-fdac78c1b154 03/02/23 14:01:08.959
STEP: Creating a pod to test consume secrets 03/02/23 14:01:08.963
Mar  2 14:01:08.970: INFO: Waiting up to 5m0s for pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54" in namespace "secrets-8149" to be "Succeeded or Failed"
Mar  2 14:01:08.979: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54": Phase="Pending", Reason="", readiness=false. Elapsed: 8.262854ms
Mar  2 14:01:11.002: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030606785s
Mar  2 14:01:12.990: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54": Phase="Running", Reason="", readiness=true. Elapsed: 4.018405672s
Mar  2 14:01:14.990: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54": Phase="Running", Reason="", readiness=false. Elapsed: 6.018956486s
Mar  2 14:01:16.992: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020478459s
STEP: Saw pod success 03/02/23 14:01:16.992
Mar  2 14:01:16.992: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54" satisfied condition "Succeeded or Failed"
Mar  2 14:01:16.999: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 14:01:17.006
Mar  2 14:01:17.022: INFO: Waiting for pod pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54 to disappear
Mar  2 14:01:17.025: INFO: Pod pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 14:01:17.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8149" for this suite. 03/02/23 14:01:17.03
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":261,"skipped":4552,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.105 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:01:08.931
    Mar  2 14:01:08.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 14:01:08.932
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:08.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:08.955
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-b85a27ae-7b96-4e28-b95a-fdac78c1b154 03/02/23 14:01:08.959
    STEP: Creating a pod to test consume secrets 03/02/23 14:01:08.963
    Mar  2 14:01:08.970: INFO: Waiting up to 5m0s for pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54" in namespace "secrets-8149" to be "Succeeded or Failed"
    Mar  2 14:01:08.979: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54": Phase="Pending", Reason="", readiness=false. Elapsed: 8.262854ms
    Mar  2 14:01:11.002: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030606785s
    Mar  2 14:01:12.990: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54": Phase="Running", Reason="", readiness=true. Elapsed: 4.018405672s
    Mar  2 14:01:14.990: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54": Phase="Running", Reason="", readiness=false. Elapsed: 6.018956486s
    Mar  2 14:01:16.992: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020478459s
    STEP: Saw pod success 03/02/23 14:01:16.992
    Mar  2 14:01:16.992: INFO: Pod "pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54" satisfied condition "Succeeded or Failed"
    Mar  2 14:01:16.999: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 14:01:17.006
    Mar  2 14:01:17.022: INFO: Waiting for pod pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54 to disappear
    Mar  2 14:01:17.025: INFO: Pod pod-secrets-83876cd7-475b-4b05-9be1-c861cb472f54 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 14:01:17.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8149" for this suite. 03/02/23 14:01:17.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:01:17.037
Mar  2 14:01:17.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename namespaces 03/02/23 14:01:17.039
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:17.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:17.057
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 03/02/23 14:01:17.061
Mar  2 14:01:17.065: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/02/23 14:01:17.065
Mar  2 14:01:17.076: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/02/23 14:01:17.076
Mar  2 14:01:17.088: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  2 14:01:17.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5298" for this suite. 03/02/23 14:01:17.093
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":262,"skipped":4557,"failed":0}
------------------------------
â€¢ [0.062 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:01:17.037
    Mar  2 14:01:17.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename namespaces 03/02/23 14:01:17.039
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:17.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:17.057
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 03/02/23 14:01:17.061
    Mar  2 14:01:17.065: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/02/23 14:01:17.065
    Mar  2 14:01:17.076: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/02/23 14:01:17.076
    Mar  2 14:01:17.088: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 14:01:17.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5298" for this suite. 03/02/23 14:01:17.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:01:17.1
Mar  2 14:01:17.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename cronjob 03/02/23 14:01:17.101
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:17.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:17.126
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/02/23 14:01:17.129
STEP: creating 03/02/23 14:01:17.129
STEP: getting 03/02/23 14:01:17.134
STEP: listing 03/02/23 14:01:17.136
STEP: watching 03/02/23 14:01:17.139
Mar  2 14:01:17.139: INFO: starting watch
STEP: cluster-wide listing 03/02/23 14:01:17.14
STEP: cluster-wide watching 03/02/23 14:01:17.143
Mar  2 14:01:17.143: INFO: starting watch
STEP: patching 03/02/23 14:01:17.144
STEP: updating 03/02/23 14:01:17.153
Mar  2 14:01:17.162: INFO: waiting for watch events with expected annotations
Mar  2 14:01:17.162: INFO: saw patched and updated annotations
STEP: patching /status 03/02/23 14:01:17.163
STEP: updating /status 03/02/23 14:01:17.167
STEP: get /status 03/02/23 14:01:17.173
STEP: deleting 03/02/23 14:01:17.175
STEP: deleting a collection 03/02/23 14:01:17.186
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  2 14:01:17.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7653" for this suite. 03/02/23 14:01:17.197
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":263,"skipped":4591,"failed":0}
------------------------------
â€¢ [0.101 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:01:17.1
    Mar  2 14:01:17.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename cronjob 03/02/23 14:01:17.101
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:17.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:17.126
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/02/23 14:01:17.129
    STEP: creating 03/02/23 14:01:17.129
    STEP: getting 03/02/23 14:01:17.134
    STEP: listing 03/02/23 14:01:17.136
    STEP: watching 03/02/23 14:01:17.139
    Mar  2 14:01:17.139: INFO: starting watch
    STEP: cluster-wide listing 03/02/23 14:01:17.14
    STEP: cluster-wide watching 03/02/23 14:01:17.143
    Mar  2 14:01:17.143: INFO: starting watch
    STEP: patching 03/02/23 14:01:17.144
    STEP: updating 03/02/23 14:01:17.153
    Mar  2 14:01:17.162: INFO: waiting for watch events with expected annotations
    Mar  2 14:01:17.162: INFO: saw patched and updated annotations
    STEP: patching /status 03/02/23 14:01:17.163
    STEP: updating /status 03/02/23 14:01:17.167
    STEP: get /status 03/02/23 14:01:17.173
    STEP: deleting 03/02/23 14:01:17.175
    STEP: deleting a collection 03/02/23 14:01:17.186
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  2 14:01:17.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7653" for this suite. 03/02/23 14:01:17.197
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:01:17.201
Mar  2 14:01:17.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename prestop 03/02/23 14:01:17.203
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:17.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:17.22
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-4394 03/02/23 14:01:17.224
STEP: Waiting for pods to come up. 03/02/23 14:01:17.234
Mar  2 14:01:17.234: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4394" to be "running"
Mar  2 14:01:17.239: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.591863ms
Mar  2 14:01:19.249: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014545458s
Mar  2 14:01:21.354: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.119422249s
Mar  2 14:01:21.354: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-4394 03/02/23 14:01:21.402
Mar  2 14:01:21.429: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4394" to be "running"
Mar  2 14:01:21.433: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.501635ms
Mar  2 14:01:23.439: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.0099482s
Mar  2 14:01:23.439: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/02/23 14:01:23.439
Mar  2 14:01:28.453: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/02/23 14:01:28.453
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Mar  2 14:01:28.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4394" for this suite. 03/02/23 14:01:28.498
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":264,"skipped":4594,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.305 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:01:17.201
    Mar  2 14:01:17.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename prestop 03/02/23 14:01:17.203
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:17.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:17.22
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-4394 03/02/23 14:01:17.224
    STEP: Waiting for pods to come up. 03/02/23 14:01:17.234
    Mar  2 14:01:17.234: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4394" to be "running"
    Mar  2 14:01:17.239: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.591863ms
    Mar  2 14:01:19.249: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014545458s
    Mar  2 14:01:21.354: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.119422249s
    Mar  2 14:01:21.354: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-4394 03/02/23 14:01:21.402
    Mar  2 14:01:21.429: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4394" to be "running"
    Mar  2 14:01:21.433: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.501635ms
    Mar  2 14:01:23.439: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.0099482s
    Mar  2 14:01:23.439: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/02/23 14:01:23.439
    Mar  2 14:01:28.453: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/02/23 14:01:28.453
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Mar  2 14:01:28.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-4394" for this suite. 03/02/23 14:01:28.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:01:28.52
Mar  2 14:01:28.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 14:01:28.524
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:28.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:28.555
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/02/23 14:01:28.56
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/02/23 14:01:28.562
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/02/23 14:01:28.563
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/02/23 14:01:28.563
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/02/23 14:01:28.564
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/02/23 14:01:28.565
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/02/23 14:01:28.566
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 14:01:28.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7810" for this suite. 03/02/23 14:01:28.573
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":265,"skipped":4613,"failed":0}
------------------------------
â€¢ [0.061 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:01:28.52
    Mar  2 14:01:28.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 14:01:28.524
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:28.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:28.555
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/02/23 14:01:28.56
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/02/23 14:01:28.562
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/02/23 14:01:28.563
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/02/23 14:01:28.563
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/02/23 14:01:28.564
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/02/23 14:01:28.565
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/02/23 14:01:28.566
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 14:01:28.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7810" for this suite. 03/02/23 14:01:28.573
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:01:28.585
Mar  2 14:01:28.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename configmap 03/02/23 14:01:28.586
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:28.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:28.605
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-a23dbf24-ef7c-46c7-96ac-878fb2d1375d 03/02/23 14:01:28.608
STEP: Creating a pod to test consume configMaps 03/02/23 14:01:28.612
Mar  2 14:01:28.621: INFO: Waiting up to 5m0s for pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20" in namespace "configmap-4968" to be "Succeeded or Failed"
Mar  2 14:01:28.636: INFO: Pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20": Phase="Pending", Reason="", readiness=false. Elapsed: 12.438734ms
Mar  2 14:01:30.654: INFO: Pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029722004s
Mar  2 14:01:32.643: INFO: Pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019540425s
Mar  2 14:01:34.643: INFO: Pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019324667s
STEP: Saw pod success 03/02/23 14:01:34.643
Mar  2 14:01:34.644: INFO: Pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20" satisfied condition "Succeeded or Failed"
Mar  2 14:01:34.649: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 14:01:34.656
Mar  2 14:01:34.667: INFO: Waiting for pod pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20 to disappear
Mar  2 14:01:34.702: INFO: Pod pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 14:01:34.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4968" for this suite. 03/02/23 14:01:34.719
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":266,"skipped":4617,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.140 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:01:28.585
    Mar  2 14:01:28.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename configmap 03/02/23 14:01:28.586
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:28.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:28.605
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-a23dbf24-ef7c-46c7-96ac-878fb2d1375d 03/02/23 14:01:28.608
    STEP: Creating a pod to test consume configMaps 03/02/23 14:01:28.612
    Mar  2 14:01:28.621: INFO: Waiting up to 5m0s for pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20" in namespace "configmap-4968" to be "Succeeded or Failed"
    Mar  2 14:01:28.636: INFO: Pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20": Phase="Pending", Reason="", readiness=false. Elapsed: 12.438734ms
    Mar  2 14:01:30.654: INFO: Pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029722004s
    Mar  2 14:01:32.643: INFO: Pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019540425s
    Mar  2 14:01:34.643: INFO: Pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019324667s
    STEP: Saw pod success 03/02/23 14:01:34.643
    Mar  2 14:01:34.644: INFO: Pod "pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20" satisfied condition "Succeeded or Failed"
    Mar  2 14:01:34.649: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 14:01:34.656
    Mar  2 14:01:34.667: INFO: Waiting for pod pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20 to disappear
    Mar  2 14:01:34.702: INFO: Pod pod-configmaps-a711b2e3-7b33-42c2-b486-9d9564572a20 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 14:01:34.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4968" for this suite. 03/02/23 14:01:34.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:01:34.748
Mar  2 14:01:34.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 14:01:34.75
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:34.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:34.823
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 14:01:34.837
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 14:01:35.75
STEP: Deploying the webhook pod 03/02/23 14:01:35.782
STEP: Wait for the deployment to be ready 03/02/23 14:01:35.802
Mar  2 14:01:35.823: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 14:01:37.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 1, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 1, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 1, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 1, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 14:01:39.847
STEP: Verifying the service has paired with the endpoint 03/02/23 14:01:39.876
Mar  2 14:01:40.883: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Mar  2 14:01:40.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5469-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 14:01:46.424
STEP: Creating a custom resource while v1 is storage version 03/02/23 14:01:46.467
STEP: Patching Custom Resource Definition to set v2 as storage 03/02/23 14:01:48.552
STEP: Patching the custom resource while v2 is storage version 03/02/23 14:01:48.563
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 14:01:49.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5891" for this suite. 03/02/23 14:01:49.148
STEP: Destroying namespace "webhook-5891-markers" for this suite. 03/02/23 14:01:49.157
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":267,"skipped":4682,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.477 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:01:34.748
    Mar  2 14:01:34.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 14:01:34.75
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:34.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:34.823
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 14:01:34.837
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 14:01:35.75
    STEP: Deploying the webhook pod 03/02/23 14:01:35.782
    STEP: Wait for the deployment to be ready 03/02/23 14:01:35.802
    Mar  2 14:01:35.823: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 14:01:37.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 1, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 1, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 1, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 1, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 14:01:39.847
    STEP: Verifying the service has paired with the endpoint 03/02/23 14:01:39.876
    Mar  2 14:01:40.883: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Mar  2 14:01:40.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5469-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 14:01:46.424
    STEP: Creating a custom resource while v1 is storage version 03/02/23 14:01:46.467
    STEP: Patching Custom Resource Definition to set v2 as storage 03/02/23 14:01:48.552
    STEP: Patching the custom resource while v2 is storage version 03/02/23 14:01:48.563
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 14:01:49.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5891" for this suite. 03/02/23 14:01:49.148
    STEP: Destroying namespace "webhook-5891-markers" for this suite. 03/02/23 14:01:49.157
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:01:49.255
Mar  2 14:01:49.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename podtemplate 03/02/23 14:01:49.256
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:49.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:49.34
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  2 14:01:49.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4724" for this suite. 03/02/23 14:01:49.407
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":268,"skipped":4686,"failed":0}
------------------------------
â€¢ [0.161 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:01:49.255
    Mar  2 14:01:49.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename podtemplate 03/02/23 14:01:49.256
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:49.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:49.34
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  2 14:01:49.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-4724" for this suite. 03/02/23 14:01:49.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:01:49.424
Mar  2 14:01:49.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pods 03/02/23 14:01:49.426
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:49.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:49.46
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Mar  2 14:01:49.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: creating the pod 03/02/23 14:01:49.466
STEP: submitting the pod to kubernetes 03/02/23 14:01:49.467
Mar  2 14:01:49.479: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f" in namespace "pods-4866" to be "running and ready"
Mar  2 14:01:49.488: INFO: Pod "pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.044115ms
Mar  2 14:01:49.489: INFO: The phase of Pod pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:01:51.496: INFO: Pod "pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017003616s
Mar  2 14:01:51.497: INFO: The phase of Pod pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:01:53.495: INFO: Pod "pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f": Phase="Running", Reason="", readiness=true. Elapsed: 4.01562154s
Mar  2 14:01:53.495: INFO: The phase of Pod pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f is Running (Ready = true)
Mar  2 14:01:53.495: INFO: Pod "pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 14:01:53.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4866" for this suite. 03/02/23 14:01:53.61
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":269,"skipped":4703,"failed":0}
------------------------------
â€¢ [4.194 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:01:49.424
    Mar  2 14:01:49.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pods 03/02/23 14:01:49.426
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:49.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:49.46
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Mar  2 14:01:49.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: creating the pod 03/02/23 14:01:49.466
    STEP: submitting the pod to kubernetes 03/02/23 14:01:49.467
    Mar  2 14:01:49.479: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f" in namespace "pods-4866" to be "running and ready"
    Mar  2 14:01:49.488: INFO: Pod "pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.044115ms
    Mar  2 14:01:49.489: INFO: The phase of Pod pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:01:51.496: INFO: Pod "pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017003616s
    Mar  2 14:01:51.497: INFO: The phase of Pod pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:01:53.495: INFO: Pod "pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f": Phase="Running", Reason="", readiness=true. Elapsed: 4.01562154s
    Mar  2 14:01:53.495: INFO: The phase of Pod pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f is Running (Ready = true)
    Mar  2 14:01:53.495: INFO: Pod "pod-exec-websocket-94b2689e-ce1a-4996-882c-203c5755710f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 14:01:53.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4866" for this suite. 03/02/23 14:01:53.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:01:53.629
Mar  2 14:01:53.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 14:01:53.63
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:53.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:53.651
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-7f70a27a-59dd-434a-851d-b1a9eaac00aa 03/02/23 14:01:53.654
STEP: Creating a pod to test consume configMaps 03/02/23 14:01:53.657
Mar  2 14:01:53.663: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d" in namespace "projected-5661" to be "Succeeded or Failed"
Mar  2 14:01:53.687: INFO: Pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.809497ms
Mar  2 14:01:55.706: INFO: Pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d": Phase="Running", Reason="", readiness=true. Elapsed: 2.030500993s
Mar  2 14:01:57.692: INFO: Pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d": Phase="Running", Reason="", readiness=false. Elapsed: 4.017082113s
Mar  2 14:01:59.695: INFO: Pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019474131s
STEP: Saw pod success 03/02/23 14:01:59.695
Mar  2 14:01:59.695: INFO: Pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d" satisfied condition "Succeeded or Failed"
Mar  2 14:01:59.702: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d container agnhost-container: <nil>
STEP: delete the pod 03/02/23 14:01:59.716
Mar  2 14:01:59.738: INFO: Waiting for pod pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d to disappear
Mar  2 14:01:59.742: INFO: Pod pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 14:01:59.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5661" for this suite. 03/02/23 14:01:59.752
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":270,"skipped":4745,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.131 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:01:53.629
    Mar  2 14:01:53.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 14:01:53.63
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:53.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:53.651
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-7f70a27a-59dd-434a-851d-b1a9eaac00aa 03/02/23 14:01:53.654
    STEP: Creating a pod to test consume configMaps 03/02/23 14:01:53.657
    Mar  2 14:01:53.663: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d" in namespace "projected-5661" to be "Succeeded or Failed"
    Mar  2 14:01:53.687: INFO: Pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.809497ms
    Mar  2 14:01:55.706: INFO: Pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d": Phase="Running", Reason="", readiness=true. Elapsed: 2.030500993s
    Mar  2 14:01:57.692: INFO: Pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d": Phase="Running", Reason="", readiness=false. Elapsed: 4.017082113s
    Mar  2 14:01:59.695: INFO: Pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019474131s
    STEP: Saw pod success 03/02/23 14:01:59.695
    Mar  2 14:01:59.695: INFO: Pod "pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d" satisfied condition "Succeeded or Failed"
    Mar  2 14:01:59.702: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 14:01:59.716
    Mar  2 14:01:59.738: INFO: Waiting for pod pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d to disappear
    Mar  2 14:01:59.742: INFO: Pod pod-projected-configmaps-a8922fb1-e823-43e7-ac53-40a81a26d55d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 14:01:59.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5661" for this suite. 03/02/23 14:01:59.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:01:59.762
Mar  2 14:01:59.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 14:01:59.762
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:59.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:59.793
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/02/23 14:01:59.805
Mar  2 14:01:59.816: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5872" to be "running and ready"
Mar  2 14:01:59.821: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.561423ms
Mar  2 14:01:59.822: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:02:01.828: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012241285s
Mar  2 14:02:01.828: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  2 14:02:01.828: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 03/02/23 14:02:01.836
Mar  2 14:02:01.842: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5872" to be "running and ready"
Mar  2 14:02:01.847: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.035171ms
Mar  2 14:02:01.847: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:02:03.863: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.0206454s
Mar  2 14:02:03.863: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar  2 14:02:03.863: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/02/23 14:02:03.867
Mar  2 14:02:03.882: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 14:02:03.896: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 14:02:05.898: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 14:02:05.903: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 14:02:07.896: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 14:02:07.902: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/02/23 14:02:07.902
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  2 14:02:07.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5872" for this suite. 03/02/23 14:02:07.923
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":271,"skipped":4776,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.170 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:01:59.762
    Mar  2 14:01:59.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 14:01:59.762
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:01:59.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:01:59.793
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/02/23 14:01:59.805
    Mar  2 14:01:59.816: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5872" to be "running and ready"
    Mar  2 14:01:59.821: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.561423ms
    Mar  2 14:01:59.822: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:02:01.828: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012241285s
    Mar  2 14:02:01.828: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  2 14:02:01.828: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 03/02/23 14:02:01.836
    Mar  2 14:02:01.842: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5872" to be "running and ready"
    Mar  2 14:02:01.847: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.035171ms
    Mar  2 14:02:01.847: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:02:03.863: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.0206454s
    Mar  2 14:02:03.863: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar  2 14:02:03.863: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/02/23 14:02:03.867
    Mar  2 14:02:03.882: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar  2 14:02:03.896: INFO: Pod pod-with-prestop-http-hook still exists
    Mar  2 14:02:05.898: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar  2 14:02:05.903: INFO: Pod pod-with-prestop-http-hook still exists
    Mar  2 14:02:07.896: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar  2 14:02:07.902: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/02/23 14:02:07.902
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  2 14:02:07.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5872" for this suite. 03/02/23 14:02:07.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:02:07.935
Mar  2 14:02:07.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubelet-test 03/02/23 14:02:07.937
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:07.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:07.954
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/02/23 14:02:07.964
Mar  2 14:02:07.964: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasese057e1d5-a3b0-4043-b2dc-cb767a77d9cf" in namespace "kubelet-test-5479" to be "completed"
Mar  2 14:02:07.991: INFO: Pod "agnhost-host-aliasese057e1d5-a3b0-4043-b2dc-cb767a77d9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 26.996413ms
Mar  2 14:02:10.003: INFO: Pod "agnhost-host-aliasese057e1d5-a3b0-4043-b2dc-cb767a77d9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038892061s
Mar  2 14:02:11.995: INFO: Pod "agnhost-host-aliasese057e1d5-a3b0-4043-b2dc-cb767a77d9cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030909696s
Mar  2 14:02:11.995: INFO: Pod "agnhost-host-aliasese057e1d5-a3b0-4043-b2dc-cb767a77d9cf" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  2 14:02:12.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5479" for this suite. 03/02/23 14:02:12.009
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":272,"skipped":4799,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:02:07.935
    Mar  2 14:02:07.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubelet-test 03/02/23 14:02:07.937
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:07.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:07.954
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/02/23 14:02:07.964
    Mar  2 14:02:07.964: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasese057e1d5-a3b0-4043-b2dc-cb767a77d9cf" in namespace "kubelet-test-5479" to be "completed"
    Mar  2 14:02:07.991: INFO: Pod "agnhost-host-aliasese057e1d5-a3b0-4043-b2dc-cb767a77d9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 26.996413ms
    Mar  2 14:02:10.003: INFO: Pod "agnhost-host-aliasese057e1d5-a3b0-4043-b2dc-cb767a77d9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038892061s
    Mar  2 14:02:11.995: INFO: Pod "agnhost-host-aliasese057e1d5-a3b0-4043-b2dc-cb767a77d9cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030909696s
    Mar  2 14:02:11.995: INFO: Pod "agnhost-host-aliasese057e1d5-a3b0-4043-b2dc-cb767a77d9cf" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  2 14:02:12.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5479" for this suite. 03/02/23 14:02:12.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:02:12.042
Mar  2 14:02:12.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sysctl 03/02/23 14:02:12.044
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:12.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:12.079
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/02/23 14:02:12.085
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 14:02:12.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1533" for this suite. 03/02/23 14:02:12.097
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":273,"skipped":4808,"failed":0}
------------------------------
â€¢ [0.062 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:02:12.042
    Mar  2 14:02:12.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sysctl 03/02/23 14:02:12.044
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:12.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:12.079
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/02/23 14:02:12.085
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 14:02:12.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-1533" for this suite. 03/02/23 14:02:12.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:02:12.112
Mar  2 14:02:12.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 14:02:12.114
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:12.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:12.14
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 03/02/23 14:02:12.145
Mar  2 14:02:12.157: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca" in namespace "downward-api-5822" to be "Succeeded or Failed"
Mar  2 14:02:12.164: INFO: Pod "downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.389325ms
Mar  2 14:02:14.184: INFO: Pod "downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026397818s
Mar  2 14:02:16.172: INFO: Pod "downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013880872s
STEP: Saw pod success 03/02/23 14:02:16.172
Mar  2 14:02:16.172: INFO: Pod "downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca" satisfied condition "Succeeded or Failed"
Mar  2 14:02:16.184: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca container client-container: <nil>
STEP: delete the pod 03/02/23 14:02:16.19
Mar  2 14:02:16.208: INFO: Waiting for pod downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca to disappear
Mar  2 14:02:16.211: INFO: Pod downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 14:02:16.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5822" for this suite. 03/02/23 14:02:16.216
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":274,"skipped":4839,"failed":0}
------------------------------
â€¢ [4.108 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:02:12.112
    Mar  2 14:02:12.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 14:02:12.114
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:12.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:12.14
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 03/02/23 14:02:12.145
    Mar  2 14:02:12.157: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca" in namespace "downward-api-5822" to be "Succeeded or Failed"
    Mar  2 14:02:12.164: INFO: Pod "downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.389325ms
    Mar  2 14:02:14.184: INFO: Pod "downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026397818s
    Mar  2 14:02:16.172: INFO: Pod "downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013880872s
    STEP: Saw pod success 03/02/23 14:02:16.172
    Mar  2 14:02:16.172: INFO: Pod "downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca" satisfied condition "Succeeded or Failed"
    Mar  2 14:02:16.184: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca container client-container: <nil>
    STEP: delete the pod 03/02/23 14:02:16.19
    Mar  2 14:02:16.208: INFO: Waiting for pod downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca to disappear
    Mar  2 14:02:16.211: INFO: Pod downwardapi-volume-a7fc5f65-9f0d-463c-b362-347b178b63ca no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 14:02:16.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5822" for this suite. 03/02/23 14:02:16.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:02:16.231
Mar  2 14:02:16.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename resourcequota 03/02/23 14:02:16.233
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:16.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:16.253
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 03/02/23 14:02:16.257
STEP: Getting a ResourceQuota 03/02/23 14:02:16.261
STEP: Updating a ResourceQuota 03/02/23 14:02:16.265
STEP: Verifying a ResourceQuota was modified 03/02/23 14:02:16.271
STEP: Deleting a ResourceQuota 03/02/23 14:02:16.276
STEP: Verifying the deleted ResourceQuota 03/02/23 14:02:16.281
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 14:02:16.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1462" for this suite. 03/02/23 14:02:16.29
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":275,"skipped":4858,"failed":0}
------------------------------
â€¢ [0.063 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:02:16.231
    Mar  2 14:02:16.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename resourcequota 03/02/23 14:02:16.233
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:16.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:16.253
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 03/02/23 14:02:16.257
    STEP: Getting a ResourceQuota 03/02/23 14:02:16.261
    STEP: Updating a ResourceQuota 03/02/23 14:02:16.265
    STEP: Verifying a ResourceQuota was modified 03/02/23 14:02:16.271
    STEP: Deleting a ResourceQuota 03/02/23 14:02:16.276
    STEP: Verifying the deleted ResourceQuota 03/02/23 14:02:16.281
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 14:02:16.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1462" for this suite. 03/02/23 14:02:16.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:02:16.299
Mar  2 14:02:16.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 14:02:16.3
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:16.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:16.318
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 03/02/23 14:02:16.322
Mar  2 14:02:16.331: INFO: Waiting up to 5m0s for pod "downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf" in namespace "downward-api-587" to be "Succeeded or Failed"
Mar  2 14:02:16.336: INFO: Pod "downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.876007ms
Mar  2 14:02:18.346: INFO: Pod "downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015550594s
Mar  2 14:02:20.348: INFO: Pod "downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017335957s
STEP: Saw pod success 03/02/23 14:02:20.348
Mar  2 14:02:20.348: INFO: Pod "downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf" satisfied condition "Succeeded or Failed"
Mar  2 14:02:20.353: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf container dapi-container: <nil>
STEP: delete the pod 03/02/23 14:02:20.362
Mar  2 14:02:20.403: INFO: Waiting for pod downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf to disappear
Mar  2 14:02:20.409: INFO: Pod downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  2 14:02:20.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-587" for this suite. 03/02/23 14:02:20.416
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":276,"skipped":4919,"failed":0}
------------------------------
â€¢ [4.125 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:02:16.299
    Mar  2 14:02:16.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 14:02:16.3
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:16.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:16.318
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 03/02/23 14:02:16.322
    Mar  2 14:02:16.331: INFO: Waiting up to 5m0s for pod "downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf" in namespace "downward-api-587" to be "Succeeded or Failed"
    Mar  2 14:02:16.336: INFO: Pod "downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.876007ms
    Mar  2 14:02:18.346: INFO: Pod "downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015550594s
    Mar  2 14:02:20.348: INFO: Pod "downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017335957s
    STEP: Saw pod success 03/02/23 14:02:20.348
    Mar  2 14:02:20.348: INFO: Pod "downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf" satisfied condition "Succeeded or Failed"
    Mar  2 14:02:20.353: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf container dapi-container: <nil>
    STEP: delete the pod 03/02/23 14:02:20.362
    Mar  2 14:02:20.403: INFO: Waiting for pod downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf to disappear
    Mar  2 14:02:20.409: INFO: Pod downward-api-88c9a20e-aff2-48b1-ab31-eafc67f81bdf no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  2 14:02:20.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-587" for this suite. 03/02/23 14:02:20.416
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:02:20.426
Mar  2 14:02:20.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 14:02:20.428
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:20.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:20.498
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 03/02/23 14:02:20.501
Mar  2 14:02:20.513: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041" in namespace "projected-7877" to be "Succeeded or Failed"
Mar  2 14:02:20.517: INFO: Pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980145ms
Mar  2 14:02:22.527: INFO: Pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041": Phase="Running", Reason="", readiness=true. Elapsed: 2.014182915s
Mar  2 14:02:24.526: INFO: Pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041": Phase="Running", Reason="", readiness=false. Elapsed: 4.012707173s
Mar  2 14:02:26.522: INFO: Pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008940682s
STEP: Saw pod success 03/02/23 14:02:26.522
Mar  2 14:02:26.522: INFO: Pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041" satisfied condition "Succeeded or Failed"
Mar  2 14:02:26.527: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041 container client-container: <nil>
STEP: delete the pod 03/02/23 14:02:26.535
Mar  2 14:02:26.556: INFO: Waiting for pod downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041 to disappear
Mar  2 14:02:26.561: INFO: Pod downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 14:02:26.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7877" for this suite. 03/02/23 14:02:26.568
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":277,"skipped":4919,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.159 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:02:20.426
    Mar  2 14:02:20.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 14:02:20.428
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:20.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:20.498
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 03/02/23 14:02:20.501
    Mar  2 14:02:20.513: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041" in namespace "projected-7877" to be "Succeeded or Failed"
    Mar  2 14:02:20.517: INFO: Pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980145ms
    Mar  2 14:02:22.527: INFO: Pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041": Phase="Running", Reason="", readiness=true. Elapsed: 2.014182915s
    Mar  2 14:02:24.526: INFO: Pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041": Phase="Running", Reason="", readiness=false. Elapsed: 4.012707173s
    Mar  2 14:02:26.522: INFO: Pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008940682s
    STEP: Saw pod success 03/02/23 14:02:26.522
    Mar  2 14:02:26.522: INFO: Pod "downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041" satisfied condition "Succeeded or Failed"
    Mar  2 14:02:26.527: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041 container client-container: <nil>
    STEP: delete the pod 03/02/23 14:02:26.535
    Mar  2 14:02:26.556: INFO: Waiting for pod downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041 to disappear
    Mar  2 14:02:26.561: INFO: Pod downwardapi-volume-c48574f4-cc09-4ae0-a528-1bc0b1f94041 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 14:02:26.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7877" for this suite. 03/02/23 14:02:26.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:02:26.592
Mar  2 14:02:26.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename statefulset 03/02/23 14:02:26.593
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:26.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:26.686
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6504 03/02/23 14:02:26.692
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-6504 03/02/23 14:02:26.718
Mar  2 14:02:26.742: INFO: Found 0 stateful pods, waiting for 1
Mar  2 14:02:36.754: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/02/23 14:02:36.765
STEP: Getting /status 03/02/23 14:02:36.774
Mar  2 14:02:36.781: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/02/23 14:02:36.781
Mar  2 14:02:36.794: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/02/23 14:02:36.794
Mar  2 14:02:36.798: INFO: Observed &StatefulSet event: ADDED
Mar  2 14:02:36.798: INFO: Found Statefulset ss in namespace statefulset-6504 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 14:02:36.798: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/02/23 14:02:36.798
Mar  2 14:02:36.799: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 14:02:36.806: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/02/23 14:02:36.806
Mar  2 14:02:36.809: INFO: Observed &StatefulSet event: ADDED
Mar  2 14:02:36.810: INFO: Observed Statefulset ss in namespace statefulset-6504 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 14:02:36.810: INFO: Observed &StatefulSet event: MODIFIED
Mar  2 14:02:36.810: INFO: Found Statefulset ss in namespace statefulset-6504 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 14:02:36.811: INFO: Deleting all statefulset in ns statefulset-6504
Mar  2 14:02:36.813: INFO: Scaling statefulset ss to 0
Mar  2 14:02:46.839: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 14:02:46.845: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 14:02:46.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6504" for this suite. 03/02/23 14:02:46.886
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":278,"skipped":4976,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.310 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:02:26.592
    Mar  2 14:02:26.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename statefulset 03/02/23 14:02:26.593
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:26.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:26.686
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6504 03/02/23 14:02:26.692
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-6504 03/02/23 14:02:26.718
    Mar  2 14:02:26.742: INFO: Found 0 stateful pods, waiting for 1
    Mar  2 14:02:36.754: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/02/23 14:02:36.765
    STEP: Getting /status 03/02/23 14:02:36.774
    Mar  2 14:02:36.781: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/02/23 14:02:36.781
    Mar  2 14:02:36.794: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/02/23 14:02:36.794
    Mar  2 14:02:36.798: INFO: Observed &StatefulSet event: ADDED
    Mar  2 14:02:36.798: INFO: Found Statefulset ss in namespace statefulset-6504 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  2 14:02:36.798: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/02/23 14:02:36.798
    Mar  2 14:02:36.799: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  2 14:02:36.806: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/02/23 14:02:36.806
    Mar  2 14:02:36.809: INFO: Observed &StatefulSet event: ADDED
    Mar  2 14:02:36.810: INFO: Observed Statefulset ss in namespace statefulset-6504 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  2 14:02:36.810: INFO: Observed &StatefulSet event: MODIFIED
    Mar  2 14:02:36.810: INFO: Found Statefulset ss in namespace statefulset-6504 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 14:02:36.811: INFO: Deleting all statefulset in ns statefulset-6504
    Mar  2 14:02:36.813: INFO: Scaling statefulset ss to 0
    Mar  2 14:02:46.839: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 14:02:46.845: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 14:02:46.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6504" for this suite. 03/02/23 14:02:46.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:02:46.904
Mar  2 14:02:46.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pods 03/02/23 14:02:46.907
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:46.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:46.93
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 03/02/23 14:02:46.948
STEP: watching for Pod to be ready 03/02/23 14:02:46.962
Mar  2 14:02:46.964: INFO: observed Pod pod-test in namespace pods-473 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar  2 14:02:46.970: INFO: observed Pod pod-test in namespace pods-473 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  }]
Mar  2 14:02:46.982: INFO: observed Pod pod-test in namespace pods-473 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  }]
Mar  2 14:02:47.734: INFO: observed Pod pod-test in namespace pods-473 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  }]
Mar  2 14:02:48.891: INFO: Found Pod pod-test in namespace pods-473 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/02/23 14:02:48.897
STEP: getting the Pod and ensuring that it's patched 03/02/23 14:02:48.922
STEP: replacing the Pod's status Ready condition to False 03/02/23 14:02:48.925
STEP: check the Pod again to ensure its Ready conditions are False 03/02/23 14:02:48.938
STEP: deleting the Pod via a Collection with a LabelSelector 03/02/23 14:02:48.938
STEP: watching for the Pod to be deleted 03/02/23 14:02:48.95
Mar  2 14:02:48.952: INFO: observed event type MODIFIED
Mar  2 14:02:50.908: INFO: observed event type MODIFIED
Mar  2 14:02:51.182: INFO: observed event type MODIFIED
Mar  2 14:02:51.917: INFO: observed event type MODIFIED
Mar  2 14:02:51.932: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 14:02:51.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-473" for this suite. 03/02/23 14:02:51.96
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":279,"skipped":4981,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.078 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:02:46.904
    Mar  2 14:02:46.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pods 03/02/23 14:02:46.907
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:46.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:46.93
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 03/02/23 14:02:46.948
    STEP: watching for Pod to be ready 03/02/23 14:02:46.962
    Mar  2 14:02:46.964: INFO: observed Pod pod-test in namespace pods-473 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar  2 14:02:46.970: INFO: observed Pod pod-test in namespace pods-473 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  }]
    Mar  2 14:02:46.982: INFO: observed Pod pod-test in namespace pods-473 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  }]
    Mar  2 14:02:47.734: INFO: observed Pod pod-test in namespace pods-473 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  }]
    Mar  2 14:02:48.891: INFO: Found Pod pod-test in namespace pods-473 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:02:46 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/02/23 14:02:48.897
    STEP: getting the Pod and ensuring that it's patched 03/02/23 14:02:48.922
    STEP: replacing the Pod's status Ready condition to False 03/02/23 14:02:48.925
    STEP: check the Pod again to ensure its Ready conditions are False 03/02/23 14:02:48.938
    STEP: deleting the Pod via a Collection with a LabelSelector 03/02/23 14:02:48.938
    STEP: watching for the Pod to be deleted 03/02/23 14:02:48.95
    Mar  2 14:02:48.952: INFO: observed event type MODIFIED
    Mar  2 14:02:50.908: INFO: observed event type MODIFIED
    Mar  2 14:02:51.182: INFO: observed event type MODIFIED
    Mar  2 14:02:51.917: INFO: observed event type MODIFIED
    Mar  2 14:02:51.932: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 14:02:51.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-473" for this suite. 03/02/23 14:02:51.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:02:51.993
Mar  2 14:02:51.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename proxy 03/02/23 14:02:51.995
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:52.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:52.016
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar  2 14:02:52.019: INFO: Creating pod...
Mar  2 14:02:52.028: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9363" to be "running"
Mar  2 14:02:52.042: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039497ms
Mar  2 14:02:54.048: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.02028711s
Mar  2 14:02:54.048: INFO: Pod "agnhost" satisfied condition "running"
Mar  2 14:02:54.048: INFO: Creating service...
Mar  2 14:02:54.068: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/DELETE
Mar  2 14:02:54.105: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 14:02:54.106: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/GET
Mar  2 14:02:54.111: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  2 14:02:54.112: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/HEAD
Mar  2 14:02:54.117: INFO: http.Client request:HEAD | StatusCode:200
Mar  2 14:02:54.117: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/OPTIONS
Mar  2 14:02:54.130: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 14:02:54.130: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/PATCH
Mar  2 14:02:54.135: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 14:02:54.135: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/POST
Mar  2 14:02:54.141: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 14:02:54.141: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/PUT
Mar  2 14:02:54.148: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  2 14:02:54.148: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/DELETE
Mar  2 14:02:54.153: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 14:02:54.153: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/GET
Mar  2 14:02:54.158: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  2 14:02:54.158: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/HEAD
Mar  2 14:02:54.163: INFO: http.Client request:HEAD | StatusCode:200
Mar  2 14:02:54.163: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/OPTIONS
Mar  2 14:02:54.215: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 14:02:54.215: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/PATCH
Mar  2 14:02:54.220: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 14:02:54.220: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/POST
Mar  2 14:02:54.225: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 14:02:54.225: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/PUT
Mar  2 14:02:54.231: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  2 14:02:54.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9363" for this suite. 03/02/23 14:02:54.238
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":280,"skipped":5005,"failed":0}
------------------------------
â€¢ [2.256 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:02:51.993
    Mar  2 14:02:51.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename proxy 03/02/23 14:02:51.995
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:52.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:52.016
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar  2 14:02:52.019: INFO: Creating pod...
    Mar  2 14:02:52.028: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9363" to be "running"
    Mar  2 14:02:52.042: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039497ms
    Mar  2 14:02:54.048: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.02028711s
    Mar  2 14:02:54.048: INFO: Pod "agnhost" satisfied condition "running"
    Mar  2 14:02:54.048: INFO: Creating service...
    Mar  2 14:02:54.068: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/DELETE
    Mar  2 14:02:54.105: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  2 14:02:54.106: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/GET
    Mar  2 14:02:54.111: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar  2 14:02:54.112: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/HEAD
    Mar  2 14:02:54.117: INFO: http.Client request:HEAD | StatusCode:200
    Mar  2 14:02:54.117: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar  2 14:02:54.130: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  2 14:02:54.130: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/PATCH
    Mar  2 14:02:54.135: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  2 14:02:54.135: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/POST
    Mar  2 14:02:54.141: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  2 14:02:54.141: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/pods/agnhost/proxy/some/path/with/PUT
    Mar  2 14:02:54.148: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  2 14:02:54.148: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/DELETE
    Mar  2 14:02:54.153: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  2 14:02:54.153: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/GET
    Mar  2 14:02:54.158: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar  2 14:02:54.158: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/HEAD
    Mar  2 14:02:54.163: INFO: http.Client request:HEAD | StatusCode:200
    Mar  2 14:02:54.163: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/OPTIONS
    Mar  2 14:02:54.215: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  2 14:02:54.215: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/PATCH
    Mar  2 14:02:54.220: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  2 14:02:54.220: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/POST
    Mar  2 14:02:54.225: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  2 14:02:54.225: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9363/services/test-service/proxy/some/path/with/PUT
    Mar  2 14:02:54.231: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  2 14:02:54.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-9363" for this suite. 03/02/23 14:02:54.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:02:54.255
Mar  2 14:02:54.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 14:02:54.256
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:54.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:54.281
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 14:02:54.303
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 14:02:54.632
STEP: Deploying the webhook pod 03/02/23 14:02:54.642
STEP: Wait for the deployment to be ready 03/02/23 14:02:54.669
Mar  2 14:02:54.699: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 14:02:56.728: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 2, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 2, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 2, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 2, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 14:02:58.732
STEP: Verifying the service has paired with the endpoint 03/02/23 14:02:58.748
Mar  2 14:02:59.748: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 03/02/23 14:02:59.759
STEP: create a pod that should be denied by the webhook 03/02/23 14:02:59.794
STEP: create a pod that causes the webhook to hang 03/02/23 14:02:59.807
STEP: create a configmap that should be denied by the webhook 03/02/23 14:03:09.819
STEP: create a configmap that should be admitted by the webhook 03/02/23 14:03:09.834
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/02/23 14:03:09.845
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/02/23 14:03:09.86
STEP: create a namespace that bypass the webhook 03/02/23 14:03:09.867
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/02/23 14:03:09.877
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 14:03:09.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4798" for this suite. 03/02/23 14:03:09.921
STEP: Destroying namespace "webhook-4798-markers" for this suite. 03/02/23 14:03:09.928
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":281,"skipped":5015,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.737 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:02:54.255
    Mar  2 14:02:54.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 14:02:54.256
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:02:54.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:02:54.281
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 14:02:54.303
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 14:02:54.632
    STEP: Deploying the webhook pod 03/02/23 14:02:54.642
    STEP: Wait for the deployment to be ready 03/02/23 14:02:54.669
    Mar  2 14:02:54.699: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 14:02:56.728: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 2, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 2, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 2, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 2, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 14:02:58.732
    STEP: Verifying the service has paired with the endpoint 03/02/23 14:02:58.748
    Mar  2 14:02:59.748: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 03/02/23 14:02:59.759
    STEP: create a pod that should be denied by the webhook 03/02/23 14:02:59.794
    STEP: create a pod that causes the webhook to hang 03/02/23 14:02:59.807
    STEP: create a configmap that should be denied by the webhook 03/02/23 14:03:09.819
    STEP: create a configmap that should be admitted by the webhook 03/02/23 14:03:09.834
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/02/23 14:03:09.845
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/02/23 14:03:09.86
    STEP: create a namespace that bypass the webhook 03/02/23 14:03:09.867
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/02/23 14:03:09.877
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 14:03:09.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4798" for this suite. 03/02/23 14:03:09.921
    STEP: Destroying namespace "webhook-4798-markers" for this suite. 03/02/23 14:03:09.928
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:03:10.012
Mar  2 14:03:10.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 14:03:10.016
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:03:10.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:03:10.048
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 14:03:10.069
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 14:03:10.644
STEP: Deploying the webhook pod 03/02/23 14:03:10.651
STEP: Wait for the deployment to be ready 03/02/23 14:03:10.667
Mar  2 14:03:10.682: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 14:03:12.692: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 3, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 3, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 3, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 3, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 14:03:14.714
STEP: Verifying the service has paired with the endpoint 03/02/23 14:03:14.735
Mar  2 14:03:15.736: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/02/23 14:03:15.745
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/02/23 14:03:15.769
STEP: Creating a dummy validating-webhook-configuration object 03/02/23 14:03:15.953
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/02/23 14:03:16.026
STEP: Creating a dummy mutating-webhook-configuration object 03/02/23 14:03:16.047
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/02/23 14:03:16.059
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 14:03:16.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6392" for this suite. 03/02/23 14:03:16.143
STEP: Destroying namespace "webhook-6392-markers" for this suite. 03/02/23 14:03:16.15
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":282,"skipped":5067,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.242 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:03:10.012
    Mar  2 14:03:10.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 14:03:10.016
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:03:10.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:03:10.048
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 14:03:10.069
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 14:03:10.644
    STEP: Deploying the webhook pod 03/02/23 14:03:10.651
    STEP: Wait for the deployment to be ready 03/02/23 14:03:10.667
    Mar  2 14:03:10.682: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 14:03:12.692: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 3, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 3, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 3, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 3, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 14:03:14.714
    STEP: Verifying the service has paired with the endpoint 03/02/23 14:03:14.735
    Mar  2 14:03:15.736: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/02/23 14:03:15.745
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/02/23 14:03:15.769
    STEP: Creating a dummy validating-webhook-configuration object 03/02/23 14:03:15.953
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/02/23 14:03:16.026
    STEP: Creating a dummy mutating-webhook-configuration object 03/02/23 14:03:16.047
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/02/23 14:03:16.059
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 14:03:16.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6392" for this suite. 03/02/23 14:03:16.143
    STEP: Destroying namespace "webhook-6392-markers" for this suite. 03/02/23 14:03:16.15
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:03:16.257
Mar  2 14:03:16.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename runtimeclass 03/02/23 14:03:16.258
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:03:16.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:03:16.359
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar  2 14:03:16.453: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1244 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  2 14:03:16.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1244" for this suite. 03/02/23 14:03:16.52
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":283,"skipped":5089,"failed":0}
------------------------------
â€¢ [0.277 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:03:16.257
    Mar  2 14:03:16.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename runtimeclass 03/02/23 14:03:16.258
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:03:16.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:03:16.359
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar  2 14:03:16.453: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1244 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  2 14:03:16.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1244" for this suite. 03/02/23 14:03:16.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:03:16.536
Mar  2 14:03:16.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename svcaccounts 03/02/23 14:03:16.538
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:03:16.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:03:16.597
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  03/02/23 14:03:16.6
Mar  2 14:03:16.647: INFO: Waiting up to 5m0s for pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0" in namespace "svcaccounts-1701" to be "Succeeded or Failed"
Mar  2 14:03:16.655: INFO: Pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.874384ms
Mar  2 14:03:18.675: INFO: Pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028187446s
Mar  2 14:03:20.669: INFO: Pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021351125s
Mar  2 14:03:22.670: INFO: Pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022432052s
STEP: Saw pod success 03/02/23 14:03:22.67
Mar  2 14:03:22.670: INFO: Pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0" satisfied condition "Succeeded or Failed"
Mar  2 14:03:22.715: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 14:03:22.727
Mar  2 14:03:22.751: INFO: Waiting for pod test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0 to disappear
Mar  2 14:03:22.757: INFO: Pod test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 14:03:22.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1701" for this suite. 03/02/23 14:03:22.765
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":284,"skipped":5094,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.281 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:03:16.536
    Mar  2 14:03:16.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 14:03:16.538
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:03:16.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:03:16.597
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  03/02/23 14:03:16.6
    Mar  2 14:03:16.647: INFO: Waiting up to 5m0s for pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0" in namespace "svcaccounts-1701" to be "Succeeded or Failed"
    Mar  2 14:03:16.655: INFO: Pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.874384ms
    Mar  2 14:03:18.675: INFO: Pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028187446s
    Mar  2 14:03:20.669: INFO: Pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021351125s
    Mar  2 14:03:22.670: INFO: Pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022432052s
    STEP: Saw pod success 03/02/23 14:03:22.67
    Mar  2 14:03:22.670: INFO: Pod "test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0" satisfied condition "Succeeded or Failed"
    Mar  2 14:03:22.715: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 14:03:22.727
    Mar  2 14:03:22.751: INFO: Waiting for pod test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0 to disappear
    Mar  2 14:03:22.757: INFO: Pod test-pod-cdb187a5-f697-4bbb-b7a4-df66aaa31cf0 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 14:03:22.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1701" for this suite. 03/02/23 14:03:22.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:03:22.827
Mar  2 14:03:22.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 14:03:22.828
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:03:22.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:03:22.856
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-73668445-c70b-4304-8e28-40b42b0f89f0 03/02/23 14:03:22.86
STEP: Creating a pod to test consume secrets 03/02/23 14:03:22.864
Mar  2 14:03:22.922: INFO: Waiting up to 5m0s for pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5" in namespace "secrets-7956" to be "Succeeded or Failed"
Mar  2 14:03:22.948: INFO: Pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.311498ms
Mar  2 14:03:24.955: INFO: Pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.0219903s
Mar  2 14:03:26.955: INFO: Pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5": Phase="Running", Reason="", readiness=false. Elapsed: 4.021996304s
Mar  2 14:03:28.954: INFO: Pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020448815s
STEP: Saw pod success 03/02/23 14:03:28.954
Mar  2 14:03:28.954: INFO: Pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5" satisfied condition "Succeeded or Failed"
Mar  2 14:03:28.957: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 14:03:28.965
Mar  2 14:03:29.010: INFO: Waiting for pod pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5 to disappear
Mar  2 14:03:29.017: INFO: Pod pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 14:03:29.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7956" for this suite. 03/02/23 14:03:29.026
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":285,"skipped":5120,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.207 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:03:22.827
    Mar  2 14:03:22.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 14:03:22.828
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:03:22.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:03:22.856
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-73668445-c70b-4304-8e28-40b42b0f89f0 03/02/23 14:03:22.86
    STEP: Creating a pod to test consume secrets 03/02/23 14:03:22.864
    Mar  2 14:03:22.922: INFO: Waiting up to 5m0s for pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5" in namespace "secrets-7956" to be "Succeeded or Failed"
    Mar  2 14:03:22.948: INFO: Pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.311498ms
    Mar  2 14:03:24.955: INFO: Pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.0219903s
    Mar  2 14:03:26.955: INFO: Pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5": Phase="Running", Reason="", readiness=false. Elapsed: 4.021996304s
    Mar  2 14:03:28.954: INFO: Pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020448815s
    STEP: Saw pod success 03/02/23 14:03:28.954
    Mar  2 14:03:28.954: INFO: Pod "pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5" satisfied condition "Succeeded or Failed"
    Mar  2 14:03:28.957: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 14:03:28.965
    Mar  2 14:03:29.010: INFO: Waiting for pod pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5 to disappear
    Mar  2 14:03:29.017: INFO: Pod pod-secrets-67735b1b-6647-47a0-a80f-7c5e8e1dd9b5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 14:03:29.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7956" for this suite. 03/02/23 14:03:29.026
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:03:29.035
Mar  2 14:03:29.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 14:03:29.036
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:03:29.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:03:29.063
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-479b616e-2ca9-464f-9bcf-28926a44530d 03/02/23 14:03:29.072
STEP: Creating the pod 03/02/23 14:03:29.082
Mar  2 14:03:29.090: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468" in namespace "projected-5717" to be "running and ready"
Mar  2 14:03:29.100: INFO: Pod "pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468": Phase="Pending", Reason="", readiness=false. Elapsed: 9.725315ms
Mar  2 14:03:29.102: INFO: The phase of Pod pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:03:31.127: INFO: Pod "pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036941696s
Mar  2 14:03:31.127: INFO: The phase of Pod pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:03:33.120: INFO: Pod "pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468": Phase="Running", Reason="", readiness=true. Elapsed: 4.029958366s
Mar  2 14:03:33.125: INFO: The phase of Pod pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468 is Running (Ready = true)
Mar  2 14:03:33.125: INFO: Pod "pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-479b616e-2ca9-464f-9bcf-28926a44530d 03/02/23 14:03:33.138
STEP: waiting to observe update in volume 03/02/23 14:03:33.146
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 14:04:49.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5717" for this suite. 03/02/23 14:04:49.763
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":286,"skipped":5121,"failed":0}
------------------------------
â€¢ [SLOW TEST] [80.737 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:03:29.035
    Mar  2 14:03:29.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 14:03:29.036
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:03:29.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:03:29.063
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-479b616e-2ca9-464f-9bcf-28926a44530d 03/02/23 14:03:29.072
    STEP: Creating the pod 03/02/23 14:03:29.082
    Mar  2 14:03:29.090: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468" in namespace "projected-5717" to be "running and ready"
    Mar  2 14:03:29.100: INFO: Pod "pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468": Phase="Pending", Reason="", readiness=false. Elapsed: 9.725315ms
    Mar  2 14:03:29.102: INFO: The phase of Pod pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:03:31.127: INFO: Pod "pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036941696s
    Mar  2 14:03:31.127: INFO: The phase of Pod pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:03:33.120: INFO: Pod "pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468": Phase="Running", Reason="", readiness=true. Elapsed: 4.029958366s
    Mar  2 14:03:33.125: INFO: The phase of Pod pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468 is Running (Ready = true)
    Mar  2 14:03:33.125: INFO: Pod "pod-projected-configmaps-918cfa0a-23e5-4858-a6be-f756e4242468" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-479b616e-2ca9-464f-9bcf-28926a44530d 03/02/23 14:03:33.138
    STEP: waiting to observe update in volume 03/02/23 14:03:33.146
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 14:04:49.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5717" for this suite. 03/02/23 14:04:49.763
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:04:49.777
Mar  2 14:04:49.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 14:04:49.782
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:04:49.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:04:49.825
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 03/02/23 14:04:49.831
Mar  2 14:04:49.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce" in namespace "projected-1434" to be "Succeeded or Failed"
Mar  2 14:04:49.872: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce": Phase="Pending", Reason="", readiness=false. Elapsed: 26.315282ms
Mar  2 14:04:51.888: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041820673s
Mar  2 14:04:53.882: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce": Phase="Running", Reason="", readiness=false. Elapsed: 4.036224422s
Mar  2 14:04:55.891: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce": Phase="Running", Reason="", readiness=false. Elapsed: 6.045189895s
Mar  2 14:04:57.879: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.032997882s
STEP: Saw pod success 03/02/23 14:04:57.879
Mar  2 14:04:57.880: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce" satisfied condition "Succeeded or Failed"
Mar  2 14:04:57.884: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce container client-container: <nil>
STEP: delete the pod 03/02/23 14:04:57.89
Mar  2 14:04:57.915: INFO: Waiting for pod downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce to disappear
Mar  2 14:04:57.918: INFO: Pod downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 14:04:57.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1434" for this suite. 03/02/23 14:04:57.924
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":287,"skipped":5124,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.157 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:04:49.777
    Mar  2 14:04:49.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 14:04:49.782
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:04:49.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:04:49.825
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 03/02/23 14:04:49.831
    Mar  2 14:04:49.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce" in namespace "projected-1434" to be "Succeeded or Failed"
    Mar  2 14:04:49.872: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce": Phase="Pending", Reason="", readiness=false. Elapsed: 26.315282ms
    Mar  2 14:04:51.888: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041820673s
    Mar  2 14:04:53.882: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce": Phase="Running", Reason="", readiness=false. Elapsed: 4.036224422s
    Mar  2 14:04:55.891: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce": Phase="Running", Reason="", readiness=false. Elapsed: 6.045189895s
    Mar  2 14:04:57.879: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.032997882s
    STEP: Saw pod success 03/02/23 14:04:57.879
    Mar  2 14:04:57.880: INFO: Pod "downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce" satisfied condition "Succeeded or Failed"
    Mar  2 14:04:57.884: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce container client-container: <nil>
    STEP: delete the pod 03/02/23 14:04:57.89
    Mar  2 14:04:57.915: INFO: Waiting for pod downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce to disappear
    Mar  2 14:04:57.918: INFO: Pod downwardapi-volume-4dd14c27-c770-426b-abb1-b17147e847ce no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 14:04:57.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1434" for this suite. 03/02/23 14:04:57.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:04:57.948
Mar  2 14:04:57.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename disruption 03/02/23 14:04:57.95
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:04:57.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:04:57.982
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:04:57.987
Mar  2 14:04:57.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename disruption-2 03/02/23 14:04:57.989
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:04:58.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:04:58.016
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 03/02/23 14:04:58.027
STEP: Waiting for the pdb to be processed 03/02/23 14:05:00.044
STEP: Waiting for the pdb to be processed 03/02/23 14:05:00.062
STEP: listing a collection of PDBs across all namespaces 03/02/23 14:05:00.07
STEP: listing a collection of PDBs in namespace disruption-6767 03/02/23 14:05:00.076
STEP: deleting a collection of PDBs 03/02/23 14:05:00.081
STEP: Waiting for the PDB collection to be deleted 03/02/23 14:05:00.095
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Mar  2 14:05:00.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-2895" for this suite. 03/02/23 14:05:00.114
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  2 14:05:00.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6767" for this suite. 03/02/23 14:05:00.171
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":288,"skipped":5156,"failed":0}
------------------------------
â€¢ [2.271 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:04:57.948
    Mar  2 14:04:57.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename disruption 03/02/23 14:04:57.95
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:04:57.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:04:57.982
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:04:57.987
    Mar  2 14:04:57.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename disruption-2 03/02/23 14:04:57.989
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:04:58.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:04:58.016
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 03/02/23 14:04:58.027
    STEP: Waiting for the pdb to be processed 03/02/23 14:05:00.044
    STEP: Waiting for the pdb to be processed 03/02/23 14:05:00.062
    STEP: listing a collection of PDBs across all namespaces 03/02/23 14:05:00.07
    STEP: listing a collection of PDBs in namespace disruption-6767 03/02/23 14:05:00.076
    STEP: deleting a collection of PDBs 03/02/23 14:05:00.081
    STEP: Waiting for the PDB collection to be deleted 03/02/23 14:05:00.095
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Mar  2 14:05:00.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-2895" for this suite. 03/02/23 14:05:00.114
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  2 14:05:00.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6767" for this suite. 03/02/23 14:05:00.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:05:00.235
Mar  2 14:05:00.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename var-expansion 03/02/23 14:05:00.237
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:05:00.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:05:00.299
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 03/02/23 14:05:00.306
Mar  2 14:05:00.330: INFO: Waiting up to 5m0s for pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2" in namespace "var-expansion-2963" to be "Succeeded or Failed"
Mar  2 14:05:00.336: INFO: Pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.798865ms
Mar  2 14:05:02.344: INFO: Pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013727676s
Mar  2 14:05:04.346: INFO: Pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015651681s
Mar  2 14:05:06.345: INFO: Pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014467903s
STEP: Saw pod success 03/02/23 14:05:06.345
Mar  2 14:05:06.345: INFO: Pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2" satisfied condition "Succeeded or Failed"
Mar  2 14:05:06.351: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2 container dapi-container: <nil>
STEP: delete the pod 03/02/23 14:05:06.36
Mar  2 14:05:06.389: INFO: Waiting for pod var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2 to disappear
Mar  2 14:05:06.397: INFO: Pod var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 14:05:06.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2963" for this suite. 03/02/23 14:05:06.407
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":289,"skipped":5188,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.187 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:05:00.235
    Mar  2 14:05:00.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename var-expansion 03/02/23 14:05:00.237
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:05:00.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:05:00.299
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 03/02/23 14:05:00.306
    Mar  2 14:05:00.330: INFO: Waiting up to 5m0s for pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2" in namespace "var-expansion-2963" to be "Succeeded or Failed"
    Mar  2 14:05:00.336: INFO: Pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.798865ms
    Mar  2 14:05:02.344: INFO: Pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013727676s
    Mar  2 14:05:04.346: INFO: Pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015651681s
    Mar  2 14:05:06.345: INFO: Pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014467903s
    STEP: Saw pod success 03/02/23 14:05:06.345
    Mar  2 14:05:06.345: INFO: Pod "var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2" satisfied condition "Succeeded or Failed"
    Mar  2 14:05:06.351: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2 container dapi-container: <nil>
    STEP: delete the pod 03/02/23 14:05:06.36
    Mar  2 14:05:06.389: INFO: Waiting for pod var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2 to disappear
    Mar  2 14:05:06.397: INFO: Pod var-expansion-9465b769-0330-42c9-933a-75c26f1e36f2 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 14:05:06.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2963" for this suite. 03/02/23 14:05:06.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:05:06.424
Mar  2 14:05:06.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 14:05:06.429
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:05:06.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:05:06.457
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 03/02/23 14:05:06.467
Mar  2 14:05:06.477: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3" in namespace "downward-api-1871" to be "Succeeded or Failed"
Mar  2 14:05:06.482: INFO: Pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.10953ms
Mar  2 14:05:08.497: INFO: Pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.020269365s
Mar  2 14:05:10.530: INFO: Pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3": Phase="Running", Reason="", readiness=false. Elapsed: 4.053296476s
Mar  2 14:05:12.497: INFO: Pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020615824s
STEP: Saw pod success 03/02/23 14:05:12.497
Mar  2 14:05:12.498: INFO: Pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3" satisfied condition "Succeeded or Failed"
Mar  2 14:05:12.526: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3 container client-container: <nil>
STEP: delete the pod 03/02/23 14:05:12.54
Mar  2 14:05:12.634: INFO: Waiting for pod downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3 to disappear
Mar  2 14:05:12.640: INFO: Pod downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 14:05:12.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1871" for this suite. 03/02/23 14:05:12.65
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":290,"skipped":5194,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.245 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:05:06.424
    Mar  2 14:05:06.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 14:05:06.429
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:05:06.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:05:06.457
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 03/02/23 14:05:06.467
    Mar  2 14:05:06.477: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3" in namespace "downward-api-1871" to be "Succeeded or Failed"
    Mar  2 14:05:06.482: INFO: Pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.10953ms
    Mar  2 14:05:08.497: INFO: Pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.020269365s
    Mar  2 14:05:10.530: INFO: Pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3": Phase="Running", Reason="", readiness=false. Elapsed: 4.053296476s
    Mar  2 14:05:12.497: INFO: Pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020615824s
    STEP: Saw pod success 03/02/23 14:05:12.497
    Mar  2 14:05:12.498: INFO: Pod "downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3" satisfied condition "Succeeded or Failed"
    Mar  2 14:05:12.526: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3 container client-container: <nil>
    STEP: delete the pod 03/02/23 14:05:12.54
    Mar  2 14:05:12.634: INFO: Waiting for pod downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3 to disappear
    Mar  2 14:05:12.640: INFO: Pod downwardapi-volume-d413727b-d43d-478a-8034-105e064620b3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 14:05:12.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1871" for this suite. 03/02/23 14:05:12.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:05:12.67
Mar  2 14:05:12.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pod-network-test 03/02/23 14:05:12.671
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:05:12.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:05:12.757
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-5070 03/02/23 14:05:12.855
STEP: creating a selector 03/02/23 14:05:12.855
STEP: Creating the service pods in kubernetes 03/02/23 14:05:12.855
Mar  2 14:05:12.855: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 14:05:13.056: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5070" to be "running and ready"
Mar  2 14:05:13.064: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.187217ms
Mar  2 14:05:13.064: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:05:15.069: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013532441s
Mar  2 14:05:15.070: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:05:17.114: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058380107s
Mar  2 14:05:17.114: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:05:19.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.045987682s
Mar  2 14:05:19.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 14:05:21.071: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014810789s
Mar  2 14:05:21.071: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 14:05:23.070: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013945057s
Mar  2 14:05:23.070: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 14:05:25.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019436714s
Mar  2 14:05:25.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 14:05:27.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017911569s
Mar  2 14:05:27.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 14:05:29.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.018005868s
Mar  2 14:05:29.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 14:05:31.072: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015778981s
Mar  2 14:05:31.072: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 14:05:33.070: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013888409s
Mar  2 14:05:33.070: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 14:05:35.146: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.089887649s
Mar  2 14:05:35.146: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  2 14:05:35.146: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  2 14:05:35.165: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5070" to be "running and ready"
Mar  2 14:05:35.174: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.199642ms
Mar  2 14:05:35.174: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  2 14:05:35.174: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  2 14:05:35.176: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5070" to be "running and ready"
Mar  2 14:05:35.179: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.382603ms
Mar  2 14:05:35.179: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  2 14:05:35.179: INFO: Pod "netserver-2" satisfied condition "running and ready"
Mar  2 14:05:35.182: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5070" to be "running and ready"
Mar  2 14:05:35.184: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.41981ms
Mar  2 14:05:35.184: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Mar  2 14:05:35.184: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 03/02/23 14:05:35.188
Mar  2 14:05:35.196: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5070" to be "running"
Mar  2 14:05:35.201: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.350354ms
Mar  2 14:05:37.225: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028941523s
Mar  2 14:05:37.225: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  2 14:05:37.242: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Mar  2 14:05:37.243: INFO: Breadth first check of 10.233.123.53 on host 172.16.0.61...
Mar  2 14:05:37.254: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.70:9080/dial?request=hostname&protocol=udp&host=10.233.123.53&port=8081&tries=1'] Namespace:pod-network-test-5070 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:05:37.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 14:05:37.255: INFO: ExecWithOptions: Clientset creation
Mar  2 14:05:37.256: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5070/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.70%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.123.53%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 14:05:37.443: INFO: Waiting for responses: map[]
Mar  2 14:05:37.444: INFO: reached 10.233.123.53 after 0/1 tries
Mar  2 14:05:37.444: INFO: Breadth first check of 10.233.92.83 on host 172.16.0.138...
Mar  2 14:05:37.449: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.70:9080/dial?request=hostname&protocol=udp&host=10.233.92.83&port=8081&tries=1'] Namespace:pod-network-test-5070 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:05:37.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 14:05:37.450: INFO: ExecWithOptions: Clientset creation
Mar  2 14:05:37.450: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5070/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.70%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.92.83%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 14:05:37.636: INFO: Waiting for responses: map[]
Mar  2 14:05:37.637: INFO: reached 10.233.92.83 after 0/1 tries
Mar  2 14:05:37.637: INFO: Breadth first check of 10.233.123.102 on host 172.16.0.192...
Mar  2 14:05:37.642: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.70:9080/dial?request=hostname&protocol=udp&host=10.233.123.102&port=8081&tries=1'] Namespace:pod-network-test-5070 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:05:37.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 14:05:37.643: INFO: ExecWithOptions: Clientset creation
Mar  2 14:05:37.643: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5070/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.70%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.123.102%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 14:05:37.846: INFO: Waiting for responses: map[]
Mar  2 14:05:37.846: INFO: reached 10.233.123.102 after 0/1 tries
Mar  2 14:05:37.846: INFO: Breadth first check of 10.233.126.91 on host 172.16.0.56...
Mar  2 14:05:37.858: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.70:9080/dial?request=hostname&protocol=udp&host=10.233.126.91&port=8081&tries=1'] Namespace:pod-network-test-5070 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:05:37.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 14:05:37.859: INFO: ExecWithOptions: Clientset creation
Mar  2 14:05:37.859: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5070/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.70%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.126.91%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 14:05:38.040: INFO: Waiting for responses: map[]
Mar  2 14:05:38.040: INFO: reached 10.233.126.91 after 0/1 tries
Mar  2 14:05:38.040: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  2 14:05:38.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5070" for this suite. 03/02/23 14:05:38.055
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":291,"skipped":5209,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.398 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:05:12.67
    Mar  2 14:05:12.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pod-network-test 03/02/23 14:05:12.671
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:05:12.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:05:12.757
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-5070 03/02/23 14:05:12.855
    STEP: creating a selector 03/02/23 14:05:12.855
    STEP: Creating the service pods in kubernetes 03/02/23 14:05:12.855
    Mar  2 14:05:12.855: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  2 14:05:13.056: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5070" to be "running and ready"
    Mar  2 14:05:13.064: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.187217ms
    Mar  2 14:05:13.064: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:05:15.069: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013532441s
    Mar  2 14:05:15.070: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:05:17.114: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058380107s
    Mar  2 14:05:17.114: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:05:19.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.045987682s
    Mar  2 14:05:19.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 14:05:21.071: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014810789s
    Mar  2 14:05:21.071: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 14:05:23.070: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013945057s
    Mar  2 14:05:23.070: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 14:05:25.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019436714s
    Mar  2 14:05:25.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 14:05:27.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017911569s
    Mar  2 14:05:27.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 14:05:29.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.018005868s
    Mar  2 14:05:29.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 14:05:31.072: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015778981s
    Mar  2 14:05:31.072: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 14:05:33.070: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013888409s
    Mar  2 14:05:33.070: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 14:05:35.146: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.089887649s
    Mar  2 14:05:35.146: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  2 14:05:35.146: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  2 14:05:35.165: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5070" to be "running and ready"
    Mar  2 14:05:35.174: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.199642ms
    Mar  2 14:05:35.174: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  2 14:05:35.174: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  2 14:05:35.176: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5070" to be "running and ready"
    Mar  2 14:05:35.179: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.382603ms
    Mar  2 14:05:35.179: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  2 14:05:35.179: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Mar  2 14:05:35.182: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5070" to be "running and ready"
    Mar  2 14:05:35.184: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.41981ms
    Mar  2 14:05:35.184: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Mar  2 14:05:35.184: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 03/02/23 14:05:35.188
    Mar  2 14:05:35.196: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5070" to be "running"
    Mar  2 14:05:35.201: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.350354ms
    Mar  2 14:05:37.225: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028941523s
    Mar  2 14:05:37.225: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  2 14:05:37.242: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Mar  2 14:05:37.243: INFO: Breadth first check of 10.233.123.53 on host 172.16.0.61...
    Mar  2 14:05:37.254: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.70:9080/dial?request=hostname&protocol=udp&host=10.233.123.53&port=8081&tries=1'] Namespace:pod-network-test-5070 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 14:05:37.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 14:05:37.255: INFO: ExecWithOptions: Clientset creation
    Mar  2 14:05:37.256: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5070/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.70%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.123.53%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 14:05:37.443: INFO: Waiting for responses: map[]
    Mar  2 14:05:37.444: INFO: reached 10.233.123.53 after 0/1 tries
    Mar  2 14:05:37.444: INFO: Breadth first check of 10.233.92.83 on host 172.16.0.138...
    Mar  2 14:05:37.449: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.70:9080/dial?request=hostname&protocol=udp&host=10.233.92.83&port=8081&tries=1'] Namespace:pod-network-test-5070 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 14:05:37.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 14:05:37.450: INFO: ExecWithOptions: Clientset creation
    Mar  2 14:05:37.450: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5070/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.70%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.92.83%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 14:05:37.636: INFO: Waiting for responses: map[]
    Mar  2 14:05:37.637: INFO: reached 10.233.92.83 after 0/1 tries
    Mar  2 14:05:37.637: INFO: Breadth first check of 10.233.123.102 on host 172.16.0.192...
    Mar  2 14:05:37.642: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.70:9080/dial?request=hostname&protocol=udp&host=10.233.123.102&port=8081&tries=1'] Namespace:pod-network-test-5070 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 14:05:37.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 14:05:37.643: INFO: ExecWithOptions: Clientset creation
    Mar  2 14:05:37.643: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5070/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.70%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.123.102%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 14:05:37.846: INFO: Waiting for responses: map[]
    Mar  2 14:05:37.846: INFO: reached 10.233.123.102 after 0/1 tries
    Mar  2 14:05:37.846: INFO: Breadth first check of 10.233.126.91 on host 172.16.0.56...
    Mar  2 14:05:37.858: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.123.70:9080/dial?request=hostname&protocol=udp&host=10.233.126.91&port=8081&tries=1'] Namespace:pod-network-test-5070 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 14:05:37.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 14:05:37.859: INFO: ExecWithOptions: Clientset creation
    Mar  2 14:05:37.859: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5070/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.123.70%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.126.91%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 14:05:38.040: INFO: Waiting for responses: map[]
    Mar  2 14:05:38.040: INFO: reached 10.233.126.91 after 0/1 tries
    Mar  2 14:05:38.040: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  2 14:05:38.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5070" for this suite. 03/02/23 14:05:38.055
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:05:38.07
Mar  2 14:05:38.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 14:05:38.072
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:05:38.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:05:38.147
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Mar  2 14:05:38.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 14:05:50.816
Mar  2 14:05:50.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9354 --namespace=crd-publish-openapi-9354 create -f -'
Mar  2 14:05:52.272: INFO: stderr: ""
Mar  2 14:05:52.272: INFO: stdout: "e2e-test-crd-publish-openapi-6751-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 14:05:52.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9354 --namespace=crd-publish-openapi-9354 delete e2e-test-crd-publish-openapi-6751-crds test-cr'
Mar  2 14:05:52.373: INFO: stderr: ""
Mar  2 14:05:52.373: INFO: stdout: "e2e-test-crd-publish-openapi-6751-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  2 14:05:52.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9354 --namespace=crd-publish-openapi-9354 apply -f -'
Mar  2 14:05:52.748: INFO: stderr: ""
Mar  2 14:05:52.748: INFO: stdout: "e2e-test-crd-publish-openapi-6751-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 14:05:52.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9354 --namespace=crd-publish-openapi-9354 delete e2e-test-crd-publish-openapi-6751-crds test-cr'
Mar  2 14:05:52.916: INFO: stderr: ""
Mar  2 14:05:52.916: INFO: stdout: "e2e-test-crd-publish-openapi-6751-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/02/23 14:05:52.916
Mar  2 14:05:52.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9354 explain e2e-test-crd-publish-openapi-6751-crds'
Mar  2 14:05:53.311: INFO: stderr: ""
Mar  2 14:05:53.311: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6751-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 14:05:58.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9354" for this suite. 03/02/23 14:05:58.449
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":292,"skipped":5212,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.387 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:05:38.07
    Mar  2 14:05:38.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 14:05:38.072
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:05:38.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:05:38.147
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Mar  2 14:05:38.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 14:05:50.816
    Mar  2 14:05:50.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9354 --namespace=crd-publish-openapi-9354 create -f -'
    Mar  2 14:05:52.272: INFO: stderr: ""
    Mar  2 14:05:52.272: INFO: stdout: "e2e-test-crd-publish-openapi-6751-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar  2 14:05:52.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9354 --namespace=crd-publish-openapi-9354 delete e2e-test-crd-publish-openapi-6751-crds test-cr'
    Mar  2 14:05:52.373: INFO: stderr: ""
    Mar  2 14:05:52.373: INFO: stdout: "e2e-test-crd-publish-openapi-6751-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar  2 14:05:52.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9354 --namespace=crd-publish-openapi-9354 apply -f -'
    Mar  2 14:05:52.748: INFO: stderr: ""
    Mar  2 14:05:52.748: INFO: stdout: "e2e-test-crd-publish-openapi-6751-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar  2 14:05:52.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9354 --namespace=crd-publish-openapi-9354 delete e2e-test-crd-publish-openapi-6751-crds test-cr'
    Mar  2 14:05:52.916: INFO: stderr: ""
    Mar  2 14:05:52.916: INFO: stdout: "e2e-test-crd-publish-openapi-6751-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/02/23 14:05:52.916
    Mar  2 14:05:52.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-9354 explain e2e-test-crd-publish-openapi-6751-crds'
    Mar  2 14:05:53.311: INFO: stderr: ""
    Mar  2 14:05:53.311: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6751-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 14:05:58.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9354" for this suite. 03/02/23 14:05:58.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:05:58.465
Mar  2 14:05:58.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename dns 03/02/23 14:05:58.466
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:05:58.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:05:58.502
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/02/23 14:05:58.506
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7949 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7949;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7949 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7949;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7949.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7949.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7949.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7949.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7949.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7949.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7949.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7949.svc;check="$$(dig +notcp +noall +answer +search 89.34.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.34.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.34.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.34.89_tcp@PTR;sleep 1; done
 03/02/23 14:05:58.549
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7949 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7949;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7949 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7949;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7949.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7949.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7949.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7949.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7949.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7949.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7949.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7949.svc;check="$$(dig +notcp +noall +answer +search 89.34.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.34.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.34.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.34.89_tcp@PTR;sleep 1; done
 03/02/23 14:05:58.55
STEP: creating a pod to probe DNS 03/02/23 14:05:58.55
STEP: submitting the pod to kubernetes 03/02/23 14:05:58.552
Mar  2 14:05:58.567: INFO: Waiting up to 15m0s for pod "dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f" in namespace "dns-7949" to be "running"
Mar  2 14:05:58.596: INFO: Pod "dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.264964ms
Mar  2 14:06:00.610: INFO: Pod "dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042578726s
Mar  2 14:06:02.620: INFO: Pod "dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f": Phase="Running", Reason="", readiness=true. Elapsed: 4.05209921s
Mar  2 14:06:02.620: INFO: Pod "dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f" satisfied condition "running"
STEP: retrieving the pod 03/02/23 14:06:02.621
STEP: looking for the results for each expected name from probers 03/02/23 14:06:02.631
Mar  2 14:06:02.650: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.678: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.687: INFO: Unable to read wheezy_udp@dns-test-service.dns-7949 from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.716: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7949 from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.742: INFO: Unable to read wheezy_udp@dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.802: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.819: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.861: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.866: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.926: INFO: Unable to read jessie_udp@dns-test-service.dns-7949 from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.936: INFO: Unable to read jessie_tcp@dns-test-service.dns-7949 from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.942: INFO: Unable to read jessie_udp@dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.947: INFO: Unable to read jessie_tcp@dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.950: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:02.955: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
Mar  2 14:06:03.036: INFO: Lookups using dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7949 wheezy_tcp@dns-test-service.dns-7949 wheezy_udp@dns-test-service.dns-7949.svc wheezy_tcp@dns-test-service.dns-7949.svc wheezy_udp@_http._tcp.dns-test-service.dns-7949.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7949.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7949 jessie_tcp@dns-test-service.dns-7949 jessie_udp@dns-test-service.dns-7949.svc jessie_tcp@dns-test-service.dns-7949.svc jessie_udp@_http._tcp.dns-test-service.dns-7949.svc jessie_tcp@_http._tcp.dns-test-service.dns-7949.svc]

Mar  2 14:06:08.852: INFO: DNS probes using dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f succeeded

STEP: deleting the pod 03/02/23 14:06:08.853
STEP: deleting the test service 03/02/23 14:06:09.054
STEP: deleting the test headless service 03/02/23 14:06:09.091
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 14:06:09.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7949" for this suite. 03/02/23 14:06:09.163
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":293,"skipped":5227,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.715 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:05:58.465
    Mar  2 14:05:58.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename dns 03/02/23 14:05:58.466
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:05:58.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:05:58.502
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/02/23 14:05:58.506
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7949 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7949;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7949 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7949;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7949.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7949.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7949.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7949.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7949.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7949.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7949.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7949.svc;check="$$(dig +notcp +noall +answer +search 89.34.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.34.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.34.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.34.89_tcp@PTR;sleep 1; done
     03/02/23 14:05:58.549
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7949 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7949;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7949 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7949;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7949.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7949.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7949.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7949.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7949.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7949.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7949.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7949.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7949.svc;check="$$(dig +notcp +noall +answer +search 89.34.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.34.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.34.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.34.89_tcp@PTR;sleep 1; done
     03/02/23 14:05:58.55
    STEP: creating a pod to probe DNS 03/02/23 14:05:58.55
    STEP: submitting the pod to kubernetes 03/02/23 14:05:58.552
    Mar  2 14:05:58.567: INFO: Waiting up to 15m0s for pod "dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f" in namespace "dns-7949" to be "running"
    Mar  2 14:05:58.596: INFO: Pod "dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.264964ms
    Mar  2 14:06:00.610: INFO: Pod "dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042578726s
    Mar  2 14:06:02.620: INFO: Pod "dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f": Phase="Running", Reason="", readiness=true. Elapsed: 4.05209921s
    Mar  2 14:06:02.620: INFO: Pod "dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 14:06:02.621
    STEP: looking for the results for each expected name from probers 03/02/23 14:06:02.631
    Mar  2 14:06:02.650: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.678: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.687: INFO: Unable to read wheezy_udp@dns-test-service.dns-7949 from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.716: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7949 from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.742: INFO: Unable to read wheezy_udp@dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.802: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.819: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.861: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.866: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.926: INFO: Unable to read jessie_udp@dns-test-service.dns-7949 from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.936: INFO: Unable to read jessie_tcp@dns-test-service.dns-7949 from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.942: INFO: Unable to read jessie_udp@dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.947: INFO: Unable to read jessie_tcp@dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.950: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:02.955: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7949.svc from pod dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f: the server could not find the requested resource (get pods dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f)
    Mar  2 14:06:03.036: INFO: Lookups using dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7949 wheezy_tcp@dns-test-service.dns-7949 wheezy_udp@dns-test-service.dns-7949.svc wheezy_tcp@dns-test-service.dns-7949.svc wheezy_udp@_http._tcp.dns-test-service.dns-7949.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7949.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7949 jessie_tcp@dns-test-service.dns-7949 jessie_udp@dns-test-service.dns-7949.svc jessie_tcp@dns-test-service.dns-7949.svc jessie_udp@_http._tcp.dns-test-service.dns-7949.svc jessie_tcp@_http._tcp.dns-test-service.dns-7949.svc]

    Mar  2 14:06:08.852: INFO: DNS probes using dns-7949/dns-test-2a783258-70a5-4c8b-9b5b-b41c8c10202f succeeded

    STEP: deleting the pod 03/02/23 14:06:08.853
    STEP: deleting the test service 03/02/23 14:06:09.054
    STEP: deleting the test headless service 03/02/23 14:06:09.091
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 14:06:09.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7949" for this suite. 03/02/23 14:06:09.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:06:09.209
Mar  2 14:06:09.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sched-preemption 03/02/23 14:06:09.211
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:06:09.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:06:09.241
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 14:06:09.261: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 14:07:09.498: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 03/02/23 14:07:09.501
Mar  2 14:07:09.543: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  2 14:07:09.561: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  2 14:07:09.610: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  2 14:07:09.634: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar  2 14:07:09.689: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar  2 14:07:09.704: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Mar  2 14:07:09.755: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Mar  2 14:07:09.777: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/02/23 14:07:09.778
Mar  2 14:07:09.778: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1160" to be "running"
Mar  2 14:07:09.785: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.754078ms
Mar  2 14:07:11.797: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018497009s
Mar  2 14:07:13.822: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043324123s
Mar  2 14:07:15.827: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048781881s
Mar  2 14:07:17.792: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01330595s
Mar  2 14:07:19.806: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.027639292s
Mar  2 14:07:19.806: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar  2 14:07:19.806: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
Mar  2 14:07:19.810: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.490348ms
Mar  2 14:07:19.810: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 14:07:19.810: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
Mar  2 14:07:19.813: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.059031ms
Mar  2 14:07:21.820: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010474925s
Mar  2 14:07:21.820: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 14:07:21.820: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
Mar  2 14:07:21.826: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.483813ms
Mar  2 14:07:21.826: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 14:07:21.826: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
Mar  2 14:07:21.830: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.97746ms
Mar  2 14:07:21.830: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 14:07:21.831: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
Mar  2 14:07:21.834: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.112631ms
Mar  2 14:07:21.834: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 14:07:21.834: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
Mar  2 14:07:21.837: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.576834ms
Mar  2 14:07:21.837: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 14:07:21.837: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
Mar  2 14:07:21.840: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.672349ms
Mar  2 14:07:21.840: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/02/23 14:07:21.84
Mar  2 14:07:21.858: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar  2 14:07:21.868: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.694711ms
Mar  2 14:07:23.873: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014852904s
Mar  2 14:07:25.916: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058019314s
Mar  2 14:07:27.873: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.014964715s
Mar  2 14:07:27.874: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  2 14:07:27.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1160" for this suite. 03/02/23 14:07:27.927
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":294,"skipped":5257,"failed":0}
------------------------------
â€¢ [SLOW TEST] [78.792 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:06:09.209
    Mar  2 14:06:09.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sched-preemption 03/02/23 14:06:09.211
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:06:09.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:06:09.241
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  2 14:06:09.261: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 14:07:09.498: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 03/02/23 14:07:09.501
    Mar  2 14:07:09.543: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar  2 14:07:09.561: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar  2 14:07:09.610: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar  2 14:07:09.634: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar  2 14:07:09.689: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar  2 14:07:09.704: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Mar  2 14:07:09.755: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Mar  2 14:07:09.777: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/02/23 14:07:09.778
    Mar  2 14:07:09.778: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1160" to be "running"
    Mar  2 14:07:09.785: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.754078ms
    Mar  2 14:07:11.797: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018497009s
    Mar  2 14:07:13.822: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043324123s
    Mar  2 14:07:15.827: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048781881s
    Mar  2 14:07:17.792: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01330595s
    Mar  2 14:07:19.806: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.027639292s
    Mar  2 14:07:19.806: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar  2 14:07:19.806: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
    Mar  2 14:07:19.810: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.490348ms
    Mar  2 14:07:19.810: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 14:07:19.810: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
    Mar  2 14:07:19.813: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.059031ms
    Mar  2 14:07:21.820: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010474925s
    Mar  2 14:07:21.820: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 14:07:21.820: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
    Mar  2 14:07:21.826: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.483813ms
    Mar  2 14:07:21.826: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 14:07:21.826: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
    Mar  2 14:07:21.830: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.97746ms
    Mar  2 14:07:21.830: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 14:07:21.831: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
    Mar  2 14:07:21.834: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.112631ms
    Mar  2 14:07:21.834: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 14:07:21.834: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
    Mar  2 14:07:21.837: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.576834ms
    Mar  2 14:07:21.837: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 14:07:21.837: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1160" to be "running"
    Mar  2 14:07:21.840: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.672349ms
    Mar  2 14:07:21.840: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/02/23 14:07:21.84
    Mar  2 14:07:21.858: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar  2 14:07:21.868: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.694711ms
    Mar  2 14:07:23.873: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014852904s
    Mar  2 14:07:25.916: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058019314s
    Mar  2 14:07:27.873: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.014964715s
    Mar  2 14:07:27.874: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 14:07:27.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-1160" for this suite. 03/02/23 14:07:27.927
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:07:28.003
Mar  2 14:07:28.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 14:07:28.006
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:07:28.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:07:28.03
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-7810f55d-bc1c-4987-9df6-4e2a2930c633 03/02/23 14:07:28.034
STEP: Creating a pod to test consume configMaps 03/02/23 14:07:28.042
Mar  2 14:07:28.054: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef" in namespace "projected-9618" to be "Succeeded or Failed"
Mar  2 14:07:28.072: INFO: Pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef": Phase="Pending", Reason="", readiness=false. Elapsed: 18.324287ms
Mar  2 14:07:30.082: INFO: Pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027753184s
Mar  2 14:07:32.138: INFO: Pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083792859s
Mar  2 14:07:34.127: INFO: Pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.072923562s
STEP: Saw pod success 03/02/23 14:07:34.127
Mar  2 14:07:34.128: INFO: Pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef" satisfied condition "Succeeded or Failed"
Mar  2 14:07:34.141: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef container agnhost-container: <nil>
STEP: delete the pod 03/02/23 14:07:34.151
Mar  2 14:07:34.201: INFO: Waiting for pod pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef to disappear
Mar  2 14:07:34.204: INFO: Pod pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 14:07:34.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9618" for this suite. 03/02/23 14:07:34.22
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":295,"skipped":5259,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.251 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:07:28.003
    Mar  2 14:07:28.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 14:07:28.006
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:07:28.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:07:28.03
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-7810f55d-bc1c-4987-9df6-4e2a2930c633 03/02/23 14:07:28.034
    STEP: Creating a pod to test consume configMaps 03/02/23 14:07:28.042
    Mar  2 14:07:28.054: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef" in namespace "projected-9618" to be "Succeeded or Failed"
    Mar  2 14:07:28.072: INFO: Pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef": Phase="Pending", Reason="", readiness=false. Elapsed: 18.324287ms
    Mar  2 14:07:30.082: INFO: Pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027753184s
    Mar  2 14:07:32.138: INFO: Pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083792859s
    Mar  2 14:07:34.127: INFO: Pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.072923562s
    STEP: Saw pod success 03/02/23 14:07:34.127
    Mar  2 14:07:34.128: INFO: Pod "pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef" satisfied condition "Succeeded or Failed"
    Mar  2 14:07:34.141: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 14:07:34.151
    Mar  2 14:07:34.201: INFO: Waiting for pod pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef to disappear
    Mar  2 14:07:34.204: INFO: Pod pod-projected-configmaps-f0877ac5-71ac-4f51-adb6-7620cf9d2eef no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 14:07:34.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9618" for this suite. 03/02/23 14:07:34.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:07:34.255
Mar  2 14:07:34.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename daemonsets 03/02/23 14:07:34.257
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:07:34.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:07:34.329
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 03/02/23 14:07:34.39
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 14:07:34.408
Mar  2 14:07:34.433: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:07:34.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:07:34.444: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 14:07:35.452: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:07:35.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:07:35.475: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 14:07:36.460: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:07:36.470: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 14:07:36.470: INFO: Node aarnq-sc-k8s-node-srv2 is running 0 daemon pod, expected 1
Mar  2 14:07:37.450: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:07:37.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Mar  2 14:07:37.455: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/02/23 14:07:37.459
Mar  2 14:07:37.490: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:07:37.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 14:07:37.494: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
Mar  2 14:07:38.505: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:07:38.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 14:07:38.513: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
Mar  2 14:07:39.517: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:07:39.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 14:07:39.523: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
Mar  2 14:07:40.507: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:07:40.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 14:07:40.513: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
Mar  2 14:07:41.507: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:07:41.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Mar  2 14:07:41.514: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 14:07:41.518
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1781, will wait for the garbage collector to delete the pods 03/02/23 14:07:41.518
Mar  2 14:07:41.598: INFO: Deleting DaemonSet.extensions daemon-set took: 26.477492ms
Mar  2 14:07:41.705: INFO: Terminating DaemonSet.extensions daemon-set pods took: 107.723846ms
Mar  2 14:07:44.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:07:44.309: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 14:07:44.312: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1964604"},"items":null}

Mar  2 14:07:44.314: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1964604"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 14:07:44.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1781" for this suite. 03/02/23 14:07:44.352
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":296,"skipped":5270,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.104 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:07:34.255
    Mar  2 14:07:34.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename daemonsets 03/02/23 14:07:34.257
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:07:34.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:07:34.329
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 03/02/23 14:07:34.39
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 14:07:34.408
    Mar  2 14:07:34.433: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:07:34.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:07:34.444: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 14:07:35.452: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:07:35.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:07:35.475: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 14:07:36.460: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:07:36.470: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 14:07:36.470: INFO: Node aarnq-sc-k8s-node-srv2 is running 0 daemon pod, expected 1
    Mar  2 14:07:37.450: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:07:37.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Mar  2 14:07:37.455: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/02/23 14:07:37.459
    Mar  2 14:07:37.490: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:07:37.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 14:07:37.494: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
    Mar  2 14:07:38.505: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:07:38.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 14:07:38.513: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
    Mar  2 14:07:39.517: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:07:39.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 14:07:39.523: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
    Mar  2 14:07:40.507: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:07:40.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 14:07:40.513: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
    Mar  2 14:07:41.507: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:07:41.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Mar  2 14:07:41.514: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 14:07:41.518
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1781, will wait for the garbage collector to delete the pods 03/02/23 14:07:41.518
    Mar  2 14:07:41.598: INFO: Deleting DaemonSet.extensions daemon-set took: 26.477492ms
    Mar  2 14:07:41.705: INFO: Terminating DaemonSet.extensions daemon-set pods took: 107.723846ms
    Mar  2 14:07:44.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:07:44.309: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 14:07:44.312: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1964604"},"items":null}

    Mar  2 14:07:44.314: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1964604"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 14:07:44.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1781" for this suite. 03/02/23 14:07:44.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:07:44.38
Mar  2 14:07:44.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename deployment 03/02/23 14:07:44.381
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:07:44.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:07:44.406
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar  2 14:07:44.409: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  2 14:07:44.421: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 14:07:49.428: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 14:07:49.428
Mar  2 14:07:49.429: INFO: Creating deployment "test-rolling-update-deployment"
Mar  2 14:07:49.441: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  2 14:07:49.460: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  2 14:07:51.514: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  2 14:07:51.518: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 14:07:51.529: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5606  906c832b-ac11-451d-a74b-433a513f88ff 1964735 1 2023-03-02 14:07:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-02 14:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d4aa98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 14:07:49 +0000 UTC,LastTransitionTime:2023-03-02 14:07:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-02 14:07:50 +0000 UTC,LastTransitionTime:2023-03-02 14:07:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 14:07:51.533: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5606  1f914cec-291d-41b4-92e0-517c6887b71c 1964722 1 2023-03-02 14:07:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 906c832b-ac11-451d-a74b-433a513f88ff 0xc006d4afa7 0xc006d4afa8}] [] [{kube-controller-manager Update apps/v1 2023-03-02 14:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"906c832b-ac11-451d-a74b-433a513f88ff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d4b058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 14:07:51.533: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  2 14:07:51.533: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5606  bc5f5ced-3c45-4060-a097-9ef33ebe6041 1964734 2 2023-03-02 14:07:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 906c832b-ac11-451d-a74b-433a513f88ff 0xc006d4ae67 0xc006d4ae68}] [] [{e2e.test Update apps/v1 2023-03-02 14:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"906c832b-ac11-451d-a74b-433a513f88ff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006d4af28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 14:07:51.537: INFO: Pod "test-rolling-update-deployment-78f575d8ff-vlfds" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-vlfds test-rolling-update-deployment-78f575d8ff- deployment-5606  a7fb7b60-120c-47c3-af1e-2ffe3ace2e22 1964721 0 2023-03-02 14:07:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:a4e56dfb2512ca1e8082de218ae935387b0342dd964983db1da9f7a5c4ce31bb cni.projectcalico.org/podIP:10.233.123.74/32 cni.projectcalico.org/podIPs:10.233.123.74/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 1f914cec-291d-41b4-92e0-517c6887b71c 0xc005151b27 0xc005151b28}] [] [{kube-controller-manager Update v1 2023-03-02 14:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f914cec-291d-41b4-92e0-517c6887b71c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dpbcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dpbcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 14:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 14:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 14:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 14:07:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.74,StartTime:2023-03-02 14:07:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 14:07:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://b83d67fcdc63a515168dd9e223348c359fe92d80c65ceb3fcdb097f07f36b022,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 14:07:51.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5606" for this suite. 03/02/23 14:07:51.545
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":297,"skipped":5342,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.173 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:07:44.38
    Mar  2 14:07:44.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename deployment 03/02/23 14:07:44.381
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:07:44.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:07:44.406
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar  2 14:07:44.409: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Mar  2 14:07:44.421: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  2 14:07:49.428: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 14:07:49.428
    Mar  2 14:07:49.429: INFO: Creating deployment "test-rolling-update-deployment"
    Mar  2 14:07:49.441: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar  2 14:07:49.460: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Mar  2 14:07:51.514: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar  2 14:07:51.518: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 14:07:51.529: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5606  906c832b-ac11-451d-a74b-433a513f88ff 1964735 1 2023-03-02 14:07:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-02 14:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d4aa98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 14:07:49 +0000 UTC,LastTransitionTime:2023-03-02 14:07:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-02 14:07:50 +0000 UTC,LastTransitionTime:2023-03-02 14:07:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  2 14:07:51.533: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5606  1f914cec-291d-41b4-92e0-517c6887b71c 1964722 1 2023-03-02 14:07:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 906c832b-ac11-451d-a74b-433a513f88ff 0xc006d4afa7 0xc006d4afa8}] [] [{kube-controller-manager Update apps/v1 2023-03-02 14:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"906c832b-ac11-451d-a74b-433a513f88ff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d4b058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 14:07:51.533: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar  2 14:07:51.533: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5606  bc5f5ced-3c45-4060-a097-9ef33ebe6041 1964734 2 2023-03-02 14:07:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 906c832b-ac11-451d-a74b-433a513f88ff 0xc006d4ae67 0xc006d4ae68}] [] [{e2e.test Update apps/v1 2023-03-02 14:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"906c832b-ac11-451d-a74b-433a513f88ff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006d4af28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 14:07:51.537: INFO: Pod "test-rolling-update-deployment-78f575d8ff-vlfds" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-vlfds test-rolling-update-deployment-78f575d8ff- deployment-5606  a7fb7b60-120c-47c3-af1e-2ffe3ace2e22 1964721 0 2023-03-02 14:07:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:a4e56dfb2512ca1e8082de218ae935387b0342dd964983db1da9f7a5c4ce31bb cni.projectcalico.org/podIP:10.233.123.74/32 cni.projectcalico.org/podIPs:10.233.123.74/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 1f914cec-291d-41b4-92e0-517c6887b71c 0xc005151b27 0xc005151b28}] [] [{kube-controller-manager Update v1 2023-03-02 14:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f914cec-291d-41b4-92e0-517c6887b71c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 14:07:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.123.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dpbcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dpbcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aarnq-sc-k8s-node-srv2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 14:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 14:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 14:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 14:07:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.192,PodIP:10.233.123.74,StartTime:2023-03-02 14:07:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 14:07:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://b83d67fcdc63a515168dd9e223348c359fe92d80c65ceb3fcdb097f07f36b022,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.123.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 14:07:51.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5606" for this suite. 03/02/23 14:07:51.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:07:51.568
Mar  2 14:07:51.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 14:07:51.569
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:07:51.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:07:51.634
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar  2 14:07:51.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 14:08:52.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1608" for this suite. 03/02/23 14:08:52.962
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":298,"skipped":5393,"failed":0}
------------------------------
â€¢ [SLOW TEST] [61.412 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:07:51.568
    Mar  2 14:07:51.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 14:07:51.569
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:07:51.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:07:51.634
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar  2 14:07:51.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 14:08:52.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1608" for this suite. 03/02/23 14:08:52.962
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:08:52.988
Mar  2 14:08:52.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 14:08:52.991
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:08:53.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:08:53.048
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 03/02/23 14:08:53.052
Mar  2 14:08:53.102: INFO: Waiting up to 5m0s for pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be" in namespace "emptydir-4971" to be "Succeeded or Failed"
Mar  2 14:08:53.120: INFO: Pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be": Phase="Pending", Reason="", readiness=false. Elapsed: 18.362958ms
Mar  2 14:08:55.135: INFO: Pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be": Phase="Running", Reason="", readiness=true. Elapsed: 2.032464798s
Mar  2 14:08:57.129: INFO: Pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be": Phase="Running", Reason="", readiness=false. Elapsed: 4.026601741s
Mar  2 14:08:59.129: INFO: Pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026798754s
STEP: Saw pod success 03/02/23 14:08:59.129
Mar  2 14:08:59.130: INFO: Pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be" satisfied condition "Succeeded or Failed"
Mar  2 14:08:59.134: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-88de84fd-d527-45bd-bd8d-62c4897878be container test-container: <nil>
STEP: delete the pod 03/02/23 14:08:59.15
Mar  2 14:08:59.182: INFO: Waiting for pod pod-88de84fd-d527-45bd-bd8d-62c4897878be to disappear
Mar  2 14:08:59.189: INFO: Pod pod-88de84fd-d527-45bd-bd8d-62c4897878be no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 14:08:59.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4971" for this suite. 03/02/23 14:08:59.264
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":299,"skipped":5396,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.332 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:08:52.988
    Mar  2 14:08:52.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 14:08:52.991
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:08:53.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:08:53.048
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/02/23 14:08:53.052
    Mar  2 14:08:53.102: INFO: Waiting up to 5m0s for pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be" in namespace "emptydir-4971" to be "Succeeded or Failed"
    Mar  2 14:08:53.120: INFO: Pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be": Phase="Pending", Reason="", readiness=false. Elapsed: 18.362958ms
    Mar  2 14:08:55.135: INFO: Pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be": Phase="Running", Reason="", readiness=true. Elapsed: 2.032464798s
    Mar  2 14:08:57.129: INFO: Pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be": Phase="Running", Reason="", readiness=false. Elapsed: 4.026601741s
    Mar  2 14:08:59.129: INFO: Pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026798754s
    STEP: Saw pod success 03/02/23 14:08:59.129
    Mar  2 14:08:59.130: INFO: Pod "pod-88de84fd-d527-45bd-bd8d-62c4897878be" satisfied condition "Succeeded or Failed"
    Mar  2 14:08:59.134: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-88de84fd-d527-45bd-bd8d-62c4897878be container test-container: <nil>
    STEP: delete the pod 03/02/23 14:08:59.15
    Mar  2 14:08:59.182: INFO: Waiting for pod pod-88de84fd-d527-45bd-bd8d-62c4897878be to disappear
    Mar  2 14:08:59.189: INFO: Pod pod-88de84fd-d527-45bd-bd8d-62c4897878be no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 14:08:59.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4971" for this suite. 03/02/23 14:08:59.264
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:08:59.33
Mar  2 14:08:59.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename ephemeral-containers-test 03/02/23 14:08:59.332
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:08:59.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:08:59.37
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/02/23 14:08:59.442
Mar  2 14:08:59.460: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1966" to be "running and ready"
Mar  2 14:08:59.528: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 65.088632ms
Mar  2 14:08:59.528: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:09:01.560: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.097313926s
Mar  2 14:09:01.561: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar  2 14:09:01.561: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/02/23 14:09:01.564
Mar  2 14:09:01.595: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1966" to be "container debugger running"
Mar  2 14:09:01.604: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.393513ms
Mar  2 14:09:03.612: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016410954s
Mar  2 14:09:05.638: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.042558141s
Mar  2 14:09:05.638: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/02/23 14:09:05.638
Mar  2 14:09:05.638: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1966 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:09:05.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 14:09:05.639: INFO: ExecWithOptions: Clientset creation
Mar  2 14:09:05.639: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-1966/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar  2 14:09:05.755: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 14:09:05.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-1966" for this suite. 03/02/23 14:09:05.787
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":300,"skipped":5396,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.509 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:08:59.33
    Mar  2 14:08:59.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/02/23 14:08:59.332
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:08:59.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:08:59.37
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/02/23 14:08:59.442
    Mar  2 14:08:59.460: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1966" to be "running and ready"
    Mar  2 14:08:59.528: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 65.088632ms
    Mar  2 14:08:59.528: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:09:01.560: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.097313926s
    Mar  2 14:09:01.561: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar  2 14:09:01.561: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/02/23 14:09:01.564
    Mar  2 14:09:01.595: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1966" to be "container debugger running"
    Mar  2 14:09:01.604: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.393513ms
    Mar  2 14:09:03.612: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016410954s
    Mar  2 14:09:05.638: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.042558141s
    Mar  2 14:09:05.638: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/02/23 14:09:05.638
    Mar  2 14:09:05.638: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1966 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 14:09:05.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 14:09:05.639: INFO: ExecWithOptions: Clientset creation
    Mar  2 14:09:05.639: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-1966/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar  2 14:09:05.755: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 14:09:05.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-1966" for this suite. 03/02/23 14:09:05.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:09:05.844
Mar  2 14:09:05.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-probe 03/02/23 14:09:05.845
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:09:05.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:09:05.934
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 14:10:06.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3619" for this suite. 03/02/23 14:10:06.008
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":301,"skipped":5457,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.173 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:09:05.844
    Mar  2 14:09:05.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-probe 03/02/23 14:09:05.845
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:09:05.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:09:05.934
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 14:10:06.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3619" for this suite. 03/02/23 14:10:06.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:10:06.031
Mar  2 14:10:06.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 14:10:06.033
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:06.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:06.07
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-167 03/02/23 14:10:06.094
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/02/23 14:10:06.124
STEP: creating service externalsvc in namespace services-167 03/02/23 14:10:06.127
STEP: creating replication controller externalsvc in namespace services-167 03/02/23 14:10:06.158
I0302 14:10:06.170345      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-167, replica count: 2
I0302 14:10:09.222899      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/02/23 14:10:09.229
Mar  2 14:10:09.244: INFO: Creating new exec pod
Mar  2 14:10:09.253: INFO: Waiting up to 5m0s for pod "execpodjw8sr" in namespace "services-167" to be "running"
Mar  2 14:10:09.258: INFO: Pod "execpodjw8sr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.253729ms
Mar  2 14:10:11.267: INFO: Pod "execpodjw8sr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014536563s
Mar  2 14:10:13.265: INFO: Pod "execpodjw8sr": Phase="Running", Reason="", readiness=true. Elapsed: 4.012230155s
Mar  2 14:10:13.265: INFO: Pod "execpodjw8sr" satisfied condition "running"
Mar  2 14:10:13.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-167 exec execpodjw8sr -- /bin/sh -x -c nslookup clusterip-service.services-167.svc.cluster.local'
Mar  2 14:10:13.932: INFO: stderr: "+ nslookup clusterip-service.services-167.svc.cluster.local\n"
Mar  2 14:10:13.932: INFO: stdout: "Server:\t\t10.233.0.3\nAddress:\t10.233.0.3#53\n\nclusterip-service.services-167.svc.cluster.local\tcanonical name = externalsvc.services-167.svc.cluster.local.\nName:\texternalsvc.services-167.svc.cluster.local\nAddress: 10.233.30.27\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-167, will wait for the garbage collector to delete the pods 03/02/23 14:10:13.932
Mar  2 14:10:13.998: INFO: Deleting ReplicationController externalsvc took: 8.50988ms
Mar  2 14:10:14.201: INFO: Terminating ReplicationController externalsvc pods took: 202.278278ms
Mar  2 14:10:16.748: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 14:10:16.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-167" for this suite. 03/02/23 14:10:16.808
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":302,"skipped":5522,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.800 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:10:06.031
    Mar  2 14:10:06.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 14:10:06.033
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:06.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:06.07
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-167 03/02/23 14:10:06.094
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/02/23 14:10:06.124
    STEP: creating service externalsvc in namespace services-167 03/02/23 14:10:06.127
    STEP: creating replication controller externalsvc in namespace services-167 03/02/23 14:10:06.158
    I0302 14:10:06.170345      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-167, replica count: 2
    I0302 14:10:09.222899      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/02/23 14:10:09.229
    Mar  2 14:10:09.244: INFO: Creating new exec pod
    Mar  2 14:10:09.253: INFO: Waiting up to 5m0s for pod "execpodjw8sr" in namespace "services-167" to be "running"
    Mar  2 14:10:09.258: INFO: Pod "execpodjw8sr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.253729ms
    Mar  2 14:10:11.267: INFO: Pod "execpodjw8sr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014536563s
    Mar  2 14:10:13.265: INFO: Pod "execpodjw8sr": Phase="Running", Reason="", readiness=true. Elapsed: 4.012230155s
    Mar  2 14:10:13.265: INFO: Pod "execpodjw8sr" satisfied condition "running"
    Mar  2 14:10:13.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=services-167 exec execpodjw8sr -- /bin/sh -x -c nslookup clusterip-service.services-167.svc.cluster.local'
    Mar  2 14:10:13.932: INFO: stderr: "+ nslookup clusterip-service.services-167.svc.cluster.local\n"
    Mar  2 14:10:13.932: INFO: stdout: "Server:\t\t10.233.0.3\nAddress:\t10.233.0.3#53\n\nclusterip-service.services-167.svc.cluster.local\tcanonical name = externalsvc.services-167.svc.cluster.local.\nName:\texternalsvc.services-167.svc.cluster.local\nAddress: 10.233.30.27\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-167, will wait for the garbage collector to delete the pods 03/02/23 14:10:13.932
    Mar  2 14:10:13.998: INFO: Deleting ReplicationController externalsvc took: 8.50988ms
    Mar  2 14:10:14.201: INFO: Terminating ReplicationController externalsvc pods took: 202.278278ms
    Mar  2 14:10:16.748: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 14:10:16.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-167" for this suite. 03/02/23 14:10:16.808
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:10:16.834
Mar  2 14:10:16.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename gc 03/02/23 14:10:16.835
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:16.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:16.943
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/02/23 14:10:16.96
STEP: delete the rc 03/02/23 14:10:21.978
STEP: wait for the rc to be deleted 03/02/23 14:10:22.084
Mar  2 14:10:23.649: INFO: 80 pods remaining
Mar  2 14:10:23.649: INFO: 80 pods has nil DeletionTimestamp
Mar  2 14:10:23.649: INFO: 
Mar  2 14:10:24.348: INFO: 68 pods remaining
Mar  2 14:10:24.348: INFO: 64 pods has nil DeletionTimestamp
Mar  2 14:10:24.348: INFO: 
Mar  2 14:10:25.953: INFO: 59 pods remaining
Mar  2 14:10:26.024: INFO: 59 pods has nil DeletionTimestamp
Mar  2 14:10:26.067: INFO: 
Mar  2 14:10:26.290: INFO: 40 pods remaining
Mar  2 14:10:26.294: INFO: 40 pods has nil DeletionTimestamp
Mar  2 14:10:26.295: INFO: 
Mar  2 14:10:27.758: INFO: 28 pods remaining
Mar  2 14:10:27.758: INFO: 28 pods has nil DeletionTimestamp
Mar  2 14:10:27.758: INFO: 
Mar  2 14:10:28.192: INFO: 19 pods remaining
Mar  2 14:10:28.192: INFO: 19 pods has nil DeletionTimestamp
Mar  2 14:10:28.192: INFO: 
STEP: Gathering metrics 03/02/23 14:10:29.564
Mar  2 14:10:29.764: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
Mar  2 14:10:29.839: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 75.146205ms
Mar  2 14:10:29.839: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
Mar  2 14:10:29.839: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
Mar  2 14:10:30.640: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 14:10:30.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7043" for this suite. 03/02/23 14:10:30.652
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":303,"skipped":5523,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.848 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:10:16.834
    Mar  2 14:10:16.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename gc 03/02/23 14:10:16.835
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:16.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:16.943
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/02/23 14:10:16.96
    STEP: delete the rc 03/02/23 14:10:21.978
    STEP: wait for the rc to be deleted 03/02/23 14:10:22.084
    Mar  2 14:10:23.649: INFO: 80 pods remaining
    Mar  2 14:10:23.649: INFO: 80 pods has nil DeletionTimestamp
    Mar  2 14:10:23.649: INFO: 
    Mar  2 14:10:24.348: INFO: 68 pods remaining
    Mar  2 14:10:24.348: INFO: 64 pods has nil DeletionTimestamp
    Mar  2 14:10:24.348: INFO: 
    Mar  2 14:10:25.953: INFO: 59 pods remaining
    Mar  2 14:10:26.024: INFO: 59 pods has nil DeletionTimestamp
    Mar  2 14:10:26.067: INFO: 
    Mar  2 14:10:26.290: INFO: 40 pods remaining
    Mar  2 14:10:26.294: INFO: 40 pods has nil DeletionTimestamp
    Mar  2 14:10:26.295: INFO: 
    Mar  2 14:10:27.758: INFO: 28 pods remaining
    Mar  2 14:10:27.758: INFO: 28 pods has nil DeletionTimestamp
    Mar  2 14:10:27.758: INFO: 
    Mar  2 14:10:28.192: INFO: 19 pods remaining
    Mar  2 14:10:28.192: INFO: 19 pods has nil DeletionTimestamp
    Mar  2 14:10:28.192: INFO: 
    STEP: Gathering metrics 03/02/23 14:10:29.564
    Mar  2 14:10:29.764: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
    Mar  2 14:10:29.839: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 75.146205ms
    Mar  2 14:10:29.839: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
    Mar  2 14:10:29.839: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
    Mar  2 14:10:30.640: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 14:10:30.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7043" for this suite. 03/02/23 14:10:30.652
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:10:30.742
Mar  2 14:10:30.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename endpointslice 03/02/23 14:10:30.744
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:30.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:30.96
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  2 14:10:31.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9432" for this suite. 03/02/23 14:10:31.143
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":304,"skipped":5527,"failed":0}
------------------------------
â€¢ [0.411 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:10:30.742
    Mar  2 14:10:30.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename endpointslice 03/02/23 14:10:30.744
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:30.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:30.96
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  2 14:10:31.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9432" for this suite. 03/02/23 14:10:31.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:10:31.164
Mar  2 14:10:31.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pods 03/02/23 14:10:31.17
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:31.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:31.331
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 03/02/23 14:10:31.358
Mar  2 14:10:31.450: INFO: created test-pod-1
Mar  2 14:10:31.469: INFO: created test-pod-2
Mar  2 14:10:31.536: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/02/23 14:10:31.537
Mar  2 14:10:31.537: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4822' to be running and ready
Mar  2 14:10:31.643: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:31.643: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:31.643: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:31.643: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (0 seconds elapsed)
Mar  2 14:10:31.643: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
Mar  2 14:10:31.643: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
Mar  2 14:10:31.643: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:31.643: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:31.643: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:31.643: INFO: 
Mar  2 14:10:33.737: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:33.737: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:33.737: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:33.737: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (2 seconds elapsed)
Mar  2 14:10:33.737: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
Mar  2 14:10:33.737: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
Mar  2 14:10:33.737: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:33.738: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:33.738: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:33.738: INFO: 
Mar  2 14:10:35.750: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:35.750: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:35.750: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:35.750: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (4 seconds elapsed)
Mar  2 14:10:35.750: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
Mar  2 14:10:35.750: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
Mar  2 14:10:35.750: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:35.750: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:35.750: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:35.750: INFO: 
Mar  2 14:10:37.690: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:37.690: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:37.690: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:37.690: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (6 seconds elapsed)
Mar  2 14:10:37.690: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
Mar  2 14:10:37.690: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
Mar  2 14:10:37.690: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:37.690: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:37.690: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:37.690: INFO: 
Mar  2 14:10:39.667: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:39.667: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:39.668: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:39.668: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (8 seconds elapsed)
Mar  2 14:10:39.668: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
Mar  2 14:10:39.668: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
Mar  2 14:10:39.668: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:39.668: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:39.668: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:39.668: INFO: 
Mar  2 14:10:41.661: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:41.661: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:41.662: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:41.662: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (10 seconds elapsed)
Mar  2 14:10:41.662: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
Mar  2 14:10:41.662: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
Mar  2 14:10:41.662: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:41.662: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:41.662: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:41.662: INFO: 
Mar  2 14:10:43.662: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 14:10:43.662: INFO: 2 / 3 pods in namespace 'pods-4822' are running and ready (12 seconds elapsed)
Mar  2 14:10:43.662: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
Mar  2 14:10:43.662: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
Mar  2 14:10:43.662: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
Mar  2 14:10:43.662: INFO: 
Mar  2 14:10:45.704: INFO: 3 / 3 pods in namespace 'pods-4822' are running and ready (14 seconds elapsed)
Mar  2 14:10:45.714: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/02/23 14:10:45.764
Mar  2 14:10:45.769: INFO: Pod quantity 3 is different from expected quantity 0
Mar  2 14:10:46.780: INFO: Pod quantity 3 is different from expected quantity 0
Mar  2 14:10:47.847: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 14:10:48.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4822" for this suite. 03/02/23 14:10:48.846
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":305,"skipped":5568,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.693 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:10:31.164
    Mar  2 14:10:31.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pods 03/02/23 14:10:31.17
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:31.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:31.331
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 03/02/23 14:10:31.358
    Mar  2 14:10:31.450: INFO: created test-pod-1
    Mar  2 14:10:31.469: INFO: created test-pod-2
    Mar  2 14:10:31.536: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/02/23 14:10:31.537
    Mar  2 14:10:31.537: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4822' to be running and ready
    Mar  2 14:10:31.643: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:31.643: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:31.643: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:31.643: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (0 seconds elapsed)
    Mar  2 14:10:31.643: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
    Mar  2 14:10:31.643: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
    Mar  2 14:10:31.643: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:31.643: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:31.643: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:31.643: INFO: 
    Mar  2 14:10:33.737: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:33.737: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:33.737: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:33.737: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (2 seconds elapsed)
    Mar  2 14:10:33.737: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
    Mar  2 14:10:33.737: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
    Mar  2 14:10:33.737: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:33.738: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:33.738: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:33.738: INFO: 
    Mar  2 14:10:35.750: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:35.750: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:35.750: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:35.750: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (4 seconds elapsed)
    Mar  2 14:10:35.750: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
    Mar  2 14:10:35.750: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
    Mar  2 14:10:35.750: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:35.750: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:35.750: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:35.750: INFO: 
    Mar  2 14:10:37.690: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:37.690: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:37.690: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:37.690: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (6 seconds elapsed)
    Mar  2 14:10:37.690: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
    Mar  2 14:10:37.690: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
    Mar  2 14:10:37.690: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:37.690: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:37.690: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:37.690: INFO: 
    Mar  2 14:10:39.667: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:39.667: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:39.668: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:39.668: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (8 seconds elapsed)
    Mar  2 14:10:39.668: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
    Mar  2 14:10:39.668: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
    Mar  2 14:10:39.668: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:39.668: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:39.668: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:39.668: INFO: 
    Mar  2 14:10:41.661: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:41.661: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:41.662: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:41.662: INFO: 0 / 3 pods in namespace 'pods-4822' are running and ready (10 seconds elapsed)
    Mar  2 14:10:41.662: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
    Mar  2 14:10:41.662: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
    Mar  2 14:10:41.662: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:41.662: INFO: test-pod-2  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:41.662: INFO: test-pod-3  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:41.662: INFO: 
    Mar  2 14:10:43.662: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 14:10:43.662: INFO: 2 / 3 pods in namespace 'pods-4822' are running and ready (12 seconds elapsed)
    Mar  2 14:10:43.662: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
    Mar  2 14:10:43.662: INFO: POD         NODE                    PHASE    GRACE  CONDITIONS
    Mar  2 14:10:43.662: INFO: test-pod-1  aarnq-sc-k8s-node-srv2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:10:31 +0000 UTC  }]
    Mar  2 14:10:43.662: INFO: 
    Mar  2 14:10:45.704: INFO: 3 / 3 pods in namespace 'pods-4822' are running and ready (14 seconds elapsed)
    Mar  2 14:10:45.714: INFO: expected 0 pod replicas in namespace 'pods-4822', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/02/23 14:10:45.764
    Mar  2 14:10:45.769: INFO: Pod quantity 3 is different from expected quantity 0
    Mar  2 14:10:46.780: INFO: Pod quantity 3 is different from expected quantity 0
    Mar  2 14:10:47.847: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 14:10:48.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4822" for this suite. 03/02/23 14:10:48.846
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:10:48.864
Mar  2 14:10:48.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename gc 03/02/23 14:10:48.87
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:48.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:48.945
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/02/23 14:10:48.953
STEP: delete the rc 03/02/23 14:10:53.967
STEP: wait for all pods to be garbage collected 03/02/23 14:10:53.979
STEP: Gathering metrics 03/02/23 14:10:58.996
Mar  2 14:10:59.060: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
Mar  2 14:10:59.068: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 7.407101ms
Mar  2 14:10:59.068: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
Mar  2 14:10:59.068: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
Mar  2 14:10:59.254: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 14:10:59.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2549" for this suite. 03/02/23 14:10:59.288
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":306,"skipped":5569,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.439 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:10:48.864
    Mar  2 14:10:48.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename gc 03/02/23 14:10:48.87
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:48.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:48.945
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/02/23 14:10:48.953
    STEP: delete the rc 03/02/23 14:10:53.967
    STEP: wait for all pods to be garbage collected 03/02/23 14:10:53.979
    STEP: Gathering metrics 03/02/23 14:10:58.996
    Mar  2 14:10:59.060: INFO: Waiting up to 5m0s for pod "kube-controller-manager-aarnq-sc-k8s-ctl0" in namespace "kube-system" to be "running and ready"
    Mar  2 14:10:59.068: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0": Phase="Running", Reason="", readiness=true. Elapsed: 7.407101ms
    Mar  2 14:10:59.068: INFO: The phase of Pod kube-controller-manager-aarnq-sc-k8s-ctl0 is Running (Ready = true)
    Mar  2 14:10:59.068: INFO: Pod "kube-controller-manager-aarnq-sc-k8s-ctl0" satisfied condition "running and ready"
    Mar  2 14:10:59.254: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 14:10:59.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2549" for this suite. 03/02/23 14:10:59.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:10:59.305
Mar  2 14:10:59.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename namespaces 03/02/23 14:10:59.307
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:59.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:59.358
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 03/02/23 14:10:59.37
STEP: patching the Namespace 03/02/23 14:10:59.406
STEP: get the Namespace and ensuring it has the label 03/02/23 14:10:59.433
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  2 14:10:59.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8574" for this suite. 03/02/23 14:10:59.452
STEP: Destroying namespace "nspatchtest-b0af1fcb-55af-48a1-9c0a-ca80cd9759d9-6850" for this suite. 03/02/23 14:10:59.474
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":307,"skipped":5575,"failed":0}
------------------------------
â€¢ [0.199 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:10:59.305
    Mar  2 14:10:59.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename namespaces 03/02/23 14:10:59.307
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:59.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:59.358
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 03/02/23 14:10:59.37
    STEP: patching the Namespace 03/02/23 14:10:59.406
    STEP: get the Namespace and ensuring it has the label 03/02/23 14:10:59.433
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 14:10:59.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8574" for this suite. 03/02/23 14:10:59.452
    STEP: Destroying namespace "nspatchtest-b0af1fcb-55af-48a1-9c0a-ca80cd9759d9-6850" for this suite. 03/02/23 14:10:59.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:10:59.507
Mar  2 14:10:59.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-probe 03/02/23 14:10:59.513
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:59.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:59.549
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce in namespace container-probe-9676 03/02/23 14:10:59.556
Mar  2 14:10:59.567: INFO: Waiting up to 5m0s for pod "liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce" in namespace "container-probe-9676" to be "not pending"
Mar  2 14:10:59.584: INFO: Pod "liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce": Phase="Pending", Reason="", readiness=false. Elapsed: 17.35589ms
Mar  2 14:11:01.622: INFO: Pod "liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055384889s
Mar  2 14:11:03.592: INFO: Pod "liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce": Phase="Running", Reason="", readiness=true. Elapsed: 4.024842181s
Mar  2 14:11:03.592: INFO: Pod "liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce" satisfied condition "not pending"
Mar  2 14:11:03.592: INFO: Started pod liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce in namespace container-probe-9676
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 14:11:03.592
Mar  2 14:11:03.597: INFO: Initial restart count of pod liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce is 0
Mar  2 14:11:21.675: INFO: Restart count of pod container-probe-9676/liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce is now 1 (18.078062111s elapsed)
STEP: deleting the pod 03/02/23 14:11:21.675
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 14:11:21.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9676" for this suite. 03/02/23 14:11:21.742
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":308,"skipped":5586,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.244 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:10:59.507
    Mar  2 14:10:59.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-probe 03/02/23 14:10:59.513
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:10:59.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:10:59.549
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce in namespace container-probe-9676 03/02/23 14:10:59.556
    Mar  2 14:10:59.567: INFO: Waiting up to 5m0s for pod "liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce" in namespace "container-probe-9676" to be "not pending"
    Mar  2 14:10:59.584: INFO: Pod "liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce": Phase="Pending", Reason="", readiness=false. Elapsed: 17.35589ms
    Mar  2 14:11:01.622: INFO: Pod "liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055384889s
    Mar  2 14:11:03.592: INFO: Pod "liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce": Phase="Running", Reason="", readiness=true. Elapsed: 4.024842181s
    Mar  2 14:11:03.592: INFO: Pod "liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce" satisfied condition "not pending"
    Mar  2 14:11:03.592: INFO: Started pod liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce in namespace container-probe-9676
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 14:11:03.592
    Mar  2 14:11:03.597: INFO: Initial restart count of pod liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce is 0
    Mar  2 14:11:21.675: INFO: Restart count of pod container-probe-9676/liveness-0710cbf3-7b74-4a61-8bcb-3b97a87bedce is now 1 (18.078062111s elapsed)
    STEP: deleting the pod 03/02/23 14:11:21.675
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 14:11:21.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9676" for this suite. 03/02/23 14:11:21.742
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:11:21.753
Mar  2 14:11:21.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename var-expansion 03/02/23 14:11:21.757
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:11:21.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:11:21.817
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 03/02/23 14:11:21.826
Mar  2 14:11:21.837: INFO: Waiting up to 5m0s for pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb" in namespace "var-expansion-7267" to be "Succeeded or Failed"
Mar  2 14:11:21.852: INFO: Pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.704298ms
Mar  2 14:11:23.856: INFO: Pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018843871s
Mar  2 14:11:25.866: INFO: Pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02880892s
Mar  2 14:11:27.862: INFO: Pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024912976s
STEP: Saw pod success 03/02/23 14:11:27.862
Mar  2 14:11:27.863: INFO: Pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb" satisfied condition "Succeeded or Failed"
Mar  2 14:11:27.871: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb container dapi-container: <nil>
STEP: delete the pod 03/02/23 14:11:27.884
Mar  2 14:11:27.900: INFO: Waiting for pod var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb to disappear
Mar  2 14:11:27.908: INFO: Pod var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 14:11:27.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7267" for this suite. 03/02/23 14:11:27.923
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":309,"skipped":5588,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.180 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:11:21.753
    Mar  2 14:11:21.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename var-expansion 03/02/23 14:11:21.757
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:11:21.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:11:21.817
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 03/02/23 14:11:21.826
    Mar  2 14:11:21.837: INFO: Waiting up to 5m0s for pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb" in namespace "var-expansion-7267" to be "Succeeded or Failed"
    Mar  2 14:11:21.852: INFO: Pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.704298ms
    Mar  2 14:11:23.856: INFO: Pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018843871s
    Mar  2 14:11:25.866: INFO: Pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02880892s
    Mar  2 14:11:27.862: INFO: Pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024912976s
    STEP: Saw pod success 03/02/23 14:11:27.862
    Mar  2 14:11:27.863: INFO: Pod "var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb" satisfied condition "Succeeded or Failed"
    Mar  2 14:11:27.871: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb container dapi-container: <nil>
    STEP: delete the pod 03/02/23 14:11:27.884
    Mar  2 14:11:27.900: INFO: Waiting for pod var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb to disappear
    Mar  2 14:11:27.908: INFO: Pod var-expansion-7bf2a4f4-816a-45f7-a6b6-e48a33d0a0fb no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 14:11:27.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7267" for this suite. 03/02/23 14:11:27.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:11:27.937
Mar  2 14:11:27.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename job 03/02/23 14:11:27.938
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:11:27.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:11:27.98
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 03/02/23 14:11:27.986
STEP: Ensure pods equal to paralellism count is attached to the job 03/02/23 14:11:28.003
STEP: patching /status 03/02/23 14:11:32.033
STEP: updating /status 03/02/23 14:11:32.058
STEP: get /status 03/02/23 14:11:32.066
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 14:11:32.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3657" for this suite. 03/02/23 14:11:32.106
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":310,"skipped":5630,"failed":0}
------------------------------
â€¢ [4.182 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:11:27.937
    Mar  2 14:11:27.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename job 03/02/23 14:11:27.938
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:11:27.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:11:27.98
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 03/02/23 14:11:27.986
    STEP: Ensure pods equal to paralellism count is attached to the job 03/02/23 14:11:28.003
    STEP: patching /status 03/02/23 14:11:32.033
    STEP: updating /status 03/02/23 14:11:32.058
    STEP: get /status 03/02/23 14:11:32.066
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 14:11:32.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3657" for this suite. 03/02/23 14:11:32.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:11:32.123
Mar  2 14:11:32.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename containers 03/02/23 14:11:32.124
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:11:32.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:11:32.159
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 03/02/23 14:11:32.167
Mar  2 14:11:32.256: INFO: Waiting up to 5m0s for pod "client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0" in namespace "containers-5024" to be "Succeeded or Failed"
Mar  2 14:11:32.263: INFO: Pod "client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913952ms
Mar  2 14:11:34.268: INFO: Pod "client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01191029s
Mar  2 14:11:36.282: INFO: Pod "client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02602184s
STEP: Saw pod success 03/02/23 14:11:36.282
Mar  2 14:11:36.287: INFO: Pod "client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0" satisfied condition "Succeeded or Failed"
Mar  2 14:11:36.293: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 14:11:36.31
Mar  2 14:11:36.349: INFO: Waiting for pod client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0 to disappear
Mar  2 14:11:36.356: INFO: Pod client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  2 14:11:36.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5024" for this suite. 03/02/23 14:11:36.369
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":311,"skipped":5687,"failed":0}
------------------------------
â€¢ [4.281 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:11:32.123
    Mar  2 14:11:32.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename containers 03/02/23 14:11:32.124
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:11:32.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:11:32.159
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 03/02/23 14:11:32.167
    Mar  2 14:11:32.256: INFO: Waiting up to 5m0s for pod "client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0" in namespace "containers-5024" to be "Succeeded or Failed"
    Mar  2 14:11:32.263: INFO: Pod "client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913952ms
    Mar  2 14:11:34.268: INFO: Pod "client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01191029s
    Mar  2 14:11:36.282: INFO: Pod "client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02602184s
    STEP: Saw pod success 03/02/23 14:11:36.282
    Mar  2 14:11:36.287: INFO: Pod "client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0" satisfied condition "Succeeded or Failed"
    Mar  2 14:11:36.293: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 14:11:36.31
    Mar  2 14:11:36.349: INFO: Waiting for pod client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0 to disappear
    Mar  2 14:11:36.356: INFO: Pod client-containers-d9bcf91c-b12e-43d9-9195-a932657c8fb0 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  2 14:11:36.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5024" for this suite. 03/02/23 14:11:36.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:11:36.43
Mar  2 14:11:36.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename resourcequota 03/02/23 14:11:36.431
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:11:36.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:11:36.458
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 03/02/23 14:11:36.461
STEP: Creating a ResourceQuota 03/02/23 14:11:41.465
STEP: Ensuring resource quota status is calculated 03/02/23 14:11:41.479
STEP: Creating a Service 03/02/23 14:11:43.517
STEP: Creating a NodePort Service 03/02/23 14:11:43.551
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/02/23 14:11:43.623
STEP: Ensuring resource quota status captures service creation 03/02/23 14:11:43.646
STEP: Deleting Services 03/02/23 14:11:45.652
STEP: Ensuring resource quota status released usage 03/02/23 14:11:45.707
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 14:11:47.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2849" for this suite. 03/02/23 14:11:47.722
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":312,"skipped":5707,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.303 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:11:36.43
    Mar  2 14:11:36.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename resourcequota 03/02/23 14:11:36.431
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:11:36.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:11:36.458
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 03/02/23 14:11:36.461
    STEP: Creating a ResourceQuota 03/02/23 14:11:41.465
    STEP: Ensuring resource quota status is calculated 03/02/23 14:11:41.479
    STEP: Creating a Service 03/02/23 14:11:43.517
    STEP: Creating a NodePort Service 03/02/23 14:11:43.551
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/02/23 14:11:43.623
    STEP: Ensuring resource quota status captures service creation 03/02/23 14:11:43.646
    STEP: Deleting Services 03/02/23 14:11:45.652
    STEP: Ensuring resource quota status released usage 03/02/23 14:11:45.707
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 14:11:47.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2849" for this suite. 03/02/23 14:11:47.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:11:47.739
Mar  2 14:11:47.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-probe 03/02/23 14:11:47.74
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:11:47.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:11:47.761
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-ba41158c-63d6-4fa7-8205-576dccff4774 in namespace container-probe-5499 03/02/23 14:11:47.765
Mar  2 14:11:47.781: INFO: Waiting up to 5m0s for pod "liveness-ba41158c-63d6-4fa7-8205-576dccff4774" in namespace "container-probe-5499" to be "not pending"
Mar  2 14:11:47.786: INFO: Pod "liveness-ba41158c-63d6-4fa7-8205-576dccff4774": Phase="Pending", Reason="", readiness=false. Elapsed: 4.593333ms
Mar  2 14:11:49.794: INFO: Pod "liveness-ba41158c-63d6-4fa7-8205-576dccff4774": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012802302s
Mar  2 14:11:51.818: INFO: Pod "liveness-ba41158c-63d6-4fa7-8205-576dccff4774": Phase="Running", Reason="", readiness=true. Elapsed: 4.03653618s
Mar  2 14:11:51.818: INFO: Pod "liveness-ba41158c-63d6-4fa7-8205-576dccff4774" satisfied condition "not pending"
Mar  2 14:11:51.818: INFO: Started pod liveness-ba41158c-63d6-4fa7-8205-576dccff4774 in namespace container-probe-5499
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 14:11:51.818
Mar  2 14:11:51.823: INFO: Initial restart count of pod liveness-ba41158c-63d6-4fa7-8205-576dccff4774 is 0
STEP: deleting the pod 03/02/23 14:15:52.876
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 14:15:52.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5499" for this suite. 03/02/23 14:15:52.925
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":313,"skipped":5735,"failed":0}
------------------------------
â€¢ [SLOW TEST] [245.201 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:11:47.739
    Mar  2 14:11:47.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-probe 03/02/23 14:11:47.74
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:11:47.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:11:47.761
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-ba41158c-63d6-4fa7-8205-576dccff4774 in namespace container-probe-5499 03/02/23 14:11:47.765
    Mar  2 14:11:47.781: INFO: Waiting up to 5m0s for pod "liveness-ba41158c-63d6-4fa7-8205-576dccff4774" in namespace "container-probe-5499" to be "not pending"
    Mar  2 14:11:47.786: INFO: Pod "liveness-ba41158c-63d6-4fa7-8205-576dccff4774": Phase="Pending", Reason="", readiness=false. Elapsed: 4.593333ms
    Mar  2 14:11:49.794: INFO: Pod "liveness-ba41158c-63d6-4fa7-8205-576dccff4774": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012802302s
    Mar  2 14:11:51.818: INFO: Pod "liveness-ba41158c-63d6-4fa7-8205-576dccff4774": Phase="Running", Reason="", readiness=true. Elapsed: 4.03653618s
    Mar  2 14:11:51.818: INFO: Pod "liveness-ba41158c-63d6-4fa7-8205-576dccff4774" satisfied condition "not pending"
    Mar  2 14:11:51.818: INFO: Started pod liveness-ba41158c-63d6-4fa7-8205-576dccff4774 in namespace container-probe-5499
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 14:11:51.818
    Mar  2 14:11:51.823: INFO: Initial restart count of pod liveness-ba41158c-63d6-4fa7-8205-576dccff4774 is 0
    STEP: deleting the pod 03/02/23 14:15:52.876
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 14:15:52.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5499" for this suite. 03/02/23 14:15:52.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:15:52.945
Mar  2 14:15:52.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename services 03/02/23 14:15:52.948
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:15:52.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:15:52.984
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 03/02/23 14:15:52.998
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 14:15:53.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3861" for this suite. 03/02/23 14:15:53.009
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":314,"skipped":5761,"failed":0}
------------------------------
â€¢ [0.071 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:15:52.945
    Mar  2 14:15:52.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename services 03/02/23 14:15:52.948
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:15:52.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:15:52.984
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 03/02/23 14:15:52.998
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 14:15:53.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3861" for this suite. 03/02/23 14:15:53.009
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:15:53.017
Mar  2 14:15:53.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 14:15:53.019
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:15:53.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:15:53.039
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 03/02/23 14:15:53.045
Mar  2 14:15:53.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1" in namespace "downward-api-8629" to be "Succeeded or Failed"
Mar  2 14:15:53.056: INFO: Pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.966327ms
Mar  2 14:15:55.066: INFO: Pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014352753s
Mar  2 14:15:57.068: INFO: Pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016103668s
Mar  2 14:15:59.068: INFO: Pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016074566s
STEP: Saw pod success 03/02/23 14:15:59.069
Mar  2 14:15:59.069: INFO: Pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1" satisfied condition "Succeeded or Failed"
Mar  2 14:15:59.108: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1 container client-container: <nil>
STEP: delete the pod 03/02/23 14:15:59.134
Mar  2 14:15:59.146: INFO: Waiting for pod downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1 to disappear
Mar  2 14:15:59.150: INFO: Pod downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 14:15:59.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8629" for this suite. 03/02/23 14:15:59.158
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":315,"skipped":5769,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.149 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:15:53.017
    Mar  2 14:15:53.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 14:15:53.019
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:15:53.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:15:53.039
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 03/02/23 14:15:53.045
    Mar  2 14:15:53.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1" in namespace "downward-api-8629" to be "Succeeded or Failed"
    Mar  2 14:15:53.056: INFO: Pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.966327ms
    Mar  2 14:15:55.066: INFO: Pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014352753s
    Mar  2 14:15:57.068: INFO: Pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016103668s
    Mar  2 14:15:59.068: INFO: Pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016074566s
    STEP: Saw pod success 03/02/23 14:15:59.069
    Mar  2 14:15:59.069: INFO: Pod "downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1" satisfied condition "Succeeded or Failed"
    Mar  2 14:15:59.108: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1 container client-container: <nil>
    STEP: delete the pod 03/02/23 14:15:59.134
    Mar  2 14:15:59.146: INFO: Waiting for pod downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1 to disappear
    Mar  2 14:15:59.150: INFO: Pod downwardapi-volume-81d8e751-dc08-45dd-90db-e4c096f718b1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 14:15:59.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8629" for this suite. 03/02/23 14:15:59.158
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:15:59.171
Mar  2 14:15:59.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 14:15:59.212
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:15:59.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:15:59.331
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/02/23 14:15:59.335
Mar  2 14:15:59.344: INFO: Waiting up to 5m0s for pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30" in namespace "emptydir-5301" to be "Succeeded or Failed"
Mar  2 14:15:59.350: INFO: Pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30": Phase="Pending", Reason="", readiness=false. Elapsed: 5.802129ms
Mar  2 14:16:01.358: INFO: Pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30": Phase="Running", Reason="", readiness=true. Elapsed: 2.013866201s
Mar  2 14:16:03.358: INFO: Pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30": Phase="Running", Reason="", readiness=false. Elapsed: 4.014432441s
Mar  2 14:16:05.357: INFO: Pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013131151s
STEP: Saw pod success 03/02/23 14:16:05.358
Mar  2 14:16:05.358: INFO: Pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30" satisfied condition "Succeeded or Failed"
Mar  2 14:16:05.363: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30 container test-container: <nil>
STEP: delete the pod 03/02/23 14:16:05.378
Mar  2 14:16:05.397: INFO: Waiting for pod pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30 to disappear
Mar  2 14:16:05.410: INFO: Pod pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 14:16:05.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5301" for this suite. 03/02/23 14:16:05.415
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":316,"skipped":5773,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.251 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:15:59.171
    Mar  2 14:15:59.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 14:15:59.212
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:15:59.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:15:59.331
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/02/23 14:15:59.335
    Mar  2 14:15:59.344: INFO: Waiting up to 5m0s for pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30" in namespace "emptydir-5301" to be "Succeeded or Failed"
    Mar  2 14:15:59.350: INFO: Pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30": Phase="Pending", Reason="", readiness=false. Elapsed: 5.802129ms
    Mar  2 14:16:01.358: INFO: Pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30": Phase="Running", Reason="", readiness=true. Elapsed: 2.013866201s
    Mar  2 14:16:03.358: INFO: Pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30": Phase="Running", Reason="", readiness=false. Elapsed: 4.014432441s
    Mar  2 14:16:05.357: INFO: Pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013131151s
    STEP: Saw pod success 03/02/23 14:16:05.358
    Mar  2 14:16:05.358: INFO: Pod "pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30" satisfied condition "Succeeded or Failed"
    Mar  2 14:16:05.363: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30 container test-container: <nil>
    STEP: delete the pod 03/02/23 14:16:05.378
    Mar  2 14:16:05.397: INFO: Waiting for pod pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30 to disappear
    Mar  2 14:16:05.410: INFO: Pod pod-65afdb51-b3a0-487e-a9cb-1c349fef3d30 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 14:16:05.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5301" for this suite. 03/02/23 14:16:05.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:16:05.425
Mar  2 14:16:05.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename replicaset 03/02/23 14:16:05.426
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:16:05.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:16:05.462
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar  2 14:16:05.466: INFO: Creating ReplicaSet my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe
Mar  2 14:16:05.516: INFO: Pod name my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe: Found 1 pods out of 1
Mar  2 14:16:05.516: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe" is running
Mar  2 14:16:05.516: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj" in namespace "replicaset-7976" to be "running"
Mar  2 14:16:05.523: INFO: Pod "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.899108ms
Mar  2 14:16:07.536: INFO: Pod "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj": Phase="Running", Reason="", readiness=true. Elapsed: 2.019559361s
Mar  2 14:16:07.536: INFO: Pod "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj" satisfied condition "running"
Mar  2 14:16:07.536: INFO: Pod "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 14:16:05 +0000 UTC Reason: Message:}])
Mar  2 14:16:07.537: INFO: Trying to dial the pod
Mar  2 14:16:12.566: INFO: Controller my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe: Got expected result from replica 1 [my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj]: "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 14:16:12.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7976" for this suite. 03/02/23 14:16:12.594
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":317,"skipped":5792,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.179 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:16:05.425
    Mar  2 14:16:05.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename replicaset 03/02/23 14:16:05.426
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:16:05.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:16:05.462
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar  2 14:16:05.466: INFO: Creating ReplicaSet my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe
    Mar  2 14:16:05.516: INFO: Pod name my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe: Found 1 pods out of 1
    Mar  2 14:16:05.516: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe" is running
    Mar  2 14:16:05.516: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj" in namespace "replicaset-7976" to be "running"
    Mar  2 14:16:05.523: INFO: Pod "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.899108ms
    Mar  2 14:16:07.536: INFO: Pod "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj": Phase="Running", Reason="", readiness=true. Elapsed: 2.019559361s
    Mar  2 14:16:07.536: INFO: Pod "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj" satisfied condition "running"
    Mar  2 14:16:07.536: INFO: Pod "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 14:16:05 +0000 UTC Reason: Message:}])
    Mar  2 14:16:07.537: INFO: Trying to dial the pod
    Mar  2 14:16:12.566: INFO: Controller my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe: Got expected result from replica 1 [my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj]: "my-hostname-basic-b338e239-81be-44be-bb9f-312e9db52ffe-jhcgj", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 14:16:12.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7976" for this suite. 03/02/23 14:16:12.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:16:12.608
Mar  2 14:16:12.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename security-context-test 03/02/23 14:16:12.61
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:16:12.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:16:12.673
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Mar  2 14:16:12.698: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66" in namespace "security-context-test-5560" to be "Succeeded or Failed"
Mar  2 14:16:12.721: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 23.277969ms
Mar  2 14:16:14.731: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033255419s
Mar  2 14:16:16.729: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030297253s
Mar  2 14:16:18.731: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032553991s
Mar  2 14:16:20.731: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033208248s
Mar  2 14:16:22.729: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030776585s
Mar  2 14:16:24.735: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036401395s
Mar  2 14:16:26.728: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.02978174s
Mar  2 14:16:26.729: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 14:16:26.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5560" for this suite. 03/02/23 14:16:26.75
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":318,"skipped":5799,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.149 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:16:12.608
    Mar  2 14:16:12.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename security-context-test 03/02/23 14:16:12.61
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:16:12.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:16:12.673
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Mar  2 14:16:12.698: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66" in namespace "security-context-test-5560" to be "Succeeded or Failed"
    Mar  2 14:16:12.721: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 23.277969ms
    Mar  2 14:16:14.731: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033255419s
    Mar  2 14:16:16.729: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030297253s
    Mar  2 14:16:18.731: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032553991s
    Mar  2 14:16:20.731: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033208248s
    Mar  2 14:16:22.729: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030776585s
    Mar  2 14:16:24.735: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036401395s
    Mar  2 14:16:26.728: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.02978174s
    Mar  2 14:16:26.729: INFO: Pod "alpine-nnp-false-357d573b-8a0b-4893-b6ae-9645b7b9db66" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 14:16:26.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5560" for this suite. 03/02/23 14:16:26.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:16:26.763
Mar  2 14:16:26.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename secrets 03/02/23 14:16:26.764
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:16:26.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:16:26.799
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-8c919ae9-1643-4260-8cf8-c4780a459a93 03/02/23 14:16:26.809
STEP: Creating secret with name s-test-opt-upd-a40cf999-a72f-4097-806d-f970640b105f 03/02/23 14:16:26.815
STEP: Creating the pod 03/02/23 14:16:26.819
Mar  2 14:16:26.829: INFO: Waiting up to 5m0s for pod "pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17" in namespace "secrets-8479" to be "running and ready"
Mar  2 14:16:26.848: INFO: Pod "pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17": Phase="Pending", Reason="", readiness=false. Elapsed: 18.53608ms
Mar  2 14:16:26.848: INFO: The phase of Pod pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:16:28.870: INFO: Pod "pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041000865s
Mar  2 14:16:28.870: INFO: The phase of Pod pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:16:30.867: INFO: Pod "pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17": Phase="Running", Reason="", readiness=true. Elapsed: 4.037602307s
Mar  2 14:16:30.867: INFO: The phase of Pod pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17 is Running (Ready = true)
Mar  2 14:16:30.867: INFO: Pod "pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-8c919ae9-1643-4260-8cf8-c4780a459a93 03/02/23 14:16:30.892
STEP: Updating secret s-test-opt-upd-a40cf999-a72f-4097-806d-f970640b105f 03/02/23 14:16:30.898
STEP: Creating secret with name s-test-opt-create-a13fe5cd-d058-4384-aa3b-06c112814a2e 03/02/23 14:16:30.906
STEP: waiting to observe update in volume 03/02/23 14:16:30.917
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 14:17:41.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8479" for this suite. 03/02/23 14:17:41.442
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":319,"skipped":5815,"failed":0}
------------------------------
â€¢ [SLOW TEST] [74.685 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:16:26.763
    Mar  2 14:16:26.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename secrets 03/02/23 14:16:26.764
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:16:26.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:16:26.799
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-8c919ae9-1643-4260-8cf8-c4780a459a93 03/02/23 14:16:26.809
    STEP: Creating secret with name s-test-opt-upd-a40cf999-a72f-4097-806d-f970640b105f 03/02/23 14:16:26.815
    STEP: Creating the pod 03/02/23 14:16:26.819
    Mar  2 14:16:26.829: INFO: Waiting up to 5m0s for pod "pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17" in namespace "secrets-8479" to be "running and ready"
    Mar  2 14:16:26.848: INFO: Pod "pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17": Phase="Pending", Reason="", readiness=false. Elapsed: 18.53608ms
    Mar  2 14:16:26.848: INFO: The phase of Pod pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:16:28.870: INFO: Pod "pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041000865s
    Mar  2 14:16:28.870: INFO: The phase of Pod pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:16:30.867: INFO: Pod "pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17": Phase="Running", Reason="", readiness=true. Elapsed: 4.037602307s
    Mar  2 14:16:30.867: INFO: The phase of Pod pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17 is Running (Ready = true)
    Mar  2 14:16:30.867: INFO: Pod "pod-secrets-019e13a6-f09a-43d3-809d-9345829faa17" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-8c919ae9-1643-4260-8cf8-c4780a459a93 03/02/23 14:16:30.892
    STEP: Updating secret s-test-opt-upd-a40cf999-a72f-4097-806d-f970640b105f 03/02/23 14:16:30.898
    STEP: Creating secret with name s-test-opt-create-a13fe5cd-d058-4384-aa3b-06c112814a2e 03/02/23 14:16:30.906
    STEP: waiting to observe update in volume 03/02/23 14:16:30.917
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 14:17:41.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8479" for this suite. 03/02/23 14:17:41.442
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:17:41.467
Mar  2 14:17:41.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename hostport 03/02/23 14:17:41.469
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:17:41.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:17:41.495
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/02/23 14:17:41.505
Mar  2 14:17:41.513: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2707" to be "running and ready"
Mar  2 14:17:41.518: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.265755ms
Mar  2 14:17:41.518: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:17:43.531: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017065268s
Mar  2 14:17:43.531: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:17:45.527: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.013815022s
Mar  2 14:17:45.527: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  2 14:17:45.527: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.16.0.56 on the node which pod1 resides and expect scheduled 03/02/23 14:17:45.527
Mar  2 14:17:45.536: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2707" to be "running and ready"
Mar  2 14:17:45.542: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.568134ms
Mar  2 14:17:45.542: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:17:47.553: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016170759s
Mar  2 14:17:47.553: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:17:49.552: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.01554753s
Mar  2 14:17:49.553: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  2 14:17:49.553: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.16.0.56 but use UDP protocol on the node which pod2 resides 03/02/23 14:17:49.553
Mar  2 14:17:49.568: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2707" to be "running and ready"
Mar  2 14:17:49.572: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.37579ms
Mar  2 14:17:49.573: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:17:51.580: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011312406s
Mar  2 14:17:51.580: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar  2 14:17:51.580: INFO: Pod "pod3" satisfied condition "running and ready"
Mar  2 14:17:51.593: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2707" to be "running and ready"
Mar  2 14:17:51.598: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914247ms
Mar  2 14:17:51.598: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:17:53.610: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.017014549s
Mar  2 14:17:53.610: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar  2 14:17:53.610: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/02/23 14:17:53.616
Mar  2 14:17:53.617: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.0.56 http://127.0.0.1:54323/hostname] Namespace:hostport-2707 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:17:53.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 14:17:53.619: INFO: ExecWithOptions: Clientset creation
Mar  2 14:17:53.619: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2707/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.16.0.56+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.56, port: 54323 03/02/23 14:17:53.735
Mar  2 14:17:53.735: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.0.56:54323/hostname] Namespace:hostport-2707 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:17:53.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 14:17:53.736: INFO: ExecWithOptions: Clientset creation
Mar  2 14:17:53.737: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2707/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.16.0.56%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.56, port: 54323 UDP 03/02/23 14:17:53.843
Mar  2 14:17:53.843: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.16.0.56 54323] Namespace:hostport-2707 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:17:53.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
Mar  2 14:17:53.844: INFO: ExecWithOptions: Clientset creation
Mar  2 14:17:53.844: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2707/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.16.0.56+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Mar  2 14:17:58.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-2707" for this suite. 03/02/23 14:17:58.981
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":320,"skipped":5819,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.543 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:17:41.467
    Mar  2 14:17:41.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename hostport 03/02/23 14:17:41.469
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:17:41.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:17:41.495
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/02/23 14:17:41.505
    Mar  2 14:17:41.513: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2707" to be "running and ready"
    Mar  2 14:17:41.518: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.265755ms
    Mar  2 14:17:41.518: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:17:43.531: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017065268s
    Mar  2 14:17:43.531: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:17:45.527: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.013815022s
    Mar  2 14:17:45.527: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  2 14:17:45.527: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.16.0.56 on the node which pod1 resides and expect scheduled 03/02/23 14:17:45.527
    Mar  2 14:17:45.536: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2707" to be "running and ready"
    Mar  2 14:17:45.542: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.568134ms
    Mar  2 14:17:45.542: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:17:47.553: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016170759s
    Mar  2 14:17:47.553: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:17:49.552: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.01554753s
    Mar  2 14:17:49.553: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  2 14:17:49.553: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.16.0.56 but use UDP protocol on the node which pod2 resides 03/02/23 14:17:49.553
    Mar  2 14:17:49.568: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2707" to be "running and ready"
    Mar  2 14:17:49.572: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.37579ms
    Mar  2 14:17:49.573: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:17:51.580: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011312406s
    Mar  2 14:17:51.580: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar  2 14:17:51.580: INFO: Pod "pod3" satisfied condition "running and ready"
    Mar  2 14:17:51.593: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2707" to be "running and ready"
    Mar  2 14:17:51.598: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914247ms
    Mar  2 14:17:51.598: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:17:53.610: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.017014549s
    Mar  2 14:17:53.610: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar  2 14:17:53.610: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/02/23 14:17:53.616
    Mar  2 14:17:53.617: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.0.56 http://127.0.0.1:54323/hostname] Namespace:hostport-2707 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 14:17:53.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 14:17:53.619: INFO: ExecWithOptions: Clientset creation
    Mar  2 14:17:53.619: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2707/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.16.0.56+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.56, port: 54323 03/02/23 14:17:53.735
    Mar  2 14:17:53.735: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.0.56:54323/hostname] Namespace:hostport-2707 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 14:17:53.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 14:17:53.736: INFO: ExecWithOptions: Clientset creation
    Mar  2 14:17:53.737: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2707/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.16.0.56%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.56, port: 54323 UDP 03/02/23 14:17:53.843
    Mar  2 14:17:53.843: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.16.0.56 54323] Namespace:hostport-2707 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 14:17:53.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    Mar  2 14:17:53.844: INFO: ExecWithOptions: Clientset creation
    Mar  2 14:17:53.844: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2707/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.16.0.56+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Mar  2 14:17:58.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-2707" for this suite. 03/02/23 14:17:58.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:17:59.017
Mar  2 14:17:59.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename daemonsets 03/02/23 14:17:59.031
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:17:59.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:17:59.063
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 03/02/23 14:17:59.133
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 14:17:59.153
Mar  2 14:17:59.175: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:17:59.257: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:17:59.257: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 14:18:00.266: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:18:00.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:18:00.347: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 14:18:01.265: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:18:01.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 14:18:01.272: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
Mar  2 14:18:02.268: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:18:02.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Mar  2 14:18:02.278: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/02/23 14:18:02.287
Mar  2 14:18:02.363: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:18:02.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 14:18:02.406: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
Mar  2 14:18:03.419: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:18:03.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 14:18:03.426: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
Mar  2 14:18:04.417: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:18:04.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Mar  2 14:18:04.422: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/02/23 14:18:04.422
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 14:18:04.437
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8461, will wait for the garbage collector to delete the pods 03/02/23 14:18:04.438
Mar  2 14:18:04.510: INFO: Deleting DaemonSet.extensions daemon-set took: 9.823845ms
Mar  2 14:18:04.610: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.55757ms
Mar  2 14:18:07.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:18:07.420: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 14:18:07.424: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1969849"},"items":null}

Mar  2 14:18:07.428: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1969849"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 14:18:07.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8461" for this suite. 03/02/23 14:18:07.47
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":321,"skipped":5826,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.460 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:17:59.017
    Mar  2 14:17:59.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename daemonsets 03/02/23 14:17:59.031
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:17:59.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:17:59.063
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 03/02/23 14:17:59.133
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 14:17:59.153
    Mar  2 14:17:59.175: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:17:59.257: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:17:59.257: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 14:18:00.266: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:18:00.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:18:00.347: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 14:18:01.265: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:18:01.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 14:18:01.272: INFO: Node aarnq-sc-k8s-node-srv3 is running 0 daemon pod, expected 1
    Mar  2 14:18:02.268: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:18:02.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Mar  2 14:18:02.278: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/02/23 14:18:02.287
    Mar  2 14:18:02.363: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:18:02.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 14:18:02.406: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
    Mar  2 14:18:03.419: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:18:03.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 14:18:03.426: INFO: Node aarnq-sc-k8s-node-srv1 is running 0 daemon pod, expected 1
    Mar  2 14:18:04.417: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:18:04.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Mar  2 14:18:04.422: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/02/23 14:18:04.422
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 14:18:04.437
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8461, will wait for the garbage collector to delete the pods 03/02/23 14:18:04.438
    Mar  2 14:18:04.510: INFO: Deleting DaemonSet.extensions daemon-set took: 9.823845ms
    Mar  2 14:18:04.610: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.55757ms
    Mar  2 14:18:07.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:18:07.420: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 14:18:07.424: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1969849"},"items":null}

    Mar  2 14:18:07.428: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1969849"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 14:18:07.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8461" for this suite. 03/02/23 14:18:07.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:18:07.495
Mar  2 14:18:07.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-probe 03/02/23 14:18:07.503
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:18:07.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:18:07.526
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Mar  2 14:18:07.536: INFO: Waiting up to 5m0s for pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf" in namespace "container-probe-1453" to be "running and ready"
Mar  2 14:18:07.540: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.88881ms
Mar  2 14:18:07.540: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:18:09.558: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 2.021489927s
Mar  2 14:18:09.558: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
Mar  2 14:18:11.549: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 4.012401449s
Mar  2 14:18:11.549: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
Mar  2 14:18:13.549: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 6.01227347s
Mar  2 14:18:13.549: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
Mar  2 14:18:15.552: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 8.015129044s
Mar  2 14:18:15.552: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
Mar  2 14:18:17.555: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 10.018276615s
Mar  2 14:18:17.555: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
Mar  2 14:18:19.552: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 12.015716606s
Mar  2 14:18:19.552: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
Mar  2 14:18:21.552: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 14.015142313s
Mar  2 14:18:21.552: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
Mar  2 14:18:23.547: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 16.010328847s
Mar  2 14:18:23.547: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
Mar  2 14:18:25.548: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 18.011128953s
Mar  2 14:18:25.548: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
Mar  2 14:18:27.549: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 20.012682884s
Mar  2 14:18:27.549: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
Mar  2 14:18:29.547: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=true. Elapsed: 22.010798971s
Mar  2 14:18:29.547: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = true)
Mar  2 14:18:29.547: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf" satisfied condition "running and ready"
Mar  2 14:18:29.552: INFO: Container started at 2023-03-02 14:18:08 +0000 UTC, pod became ready at 2023-03-02 14:18:27 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 14:18:29.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1453" for this suite. 03/02/23 14:18:29.56
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":322,"skipped":5853,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.071 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:18:07.495
    Mar  2 14:18:07.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-probe 03/02/23 14:18:07.503
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:18:07.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:18:07.526
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Mar  2 14:18:07.536: INFO: Waiting up to 5m0s for pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf" in namespace "container-probe-1453" to be "running and ready"
    Mar  2 14:18:07.540: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.88881ms
    Mar  2 14:18:07.540: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:18:09.558: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 2.021489927s
    Mar  2 14:18:09.558: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
    Mar  2 14:18:11.549: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 4.012401449s
    Mar  2 14:18:11.549: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
    Mar  2 14:18:13.549: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 6.01227347s
    Mar  2 14:18:13.549: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
    Mar  2 14:18:15.552: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 8.015129044s
    Mar  2 14:18:15.552: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
    Mar  2 14:18:17.555: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 10.018276615s
    Mar  2 14:18:17.555: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
    Mar  2 14:18:19.552: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 12.015716606s
    Mar  2 14:18:19.552: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
    Mar  2 14:18:21.552: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 14.015142313s
    Mar  2 14:18:21.552: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
    Mar  2 14:18:23.547: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 16.010328847s
    Mar  2 14:18:23.547: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
    Mar  2 14:18:25.548: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 18.011128953s
    Mar  2 14:18:25.548: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
    Mar  2 14:18:27.549: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=false. Elapsed: 20.012682884s
    Mar  2 14:18:27.549: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = false)
    Mar  2 14:18:29.547: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf": Phase="Running", Reason="", readiness=true. Elapsed: 22.010798971s
    Mar  2 14:18:29.547: INFO: The phase of Pod test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf is Running (Ready = true)
    Mar  2 14:18:29.547: INFO: Pod "test-webserver-f5e5221a-808b-4176-bdda-312d3459aacf" satisfied condition "running and ready"
    Mar  2 14:18:29.552: INFO: Container started at 2023-03-02 14:18:08 +0000 UTC, pod became ready at 2023-03-02 14:18:27 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 14:18:29.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1453" for this suite. 03/02/23 14:18:29.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:18:29.571
Mar  2 14:18:29.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename job 03/02/23 14:18:29.574
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:18:29.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:18:29.601
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 03/02/23 14:18:29.604
STEP: Ensuring active pods == parallelism 03/02/23 14:18:29.609
STEP: delete a job 03/02/23 14:18:33.619
STEP: deleting Job.batch foo in namespace job-5022, will wait for the garbage collector to delete the pods 03/02/23 14:18:33.62
Mar  2 14:18:33.692: INFO: Deleting Job.batch foo took: 17.09457ms
Mar  2 14:18:33.793: INFO: Terminating Job.batch foo pods took: 101.274465ms
STEP: Ensuring job was deleted 03/02/23 14:19:05.397
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 14:19:05.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5022" for this suite. 03/02/23 14:19:05.414
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":323,"skipped":5859,"failed":0}
------------------------------
â€¢ [SLOW TEST] [35.852 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:18:29.571
    Mar  2 14:18:29.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename job 03/02/23 14:18:29.574
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:18:29.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:18:29.601
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 03/02/23 14:18:29.604
    STEP: Ensuring active pods == parallelism 03/02/23 14:18:29.609
    STEP: delete a job 03/02/23 14:18:33.619
    STEP: deleting Job.batch foo in namespace job-5022, will wait for the garbage collector to delete the pods 03/02/23 14:18:33.62
    Mar  2 14:18:33.692: INFO: Deleting Job.batch foo took: 17.09457ms
    Mar  2 14:18:33.793: INFO: Terminating Job.batch foo pods took: 101.274465ms
    STEP: Ensuring job was deleted 03/02/23 14:19:05.397
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 14:19:05.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5022" for this suite. 03/02/23 14:19:05.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:19:05.428
Mar  2 14:19:05.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sched-preemption 03/02/23 14:19:05.429
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:19:05.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:19:05.452
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 14:19:05.469: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 14:20:05.770: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:20:05.783
Mar  2 14:20:05.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sched-preemption-path 03/02/23 14:20:05.785
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:05.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:05.836
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 03/02/23 14:20:05.841
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 14:20:05.841
Mar  2 14:20:05.865: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5274" to be "running"
Mar  2 14:20:05.899: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 33.876007ms
Mar  2 14:20:07.913: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047931829s
Mar  2 14:20:09.910: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.045682525s
Mar  2 14:20:09.911: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 14:20:09.917
Mar  2 14:20:09.942: INFO: found a healthy node: aarnq-sc-k8s-node-srv2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Mar  2 14:20:22.096: INFO: pods created so far: [1 1 1]
Mar  2 14:20:22.097: INFO: length of pods created so far: 3
Mar  2 14:20:26.122: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Mar  2 14:20:33.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5274" for this suite. 03/02/23 14:20:33.134
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  2 14:20:33.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7090" for this suite. 03/02/23 14:20:33.181
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":324,"skipped":5906,"failed":0}
------------------------------
â€¢ [SLOW TEST] [87.854 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:19:05.428
    Mar  2 14:19:05.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sched-preemption 03/02/23 14:19:05.429
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:19:05.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:19:05.452
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  2 14:19:05.469: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 14:20:05.770: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:20:05.783
    Mar  2 14:20:05.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sched-preemption-path 03/02/23 14:20:05.785
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:05.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:05.836
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 03/02/23 14:20:05.841
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 14:20:05.841
    Mar  2 14:20:05.865: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5274" to be "running"
    Mar  2 14:20:05.899: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 33.876007ms
    Mar  2 14:20:07.913: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047931829s
    Mar  2 14:20:09.910: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.045682525s
    Mar  2 14:20:09.911: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 14:20:09.917
    Mar  2 14:20:09.942: INFO: found a healthy node: aarnq-sc-k8s-node-srv2
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Mar  2 14:20:22.096: INFO: pods created so far: [1 1 1]
    Mar  2 14:20:22.097: INFO: length of pods created so far: 3
    Mar  2 14:20:26.122: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Mar  2 14:20:33.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-5274" for this suite. 03/02/23 14:20:33.134
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 14:20:33.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7090" for this suite. 03/02/23 14:20:33.181
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:20:33.292
Mar  2 14:20:33.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename replication-controller 03/02/23 14:20:33.293
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:33.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:33.324
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 03/02/23 14:20:33.33
STEP: waiting for RC to be added 03/02/23 14:20:33.336
STEP: waiting for available Replicas 03/02/23 14:20:33.336
STEP: patching ReplicationController 03/02/23 14:20:34.766
STEP: waiting for RC to be modified 03/02/23 14:20:34.783
STEP: patching ReplicationController status 03/02/23 14:20:34.783
STEP: waiting for RC to be modified 03/02/23 14:20:34.799
STEP: waiting for available Replicas 03/02/23 14:20:34.799
STEP: fetching ReplicationController status 03/02/23 14:20:34.799
STEP: patching ReplicationController scale 03/02/23 14:20:34.805
STEP: waiting for RC to be modified 03/02/23 14:20:34.811
STEP: waiting for ReplicationController's scale to be the max amount 03/02/23 14:20:34.811
STEP: fetching ReplicationController; ensuring that it's patched 03/02/23 14:20:37.22
STEP: updating ReplicationController status 03/02/23 14:20:37.227
STEP: waiting for RC to be modified 03/02/23 14:20:37.243
STEP: listing all ReplicationControllers 03/02/23 14:20:37.244
STEP: checking that ReplicationController has expected values 03/02/23 14:20:37.247
STEP: deleting ReplicationControllers by collection 03/02/23 14:20:37.248
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/02/23 14:20:37.257
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  2 14:20:37.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5663" for this suite. 03/02/23 14:20:37.355
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":325,"skipped":5927,"failed":0}
------------------------------
â€¢ [4.071 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:20:33.292
    Mar  2 14:20:33.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename replication-controller 03/02/23 14:20:33.293
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:33.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:33.324
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 03/02/23 14:20:33.33
    STEP: waiting for RC to be added 03/02/23 14:20:33.336
    STEP: waiting for available Replicas 03/02/23 14:20:33.336
    STEP: patching ReplicationController 03/02/23 14:20:34.766
    STEP: waiting for RC to be modified 03/02/23 14:20:34.783
    STEP: patching ReplicationController status 03/02/23 14:20:34.783
    STEP: waiting for RC to be modified 03/02/23 14:20:34.799
    STEP: waiting for available Replicas 03/02/23 14:20:34.799
    STEP: fetching ReplicationController status 03/02/23 14:20:34.799
    STEP: patching ReplicationController scale 03/02/23 14:20:34.805
    STEP: waiting for RC to be modified 03/02/23 14:20:34.811
    STEP: waiting for ReplicationController's scale to be the max amount 03/02/23 14:20:34.811
    STEP: fetching ReplicationController; ensuring that it's patched 03/02/23 14:20:37.22
    STEP: updating ReplicationController status 03/02/23 14:20:37.227
    STEP: waiting for RC to be modified 03/02/23 14:20:37.243
    STEP: listing all ReplicationControllers 03/02/23 14:20:37.244
    STEP: checking that ReplicationController has expected values 03/02/23 14:20:37.247
    STEP: deleting ReplicationControllers by collection 03/02/23 14:20:37.248
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/02/23 14:20:37.257
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  2 14:20:37.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5663" for this suite. 03/02/23 14:20:37.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:20:37.366
Mar  2 14:20:37.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename lease-test 03/02/23 14:20:37.368
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:37.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:37.387
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Mar  2 14:20:37.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-742" for this suite. 03/02/23 14:20:37.438
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":326,"skipped":5937,"failed":0}
------------------------------
â€¢ [0.080 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:20:37.366
    Mar  2 14:20:37.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename lease-test 03/02/23 14:20:37.368
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:37.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:37.387
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Mar  2 14:20:37.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-742" for this suite. 03/02/23 14:20:37.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:20:37.456
Mar  2 14:20:37.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename job 03/02/23 14:20:37.457
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:37.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:37.491
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 03/02/23 14:20:37.506
STEP: Patching the Job 03/02/23 14:20:37.511
STEP: Watching for Job to be patched 03/02/23 14:20:37.535
Mar  2 14:20:37.537: INFO: Event ADDED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar  2 14:20:37.538: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar  2 14:20:37.538: INFO: Event MODIFIED found for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/02/23 14:20:37.538
STEP: Watching for Job to be updated 03/02/23 14:20:37.546
Mar  2 14:20:37.550: INFO: Event MODIFIED found for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:37.550: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/02/23 14:20:37.55
Mar  2 14:20:37.561: INFO: Job: e2e-57flk as labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk]
STEP: Waiting for job to complete 03/02/23 14:20:37.561
STEP: Delete a job collection with a labelselector 03/02/23 14:20:51.569
STEP: Watching for Job to be deleted 03/02/23 14:20:51.579
Mar  2 14:20:51.584: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:51.584: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:51.585: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:51.585: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:51.585: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:51.586: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:51.586: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:51.586: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:51.587: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:51.587: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:51.587: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 14:20:51.587: INFO: Event DELETED found for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/02/23 14:20:51.587
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 14:20:51.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9597" for this suite. 03/02/23 14:20:51.6
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":327,"skipped":5972,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.164 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:20:37.456
    Mar  2 14:20:37.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename job 03/02/23 14:20:37.457
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:37.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:37.491
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 03/02/23 14:20:37.506
    STEP: Patching the Job 03/02/23 14:20:37.511
    STEP: Watching for Job to be patched 03/02/23 14:20:37.535
    Mar  2 14:20:37.537: INFO: Event ADDED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar  2 14:20:37.538: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar  2 14:20:37.538: INFO: Event MODIFIED found for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/02/23 14:20:37.538
    STEP: Watching for Job to be updated 03/02/23 14:20:37.546
    Mar  2 14:20:37.550: INFO: Event MODIFIED found for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:37.550: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/02/23 14:20:37.55
    Mar  2 14:20:37.561: INFO: Job: e2e-57flk as labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk]
    STEP: Waiting for job to complete 03/02/23 14:20:37.561
    STEP: Delete a job collection with a labelselector 03/02/23 14:20:51.569
    STEP: Watching for Job to be deleted 03/02/23 14:20:51.579
    Mar  2 14:20:51.584: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:51.584: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:51.585: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:51.585: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:51.585: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:51.586: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:51.586: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:51.586: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:51.587: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:51.587: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:51.587: INFO: Event MODIFIED observed for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 14:20:51.587: INFO: Event DELETED found for Job e2e-57flk in namespace job-9597 with labels: map[e2e-57flk:patched e2e-job-label:e2e-57flk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/02/23 14:20:51.587
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 14:20:51.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9597" for this suite. 03/02/23 14:20:51.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:20:51.643
Mar  2 14:20:51.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename tables 03/02/23 14:20:51.644
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:51.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:51.665
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Mar  2 14:20:51.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-178" for this suite. 03/02/23 14:20:51.678
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":328,"skipped":5998,"failed":0}
------------------------------
â€¢ [0.040 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:20:51.643
    Mar  2 14:20:51.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename tables 03/02/23 14:20:51.644
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:51.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:51.665
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Mar  2 14:20:51.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-178" for this suite. 03/02/23 14:20:51.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:20:51.69
Mar  2 14:20:51.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename svcaccounts 03/02/23 14:20:51.697
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:51.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:51.717
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 03/02/23 14:20:51.721
STEP: watching for the ServiceAccount to be added 03/02/23 14:20:51.728
STEP: patching the ServiceAccount 03/02/23 14:20:51.73
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/02/23 14:20:51.737
STEP: deleting the ServiceAccount 03/02/23 14:20:51.745
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 14:20:51.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-951" for this suite. 03/02/23 14:20:51.757
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":329,"skipped":6010,"failed":0}
------------------------------
â€¢ [0.077 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:20:51.69
    Mar  2 14:20:51.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 14:20:51.697
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:51.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:51.717
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 03/02/23 14:20:51.721
    STEP: watching for the ServiceAccount to be added 03/02/23 14:20:51.728
    STEP: patching the ServiceAccount 03/02/23 14:20:51.73
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/02/23 14:20:51.737
    STEP: deleting the ServiceAccount 03/02/23 14:20:51.745
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 14:20:51.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-951" for this suite. 03/02/23 14:20:51.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:20:51.78
Mar  2 14:20:51.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 14:20:51.781
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:51.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:51.802
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Mar  2 14:20:51.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 14:21:02.829
Mar  2 14:21:02.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-6728 --namespace=crd-publish-openapi-6728 create -f -'
Mar  2 14:21:03.871: INFO: stderr: ""
Mar  2 14:21:03.871: INFO: stdout: "e2e-test-crd-publish-openapi-9118-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 14:21:03.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-6728 --namespace=crd-publish-openapi-6728 delete e2e-test-crd-publish-openapi-9118-crds test-cr'
Mar  2 14:21:03.966: INFO: stderr: ""
Mar  2 14:21:03.966: INFO: stdout: "e2e-test-crd-publish-openapi-9118-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  2 14:21:03.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-6728 --namespace=crd-publish-openapi-6728 apply -f -'
Mar  2 14:21:04.314: INFO: stderr: ""
Mar  2 14:21:04.314: INFO: stdout: "e2e-test-crd-publish-openapi-9118-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 14:21:04.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-6728 --namespace=crd-publish-openapi-6728 delete e2e-test-crd-publish-openapi-9118-crds test-cr'
Mar  2 14:21:04.401: INFO: stderr: ""
Mar  2 14:21:04.401: INFO: stdout: "e2e-test-crd-publish-openapi-9118-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/02/23 14:21:04.401
Mar  2 14:21:04.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-6728 explain e2e-test-crd-publish-openapi-9118-crds'
Mar  2 14:21:04.749: INFO: stderr: ""
Mar  2 14:21:04.749: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9118-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 14:21:10.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6728" for this suite. 03/02/23 14:21:10.01
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":330,"skipped":6026,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.238 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:20:51.78
    Mar  2 14:20:51.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 14:20:51.781
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:20:51.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:20:51.802
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Mar  2 14:20:51.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 14:21:02.829
    Mar  2 14:21:02.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-6728 --namespace=crd-publish-openapi-6728 create -f -'
    Mar  2 14:21:03.871: INFO: stderr: ""
    Mar  2 14:21:03.871: INFO: stdout: "e2e-test-crd-publish-openapi-9118-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar  2 14:21:03.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-6728 --namespace=crd-publish-openapi-6728 delete e2e-test-crd-publish-openapi-9118-crds test-cr'
    Mar  2 14:21:03.966: INFO: stderr: ""
    Mar  2 14:21:03.966: INFO: stdout: "e2e-test-crd-publish-openapi-9118-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar  2 14:21:03.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-6728 --namespace=crd-publish-openapi-6728 apply -f -'
    Mar  2 14:21:04.314: INFO: stderr: ""
    Mar  2 14:21:04.314: INFO: stdout: "e2e-test-crd-publish-openapi-9118-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar  2 14:21:04.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-6728 --namespace=crd-publish-openapi-6728 delete e2e-test-crd-publish-openapi-9118-crds test-cr'
    Mar  2 14:21:04.401: INFO: stderr: ""
    Mar  2 14:21:04.401: INFO: stdout: "e2e-test-crd-publish-openapi-9118-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/02/23 14:21:04.401
    Mar  2 14:21:04.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-6728 explain e2e-test-crd-publish-openapi-9118-crds'
    Mar  2 14:21:04.749: INFO: stderr: ""
    Mar  2 14:21:04.749: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9118-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 14:21:10.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6728" for this suite. 03/02/23 14:21:10.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:21:10.021
Mar  2 14:21:10.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename aggregator 03/02/23 14:21:10.022
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:21:10.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:21:10.049
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar  2 14:21:10.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/02/23 14:21:10.055
Mar  2 14:21:10.522: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Mar  2 14:21:12.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:14.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:16.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:18.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:20.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:22.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:24.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:26.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:28.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:30.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:32.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:34.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:21:36.761: INFO: Waited 129.724599ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 03/02/23 14:21:36.874
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/02/23 14:21:36.877
STEP: List APIServices 03/02/23 14:21:36.884
Mar  2 14:21:36.892: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Mar  2 14:21:37.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-790" for this suite. 03/02/23 14:21:37.5
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":331,"skipped":6081,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.531 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:21:10.021
    Mar  2 14:21:10.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename aggregator 03/02/23 14:21:10.022
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:21:10.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:21:10.049
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar  2 14:21:10.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/02/23 14:21:10.055
    Mar  2 14:21:10.522: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Mar  2 14:21:12.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:14.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:16.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:18.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:20.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:22.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:24.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:26.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:28.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:30.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:32.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:34.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 21, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:21:36.761: INFO: Waited 129.724599ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 03/02/23 14:21:36.874
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/02/23 14:21:36.877
    STEP: List APIServices 03/02/23 14:21:36.884
    Mar  2 14:21:36.892: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Mar  2 14:21:37.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-790" for this suite. 03/02/23 14:21:37.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:21:37.557
Mar  2 14:21:37.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename statefulset 03/02/23 14:21:37.563
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:21:37.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:21:37.617
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7656 03/02/23 14:21:37.621
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Mar  2 14:21:37.668: INFO: Found 0 stateful pods, waiting for 1
Mar  2 14:21:47.673: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/02/23 14:21:47.681
W0302 14:21:47.690497      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  2 14:21:47.701: INFO: Found 1 stateful pods, waiting for 2
Mar  2 14:21:57.715: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 14:21:57.715: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/02/23 14:21:57.729
STEP: Delete all of the StatefulSets 03/02/23 14:21:57.74
STEP: Verify that StatefulSets have been deleted 03/02/23 14:21:57.747
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 14:21:57.753: INFO: Deleting all statefulset in ns statefulset-7656
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 14:21:57.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7656" for this suite. 03/02/23 14:21:57.837
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":332,"skipped":6175,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.293 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:21:37.557
    Mar  2 14:21:37.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename statefulset 03/02/23 14:21:37.563
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:21:37.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:21:37.617
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7656 03/02/23 14:21:37.621
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Mar  2 14:21:37.668: INFO: Found 0 stateful pods, waiting for 1
    Mar  2 14:21:47.673: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/02/23 14:21:47.681
    W0302 14:21:47.690497      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  2 14:21:47.701: INFO: Found 1 stateful pods, waiting for 2
    Mar  2 14:21:57.715: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 14:21:57.715: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/02/23 14:21:57.729
    STEP: Delete all of the StatefulSets 03/02/23 14:21:57.74
    STEP: Verify that StatefulSets have been deleted 03/02/23 14:21:57.747
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 14:21:57.753: INFO: Deleting all statefulset in ns statefulset-7656
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 14:21:57.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7656" for this suite. 03/02/23 14:21:57.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:21:57.857
Mar  2 14:21:57.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename certificates 03/02/23 14:21:57.863
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:21:57.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:21:57.89
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/02/23 14:21:59.496
STEP: getting /apis/certificates.k8s.io 03/02/23 14:21:59.519
STEP: getting /apis/certificates.k8s.io/v1 03/02/23 14:21:59.522
STEP: creating 03/02/23 14:21:59.523
STEP: getting 03/02/23 14:21:59.553
STEP: listing 03/02/23 14:21:59.555
STEP: watching 03/02/23 14:21:59.558
Mar  2 14:21:59.559: INFO: starting watch
STEP: patching 03/02/23 14:21:59.56
STEP: updating 03/02/23 14:21:59.571
Mar  2 14:21:59.581: INFO: waiting for watch events with expected annotations
Mar  2 14:21:59.582: INFO: saw patched and updated annotations
STEP: getting /approval 03/02/23 14:21:59.582
STEP: patching /approval 03/02/23 14:21:59.589
STEP: updating /approval 03/02/23 14:21:59.601
STEP: getting /status 03/02/23 14:21:59.609
STEP: patching /status 03/02/23 14:21:59.614
STEP: updating /status 03/02/23 14:21:59.624
STEP: deleting 03/02/23 14:21:59.63
STEP: deleting a collection 03/02/23 14:21:59.648
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 14:21:59.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3055" for this suite. 03/02/23 14:21:59.692
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":333,"skipped":6219,"failed":0}
------------------------------
â€¢ [1.843 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:21:57.857
    Mar  2 14:21:57.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename certificates 03/02/23 14:21:57.863
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:21:57.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:21:57.89
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/02/23 14:21:59.496
    STEP: getting /apis/certificates.k8s.io 03/02/23 14:21:59.519
    STEP: getting /apis/certificates.k8s.io/v1 03/02/23 14:21:59.522
    STEP: creating 03/02/23 14:21:59.523
    STEP: getting 03/02/23 14:21:59.553
    STEP: listing 03/02/23 14:21:59.555
    STEP: watching 03/02/23 14:21:59.558
    Mar  2 14:21:59.559: INFO: starting watch
    STEP: patching 03/02/23 14:21:59.56
    STEP: updating 03/02/23 14:21:59.571
    Mar  2 14:21:59.581: INFO: waiting for watch events with expected annotations
    Mar  2 14:21:59.582: INFO: saw patched and updated annotations
    STEP: getting /approval 03/02/23 14:21:59.582
    STEP: patching /approval 03/02/23 14:21:59.589
    STEP: updating /approval 03/02/23 14:21:59.601
    STEP: getting /status 03/02/23 14:21:59.609
    STEP: patching /status 03/02/23 14:21:59.614
    STEP: updating /status 03/02/23 14:21:59.624
    STEP: deleting 03/02/23 14:21:59.63
    STEP: deleting a collection 03/02/23 14:21:59.648
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 14:21:59.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-3055" for this suite. 03/02/23 14:21:59.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:21:59.715
Mar  2 14:21:59.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 14:21:59.718
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:21:59.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:21:59.742
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Mar  2 14:21:59.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 create -f -'
Mar  2 14:22:01.461: INFO: stderr: ""
Mar  2 14:22:01.461: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar  2 14:22:01.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 create -f -'
Mar  2 14:22:01.922: INFO: stderr: ""
Mar  2 14:22:01.922: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/02/23 14:22:01.922
Mar  2 14:22:02.930: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 14:22:02.930: INFO: Found 0 / 1
Mar  2 14:22:03.928: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 14:22:03.928: INFO: Found 0 / 1
Mar  2 14:22:04.927: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 14:22:04.927: INFO: Found 1 / 1
Mar  2 14:22:04.928: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 14:22:04.930: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 14:22:04.930: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 14:22:04.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 describe pod agnhost-primary-wfbwz'
Mar  2 14:22:05.131: INFO: stderr: ""
Mar  2 14:22:05.132: INFO: stdout: "Name:             agnhost-primary-wfbwz\nNamespace:        kubectl-6697\nPriority:         0\nService Account:  default\nNode:             aarnq-sc-k8s-node-srv2/172.16.0.192\nStart Time:       Thu, 02 Mar 2023 14:22:01 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 60143b0b118a3b2d18c9b4038c25d7b66cfe3d51b9b440c650753a824e2b06ac\n                  cni.projectcalico.org/podIP: 10.233.123.81/32\n                  cni.projectcalico.org/podIPs: 10.233.123.81/32\nStatus:           Running\nIP:               10.233.123.81\nIPs:\n  IP:           10.233.123.81\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://aee6f72f151eb4433effd666fe85a4deffb6970d3e1881ad3243f1861aaeba7c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 02 Mar 2023 14:22:03 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qgsmf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qgsmf:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-6697/agnhost-primary-wfbwz to aarnq-sc-k8s-node-srv2\n  Normal  Pulled     3s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Mar  2 14:22:05.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 describe rc agnhost-primary'
Mar  2 14:22:05.334: INFO: stderr: ""
Mar  2 14:22:05.334: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6697\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-wfbwz\n"
Mar  2 14:22:05.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 describe service agnhost-primary'
Mar  2 14:22:05.534: INFO: stderr: ""
Mar  2 14:22:05.535: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6697\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.7.56\nIPs:               10.233.7.56\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.123.81:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  2 14:22:05.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 describe node aarnq-sc-k8s-ctl0'
Mar  2 14:22:05.871: INFO: stderr: ""
Mar  2 14:22:05.871: INFO: stdout: "Name:               aarnq-sc-k8s-ctl0\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=s.2C4GB50\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=Kna1\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=aarnq-sc-k8s-ctl0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=s.2C4GB50\n                    topology.kubernetes.io/region=Kna1\n                    topology.kubernetes.io/zone=nova\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 172.16.0.114\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.16.0.114/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.233.103.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 24 Feb 2023 13:33:14 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\n                    node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  aarnq-sc-k8s-ctl0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 02 Mar 2023 14:21:59 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 25 Feb 2023 07:11:28 +0000   Sat, 25 Feb 2023 07:11:28 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 02 Mar 2023 14:22:00 +0000   Fri, 24 Feb 2023 13:33:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 02 Mar 2023 14:22:00 +0000   Fri, 24 Feb 2023 13:33:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 02 Mar 2023 14:22:00 +0000   Fri, 24 Feb 2023 13:33:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 02 Mar 2023 14:22:00 +0000   Fri, 24 Feb 2023 13:40:33 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.16.0.114\n  ExternalIP:  188.95.226.198\n  Hostname:    aarnq-sc-k8s-ctl0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  50620216Ki\n  hugepages-2Mi:      0\n  memory:             4026080Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  46651590989\n  hugepages-2Mi:      0\n  memory:             3923680Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 bb53872a07804c94928d2e70763d6b9c\n  System UUID:                bb53872a-0780-4c94-928d-2e70763d6b9c\n  Boot ID:                    30bddef1-9d21-449e-8619-3acdec176724\n  Kernel Version:             5.4.0-139-generic\n  OS Image:                   Ubuntu 20.04 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.12\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   openstack:///bb53872a-0780-4c94-928d-2e70763d6b9c\nNon-terminated Pods:          (18 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  falco                       falco-exporter-q5krs                                       10m (0%)      100m (5%)   8Mi (0%)         16Mi (0%)      2d22h\n  falco                       falco-t84hj                                                100m (5%)     400m (20%)  48Mi (1%)        96Mi (2%)      2d22h\n  fluentd-system              fluentd-forwarder-9pzgf                                    200m (10%)    500m (25%)  300Mi (7%)       572Mi (14%)    2d22h\n  kube-system                 calico-accountant-nljfd                                    50m (2%)      250m (12%)  32Mi (0%)        64Mi (1%)      3d\n  kube-system                 calico-kube-controllers-75748cc9fd-6565c                   30m (1%)      1 (50%)     64M (1%)         256M (6%)      3d4h\n  kube-system                 calico-node-bnj9b                                          150m (7%)     300m (15%)  64M (1%)         500M (12%)     6d\n  kube-system                 coredns-588bb58b94-k4bl7                                   100m (5%)     0 (0%)      70Mi (1%)        300Mi (7%)     4d7h\n  kube-system                 dns-autoscaler-5b9959d7fc-ndx58                            20m (1%)      0 (0%)      10Mi (0%)        0 (0%)         4d6h\n  kube-system                 etcd-aarnq-sc-k8s-ctl0                                     100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         6d\n  kube-system                 kube-apiserver-aarnq-sc-k8s-ctl0                           250m (12%)    0 (0%)      0 (0%)           0 (0%)         6d\n  kube-system                 kube-controller-manager-aarnq-sc-k8s-ctl0                  200m (10%)    0 (0%)      0 (0%)           0 (0%)         6d\n  kube-system                 kube-proxy-xdwlf                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d1h\n  kube-system                 kube-scheduler-aarnq-sc-k8s-ctl0                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         6d\n  kube-system                 node-local-dns-tvdns                                       25m (1%)      0 (0%)      40Mi (1%)        0 (0%)         5d23h\n  kube-system                 openstack-cloud-controller-manager-pwsbh                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         3d\n  kured                       kured-pjz8b                                                10m (0%)      200m (10%)  16Mi (0%)        32M (0%)       2d22h\n  monitoring                  kube-prometheus-stack-prometheus-node-exporter-ltt5s       50m (2%)      200m (10%)  16Mi (0%)        32Mi (0%)      3d\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-prp2d    0 (0%)        0 (0%)      0 (0%)           0 (0%)         106m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                1595m (79%)     2950m (147%)\n  memory             780360Ki (19%)  1920462080 (47%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
Mar  2 14:22:05.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 describe namespace kubectl-6697'
Mar  2 14:22:06.131: INFO: stderr: ""
Mar  2 14:22:06.131: INFO: stdout: "Name:         kubectl-6697\nLabels:       e2e-framework=kubectl\n              e2e-run=21912a32-1801-4cee-be61-4faef0396552\n              kubernetes.io/metadata.name=kubectl-6697\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 14:22:06.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6697" for this suite. 03/02/23 14:22:06.139
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":334,"skipped":6247,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.432 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:21:59.715
    Mar  2 14:21:59.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 14:21:59.718
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:21:59.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:21:59.742
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Mar  2 14:21:59.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 create -f -'
    Mar  2 14:22:01.461: INFO: stderr: ""
    Mar  2 14:22:01.461: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar  2 14:22:01.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 create -f -'
    Mar  2 14:22:01.922: INFO: stderr: ""
    Mar  2 14:22:01.922: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/02/23 14:22:01.922
    Mar  2 14:22:02.930: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 14:22:02.930: INFO: Found 0 / 1
    Mar  2 14:22:03.928: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 14:22:03.928: INFO: Found 0 / 1
    Mar  2 14:22:04.927: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 14:22:04.927: INFO: Found 1 / 1
    Mar  2 14:22:04.928: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar  2 14:22:04.930: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 14:22:04.930: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  2 14:22:04.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 describe pod agnhost-primary-wfbwz'
    Mar  2 14:22:05.131: INFO: stderr: ""
    Mar  2 14:22:05.132: INFO: stdout: "Name:             agnhost-primary-wfbwz\nNamespace:        kubectl-6697\nPriority:         0\nService Account:  default\nNode:             aarnq-sc-k8s-node-srv2/172.16.0.192\nStart Time:       Thu, 02 Mar 2023 14:22:01 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 60143b0b118a3b2d18c9b4038c25d7b66cfe3d51b9b440c650753a824e2b06ac\n                  cni.projectcalico.org/podIP: 10.233.123.81/32\n                  cni.projectcalico.org/podIPs: 10.233.123.81/32\nStatus:           Running\nIP:               10.233.123.81\nIPs:\n  IP:           10.233.123.81\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://aee6f72f151eb4433effd666fe85a4deffb6970d3e1881ad3243f1861aaeba7c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 02 Mar 2023 14:22:03 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qgsmf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qgsmf:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-6697/agnhost-primary-wfbwz to aarnq-sc-k8s-node-srv2\n  Normal  Pulled     3s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Mar  2 14:22:05.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 describe rc agnhost-primary'
    Mar  2 14:22:05.334: INFO: stderr: ""
    Mar  2 14:22:05.334: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6697\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-wfbwz\n"
    Mar  2 14:22:05.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 describe service agnhost-primary'
    Mar  2 14:22:05.534: INFO: stderr: ""
    Mar  2 14:22:05.535: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6697\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.7.56\nIPs:               10.233.7.56\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.123.81:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar  2 14:22:05.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 describe node aarnq-sc-k8s-ctl0'
    Mar  2 14:22:05.871: INFO: stderr: ""
    Mar  2 14:22:05.871: INFO: stdout: "Name:               aarnq-sc-k8s-ctl0\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=s.2C4GB50\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=Kna1\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=aarnq-sc-k8s-ctl0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=s.2C4GB50\n                    topology.kubernetes.io/region=Kna1\n                    topology.kubernetes.io/zone=nova\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 172.16.0.114\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.16.0.114/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.233.103.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 24 Feb 2023 13:33:14 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\n                    node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  aarnq-sc-k8s-ctl0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 02 Mar 2023 14:21:59 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 25 Feb 2023 07:11:28 +0000   Sat, 25 Feb 2023 07:11:28 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 02 Mar 2023 14:22:00 +0000   Fri, 24 Feb 2023 13:33:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 02 Mar 2023 14:22:00 +0000   Fri, 24 Feb 2023 13:33:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 02 Mar 2023 14:22:00 +0000   Fri, 24 Feb 2023 13:33:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 02 Mar 2023 14:22:00 +0000   Fri, 24 Feb 2023 13:40:33 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.16.0.114\n  ExternalIP:  188.95.226.198\n  Hostname:    aarnq-sc-k8s-ctl0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  50620216Ki\n  hugepages-2Mi:      0\n  memory:             4026080Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  46651590989\n  hugepages-2Mi:      0\n  memory:             3923680Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 bb53872a07804c94928d2e70763d6b9c\n  System UUID:                bb53872a-0780-4c94-928d-2e70763d6b9c\n  Boot ID:                    30bddef1-9d21-449e-8619-3acdec176724\n  Kernel Version:             5.4.0-139-generic\n  OS Image:                   Ubuntu 20.04 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.12\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   openstack:///bb53872a-0780-4c94-928d-2e70763d6b9c\nNon-terminated Pods:          (18 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  falco                       falco-exporter-q5krs                                       10m (0%)      100m (5%)   8Mi (0%)         16Mi (0%)      2d22h\n  falco                       falco-t84hj                                                100m (5%)     400m (20%)  48Mi (1%)        96Mi (2%)      2d22h\n  fluentd-system              fluentd-forwarder-9pzgf                                    200m (10%)    500m (25%)  300Mi (7%)       572Mi (14%)    2d22h\n  kube-system                 calico-accountant-nljfd                                    50m (2%)      250m (12%)  32Mi (0%)        64Mi (1%)      3d\n  kube-system                 calico-kube-controllers-75748cc9fd-6565c                   30m (1%)      1 (50%)     64M (1%)         256M (6%)      3d4h\n  kube-system                 calico-node-bnj9b                                          150m (7%)     300m (15%)  64M (1%)         500M (12%)     6d\n  kube-system                 coredns-588bb58b94-k4bl7                                   100m (5%)     0 (0%)      70Mi (1%)        300Mi (7%)     4d7h\n  kube-system                 dns-autoscaler-5b9959d7fc-ndx58                            20m (1%)      0 (0%)      10Mi (0%)        0 (0%)         4d6h\n  kube-system                 etcd-aarnq-sc-k8s-ctl0                                     100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         6d\n  kube-system                 kube-apiserver-aarnq-sc-k8s-ctl0                           250m (12%)    0 (0%)      0 (0%)           0 (0%)         6d\n  kube-system                 kube-controller-manager-aarnq-sc-k8s-ctl0                  200m (10%)    0 (0%)      0 (0%)           0 (0%)         6d\n  kube-system                 kube-proxy-xdwlf                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d1h\n  kube-system                 kube-scheduler-aarnq-sc-k8s-ctl0                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         6d\n  kube-system                 node-local-dns-tvdns                                       25m (1%)      0 (0%)      40Mi (1%)        0 (0%)         5d23h\n  kube-system                 openstack-cloud-controller-manager-pwsbh                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         3d\n  kured                       kured-pjz8b                                                10m (0%)      200m (10%)  16Mi (0%)        32M (0%)       2d22h\n  monitoring                  kube-prometheus-stack-prometheus-node-exporter-ltt5s       50m (2%)      200m (10%)  16Mi (0%)        32Mi (0%)      3d\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-prp2d    0 (0%)        0 (0%)      0 (0%)           0 (0%)         106m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                1595m (79%)     2950m (147%)\n  memory             780360Ki (19%)  1920462080 (47%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
    Mar  2 14:22:05.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-6697 describe namespace kubectl-6697'
    Mar  2 14:22:06.131: INFO: stderr: ""
    Mar  2 14:22:06.131: INFO: stdout: "Name:         kubectl-6697\nLabels:       e2e-framework=kubectl\n              e2e-run=21912a32-1801-4cee-be61-4faef0396552\n              kubernetes.io/metadata.name=kubectl-6697\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 14:22:06.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6697" for this suite. 03/02/23 14:22:06.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:22:06.147
Mar  2 14:22:06.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 14:22:06.148
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:06.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:06.169
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 03/02/23 14:22:06.22
Mar  2 14:22:06.235: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e" in namespace "projected-8833" to be "Succeeded or Failed"
Mar  2 14:22:06.243: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.593481ms
Mar  2 14:22:08.260: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e": Phase="Running", Reason="", readiness=true. Elapsed: 2.024204323s
Mar  2 14:22:10.266: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e": Phase="Running", Reason="", readiness=true. Elapsed: 4.030252769s
Mar  2 14:22:12.255: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e": Phase="Running", Reason="", readiness=false. Elapsed: 6.019457998s
Mar  2 14:22:14.250: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013825293s
STEP: Saw pod success 03/02/23 14:22:14.25
Mar  2 14:22:14.250: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e" satisfied condition "Succeeded or Failed"
Mar  2 14:22:14.253: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e container client-container: <nil>
STEP: delete the pod 03/02/23 14:22:14.298
Mar  2 14:22:14.337: INFO: Waiting for pod downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e to disappear
Mar  2 14:22:14.341: INFO: Pod downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 14:22:14.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8833" for this suite. 03/02/23 14:22:14.346
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":335,"skipped":6252,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.205 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:22:06.147
    Mar  2 14:22:06.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 14:22:06.148
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:06.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:06.169
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 03/02/23 14:22:06.22
    Mar  2 14:22:06.235: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e" in namespace "projected-8833" to be "Succeeded or Failed"
    Mar  2 14:22:06.243: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.593481ms
    Mar  2 14:22:08.260: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e": Phase="Running", Reason="", readiness=true. Elapsed: 2.024204323s
    Mar  2 14:22:10.266: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e": Phase="Running", Reason="", readiness=true. Elapsed: 4.030252769s
    Mar  2 14:22:12.255: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e": Phase="Running", Reason="", readiness=false. Elapsed: 6.019457998s
    Mar  2 14:22:14.250: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013825293s
    STEP: Saw pod success 03/02/23 14:22:14.25
    Mar  2 14:22:14.250: INFO: Pod "downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e" satisfied condition "Succeeded or Failed"
    Mar  2 14:22:14.253: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e container client-container: <nil>
    STEP: delete the pod 03/02/23 14:22:14.298
    Mar  2 14:22:14.337: INFO: Waiting for pod downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e to disappear
    Mar  2 14:22:14.341: INFO: Pod downwardapi-volume-22e9b973-15ff-4749-93b1-cfe6cedffb2e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 14:22:14.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8833" for this suite. 03/02/23 14:22:14.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:22:14.357
Mar  2 14:22:14.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename watch 03/02/23 14:22:14.358
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:14.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:14.442
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/02/23 14:22:14.447
STEP: starting a background goroutine to produce watch events 03/02/23 14:22:14.45
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/02/23 14:22:14.45
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  2 14:22:17.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6809" for this suite. 03/02/23 14:22:17.227
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":336,"skipped":6271,"failed":0}
------------------------------
â€¢ [2.911 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:22:14.357
    Mar  2 14:22:14.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename watch 03/02/23 14:22:14.358
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:14.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:14.442
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/02/23 14:22:14.447
    STEP: starting a background goroutine to produce watch events 03/02/23 14:22:14.45
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/02/23 14:22:14.45
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  2 14:22:17.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6809" for this suite. 03/02/23 14:22:17.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:22:17.268
Mar  2 14:22:17.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename disruption 03/02/23 14:22:17.269
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:17.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:17.301
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 03/02/23 14:22:17.309
STEP: Waiting for the pdb to be processed 03/02/23 14:22:17.323
STEP: First trying to evict a pod which shouldn't be evictable 03/02/23 14:22:17.35
STEP: Waiting for all pods to be running 03/02/23 14:22:17.35
Mar  2 14:22:17.356: INFO: pods: 0 < 3
Mar  2 14:22:19.361: INFO: running pods: 2 < 3
STEP: locating a running pod 03/02/23 14:22:21.368
STEP: Updating the pdb to allow a pod to be evicted 03/02/23 14:22:21.379
STEP: Waiting for the pdb to be processed 03/02/23 14:22:21.388
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/02/23 14:22:23.432
STEP: Waiting for all pods to be running 03/02/23 14:22:23.433
STEP: Waiting for the pdb to observed all healthy pods 03/02/23 14:22:23.438
STEP: Patching the pdb to disallow a pod to be evicted 03/02/23 14:22:23.459
STEP: Waiting for the pdb to be processed 03/02/23 14:22:23.506
STEP: Waiting for all pods to be running 03/02/23 14:22:23.517
Mar  2 14:22:23.522: INFO: running pods: 2 < 3
STEP: locating a running pod 03/02/23 14:22:25.528
STEP: Deleting the pdb to allow a pod to be evicted 03/02/23 14:22:25.536
STEP: Waiting for the pdb to be deleted 03/02/23 14:22:25.539
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/02/23 14:22:25.546
STEP: Waiting for all pods to be running 03/02/23 14:22:25.546
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  2 14:22:25.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9607" for this suite. 03/02/23 14:22:25.624
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":337,"skipped":6278,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.366 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:22:17.268
    Mar  2 14:22:17.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename disruption 03/02/23 14:22:17.269
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:17.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:17.301
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 03/02/23 14:22:17.309
    STEP: Waiting for the pdb to be processed 03/02/23 14:22:17.323
    STEP: First trying to evict a pod which shouldn't be evictable 03/02/23 14:22:17.35
    STEP: Waiting for all pods to be running 03/02/23 14:22:17.35
    Mar  2 14:22:17.356: INFO: pods: 0 < 3
    Mar  2 14:22:19.361: INFO: running pods: 2 < 3
    STEP: locating a running pod 03/02/23 14:22:21.368
    STEP: Updating the pdb to allow a pod to be evicted 03/02/23 14:22:21.379
    STEP: Waiting for the pdb to be processed 03/02/23 14:22:21.388
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/02/23 14:22:23.432
    STEP: Waiting for all pods to be running 03/02/23 14:22:23.433
    STEP: Waiting for the pdb to observed all healthy pods 03/02/23 14:22:23.438
    STEP: Patching the pdb to disallow a pod to be evicted 03/02/23 14:22:23.459
    STEP: Waiting for the pdb to be processed 03/02/23 14:22:23.506
    STEP: Waiting for all pods to be running 03/02/23 14:22:23.517
    Mar  2 14:22:23.522: INFO: running pods: 2 < 3
    STEP: locating a running pod 03/02/23 14:22:25.528
    STEP: Deleting the pdb to allow a pod to be evicted 03/02/23 14:22:25.536
    STEP: Waiting for the pdb to be deleted 03/02/23 14:22:25.539
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/02/23 14:22:25.546
    STEP: Waiting for all pods to be running 03/02/23 14:22:25.546
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  2 14:22:25.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9607" for this suite. 03/02/23 14:22:25.624
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:22:25.643
Mar  2 14:22:25.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename emptydir 03/02/23 14:22:25.648
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:25.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:25.688
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 03/02/23 14:22:25.691
Mar  2 14:22:25.726: INFO: Waiting up to 5m0s for pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb" in namespace "emptydir-8884" to be "Succeeded or Failed"
Mar  2 14:22:25.734: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.64487ms
Mar  2 14:22:27.750: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0236012s
Mar  2 14:22:29.740: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb": Phase="Running", Reason="", readiness=false. Elapsed: 4.013568498s
Mar  2 14:22:31.750: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb": Phase="Running", Reason="", readiness=false. Elapsed: 6.023432527s
Mar  2 14:22:33.738: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011653117s
STEP: Saw pod success 03/02/23 14:22:33.738
Mar  2 14:22:33.739: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb" satisfied condition "Succeeded or Failed"
Mar  2 14:22:33.742: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb container test-container: <nil>
STEP: delete the pod 03/02/23 14:22:33.749
Mar  2 14:22:33.761: INFO: Waiting for pod pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb to disappear
Mar  2 14:22:33.764: INFO: Pod pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 14:22:33.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8884" for this suite. 03/02/23 14:22:33.77
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":338,"skipped":6280,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:22:25.643
    Mar  2 14:22:25.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename emptydir 03/02/23 14:22:25.648
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:25.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:25.688
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/02/23 14:22:25.691
    Mar  2 14:22:25.726: INFO: Waiting up to 5m0s for pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb" in namespace "emptydir-8884" to be "Succeeded or Failed"
    Mar  2 14:22:25.734: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.64487ms
    Mar  2 14:22:27.750: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0236012s
    Mar  2 14:22:29.740: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb": Phase="Running", Reason="", readiness=false. Elapsed: 4.013568498s
    Mar  2 14:22:31.750: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb": Phase="Running", Reason="", readiness=false. Elapsed: 6.023432527s
    Mar  2 14:22:33.738: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011653117s
    STEP: Saw pod success 03/02/23 14:22:33.738
    Mar  2 14:22:33.739: INFO: Pod "pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb" satisfied condition "Succeeded or Failed"
    Mar  2 14:22:33.742: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb container test-container: <nil>
    STEP: delete the pod 03/02/23 14:22:33.749
    Mar  2 14:22:33.761: INFO: Waiting for pod pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb to disappear
    Mar  2 14:22:33.764: INFO: Pod pod-659104a1-c61c-41b9-ae48-e6fa9f8a1dbb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 14:22:33.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8884" for this suite. 03/02/23 14:22:33.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:22:33.791
Mar  2 14:22:33.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubectl 03/02/23 14:22:33.798
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:33.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:33.837
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 03/02/23 14:22:33.842
Mar  2 14:22:33.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 create -f -'
Mar  2 14:22:34.251: INFO: stderr: ""
Mar  2 14:22:34.251: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 14:22:34.251
Mar  2 14:22:34.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 14:22:34.451: INFO: stderr: ""
Mar  2 14:22:34.451: INFO: stdout: "update-demo-nautilus-7k6xj update-demo-nautilus-zfvhh "
Mar  2 14:22:34.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods update-demo-nautilus-7k6xj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 14:22:34.851: INFO: stderr: ""
Mar  2 14:22:34.851: INFO: stdout: ""
Mar  2 14:22:34.851: INFO: update-demo-nautilus-7k6xj is created but not running
Mar  2 14:22:39.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 14:22:40.069: INFO: stderr: ""
Mar  2 14:22:40.069: INFO: stdout: "update-demo-nautilus-7k6xj update-demo-nautilus-zfvhh "
Mar  2 14:22:40.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods update-demo-nautilus-7k6xj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 14:22:40.246: INFO: stderr: ""
Mar  2 14:22:40.246: INFO: stdout: "true"
Mar  2 14:22:40.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods update-demo-nautilus-7k6xj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 14:22:40.433: INFO: stderr: ""
Mar  2 14:22:40.433: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 14:22:40.433: INFO: validating pod update-demo-nautilus-7k6xj
Mar  2 14:22:40.441: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 14:22:40.441: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 14:22:40.441: INFO: update-demo-nautilus-7k6xj is verified up and running
Mar  2 14:22:40.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods update-demo-nautilus-zfvhh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 14:22:40.557: INFO: stderr: ""
Mar  2 14:22:40.557: INFO: stdout: "true"
Mar  2 14:22:40.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods update-demo-nautilus-zfvhh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 14:22:40.730: INFO: stderr: ""
Mar  2 14:22:40.730: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 14:22:40.730: INFO: validating pod update-demo-nautilus-zfvhh
Mar  2 14:22:40.749: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 14:22:40.749: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 14:22:40.749: INFO: update-demo-nautilus-zfvhh is verified up and running
STEP: using delete to clean up resources 03/02/23 14:22:40.749
Mar  2 14:22:40.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 delete --grace-period=0 --force -f -'
Mar  2 14:22:40.931: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 14:22:40.931: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 14:22:40.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get rc,svc -l name=update-demo --no-headers'
Mar  2 14:22:41.230: INFO: stderr: "No resources found in kubectl-9290 namespace.\n"
Mar  2 14:22:41.230: INFO: stdout: ""
Mar  2 14:22:41.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 14:22:41.365: INFO: stderr: ""
Mar  2 14:22:41.365: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 14:22:41.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9290" for this suite. 03/02/23 14:22:41.372
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":339,"skipped":6302,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.595 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:22:33.791
    Mar  2 14:22:33.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubectl 03/02/23 14:22:33.798
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:33.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:33.837
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 03/02/23 14:22:33.842
    Mar  2 14:22:33.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 create -f -'
    Mar  2 14:22:34.251: INFO: stderr: ""
    Mar  2 14:22:34.251: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 14:22:34.251
    Mar  2 14:22:34.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 14:22:34.451: INFO: stderr: ""
    Mar  2 14:22:34.451: INFO: stdout: "update-demo-nautilus-7k6xj update-demo-nautilus-zfvhh "
    Mar  2 14:22:34.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods update-demo-nautilus-7k6xj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 14:22:34.851: INFO: stderr: ""
    Mar  2 14:22:34.851: INFO: stdout: ""
    Mar  2 14:22:34.851: INFO: update-demo-nautilus-7k6xj is created but not running
    Mar  2 14:22:39.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 14:22:40.069: INFO: stderr: ""
    Mar  2 14:22:40.069: INFO: stdout: "update-demo-nautilus-7k6xj update-demo-nautilus-zfvhh "
    Mar  2 14:22:40.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods update-demo-nautilus-7k6xj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 14:22:40.246: INFO: stderr: ""
    Mar  2 14:22:40.246: INFO: stdout: "true"
    Mar  2 14:22:40.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods update-demo-nautilus-7k6xj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 14:22:40.433: INFO: stderr: ""
    Mar  2 14:22:40.433: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 14:22:40.433: INFO: validating pod update-demo-nautilus-7k6xj
    Mar  2 14:22:40.441: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 14:22:40.441: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 14:22:40.441: INFO: update-demo-nautilus-7k6xj is verified up and running
    Mar  2 14:22:40.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods update-demo-nautilus-zfvhh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 14:22:40.557: INFO: stderr: ""
    Mar  2 14:22:40.557: INFO: stdout: "true"
    Mar  2 14:22:40.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods update-demo-nautilus-zfvhh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 14:22:40.730: INFO: stderr: ""
    Mar  2 14:22:40.730: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 14:22:40.730: INFO: validating pod update-demo-nautilus-zfvhh
    Mar  2 14:22:40.749: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 14:22:40.749: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 14:22:40.749: INFO: update-demo-nautilus-zfvhh is verified up and running
    STEP: using delete to clean up resources 03/02/23 14:22:40.749
    Mar  2 14:22:40.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 delete --grace-period=0 --force -f -'
    Mar  2 14:22:40.931: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 14:22:40.931: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar  2 14:22:40.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get rc,svc -l name=update-demo --no-headers'
    Mar  2 14:22:41.230: INFO: stderr: "No resources found in kubectl-9290 namespace.\n"
    Mar  2 14:22:41.230: INFO: stdout: ""
    Mar  2 14:22:41.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=kubectl-9290 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  2 14:22:41.365: INFO: stderr: ""
    Mar  2 14:22:41.365: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 14:22:41.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9290" for this suite. 03/02/23 14:22:41.372
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:22:41.386
Mar  2 14:22:41.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 14:22:41.431
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:41.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:41.46
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-7859b40a-8f89-489b-87a1-98f23d6196ff 03/02/23 14:22:41.515
STEP: Creating secret with name s-test-opt-upd-28ddc95a-14e5-4875-8c55-5e08fb5db219 03/02/23 14:22:41.542
STEP: Creating the pod 03/02/23 14:22:41.549
Mar  2 14:22:41.556: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e" in namespace "projected-3293" to be "running and ready"
Mar  2 14:22:41.562: INFO: Pod "pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.397034ms
Mar  2 14:22:41.562: INFO: The phase of Pod pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:22:43.650: INFO: Pod "pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093618789s
Mar  2 14:22:43.650: INFO: The phase of Pod pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:22:45.694: INFO: Pod "pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e": Phase="Running", Reason="", readiness=true. Elapsed: 4.136860419s
Mar  2 14:22:45.694: INFO: The phase of Pod pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e is Running (Ready = true)
Mar  2 14:22:45.694: INFO: Pod "pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-7859b40a-8f89-489b-87a1-98f23d6196ff 03/02/23 14:22:45.922
STEP: Updating secret s-test-opt-upd-28ddc95a-14e5-4875-8c55-5e08fb5db219 03/02/23 14:22:46.03
STEP: Creating secret with name s-test-opt-create-f2066cee-881c-48a8-8652-c85405095d0b 03/02/23 14:22:46.044
STEP: waiting to observe update in volume 03/02/23 14:22:46.053
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 14:22:50.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3293" for this suite. 03/02/23 14:22:50.132
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":340,"skipped":6303,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.751 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:22:41.386
    Mar  2 14:22:41.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 14:22:41.431
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:41.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:41.46
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-7859b40a-8f89-489b-87a1-98f23d6196ff 03/02/23 14:22:41.515
    STEP: Creating secret with name s-test-opt-upd-28ddc95a-14e5-4875-8c55-5e08fb5db219 03/02/23 14:22:41.542
    STEP: Creating the pod 03/02/23 14:22:41.549
    Mar  2 14:22:41.556: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e" in namespace "projected-3293" to be "running and ready"
    Mar  2 14:22:41.562: INFO: Pod "pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.397034ms
    Mar  2 14:22:41.562: INFO: The phase of Pod pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:22:43.650: INFO: Pod "pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093618789s
    Mar  2 14:22:43.650: INFO: The phase of Pod pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:22:45.694: INFO: Pod "pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e": Phase="Running", Reason="", readiness=true. Elapsed: 4.136860419s
    Mar  2 14:22:45.694: INFO: The phase of Pod pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e is Running (Ready = true)
    Mar  2 14:22:45.694: INFO: Pod "pod-projected-secrets-91648ddf-4bbc-4c99-a4f0-0b1cd8a89a8e" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-7859b40a-8f89-489b-87a1-98f23d6196ff 03/02/23 14:22:45.922
    STEP: Updating secret s-test-opt-upd-28ddc95a-14e5-4875-8c55-5e08fb5db219 03/02/23 14:22:46.03
    STEP: Creating secret with name s-test-opt-create-f2066cee-881c-48a8-8652-c85405095d0b 03/02/23 14:22:46.044
    STEP: waiting to observe update in volume 03/02/23 14:22:46.053
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 14:22:50.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3293" for this suite. 03/02/23 14:22:50.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:22:50.151
Mar  2 14:22:50.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename container-runtime 03/02/23 14:22:50.152
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:50.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:50.172
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 03/02/23 14:22:50.18
STEP: wait for the container to reach Failed 03/02/23 14:22:50.197
STEP: get the container status 03/02/23 14:22:55.243
STEP: the container should be terminated 03/02/23 14:22:55.247
STEP: the termination message should be set 03/02/23 14:22:55.247
Mar  2 14:22:55.248: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/02/23 14:22:55.248
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  2 14:22:55.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3861" for this suite. 03/02/23 14:22:55.291
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":341,"skipped":6313,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.167 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:22:50.151
    Mar  2 14:22:50.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename container-runtime 03/02/23 14:22:50.152
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:50.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:50.172
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 03/02/23 14:22:50.18
    STEP: wait for the container to reach Failed 03/02/23 14:22:50.197
    STEP: get the container status 03/02/23 14:22:55.243
    STEP: the container should be terminated 03/02/23 14:22:55.247
    STEP: the termination message should be set 03/02/23 14:22:55.247
    Mar  2 14:22:55.248: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/02/23 14:22:55.248
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  2 14:22:55.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3861" for this suite. 03/02/23 14:22:55.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:22:55.343
Mar  2 14:22:55.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-webhook 03/02/23 14:22:55.344
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:55.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:55.371
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/02/23 14:22:55.382
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/02/23 14:22:56.543
STEP: Deploying the custom resource conversion webhook pod 03/02/23 14:22:56.555
STEP: Wait for the deployment to be ready 03/02/23 14:22:56.623
Mar  2 14:22:56.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 14:22:58.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 14:23:00.752
STEP: Verifying the service has paired with the endpoint 03/02/23 14:23:00.779
Mar  2 14:23:01.780: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar  2 14:23:01.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Creating a v1 custom resource 03/02/23 14:23:09.441
STEP: Create a v2 custom resource 03/02/23 14:23:09.456
STEP: List CRs in v1 03/02/23 14:23:09.531
STEP: List CRs in v2 03/02/23 14:23:09.535
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 14:23:10.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1422" for this suite. 03/02/23 14:23:10.06
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":342,"skipped":6357,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.890 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:22:55.343
    Mar  2 14:22:55.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-webhook 03/02/23 14:22:55.344
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:22:55.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:22:55.371
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/02/23 14:22:55.382
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/02/23 14:22:56.543
    STEP: Deploying the custom resource conversion webhook pod 03/02/23 14:22:56.555
    STEP: Wait for the deployment to be ready 03/02/23 14:22:56.623
    Mar  2 14:22:56.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 14:22:58.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 22, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 14:23:00.752
    STEP: Verifying the service has paired with the endpoint 03/02/23 14:23:00.779
    Mar  2 14:23:01.780: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar  2 14:23:01.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Creating a v1 custom resource 03/02/23 14:23:09.441
    STEP: Create a v2 custom resource 03/02/23 14:23:09.456
    STEP: List CRs in v1 03/02/23 14:23:09.531
    STEP: List CRs in v2 03/02/23 14:23:09.535
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 14:23:10.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-1422" for this suite. 03/02/23 14:23:10.06
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:23:10.234
Mar  2 14:23:10.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename statefulset 03/02/23 14:23:10.236
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:23:10.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:23:10.318
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4896 03/02/23 14:23:10.344
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-4896 03/02/23 14:23:10.37
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4896 03/02/23 14:23:10.402
Mar  2 14:23:10.410: INFO: Found 0 stateful pods, waiting for 1
Mar  2 14:23:20.420: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/02/23 14:23:20.42
Mar  2 14:23:20.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 14:23:20.737: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 14:23:20.737: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 14:23:20.737: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 14:23:20.741: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 14:23:30.756: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 14:23:30.756: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 14:23:30.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999566s
Mar  2 14:23:31.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987314318s
Mar  2 14:23:32.827: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.976717145s
Mar  2 14:23:33.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965166074s
Mar  2 14:23:34.839: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.959178041s
Mar  2 14:23:35.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.953394133s
Mar  2 14:23:36.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.947969649s
Mar  2 14:23:37.863: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.941589245s
Mar  2 14:23:38.868: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.929990938s
Mar  2 14:23:39.874: INFO: Verifying statefulset ss doesn't scale past 3 for another 925.163786ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4896 03/02/23 14:23:40.874
Mar  2 14:23:40.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:23:41.151: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 14:23:41.151: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 14:23:41.151: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 14:23:41.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:23:41.400: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 14:23:41.400: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 14:23:41.400: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 14:23:41.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:23:41.602: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 14:23:41.602: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 14:23:41.602: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 14:23:41.608: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar  2 14:23:51.620: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 14:23:51.620: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 14:23:51.620: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/02/23 14:23:51.62
Mar  2 14:23:51.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 14:23:51.834: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 14:23:51.834: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 14:23:51.834: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 14:23:51.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 14:23:52.028: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 14:23:52.028: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 14:23:52.028: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 14:23:52.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 14:23:52.292: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 14:23:52.292: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 14:23:52.292: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 14:23:52.292: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 14:23:52.301: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  2 14:24:02.313: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 14:24:02.313: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 14:24:02.313: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 14:24:02.352: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar  2 14:24:02.353: INFO: ss-0  aarnq-sc-k8s-node-srv2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:10 +0000 UTC  }]
Mar  2 14:24:02.353: INFO: ss-1  aarnq-sc-k8s-node-srv3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  }]
Mar  2 14:24:02.353: INFO: ss-2  aarnq-sc-k8s-node-srv0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  }]
Mar  2 14:24:02.354: INFO: 
Mar  2 14:24:02.354: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 14:24:03.359: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar  2 14:24:03.359: INFO: ss-0  aarnq-sc-k8s-node-srv2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:10 +0000 UTC  }]
Mar  2 14:24:03.359: INFO: ss-1  aarnq-sc-k8s-node-srv3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  }]
Mar  2 14:24:03.360: INFO: ss-2  aarnq-sc-k8s-node-srv0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  }]
Mar  2 14:24:03.360: INFO: 
Mar  2 14:24:03.360: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 14:24:04.364: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.967224475s
Mar  2 14:24:05.371: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.962812825s
Mar  2 14:24:06.384: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.95334839s
Mar  2 14:24:07.394: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.94268555s
Mar  2 14:24:08.402: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.928000565s
Mar  2 14:24:09.406: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.924428121s
Mar  2 14:24:10.414: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.920684886s
Mar  2 14:24:11.421: INFO: Verifying statefulset ss doesn't scale past 0 for another 912.368371ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4896 03/02/23 14:24:12.424
Mar  2 14:24:12.432: INFO: Scaling statefulset ss to 0
Mar  2 14:24:12.443: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 14:24:12.448: INFO: Deleting all statefulset in ns statefulset-4896
Mar  2 14:24:12.450: INFO: Scaling statefulset ss to 0
Mar  2 14:24:12.459: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 14:24:12.463: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 14:24:12.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4896" for this suite. 03/02/23 14:24:12.511
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":343,"skipped":6403,"failed":0}
------------------------------
â€¢ [SLOW TEST] [62.296 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:23:10.234
    Mar  2 14:23:10.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename statefulset 03/02/23 14:23:10.236
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:23:10.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:23:10.318
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4896 03/02/23 14:23:10.344
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-4896 03/02/23 14:23:10.37
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4896 03/02/23 14:23:10.402
    Mar  2 14:23:10.410: INFO: Found 0 stateful pods, waiting for 1
    Mar  2 14:23:20.420: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/02/23 14:23:20.42
    Mar  2 14:23:20.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 14:23:20.737: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 14:23:20.737: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 14:23:20.737: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 14:23:20.741: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar  2 14:23:30.756: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 14:23:30.756: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 14:23:30.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999566s
    Mar  2 14:23:31.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987314318s
    Mar  2 14:23:32.827: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.976717145s
    Mar  2 14:23:33.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965166074s
    Mar  2 14:23:34.839: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.959178041s
    Mar  2 14:23:35.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.953394133s
    Mar  2 14:23:36.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.947969649s
    Mar  2 14:23:37.863: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.941589245s
    Mar  2 14:23:38.868: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.929990938s
    Mar  2 14:23:39.874: INFO: Verifying statefulset ss doesn't scale past 3 for another 925.163786ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4896 03/02/23 14:23:40.874
    Mar  2 14:23:40.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 14:23:41.151: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 14:23:41.151: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 14:23:41.151: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 14:23:41.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 14:23:41.400: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar  2 14:23:41.400: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 14:23:41.400: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 14:23:41.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 14:23:41.602: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar  2 14:23:41.602: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 14:23:41.602: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 14:23:41.608: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Mar  2 14:23:51.620: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 14:23:51.620: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 14:23:51.620: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/02/23 14:23:51.62
    Mar  2 14:23:51.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 14:23:51.834: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 14:23:51.834: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 14:23:51.834: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 14:23:51.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 14:23:52.028: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 14:23:52.028: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 14:23:52.028: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 14:23:52.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=statefulset-4896 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 14:23:52.292: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 14:23:52.292: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 14:23:52.292: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 14:23:52.292: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 14:23:52.301: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Mar  2 14:24:02.313: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 14:24:02.313: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 14:24:02.313: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 14:24:02.352: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
    Mar  2 14:24:02.353: INFO: ss-0  aarnq-sc-k8s-node-srv2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:10 +0000 UTC  }]
    Mar  2 14:24:02.353: INFO: ss-1  aarnq-sc-k8s-node-srv3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  }]
    Mar  2 14:24:02.353: INFO: ss-2  aarnq-sc-k8s-node-srv0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  }]
    Mar  2 14:24:02.354: INFO: 
    Mar  2 14:24:02.354: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar  2 14:24:03.359: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
    Mar  2 14:24:03.359: INFO: ss-0  aarnq-sc-k8s-node-srv2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:10 +0000 UTC  }]
    Mar  2 14:24:03.359: INFO: ss-1  aarnq-sc-k8s-node-srv3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  }]
    Mar  2 14:24:03.360: INFO: ss-2  aarnq-sc-k8s-node-srv0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 14:23:30 +0000 UTC  }]
    Mar  2 14:24:03.360: INFO: 
    Mar  2 14:24:03.360: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar  2 14:24:04.364: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.967224475s
    Mar  2 14:24:05.371: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.962812825s
    Mar  2 14:24:06.384: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.95334839s
    Mar  2 14:24:07.394: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.94268555s
    Mar  2 14:24:08.402: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.928000565s
    Mar  2 14:24:09.406: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.924428121s
    Mar  2 14:24:10.414: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.920684886s
    Mar  2 14:24:11.421: INFO: Verifying statefulset ss doesn't scale past 0 for another 912.368371ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4896 03/02/23 14:24:12.424
    Mar  2 14:24:12.432: INFO: Scaling statefulset ss to 0
    Mar  2 14:24:12.443: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 14:24:12.448: INFO: Deleting all statefulset in ns statefulset-4896
    Mar  2 14:24:12.450: INFO: Scaling statefulset ss to 0
    Mar  2 14:24:12.459: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 14:24:12.463: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 14:24:12.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4896" for this suite. 03/02/23 14:24:12.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:24:12.539
Mar  2 14:24:12.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename webhook 03/02/23 14:24:12.541
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:24:12.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:24:12.568
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 14:24:12.59
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 14:24:12.94
STEP: Deploying the webhook pod 03/02/23 14:24:12.952
STEP: Wait for the deployment to be ready 03/02/23 14:24:12.968
Mar  2 14:24:13.006: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 14:24:15.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 24, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 24, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 14:24:17.029
STEP: Verifying the service has paired with the endpoint 03/02/23 14:24:17.043
Mar  2 14:24:18.043: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/02/23 14:24:18.047
Mar  2 14:24:18.100: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be updated by the webhook 03/02/23 14:24:18.235
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 14:24:18.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9835" for this suite. 03/02/23 14:24:18.294
STEP: Destroying namespace "webhook-9835-markers" for this suite. 03/02/23 14:24:18.308
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":344,"skipped":6416,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.891 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:24:12.539
    Mar  2 14:24:12.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename webhook 03/02/23 14:24:12.541
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:24:12.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:24:12.568
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 14:24:12.59
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 14:24:12.94
    STEP: Deploying the webhook pod 03/02/23 14:24:12.952
    STEP: Wait for the deployment to be ready 03/02/23 14:24:12.968
    Mar  2 14:24:13.006: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 14:24:15.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 14, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 24, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 14, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 14, 24, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 14:24:17.029
    STEP: Verifying the service has paired with the endpoint 03/02/23 14:24:17.043
    Mar  2 14:24:18.043: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/02/23 14:24:18.047
    Mar  2 14:24:18.100: INFO: Waiting for webhook configuration to be ready...
    STEP: create a pod that should be updated by the webhook 03/02/23 14:24:18.235
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 14:24:18.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9835" for this suite. 03/02/23 14:24:18.294
    STEP: Destroying namespace "webhook-9835-markers" for this suite. 03/02/23 14:24:18.308
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:24:18.431
Mar  2 14:24:18.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename events 03/02/23 14:24:18.432
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:24:18.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:24:18.571
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/02/23 14:24:18.666
STEP: listing all events in all namespaces 03/02/23 14:24:18.696
STEP: patching the test event 03/02/23 14:24:18.754
STEP: fetching the test event 03/02/23 14:24:18.829
STEP: updating the test event 03/02/23 14:24:18.832
STEP: getting the test event 03/02/23 14:24:18.839
STEP: deleting the test event 03/02/23 14:24:18.843
STEP: listing all events in all namespaces 03/02/23 14:24:18.863
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar  2 14:24:18.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2518" for this suite. 03/02/23 14:24:18.929
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":345,"skipped":6427,"failed":0}
------------------------------
â€¢ [0.504 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:24:18.431
    Mar  2 14:24:18.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename events 03/02/23 14:24:18.432
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:24:18.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:24:18.571
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/02/23 14:24:18.666
    STEP: listing all events in all namespaces 03/02/23 14:24:18.696
    STEP: patching the test event 03/02/23 14:24:18.754
    STEP: fetching the test event 03/02/23 14:24:18.829
    STEP: updating the test event 03/02/23 14:24:18.832
    STEP: getting the test event 03/02/23 14:24:18.839
    STEP: deleting the test event 03/02/23 14:24:18.843
    STEP: listing all events in all namespaces 03/02/23 14:24:18.863
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar  2 14:24:18.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2518" for this suite. 03/02/23 14:24:18.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:24:18.944
Mar  2 14:24:18.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename downward-api 03/02/23 14:24:18.946
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:24:18.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:24:18.969
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 03/02/23 14:24:19.044
Mar  2 14:24:19.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50" in namespace "downward-api-5381" to be "Succeeded or Failed"
Mar  2 14:24:19.093: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Pending", Reason="", readiness=false. Elapsed: 37.407153ms
Mar  2 14:24:21.127: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071561614s
Mar  2 14:24:23.103: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048179683s
Mar  2 14:24:25.100: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045006103s
Mar  2 14:24:27.125: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Running", Reason="", readiness=true. Elapsed: 8.069604312s
Mar  2 14:24:29.145: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Running", Reason="", readiness=false. Elapsed: 10.089607512s
Mar  2 14:24:31.098: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.04297383s
STEP: Saw pod success 03/02/23 14:24:31.098
Mar  2 14:24:31.099: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50" satisfied condition "Succeeded or Failed"
Mar  2 14:24:31.122: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50 container client-container: <nil>
STEP: delete the pod 03/02/23 14:24:31.131
Mar  2 14:24:31.150: INFO: Waiting for pod downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50 to disappear
Mar  2 14:24:31.155: INFO: Pod downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 14:24:31.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5381" for this suite. 03/02/23 14:24:31.161
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":346,"skipped":6452,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.221 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:24:18.944
    Mar  2 14:24:18.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename downward-api 03/02/23 14:24:18.946
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:24:18.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:24:18.969
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 03/02/23 14:24:19.044
    Mar  2 14:24:19.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50" in namespace "downward-api-5381" to be "Succeeded or Failed"
    Mar  2 14:24:19.093: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Pending", Reason="", readiness=false. Elapsed: 37.407153ms
    Mar  2 14:24:21.127: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071561614s
    Mar  2 14:24:23.103: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048179683s
    Mar  2 14:24:25.100: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045006103s
    Mar  2 14:24:27.125: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Running", Reason="", readiness=true. Elapsed: 8.069604312s
    Mar  2 14:24:29.145: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Running", Reason="", readiness=false. Elapsed: 10.089607512s
    Mar  2 14:24:31.098: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.04297383s
    STEP: Saw pod success 03/02/23 14:24:31.098
    Mar  2 14:24:31.099: INFO: Pod "downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50" satisfied condition "Succeeded or Failed"
    Mar  2 14:24:31.122: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50 container client-container: <nil>
    STEP: delete the pod 03/02/23 14:24:31.131
    Mar  2 14:24:31.150: INFO: Waiting for pod downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50 to disappear
    Mar  2 14:24:31.155: INFO: Pod downwardapi-volume-a54387a3-a3fa-49d2-9b1d-dc18c3b3bc50 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 14:24:31.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5381" for this suite. 03/02/23 14:24:31.161
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:24:31.168
Mar  2 14:24:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pods 03/02/23 14:24:31.198
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:24:31.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:24:31.238
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 03/02/23 14:24:31.242
Mar  2 14:24:31.249: INFO: Waiting up to 5m0s for pod "pod-hostip-d042021e-b041-4be7-b242-2123622c473d" in namespace "pods-6508" to be "running and ready"
Mar  2 14:24:31.254: INFO: Pod "pod-hostip-d042021e-b041-4be7-b242-2123622c473d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.431752ms
Mar  2 14:24:31.254: INFO: The phase of Pod pod-hostip-d042021e-b041-4be7-b242-2123622c473d is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:24:33.265: INFO: Pod "pod-hostip-d042021e-b041-4be7-b242-2123622c473d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015357287s
Mar  2 14:24:33.265: INFO: The phase of Pod pod-hostip-d042021e-b041-4be7-b242-2123622c473d is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:24:35.258: INFO: Pod "pod-hostip-d042021e-b041-4be7-b242-2123622c473d": Phase="Running", Reason="", readiness=true. Elapsed: 4.008655159s
Mar  2 14:24:35.258: INFO: The phase of Pod pod-hostip-d042021e-b041-4be7-b242-2123622c473d is Running (Ready = true)
Mar  2 14:24:35.258: INFO: Pod "pod-hostip-d042021e-b041-4be7-b242-2123622c473d" satisfied condition "running and ready"
Mar  2 14:24:35.272: INFO: Pod pod-hostip-d042021e-b041-4be7-b242-2123622c473d has hostIP: 172.16.0.192
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 14:24:35.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6508" for this suite. 03/02/23 14:24:35.283
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":347,"skipped":6453,"failed":0}
------------------------------
â€¢ [4.152 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:24:31.168
    Mar  2 14:24:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pods 03/02/23 14:24:31.198
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:24:31.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:24:31.238
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 03/02/23 14:24:31.242
    Mar  2 14:24:31.249: INFO: Waiting up to 5m0s for pod "pod-hostip-d042021e-b041-4be7-b242-2123622c473d" in namespace "pods-6508" to be "running and ready"
    Mar  2 14:24:31.254: INFO: Pod "pod-hostip-d042021e-b041-4be7-b242-2123622c473d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.431752ms
    Mar  2 14:24:31.254: INFO: The phase of Pod pod-hostip-d042021e-b041-4be7-b242-2123622c473d is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:24:33.265: INFO: Pod "pod-hostip-d042021e-b041-4be7-b242-2123622c473d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015357287s
    Mar  2 14:24:33.265: INFO: The phase of Pod pod-hostip-d042021e-b041-4be7-b242-2123622c473d is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:24:35.258: INFO: Pod "pod-hostip-d042021e-b041-4be7-b242-2123622c473d": Phase="Running", Reason="", readiness=true. Elapsed: 4.008655159s
    Mar  2 14:24:35.258: INFO: The phase of Pod pod-hostip-d042021e-b041-4be7-b242-2123622c473d is Running (Ready = true)
    Mar  2 14:24:35.258: INFO: Pod "pod-hostip-d042021e-b041-4be7-b242-2123622c473d" satisfied condition "running and ready"
    Mar  2 14:24:35.272: INFO: Pod pod-hostip-d042021e-b041-4be7-b242-2123622c473d has hostIP: 172.16.0.192
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 14:24:35.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6508" for this suite. 03/02/23 14:24:35.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:24:35.344
Mar  2 14:24:35.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename subpath 03/02/23 14:24:35.345
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:24:35.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:24:35.389
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/02/23 14:24:35.396
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-hbzb 03/02/23 14:24:35.412
STEP: Creating a pod to test atomic-volume-subpath 03/02/23 14:24:35.413
Mar  2 14:24:35.426: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hbzb" in namespace "subpath-8503" to be "Succeeded or Failed"
Mar  2 14:24:35.434: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.983249ms
Mar  2 14:24:37.452: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 2.026090994s
Mar  2 14:24:39.449: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 4.022863739s
Mar  2 14:24:41.448: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 6.022084594s
Mar  2 14:24:43.443: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 8.017364792s
Mar  2 14:24:45.442: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 10.015939643s
Mar  2 14:24:47.444: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 12.018255176s
Mar  2 14:24:49.441: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 14.015000368s
Mar  2 14:24:51.444: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 16.017800513s
Mar  2 14:24:53.443: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 18.017048143s
Mar  2 14:24:55.442: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 20.016540106s
Mar  2 14:24:57.444: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=false. Elapsed: 22.018261801s
Mar  2 14:24:59.443: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=false. Elapsed: 24.017047822s
Mar  2 14:25:01.445: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.018605037s
STEP: Saw pod success 03/02/23 14:25:01.445
Mar  2 14:25:01.445: INFO: Pod "pod-subpath-test-projected-hbzb" satisfied condition "Succeeded or Failed"
Mar  2 14:25:01.452: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-subpath-test-projected-hbzb container test-container-subpath-projected-hbzb: <nil>
STEP: delete the pod 03/02/23 14:25:01.461
Mar  2 14:25:01.483: INFO: Waiting for pod pod-subpath-test-projected-hbzb to disappear
Mar  2 14:25:01.502: INFO: Pod pod-subpath-test-projected-hbzb no longer exists
STEP: Deleting pod pod-subpath-test-projected-hbzb 03/02/23 14:25:01.502
Mar  2 14:25:01.502: INFO: Deleting pod "pod-subpath-test-projected-hbzb" in namespace "subpath-8503"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  2 14:25:01.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8503" for this suite. 03/02/23 14:25:01.54
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":348,"skipped":6497,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.202 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:24:35.344
    Mar  2 14:24:35.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename subpath 03/02/23 14:24:35.345
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:24:35.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:24:35.389
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/02/23 14:24:35.396
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-hbzb 03/02/23 14:24:35.412
    STEP: Creating a pod to test atomic-volume-subpath 03/02/23 14:24:35.413
    Mar  2 14:24:35.426: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hbzb" in namespace "subpath-8503" to be "Succeeded or Failed"
    Mar  2 14:24:35.434: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.983249ms
    Mar  2 14:24:37.452: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 2.026090994s
    Mar  2 14:24:39.449: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 4.022863739s
    Mar  2 14:24:41.448: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 6.022084594s
    Mar  2 14:24:43.443: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 8.017364792s
    Mar  2 14:24:45.442: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 10.015939643s
    Mar  2 14:24:47.444: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 12.018255176s
    Mar  2 14:24:49.441: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 14.015000368s
    Mar  2 14:24:51.444: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 16.017800513s
    Mar  2 14:24:53.443: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 18.017048143s
    Mar  2 14:24:55.442: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=true. Elapsed: 20.016540106s
    Mar  2 14:24:57.444: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=false. Elapsed: 22.018261801s
    Mar  2 14:24:59.443: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Running", Reason="", readiness=false. Elapsed: 24.017047822s
    Mar  2 14:25:01.445: INFO: Pod "pod-subpath-test-projected-hbzb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.018605037s
    STEP: Saw pod success 03/02/23 14:25:01.445
    Mar  2 14:25:01.445: INFO: Pod "pod-subpath-test-projected-hbzb" satisfied condition "Succeeded or Failed"
    Mar  2 14:25:01.452: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-subpath-test-projected-hbzb container test-container-subpath-projected-hbzb: <nil>
    STEP: delete the pod 03/02/23 14:25:01.461
    Mar  2 14:25:01.483: INFO: Waiting for pod pod-subpath-test-projected-hbzb to disappear
    Mar  2 14:25:01.502: INFO: Pod pod-subpath-test-projected-hbzb no longer exists
    STEP: Deleting pod pod-subpath-test-projected-hbzb 03/02/23 14:25:01.502
    Mar  2 14:25:01.502: INFO: Deleting pod "pod-subpath-test-projected-hbzb" in namespace "subpath-8503"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  2 14:25:01.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8503" for this suite. 03/02/23 14:25:01.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:25:01.546
Mar  2 14:25:01.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 14:25:01.549
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:01.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:01.588
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-f999b2c6-1ba9-4bf3-94aa-73d366d61a44 03/02/23 14:25:01.594
STEP: Creating a pod to test consume configMaps 03/02/23 14:25:01.605
Mar  2 14:25:01.627: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509" in namespace "projected-4644" to be "Succeeded or Failed"
Mar  2 14:25:01.631: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621162ms
Mar  2 14:25:03.650: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022844525s
Mar  2 14:25:05.637: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509": Phase="Running", Reason="", readiness=false. Elapsed: 4.010129853s
Mar  2 14:25:07.650: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509": Phase="Running", Reason="", readiness=false. Elapsed: 6.022778813s
Mar  2 14:25:09.637: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009574186s
STEP: Saw pod success 03/02/23 14:25:09.637
Mar  2 14:25:09.637: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509" satisfied condition "Succeeded or Failed"
Mar  2 14:25:09.642: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509 container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/02/23 14:25:09.647
Mar  2 14:25:09.660: INFO: Waiting for pod pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509 to disappear
Mar  2 14:25:09.667: INFO: Pod pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 14:25:09.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4644" for this suite. 03/02/23 14:25:09.726
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":349,"skipped":6507,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.192 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:25:01.546
    Mar  2 14:25:01.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 14:25:01.549
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:01.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:01.588
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-f999b2c6-1ba9-4bf3-94aa-73d366d61a44 03/02/23 14:25:01.594
    STEP: Creating a pod to test consume configMaps 03/02/23 14:25:01.605
    Mar  2 14:25:01.627: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509" in namespace "projected-4644" to be "Succeeded or Failed"
    Mar  2 14:25:01.631: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621162ms
    Mar  2 14:25:03.650: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022844525s
    Mar  2 14:25:05.637: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509": Phase="Running", Reason="", readiness=false. Elapsed: 4.010129853s
    Mar  2 14:25:07.650: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509": Phase="Running", Reason="", readiness=false. Elapsed: 6.022778813s
    Mar  2 14:25:09.637: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009574186s
    STEP: Saw pod success 03/02/23 14:25:09.637
    Mar  2 14:25:09.637: INFO: Pod "pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509" satisfied condition "Succeeded or Failed"
    Mar  2 14:25:09.642: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/02/23 14:25:09.647
    Mar  2 14:25:09.660: INFO: Waiting for pod pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509 to disappear
    Mar  2 14:25:09.667: INFO: Pod pod-projected-configmaps-77ca1b56-67f2-4165-890f-2d07fff84509 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 14:25:09.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4644" for this suite. 03/02/23 14:25:09.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:25:09.754
Mar  2 14:25:09.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 14:25:09.755
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:09.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:09.86
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Mar  2 14:25:09.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/02/23 14:25:22.158
Mar  2 14:25:22.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 create -f -'
Mar  2 14:25:23.474: INFO: stderr: ""
Mar  2 14:25:23.474: INFO: stdout: "e2e-test-crd-publish-openapi-6345-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 14:25:23.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 delete e2e-test-crd-publish-openapi-6345-crds test-foo'
Mar  2 14:25:23.595: INFO: stderr: ""
Mar  2 14:25:23.595: INFO: stdout: "e2e-test-crd-publish-openapi-6345-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  2 14:25:23.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 apply -f -'
Mar  2 14:25:23.989: INFO: stderr: ""
Mar  2 14:25:23.989: INFO: stdout: "e2e-test-crd-publish-openapi-6345-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 14:25:23.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 delete e2e-test-crd-publish-openapi-6345-crds test-foo'
Mar  2 14:25:24.074: INFO: stderr: ""
Mar  2 14:25:24.074: INFO: stdout: "e2e-test-crd-publish-openapi-6345-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/02/23 14:25:24.074
Mar  2 14:25:24.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 create -f -'
Mar  2 14:25:24.503: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/02/23 14:25:24.503
Mar  2 14:25:24.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 create -f -'
Mar  2 14:25:25.010: INFO: rc: 1
Mar  2 14:25:25.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 apply -f -'
Mar  2 14:25:25.430: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/02/23 14:25:25.43
Mar  2 14:25:25.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 create -f -'
Mar  2 14:25:25.801: INFO: rc: 1
Mar  2 14:25:25.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 apply -f -'
Mar  2 14:25:26.253: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/02/23 14:25:26.253
Mar  2 14:25:26.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 explain e2e-test-crd-publish-openapi-6345-crds'
Mar  2 14:25:26.664: INFO: stderr: ""
Mar  2 14:25:26.664: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6345-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/02/23 14:25:26.664
Mar  2 14:25:26.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 explain e2e-test-crd-publish-openapi-6345-crds.metadata'
Mar  2 14:25:27.032: INFO: stderr: ""
Mar  2 14:25:27.032: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6345-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  2 14:25:27.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 explain e2e-test-crd-publish-openapi-6345-crds.spec'
Mar  2 14:25:27.438: INFO: stderr: ""
Mar  2 14:25:27.438: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6345-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  2 14:25:27.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 explain e2e-test-crd-publish-openapi-6345-crds.spec.bars'
Mar  2 14:25:27.813: INFO: stderr: ""
Mar  2 14:25:27.813: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6345-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/02/23 14:25:27.813
Mar  2 14:25:27.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 explain e2e-test-crd-publish-openapi-6345-crds.spec.bars2'
Mar  2 14:25:28.198: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 14:25:33.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8514" for this suite. 03/02/23 14:25:33.98
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":350,"skipped":6567,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.238 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:25:09.754
    Mar  2 14:25:09.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 14:25:09.755
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:09.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:09.86
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Mar  2 14:25:09.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/02/23 14:25:22.158
    Mar  2 14:25:22.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 create -f -'
    Mar  2 14:25:23.474: INFO: stderr: ""
    Mar  2 14:25:23.474: INFO: stdout: "e2e-test-crd-publish-openapi-6345-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar  2 14:25:23.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 delete e2e-test-crd-publish-openapi-6345-crds test-foo'
    Mar  2 14:25:23.595: INFO: stderr: ""
    Mar  2 14:25:23.595: INFO: stdout: "e2e-test-crd-publish-openapi-6345-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar  2 14:25:23.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 apply -f -'
    Mar  2 14:25:23.989: INFO: stderr: ""
    Mar  2 14:25:23.989: INFO: stdout: "e2e-test-crd-publish-openapi-6345-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar  2 14:25:23.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 delete e2e-test-crd-publish-openapi-6345-crds test-foo'
    Mar  2 14:25:24.074: INFO: stderr: ""
    Mar  2 14:25:24.074: INFO: stdout: "e2e-test-crd-publish-openapi-6345-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/02/23 14:25:24.074
    Mar  2 14:25:24.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 create -f -'
    Mar  2 14:25:24.503: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/02/23 14:25:24.503
    Mar  2 14:25:24.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 create -f -'
    Mar  2 14:25:25.010: INFO: rc: 1
    Mar  2 14:25:25.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 apply -f -'
    Mar  2 14:25:25.430: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/02/23 14:25:25.43
    Mar  2 14:25:25.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 create -f -'
    Mar  2 14:25:25.801: INFO: rc: 1
    Mar  2 14:25:25.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 --namespace=crd-publish-openapi-8514 apply -f -'
    Mar  2 14:25:26.253: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/02/23 14:25:26.253
    Mar  2 14:25:26.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 explain e2e-test-crd-publish-openapi-6345-crds'
    Mar  2 14:25:26.664: INFO: stderr: ""
    Mar  2 14:25:26.664: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6345-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/02/23 14:25:26.664
    Mar  2 14:25:26.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 explain e2e-test-crd-publish-openapi-6345-crds.metadata'
    Mar  2 14:25:27.032: INFO: stderr: ""
    Mar  2 14:25:27.032: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6345-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar  2 14:25:27.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 explain e2e-test-crd-publish-openapi-6345-crds.spec'
    Mar  2 14:25:27.438: INFO: stderr: ""
    Mar  2 14:25:27.438: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6345-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar  2 14:25:27.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 explain e2e-test-crd-publish-openapi-6345-crds.spec.bars'
    Mar  2 14:25:27.813: INFO: stderr: ""
    Mar  2 14:25:27.813: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6345-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/02/23 14:25:27.813
    Mar  2 14:25:27.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2815392237 --namespace=crd-publish-openapi-8514 explain e2e-test-crd-publish-openapi-6345-crds.spec.bars2'
    Mar  2 14:25:28.198: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 14:25:33.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8514" for this suite. 03/02/23 14:25:33.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:25:34.002
Mar  2 14:25:34.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename limitrange 03/02/23 14:25:34.003
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:34.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:34.034
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 03/02/23 14:25:34.037
STEP: Setting up watch 03/02/23 14:25:34.037
STEP: Submitting a LimitRange 03/02/23 14:25:34.14
STEP: Verifying LimitRange creation was observed 03/02/23 14:25:34.15
STEP: Fetching the LimitRange to ensure it has proper values 03/02/23 14:25:34.15
Mar  2 14:25:34.154: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 14:25:34.154: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/02/23 14:25:34.154
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/02/23 14:25:34.166
Mar  2 14:25:34.186: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 14:25:34.186: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/02/23 14:25:34.186
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/02/23 14:25:34.215
Mar  2 14:25:34.226: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar  2 14:25:34.226: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/02/23 14:25:34.226
STEP: Failing to create a Pod with more than max resources 03/02/23 14:25:34.23
STEP: Updating a LimitRange 03/02/23 14:25:34.237
STEP: Verifying LimitRange updating is effective 03/02/23 14:25:34.271
STEP: Creating a Pod with less than former min resources 03/02/23 14:25:36.279
STEP: Failing to create a Pod with more than max resources 03/02/23 14:25:36.288
STEP: Deleting a LimitRange 03/02/23 14:25:36.295
STEP: Verifying the LimitRange was deleted 03/02/23 14:25:36.311
Mar  2 14:25:41.327: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/02/23 14:25:41.327
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Mar  2 14:25:41.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7750" for this suite. 03/02/23 14:25:41.348
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":351,"skipped":6585,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.375 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:25:34.002
    Mar  2 14:25:34.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename limitrange 03/02/23 14:25:34.003
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:34.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:34.034
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 03/02/23 14:25:34.037
    STEP: Setting up watch 03/02/23 14:25:34.037
    STEP: Submitting a LimitRange 03/02/23 14:25:34.14
    STEP: Verifying LimitRange creation was observed 03/02/23 14:25:34.15
    STEP: Fetching the LimitRange to ensure it has proper values 03/02/23 14:25:34.15
    Mar  2 14:25:34.154: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar  2 14:25:34.154: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/02/23 14:25:34.154
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/02/23 14:25:34.166
    Mar  2 14:25:34.186: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar  2 14:25:34.186: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/02/23 14:25:34.186
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/02/23 14:25:34.215
    Mar  2 14:25:34.226: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar  2 14:25:34.226: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/02/23 14:25:34.226
    STEP: Failing to create a Pod with more than max resources 03/02/23 14:25:34.23
    STEP: Updating a LimitRange 03/02/23 14:25:34.237
    STEP: Verifying LimitRange updating is effective 03/02/23 14:25:34.271
    STEP: Creating a Pod with less than former min resources 03/02/23 14:25:36.279
    STEP: Failing to create a Pod with more than max resources 03/02/23 14:25:36.288
    STEP: Deleting a LimitRange 03/02/23 14:25:36.295
    STEP: Verifying the LimitRange was deleted 03/02/23 14:25:36.311
    Mar  2 14:25:41.327: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/02/23 14:25:41.327
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Mar  2 14:25:41.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-7750" for this suite. 03/02/23 14:25:41.348
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:25:41.382
Mar  2 14:25:41.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename discovery 03/02/23 14:25:41.383
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:41.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:41.471
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/02/23 14:25:41.492
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar  2 14:25:42.394: INFO: Checking APIGroup: apiregistration.k8s.io
Mar  2 14:25:42.395: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar  2 14:25:42.395: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar  2 14:25:42.395: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar  2 14:25:42.395: INFO: Checking APIGroup: apps
Mar  2 14:25:42.396: INFO: PreferredVersion.GroupVersion: apps/v1
Mar  2 14:25:42.396: INFO: Versions found [{apps/v1 v1}]
Mar  2 14:25:42.396: INFO: apps/v1 matches apps/v1
Mar  2 14:25:42.396: INFO: Checking APIGroup: events.k8s.io
Mar  2 14:25:42.397: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar  2 14:25:42.397: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar  2 14:25:42.397: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar  2 14:25:42.397: INFO: Checking APIGroup: authentication.k8s.io
Mar  2 14:25:42.398: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar  2 14:25:42.398: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar  2 14:25:42.398: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar  2 14:25:42.398: INFO: Checking APIGroup: authorization.k8s.io
Mar  2 14:25:42.400: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar  2 14:25:42.400: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar  2 14:25:42.400: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar  2 14:25:42.400: INFO: Checking APIGroup: autoscaling
Mar  2 14:25:42.401: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar  2 14:25:42.401: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Mar  2 14:25:42.401: INFO: autoscaling/v2 matches autoscaling/v2
Mar  2 14:25:42.401: INFO: Checking APIGroup: batch
Mar  2 14:25:42.402: INFO: PreferredVersion.GroupVersion: batch/v1
Mar  2 14:25:42.403: INFO: Versions found [{batch/v1 v1}]
Mar  2 14:25:42.403: INFO: batch/v1 matches batch/v1
Mar  2 14:25:42.403: INFO: Checking APIGroup: certificates.k8s.io
Mar  2 14:25:42.404: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar  2 14:25:42.404: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar  2 14:25:42.404: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar  2 14:25:42.405: INFO: Checking APIGroup: networking.k8s.io
Mar  2 14:25:42.406: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar  2 14:25:42.406: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar  2 14:25:42.406: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar  2 14:25:42.406: INFO: Checking APIGroup: policy
Mar  2 14:25:42.408: INFO: PreferredVersion.GroupVersion: policy/v1
Mar  2 14:25:42.408: INFO: Versions found [{policy/v1 v1}]
Mar  2 14:25:42.408: INFO: policy/v1 matches policy/v1
Mar  2 14:25:42.408: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar  2 14:25:42.409: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar  2 14:25:42.410: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar  2 14:25:42.410: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar  2 14:25:42.410: INFO: Checking APIGroup: storage.k8s.io
Mar  2 14:25:42.412: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar  2 14:25:42.412: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar  2 14:25:42.412: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar  2 14:25:42.413: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar  2 14:25:42.414: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar  2 14:25:42.415: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar  2 14:25:42.415: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar  2 14:25:42.415: INFO: Checking APIGroup: apiextensions.k8s.io
Mar  2 14:25:42.416: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar  2 14:25:42.416: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar  2 14:25:42.417: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar  2 14:25:42.417: INFO: Checking APIGroup: scheduling.k8s.io
Mar  2 14:25:42.418: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar  2 14:25:42.419: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar  2 14:25:42.419: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar  2 14:25:42.419: INFO: Checking APIGroup: coordination.k8s.io
Mar  2 14:25:42.421: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar  2 14:25:42.421: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar  2 14:25:42.421: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar  2 14:25:42.422: INFO: Checking APIGroup: node.k8s.io
Mar  2 14:25:42.423: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar  2 14:25:42.424: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar  2 14:25:42.424: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar  2 14:25:42.424: INFO: Checking APIGroup: discovery.k8s.io
Mar  2 14:25:42.426: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar  2 14:25:42.426: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar  2 14:25:42.426: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar  2 14:25:42.426: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar  2 14:25:42.427: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Mar  2 14:25:42.427: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar  2 14:25:42.427: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Mar  2 14:25:42.428: INFO: Checking APIGroup: acme.cert-manager.io
Mar  2 14:25:42.429: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Mar  2 14:25:42.429: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Mar  2 14:25:42.429: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Mar  2 14:25:42.429: INFO: Checking APIGroup: cert-manager.io
Mar  2 14:25:42.430: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Mar  2 14:25:42.430: INFO: Versions found [{cert-manager.io/v1 v1}]
Mar  2 14:25:42.431: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Mar  2 14:25:42.431: INFO: Checking APIGroup: crd.projectcalico.org
Mar  2 14:25:42.432: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar  2 14:25:42.432: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar  2 14:25:42.432: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar  2 14:25:42.433: INFO: Checking APIGroup: dex.coreos.com
Mar  2 14:25:42.434: INFO: PreferredVersion.GroupVersion: dex.coreos.com/v1
Mar  2 14:25:42.434: INFO: Versions found [{dex.coreos.com/v1 v1}]
Mar  2 14:25:42.434: INFO: dex.coreos.com/v1 matches dex.coreos.com/v1
Mar  2 14:25:42.435: INFO: Checking APIGroup: monitoring.coreos.com
Mar  2 14:25:42.436: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Mar  2 14:25:42.436: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Mar  2 14:25:42.436: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Mar  2 14:25:42.436: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar  2 14:25:42.437: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Mar  2 14:25:42.438: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Mar  2 14:25:42.438: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Mar  2 14:25:42.438: INFO: Checking APIGroup: velero.io
Mar  2 14:25:42.439: INFO: PreferredVersion.GroupVersion: velero.io/v1
Mar  2 14:25:42.439: INFO: Versions found [{velero.io/v1 v1}]
Mar  2 14:25:42.439: INFO: velero.io/v1 matches velero.io/v1
Mar  2 14:25:42.439: INFO: Checking APIGroup: aquasecurity.github.io
Mar  2 14:25:42.440: INFO: PreferredVersion.GroupVersion: aquasecurity.github.io/v1alpha1
Mar  2 14:25:42.440: INFO: Versions found [{aquasecurity.github.io/v1alpha1 v1alpha1}]
Mar  2 14:25:42.440: INFO: aquasecurity.github.io/v1alpha1 matches aquasecurity.github.io/v1alpha1
Mar  2 14:25:42.440: INFO: Checking APIGroup: metrics.k8s.io
Mar  2 14:25:42.441: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar  2 14:25:42.442: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar  2 14:25:42.442: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Mar  2 14:25:42.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8125" for this suite. 03/02/23 14:25:42.448
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":352,"skipped":6585,"failed":0}
------------------------------
â€¢ [1.072 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:25:41.382
    Mar  2 14:25:41.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename discovery 03/02/23 14:25:41.383
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:41.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:41.471
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/02/23 14:25:41.492
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar  2 14:25:42.394: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar  2 14:25:42.395: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar  2 14:25:42.395: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar  2 14:25:42.395: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar  2 14:25:42.395: INFO: Checking APIGroup: apps
    Mar  2 14:25:42.396: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar  2 14:25:42.396: INFO: Versions found [{apps/v1 v1}]
    Mar  2 14:25:42.396: INFO: apps/v1 matches apps/v1
    Mar  2 14:25:42.396: INFO: Checking APIGroup: events.k8s.io
    Mar  2 14:25:42.397: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar  2 14:25:42.397: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar  2 14:25:42.397: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar  2 14:25:42.397: INFO: Checking APIGroup: authentication.k8s.io
    Mar  2 14:25:42.398: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar  2 14:25:42.398: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar  2 14:25:42.398: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar  2 14:25:42.398: INFO: Checking APIGroup: authorization.k8s.io
    Mar  2 14:25:42.400: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar  2 14:25:42.400: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar  2 14:25:42.400: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar  2 14:25:42.400: INFO: Checking APIGroup: autoscaling
    Mar  2 14:25:42.401: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar  2 14:25:42.401: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Mar  2 14:25:42.401: INFO: autoscaling/v2 matches autoscaling/v2
    Mar  2 14:25:42.401: INFO: Checking APIGroup: batch
    Mar  2 14:25:42.402: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar  2 14:25:42.403: INFO: Versions found [{batch/v1 v1}]
    Mar  2 14:25:42.403: INFO: batch/v1 matches batch/v1
    Mar  2 14:25:42.403: INFO: Checking APIGroup: certificates.k8s.io
    Mar  2 14:25:42.404: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar  2 14:25:42.404: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar  2 14:25:42.404: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar  2 14:25:42.405: INFO: Checking APIGroup: networking.k8s.io
    Mar  2 14:25:42.406: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar  2 14:25:42.406: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar  2 14:25:42.406: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar  2 14:25:42.406: INFO: Checking APIGroup: policy
    Mar  2 14:25:42.408: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar  2 14:25:42.408: INFO: Versions found [{policy/v1 v1}]
    Mar  2 14:25:42.408: INFO: policy/v1 matches policy/v1
    Mar  2 14:25:42.408: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar  2 14:25:42.409: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar  2 14:25:42.410: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar  2 14:25:42.410: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar  2 14:25:42.410: INFO: Checking APIGroup: storage.k8s.io
    Mar  2 14:25:42.412: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar  2 14:25:42.412: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar  2 14:25:42.412: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar  2 14:25:42.413: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar  2 14:25:42.414: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar  2 14:25:42.415: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar  2 14:25:42.415: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar  2 14:25:42.415: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar  2 14:25:42.416: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar  2 14:25:42.416: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar  2 14:25:42.417: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar  2 14:25:42.417: INFO: Checking APIGroup: scheduling.k8s.io
    Mar  2 14:25:42.418: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar  2 14:25:42.419: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar  2 14:25:42.419: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar  2 14:25:42.419: INFO: Checking APIGroup: coordination.k8s.io
    Mar  2 14:25:42.421: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar  2 14:25:42.421: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar  2 14:25:42.421: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar  2 14:25:42.422: INFO: Checking APIGroup: node.k8s.io
    Mar  2 14:25:42.423: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar  2 14:25:42.424: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar  2 14:25:42.424: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar  2 14:25:42.424: INFO: Checking APIGroup: discovery.k8s.io
    Mar  2 14:25:42.426: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar  2 14:25:42.426: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar  2 14:25:42.426: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar  2 14:25:42.426: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar  2 14:25:42.427: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Mar  2 14:25:42.427: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Mar  2 14:25:42.427: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Mar  2 14:25:42.428: INFO: Checking APIGroup: acme.cert-manager.io
    Mar  2 14:25:42.429: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    Mar  2 14:25:42.429: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
    Mar  2 14:25:42.429: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    Mar  2 14:25:42.429: INFO: Checking APIGroup: cert-manager.io
    Mar  2 14:25:42.430: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    Mar  2 14:25:42.430: INFO: Versions found [{cert-manager.io/v1 v1}]
    Mar  2 14:25:42.431: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    Mar  2 14:25:42.431: INFO: Checking APIGroup: crd.projectcalico.org
    Mar  2 14:25:42.432: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Mar  2 14:25:42.432: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Mar  2 14:25:42.432: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Mar  2 14:25:42.433: INFO: Checking APIGroup: dex.coreos.com
    Mar  2 14:25:42.434: INFO: PreferredVersion.GroupVersion: dex.coreos.com/v1
    Mar  2 14:25:42.434: INFO: Versions found [{dex.coreos.com/v1 v1}]
    Mar  2 14:25:42.434: INFO: dex.coreos.com/v1 matches dex.coreos.com/v1
    Mar  2 14:25:42.435: INFO: Checking APIGroup: monitoring.coreos.com
    Mar  2 14:25:42.436: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Mar  2 14:25:42.436: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Mar  2 14:25:42.436: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Mar  2 14:25:42.436: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Mar  2 14:25:42.437: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Mar  2 14:25:42.438: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Mar  2 14:25:42.438: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Mar  2 14:25:42.438: INFO: Checking APIGroup: velero.io
    Mar  2 14:25:42.439: INFO: PreferredVersion.GroupVersion: velero.io/v1
    Mar  2 14:25:42.439: INFO: Versions found [{velero.io/v1 v1}]
    Mar  2 14:25:42.439: INFO: velero.io/v1 matches velero.io/v1
    Mar  2 14:25:42.439: INFO: Checking APIGroup: aquasecurity.github.io
    Mar  2 14:25:42.440: INFO: PreferredVersion.GroupVersion: aquasecurity.github.io/v1alpha1
    Mar  2 14:25:42.440: INFO: Versions found [{aquasecurity.github.io/v1alpha1 v1alpha1}]
    Mar  2 14:25:42.440: INFO: aquasecurity.github.io/v1alpha1 matches aquasecurity.github.io/v1alpha1
    Mar  2 14:25:42.440: INFO: Checking APIGroup: metrics.k8s.io
    Mar  2 14:25:42.441: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Mar  2 14:25:42.442: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Mar  2 14:25:42.442: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Mar  2 14:25:42.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-8125" for this suite. 03/02/23 14:25:42.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:25:42.46
Mar  2 14:25:42.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename daemonsets 03/02/23 14:25:42.461
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:42.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:42.497
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 03/02/23 14:25:42.543
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 14:25:42.548
Mar  2 14:25:42.556: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:25:42.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:25:42.562: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 14:25:43.573: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:25:43.619: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:25:43.619: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
Mar  2 14:25:44.573: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:25:44.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 14:25:44.585: INFO: Node aarnq-sc-k8s-node-srv2 is running 0 daemon pod, expected 1
Mar  2 14:25:45.580: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 14:25:45.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Mar  2 14:25:45.587: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Getting /status 03/02/23 14:25:45.594
Mar  2 14:25:45.601: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/02/23 14:25:45.602
Mar  2 14:25:45.611: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/02/23 14:25:45.611
Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: ADDED
Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.615: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.615: INFO: Found daemon set daemon-set in namespace daemonsets-8644 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 14:25:45.615: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/02/23 14:25:45.615
STEP: watching for the daemon set status to be patched 03/02/23 14:25:45.623
Mar  2 14:25:45.627: INFO: Observed &DaemonSet event: ADDED
Mar  2 14:25:45.627: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.629: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.631: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.631: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.632: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.632: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.632: INFO: Observed daemon set daemon-set in namespace daemonsets-8644 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 14:25:45.633: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 14:25:45.633: INFO: Found daemon set daemon-set in namespace daemonsets-8644 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar  2 14:25:45.633: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 14:25:45.636
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8644, will wait for the garbage collector to delete the pods 03/02/23 14:25:45.636
Mar  2 14:25:45.696: INFO: Deleting DaemonSet.extensions daemon-set took: 6.157008ms
Mar  2 14:25:45.797: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.525366ms
Mar  2 14:25:48.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 14:25:48.126: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 14:25:48.131: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1974011"},"items":null}

Mar  2 14:25:48.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1974011"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 14:25:48.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8644" for this suite. 03/02/23 14:25:48.167
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":353,"skipped":6597,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.720 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:25:42.46
    Mar  2 14:25:42.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename daemonsets 03/02/23 14:25:42.461
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:42.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:42.497
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 03/02/23 14:25:42.543
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 14:25:42.548
    Mar  2 14:25:42.556: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:25:42.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:25:42.562: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 14:25:43.573: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:25:43.619: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:25:43.619: INFO: Node aarnq-sc-k8s-node-srv0 is running 0 daemon pod, expected 1
    Mar  2 14:25:44.573: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:25:44.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 14:25:44.585: INFO: Node aarnq-sc-k8s-node-srv2 is running 0 daemon pod, expected 1
    Mar  2 14:25:45.580: INFO: DaemonSet pods can't tolerate node aarnq-sc-k8s-ctl0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  2 14:25:45.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Mar  2 14:25:45.587: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Getting /status 03/02/23 14:25:45.594
    Mar  2 14:25:45.601: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/02/23 14:25:45.602
    Mar  2 14:25:45.611: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/02/23 14:25:45.611
    Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: ADDED
    Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.614: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.615: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.615: INFO: Found daemon set daemon-set in namespace daemonsets-8644 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  2 14:25:45.615: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/02/23 14:25:45.615
    STEP: watching for the daemon set status to be patched 03/02/23 14:25:45.623
    Mar  2 14:25:45.627: INFO: Observed &DaemonSet event: ADDED
    Mar  2 14:25:45.627: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.629: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.631: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.631: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.632: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.632: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.632: INFO: Observed daemon set daemon-set in namespace daemonsets-8644 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  2 14:25:45.633: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 14:25:45.633: INFO: Found daemon set daemon-set in namespace daemonsets-8644 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar  2 14:25:45.633: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 14:25:45.636
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8644, will wait for the garbage collector to delete the pods 03/02/23 14:25:45.636
    Mar  2 14:25:45.696: INFO: Deleting DaemonSet.extensions daemon-set took: 6.157008ms
    Mar  2 14:25:45.797: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.525366ms
    Mar  2 14:25:48.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 14:25:48.126: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 14:25:48.131: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1974011"},"items":null}

    Mar  2 14:25:48.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1974011"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 14:25:48.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8644" for this suite. 03/02/23 14:25:48.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:25:48.181
Mar  2 14:25:48.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename sched-pred 03/02/23 14:25:48.182
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:48.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:48.216
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 14:25:48.222: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 14:25:48.240: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 14:25:48.245: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv0 before test
Mar  2 14:25:48.289: INFO: falco-exporter-r2zfx from falco started at 2023-02-27 15:51:02 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.289: INFO: 	Container falco-exporter ready: true, restart count 4
Mar  2 14:25:48.289: INFO: falco-falcosidekick-5d5c7d4db-r6952 from falco started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.289: INFO: 	Container falcosidekick ready: true, restart count 0
Mar  2 14:25:48.289: INFO: falco-trjnh from falco started at 2023-02-27 15:53:31 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.289: INFO: 	Container falco ready: true, restart count 2
Mar  2 14:25:48.289: INFO: fluentd-forwarder-qxbtj from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.289: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 14:25:48.289: INFO: ingress-nginx-controller-qprg2 from ingress-nginx started at 2023-02-27 13:57:01 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.289: INFO: 	Container controller ready: true, restart count 2
Mar  2 14:25:48.289: INFO: calico-accountant-k6t4p from kube-system started at 2023-02-27 13:50:57 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 14:25:48.290: INFO: calico-node-dz84l from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 14:25:48.290: INFO: coredns-588bb58b94-k6j76 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container coredns ready: true, restart count 0
Mar  2 14:25:48.290: INFO: csi-cinder-nodeplugin-9nct9 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
Mar  2 14:25:48.290: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 14:25:48.290: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 14:25:48.290: INFO: kube-proxy-7bm9z from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 14:25:48.290: INFO: nginx-proxy-aarnq-sc-k8s-node-srv0 from kube-system started at 2023-02-27 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 14:25:48.290: INFO: node-local-dns-dk8hd from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 14:25:48.290: INFO: snapshot-controller-7d445c66c9-6w4w5 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 14:25:48.290: INFO: kured-hdkrw from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container kured ready: true, restart count 3
Mar  2 14:25:48.290: INFO: alertmanager-kube-prometheus-stack-alertmanager-0 from monitoring started at 2023-03-01 07:32:10 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 14:25:48.290: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 14:25:48.290: INFO: kube-prometheus-stack-grafana-84f79f467b-sr7kl from monitoring started at 2023-02-28 08:03:49 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container grafana ready: true, restart count 0
Mar  2 14:25:48.290: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar  2 14:25:48.290: INFO: kube-prometheus-stack-prometheus-node-exporter-nl9pw from monitoring started at 2023-02-27 13:49:55 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 14:25:48.290: INFO: prometheus-blackbox-exporter-677b579798-7xm9d from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar  2 14:25:48.290: INFO: s3-exporter-867c5b9457-lfcsf from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container s3-exporter ready: true, restart count 0
Mar  2 14:25:48.290: INFO: opensearch-dashboards-58c8d95f7b-9spcv from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container dashboards ready: true, restart count 0
Mar  2 14:25:48.290: INFO: opensearch-master-2 from opensearch-system started at 2023-02-28 08:03:54 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 14:25:48.290: INFO: prometheus-opensearch-exporter-5688c84dcd-95vjh from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container exporter ready: true, restart count 0
Mar  2 14:25:48.290: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-qv9vz from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 14:25:48.290: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 14:25:48.290: INFO: thanos-query-query-69fc6f554b-db7w4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container query ready: true, restart count 0
Mar  2 14:25:48.290: INFO: thanos-receiver-bucketweb-b4955fcf8-8w2xg from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container bucketweb ready: true, restart count 0
Mar  2 14:25:48.290: INFO: thanos-receiver-compactor-848df7b5d7-z2jh4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container compactor ready: true, restart count 0
Mar  2 14:25:48.290: INFO: thanos-receiver-receive-0 from thanos started at 2023-02-28 08:04:00 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container receive ready: true, restart count 0
Mar  2 14:25:48.290: INFO: thanos-receiver-receive-distributor-779c5d74d8-7hhmb from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container receive ready: true, restart count 0
Mar  2 14:25:48.290: INFO: thanos-receiver-ruler-0 from thanos started at 2023-02-28 08:03:52 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 14:25:48.290: INFO: 	Container ruler ready: true, restart count 0
Mar  2 14:25:48.290: INFO: restic-k4z4s from velero started at 2023-02-27 13:13:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container restic ready: true, restart count 2
Mar  2 14:25:48.290: INFO: velero-7bbd458dfc-s8n2h from velero started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.290: INFO: 	Container velero ready: true, restart count 0
Mar  2 14:25:48.290: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv1 before test
Mar  2 14:25:48.322: INFO: cert-manager-754b766f8b-fvh5z from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.322: INFO: 	Container cert-manager-controller ready: true, restart count 0
Mar  2 14:25:48.322: INFO: cert-manager-webhook-875cdf98f-lfgn7 from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.322: INFO: 	Container cert-manager-webhook ready: true, restart count 0
Mar  2 14:25:48.322: INFO: dex-58d8c68494-flfvv from dex started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.322: INFO: 	Container dex ready: true, restart count 0
Mar  2 14:25:48.322: INFO: falco-8cz2x from falco started at 2023-02-27 15:54:18 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.322: INFO: 	Container falco ready: true, restart count 2
Mar  2 14:25:48.322: INFO: falco-exporter-srbrb from falco started at 2023-02-27 15:51:10 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.322: INFO: 	Container falco-exporter ready: true, restart count 4
Mar  2 14:25:48.322: INFO: fluentd-aggregator-0 from fluentd-system started at 2023-02-28 08:16:03 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.322: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 14:25:48.322: INFO: fluentd-forwarder-zgcds from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 14:25:48.323: INFO: harbor-chartmuseum-5c9477455d-hp9zb from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container chartmuseum ready: true, restart count 0
Mar  2 14:25:48.323: INFO: harbor-core-58dc955656-2vz5k from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container core ready: true, restart count 1
Mar  2 14:25:48.323: INFO: harbor-database-0 from harbor started at 2023-02-28 08:16:02 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container database ready: true, restart count 0
Mar  2 14:25:48.323: INFO: harbor-jobservice-69c4c778fb-8qt7s from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container jobservice ready: true, restart count 2
Mar  2 14:25:48.323: INFO: harbor-notary-server-6cfdf66b5-sxpqw from harbor started at 2023-02-28 08:15:44 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container notary-server ready: true, restart count 2
Mar  2 14:25:48.323: INFO: harbor-notary-signer-5d6d45f584-rfqm7 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container notary-signer ready: true, restart count 2
Mar  2 14:25:48.323: INFO: harbor-portal-77d6c78fd9-p7t57 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container portal ready: true, restart count 0
Mar  2 14:25:48.323: INFO: harbor-redis-0 from harbor started at 2023-02-28 08:16:04 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container redis ready: true, restart count 0
Mar  2 14:25:48.323: INFO: harbor-registry-787bfb74d7-9vbht from harbor started at 2023-02-28 08:15:42 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container registry ready: true, restart count 0
Mar  2 14:25:48.323: INFO: 	Container registryctl ready: true, restart count 0
Mar  2 14:25:48.323: INFO: harbor-trivy-0 from harbor started at 2023-02-28 08:15:58 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container trivy ready: true, restart count 0
Mar  2 14:25:48.323: INFO: ingress-nginx-controller-8jd6t from ingress-nginx started at 2023-02-27 13:52:07 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container controller ready: true, restart count 2
Mar  2 14:25:48.323: INFO: calico-accountant-sfvmv from kube-system started at 2023-02-27 13:50:54 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 14:25:48.323: INFO: calico-node-vj6gp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 14:25:48.323: INFO: csi-cinder-nodeplugin-lvpvh from kube-system started at 2023-02-27 13:28:22 +0000 UTC (3 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container cinder-csi-plugin ready: true, restart count 2
Mar  2 14:25:48.323: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 14:25:48.323: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 14:25:48.323: INFO: kube-proxy-nrgbs from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 14:25:48.323: INFO: metrics-server-d9dcc77d6-z4sx7 from kube-system started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container metrics-server ready: true, restart count 0
Mar  2 14:25:48.323: INFO: nginx-proxy-aarnq-sc-k8s-node-srv1 from kube-system started at 2023-02-27 14:17:16 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 14:25:48.323: INFO: node-local-dns-b8kzp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 14:25:48.323: INFO: kured-kbmf8 from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container kured ready: true, restart count 3
Mar  2 14:25:48.323: INFO: alertmanager-kube-prometheus-stack-alertmanager-1 from monitoring started at 2023-02-28 08:16:00 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 14:25:48.323: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 14:25:48.323: INFO: kube-prometheus-stack-prometheus-node-exporter-jmbsj from monitoring started at 2023-02-27 13:49:59 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.323: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 14:25:48.324: INFO: user-grafana-6f7c7d589-q6clc from monitoring started at 2023-02-28 08:15:43 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.324: INFO: 	Container grafana ready: true, restart count 0
Mar  2 14:25:48.324: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar  2 14:25:48.324: INFO: opensearch-master-0 from opensearch-system started at 2023-02-28 08:16:01 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.324: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 14:25:48.324: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-j5shm from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 14:25:48.324: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 14:25:48.324: INFO: thanos-query-query-69fc6f554b-d2q5v from thanos started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.324: INFO: 	Container query ready: true, restart count 0
Mar  2 14:25:48.324: INFO: thanos-receiver-receive-2 from thanos started at 2023-02-28 08:16:05 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.324: INFO: 	Container receive ready: true, restart count 0
Mar  2 14:25:48.324: INFO: thanos-receiver-ruler-1 from thanos started at 2023-02-28 08:15:57 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.324: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 14:25:48.324: INFO: 	Container ruler ready: true, restart count 0
Mar  2 14:25:48.324: INFO: restic-zvhnj from velero started at 2023-02-27 13:13:38 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.324: INFO: 	Container restic ready: true, restart count 2
Mar  2 14:25:48.324: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv2 before test
Mar  2 14:25:48.347: INFO: falco-exporter-xkfjc from falco started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container falco-exporter ready: true, restart count 2
Mar  2 14:25:48.347: INFO: falco-z6q99 from falco started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container falco ready: true, restart count 2
Mar  2 14:25:48.347: INFO: fluentd-forwarder-xv4xh from fluentd-system started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container fluentd-forwarder ready: true, restart count 0
Mar  2 14:25:48.347: INFO: ingress-nginx-controller-vr99f from ingress-nginx started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container controller ready: true, restart count 0
Mar  2 14:25:48.347: INFO: calico-accountant-hgrw9 from kube-system started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container calico-accountant ready: true, restart count 0
Mar  2 14:25:48.347: INFO: calico-node-9ps2k from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container calico-node ready: true, restart count 3
Mar  2 14:25:48.347: INFO: csi-cinder-nodeplugin-lll9b from kube-system started at 2023-03-02 13:52:56 +0000 UTC (3 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  2 14:25:48.347: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 14:25:48.347: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  2 14:25:48.347: INFO: kube-proxy-nrj68 from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container kube-proxy ready: true, restart count 3
Mar  2 14:25:48.347: INFO: nginx-proxy-aarnq-sc-k8s-node-srv2 from kube-system started at 2023-02-28 07:06:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container nginx-proxy ready: true, restart count 3
Mar  2 14:25:48.347: INFO: node-local-dns-pwwsn from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container node-cache ready: true, restart count 3
Mar  2 14:25:48.347: INFO: kured-879hb from kured started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container kured ready: true, restart count 0
Mar  2 14:25:48.347: INFO: kube-prometheus-stack-prometheus-node-exporter-bp2wd from monitoring started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 14:25:48.347: INFO: opensearch-curator-27962785-zjbfs from opensearch-system started at 2023-03-02 14:25:00 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container opensearch-curator ready: false, restart count 0
Mar  2 14:25:48.347: INFO: sonobuoy from sonobuoy started at 2023-03-02 12:35:12 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 14:25:48.347: INFO: sonobuoy-e2e-job-eae18696d9844ddc from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container e2e ready: true, restart count 0
Mar  2 14:25:48.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 14:25:48.347: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-zf5bk from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 14:25:48.347: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 14:25:48.347: INFO: restic-tvgh4 from velero started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container restic ready: true, restart count 0
Mar  2 14:25:48.347: INFO: webhook-to-be-mutated from webhook-9835 started at 2023-03-02 14:24:18 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.347: INFO: 	Container example ready: false, restart count 0
Mar  2 14:25:48.347: INFO: 
Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv3 before test
Mar  2 14:25:48.381: INFO: cert-manager-cainjector-655cfbc4d-vc586 from cert-manager started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.382: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
Mar  2 14:25:48.382: INFO: dex-58d8c68494-gnrr5 from dex started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.382: INFO: 	Container dex ready: true, restart count 0
Mar  2 14:25:48.382: INFO: falco-9v9b5 from falco started at 2023-02-27 15:52:43 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.382: INFO: 	Container falco ready: true, restart count 1
Mar  2 14:25:48.383: INFO: falco-exporter-457cd from falco started at 2023-02-27 15:51:13 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.383: INFO: 	Container falco-exporter ready: true, restart count 3
Mar  2 14:25:48.383: INFO: falco-falcosidekick-5d5c7d4db-hddwc from falco started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.383: INFO: 	Container falcosidekick ready: true, restart count 0
Mar  2 14:25:48.383: INFO: aarnq-sc-logs-logs-compaction-27960390-dkhl6 from fluentd-system started at 2023-02-28 22:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.383: INFO: 	Container compaction ready: false, restart count 0
Mar  2 14:25:48.383: INFO: aarnq-sc-logs-logs-retention-27960450-xntms from fluentd-system started at 2023-02-28 23:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.383: INFO: 	Container retention ready: false, restart count 0
Mar  2 14:25:48.384: INFO: fluentd-forwarder-9smtw from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.384: INFO: 	Container fluentd-forwarder ready: true, restart count 1
Mar  2 14:25:48.384: INFO: harbor-backup-cronjob-27960480-trvs5 from harbor started at 2023-03-01 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.384: INFO: 	Container run ready: false, restart count 0
Mar  2 14:25:48.384: INFO: ingress-nginx-controller-4bgc8 from ingress-nginx started at 2023-02-27 13:54:29 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.384: INFO: 	Container controller ready: true, restart count 2
Mar  2 14:25:48.385: INFO: ingress-nginx-default-backend-64599cb78d-t9m7m from ingress-nginx started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.385: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Mar  2 14:25:48.385: INFO: calico-accountant-wgpwj from kube-system started at 2023-02-27 13:50:52 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.385: INFO: 	Container calico-accountant ready: true, restart count 2
Mar  2 14:25:48.385: INFO: calico-node-7vgvf from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.385: INFO: 	Container calico-node ready: true, restart count 2
Mar  2 14:25:48.386: INFO: csi-cinder-controllerplugin-6fdb685467-qppqd from kube-system started at 2023-03-01 07:31:56 +0000 UTC (6 container statuses recorded)
Mar  2 14:25:48.386: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  2 14:25:48.386: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  2 14:25:48.386: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  2 14:25:48.386: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  2 14:25:48.386: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar  2 14:25:48.386: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 14:25:48.386: INFO: csi-cinder-nodeplugin-wn26q from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
Mar  2 14:25:48.387: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
Mar  2 14:25:48.387: INFO: 	Container liveness-probe ready: true, restart count 2
Mar  2 14:25:48.387: INFO: 	Container node-driver-registrar ready: true, restart count 2
Mar  2 14:25:48.387: INFO: kube-proxy-t9sqm from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.387: INFO: 	Container kube-proxy ready: true, restart count 2
Mar  2 14:25:48.387: INFO: nginx-proxy-aarnq-sc-k8s-node-srv3 from kube-system started at 2023-02-27 13:14:14 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.387: INFO: 	Container nginx-proxy ready: true, restart count 2
Mar  2 14:25:48.387: INFO: node-local-dns-jf9nv from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.388: INFO: 	Container node-cache ready: true, restart count 2
Mar  2 14:25:48.388: INFO: kured-g9qpk from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.388: INFO: 	Container kured ready: true, restart count 3
Mar  2 14:25:48.388: INFO: ciskubebench-exporter-68bcb66c46-wjl7g from monitoring started at 2023-03-02 13:52:30 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.388: INFO: 	Container metrics-collector ready: true, restart count 0
Mar  2 14:25:48.388: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 14:25:48.388: INFO: grafana-label-enforcer-ff6966584-d9872 from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.389: INFO: 	Container prom-label-enforcer ready: true, restart count 0
Mar  2 14:25:48.389: INFO: kube-prometheus-stack-kube-state-metrics-5584579f7d-jmrqj from monitoring started at 2023-03-01 07:31:57 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.389: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 14:25:48.389: INFO: kube-prometheus-stack-operator-6bd84664f-wxlxc from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.389: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Mar  2 14:25:48.389: INFO: kube-prometheus-stack-prometheus-node-exporter-9v6v5 from monitoring started at 2023-02-27 13:50:02 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.389: INFO: 	Container node-exporter ready: true, restart count 2
Mar  2 14:25:48.390: INFO: prometheus-kube-prometheus-stack-prometheus-0 from monitoring started at 2023-03-01 07:32:02 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.390: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 14:25:48.390: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 14:25:48.390: INFO: starboard-operator-7f84bbf756-t9srl from monitoring started at 2023-03-02 13:52:30 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.390: INFO: 	Container starboard-operator ready: true, restart count 0
Mar  2 14:25:48.390: INFO: vulnerability-exporter-8485469578-tcxmv from monitoring started at 2023-03-02 13:52:30 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.390: INFO: 	Container metrics-collector ready: true, restart count 0
Mar  2 14:25:48.390: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 14:25:48.390: INFO: opensearch-master-1 from opensearch-system started at 2023-03-01 07:32:10 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.390: INFO: 	Container opensearch ready: true, restart count 0
Mar  2 14:25:48.391: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-m5t49 from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
Mar  2 14:25:48.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 14:25:48.391: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 14:25:48.391: INFO: thanos-query-query-frontend-6c58dbdc6-6g7jx from thanos started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.391: INFO: 	Container query-frontend ready: true, restart count 0
Mar  2 14:25:48.391: INFO: thanos-receiver-receive-1 from thanos started at 2023-03-01 07:32:11 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.391: INFO: 	Container receive ready: true, restart count 0
Mar  2 14:25:48.391: INFO: thanos-receiver-storegateway-0 from thanos started at 2023-03-01 07:32:06 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.391: INFO: 	Container storegateway ready: true, restart count 0
Mar  2 14:25:48.391: INFO: restic-hwpfm from velero started at 2023-02-27 13:13:47 +0000 UTC (1 container statuses recorded)
Mar  2 14:25:48.391: INFO: 	Container restic ready: true, restart count 3
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 14:25:48.392
Mar  2 14:25:48.402: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5707" to be "running"
Mar  2 14:25:48.407: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.092213ms
Mar  2 14:25:50.413: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01052716s
Mar  2 14:25:52.415: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.013165929s
Mar  2 14:25:52.415: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 14:25:52.42
STEP: Trying to apply a random label on the found node. 03/02/23 14:25:52.44
STEP: verifying the node has the label kubernetes.io/e2e-aa3d1aa4-0bb8-4526-8728-18f2bfb3e2ad 42 03/02/23 14:25:52.451
STEP: Trying to relaunch the pod, now with labels. 03/02/23 14:25:52.461
Mar  2 14:25:52.469: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5707" to be "not pending"
Mar  2 14:25:52.510: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 40.952982ms
Mar  2 14:25:54.518: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048525907s
Mar  2 14:25:56.518: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.048426249s
Mar  2 14:25:56.518: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-aa3d1aa4-0bb8-4526-8728-18f2bfb3e2ad off the node aarnq-sc-k8s-node-srv2 03/02/23 14:25:56.524
STEP: verifying the node doesn't have the label kubernetes.io/e2e-aa3d1aa4-0bb8-4526-8728-18f2bfb3e2ad 03/02/23 14:25:56.545
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  2 14:25:56.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5707" for this suite. 03/02/23 14:25:56.556
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":354,"skipped":6619,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.389 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:25:48.181
    Mar  2 14:25:48.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename sched-pred 03/02/23 14:25:48.182
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:48.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:48.216
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  2 14:25:48.222: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  2 14:25:48.240: INFO: Waiting for terminating namespaces to be deleted...
    Mar  2 14:25:48.245: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv0 before test
    Mar  2 14:25:48.289: INFO: falco-exporter-r2zfx from falco started at 2023-02-27 15:51:02 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.289: INFO: 	Container falco-exporter ready: true, restart count 4
    Mar  2 14:25:48.289: INFO: falco-falcosidekick-5d5c7d4db-r6952 from falco started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.289: INFO: 	Container falcosidekick ready: true, restart count 0
    Mar  2 14:25:48.289: INFO: falco-trjnh from falco started at 2023-02-27 15:53:31 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.289: INFO: 	Container falco ready: true, restart count 2
    Mar  2 14:25:48.289: INFO: fluentd-forwarder-qxbtj from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.289: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 14:25:48.289: INFO: ingress-nginx-controller-qprg2 from ingress-nginx started at 2023-02-27 13:57:01 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.289: INFO: 	Container controller ready: true, restart count 2
    Mar  2 14:25:48.289: INFO: calico-accountant-k6t4p from kube-system started at 2023-02-27 13:50:57 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 14:25:48.290: INFO: calico-node-dz84l from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 14:25:48.290: INFO: coredns-588bb58b94-k6j76 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container coredns ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: csi-cinder-nodeplugin-9nct9 from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
    Mar  2 14:25:48.290: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 14:25:48.290: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 14:25:48.290: INFO: kube-proxy-7bm9z from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 14:25:48.290: INFO: nginx-proxy-aarnq-sc-k8s-node-srv0 from kube-system started at 2023-02-27 14:10:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 14:25:48.290: INFO: node-local-dns-dk8hd from kube-system started at 2023-02-27 13:13:11 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 14:25:48.290: INFO: snapshot-controller-7d445c66c9-6w4w5 from kube-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: kured-hdkrw from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container kured ready: true, restart count 3
    Mar  2 14:25:48.290: INFO: alertmanager-kube-prometheus-stack-alertmanager-0 from monitoring started at 2023-03-01 07:32:10 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container alertmanager ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: kube-prometheus-stack-grafana-84f79f467b-sr7kl from monitoring started at 2023-02-28 08:03:49 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container grafana ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: kube-prometheus-stack-prometheus-node-exporter-nl9pw from monitoring started at 2023-02-27 13:49:55 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 14:25:48.290: INFO: prometheus-blackbox-exporter-677b579798-7xm9d from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: s3-exporter-867c5b9457-lfcsf from monitoring started at 2023-02-28 08:03:49 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container s3-exporter ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: opensearch-dashboards-58c8d95f7b-9spcv from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container dashboards ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: opensearch-master-2 from opensearch-system started at 2023-02-28 08:03:54 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: prometheus-opensearch-exporter-5688c84dcd-95vjh from opensearch-system started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container exporter ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-qv9vz from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: thanos-query-query-69fc6f554b-db7w4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container query ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: thanos-receiver-bucketweb-b4955fcf8-8w2xg from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container bucketweb ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: thanos-receiver-compactor-848df7b5d7-z2jh4 from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container compactor ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: thanos-receiver-receive-0 from thanos started at 2023-02-28 08:04:00 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container receive ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: thanos-receiver-receive-distributor-779c5d74d8-7hhmb from thanos started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container receive ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: thanos-receiver-ruler-0 from thanos started at 2023-02-28 08:03:52 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: 	Container ruler ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: restic-k4z4s from velero started at 2023-02-27 13:13:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container restic ready: true, restart count 2
    Mar  2 14:25:48.290: INFO: velero-7bbd458dfc-s8n2h from velero started at 2023-02-28 08:03:48 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.290: INFO: 	Container velero ready: true, restart count 0
    Mar  2 14:25:48.290: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv1 before test
    Mar  2 14:25:48.322: INFO: cert-manager-754b766f8b-fvh5z from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.322: INFO: 	Container cert-manager-controller ready: true, restart count 0
    Mar  2 14:25:48.322: INFO: cert-manager-webhook-875cdf98f-lfgn7 from cert-manager started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.322: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    Mar  2 14:25:48.322: INFO: dex-58d8c68494-flfvv from dex started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.322: INFO: 	Container dex ready: true, restart count 0
    Mar  2 14:25:48.322: INFO: falco-8cz2x from falco started at 2023-02-27 15:54:18 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.322: INFO: 	Container falco ready: true, restart count 2
    Mar  2 14:25:48.322: INFO: falco-exporter-srbrb from falco started at 2023-02-27 15:51:10 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.322: INFO: 	Container falco-exporter ready: true, restart count 4
    Mar  2 14:25:48.322: INFO: fluentd-aggregator-0 from fluentd-system started at 2023-02-28 08:16:03 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.322: INFO: 	Container fluentd ready: true, restart count 0
    Mar  2 14:25:48.322: INFO: fluentd-forwarder-zgcds from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 14:25:48.323: INFO: harbor-chartmuseum-5c9477455d-hp9zb from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container chartmuseum ready: true, restart count 0
    Mar  2 14:25:48.323: INFO: harbor-core-58dc955656-2vz5k from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container core ready: true, restart count 1
    Mar  2 14:25:48.323: INFO: harbor-database-0 from harbor started at 2023-02-28 08:16:02 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container database ready: true, restart count 0
    Mar  2 14:25:48.323: INFO: harbor-jobservice-69c4c778fb-8qt7s from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container jobservice ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: harbor-notary-server-6cfdf66b5-sxpqw from harbor started at 2023-02-28 08:15:44 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container notary-server ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: harbor-notary-signer-5d6d45f584-rfqm7 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container notary-signer ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: harbor-portal-77d6c78fd9-p7t57 from harbor started at 2023-02-28 08:15:42 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container portal ready: true, restart count 0
    Mar  2 14:25:48.323: INFO: harbor-redis-0 from harbor started at 2023-02-28 08:16:04 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container redis ready: true, restart count 0
    Mar  2 14:25:48.323: INFO: harbor-registry-787bfb74d7-9vbht from harbor started at 2023-02-28 08:15:42 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container registry ready: true, restart count 0
    Mar  2 14:25:48.323: INFO: 	Container registryctl ready: true, restart count 0
    Mar  2 14:25:48.323: INFO: harbor-trivy-0 from harbor started at 2023-02-28 08:15:58 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container trivy ready: true, restart count 0
    Mar  2 14:25:48.323: INFO: ingress-nginx-controller-8jd6t from ingress-nginx started at 2023-02-27 13:52:07 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container controller ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: calico-accountant-sfvmv from kube-system started at 2023-02-27 13:50:54 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: calico-node-vj6gp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: csi-cinder-nodeplugin-lvpvh from kube-system started at 2023-02-27 13:28:22 +0000 UTC (3 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container cinder-csi-plugin ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: kube-proxy-nrgbs from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: metrics-server-d9dcc77d6-z4sx7 from kube-system started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container metrics-server ready: true, restart count 0
    Mar  2 14:25:48.323: INFO: nginx-proxy-aarnq-sc-k8s-node-srv1 from kube-system started at 2023-02-27 14:17:16 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: node-local-dns-b8kzp from kube-system started at 2023-02-27 13:13:13 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 14:25:48.323: INFO: kured-kbmf8 from kured started at 2023-02-27 15:31:12 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container kured ready: true, restart count 3
    Mar  2 14:25:48.323: INFO: alertmanager-kube-prometheus-stack-alertmanager-1 from monitoring started at 2023-02-28 08:16:00 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container alertmanager ready: true, restart count 0
    Mar  2 14:25:48.323: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 14:25:48.323: INFO: kube-prometheus-stack-prometheus-node-exporter-jmbsj from monitoring started at 2023-02-27 13:49:59 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.323: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 14:25:48.324: INFO: user-grafana-6f7c7d589-q6clc from monitoring started at 2023-02-28 08:15:43 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.324: INFO: 	Container grafana ready: true, restart count 0
    Mar  2 14:25:48.324: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Mar  2 14:25:48.324: INFO: opensearch-master-0 from opensearch-system started at 2023-02-28 08:16:01 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.324: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 14:25:48.324: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-j5shm from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 14:25:48.324: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 14:25:48.324: INFO: thanos-query-query-69fc6f554b-d2q5v from thanos started at 2023-02-28 08:15:43 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.324: INFO: 	Container query ready: true, restart count 0
    Mar  2 14:25:48.324: INFO: thanos-receiver-receive-2 from thanos started at 2023-02-28 08:16:05 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.324: INFO: 	Container receive ready: true, restart count 0
    Mar  2 14:25:48.324: INFO: thanos-receiver-ruler-1 from thanos started at 2023-02-28 08:15:57 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.324: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 14:25:48.324: INFO: 	Container ruler ready: true, restart count 0
    Mar  2 14:25:48.324: INFO: restic-zvhnj from velero started at 2023-02-27 13:13:38 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.324: INFO: 	Container restic ready: true, restart count 2
    Mar  2 14:25:48.324: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv2 before test
    Mar  2 14:25:48.347: INFO: falco-exporter-xkfjc from falco started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container falco-exporter ready: true, restart count 2
    Mar  2 14:25:48.347: INFO: falco-z6q99 from falco started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container falco ready: true, restart count 2
    Mar  2 14:25:48.347: INFO: fluentd-forwarder-xv4xh from fluentd-system started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container fluentd-forwarder ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: ingress-nginx-controller-vr99f from ingress-nginx started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container controller ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: calico-accountant-hgrw9 from kube-system started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container calico-accountant ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: calico-node-9ps2k from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container calico-node ready: true, restart count 3
    Mar  2 14:25:48.347: INFO: csi-cinder-nodeplugin-lll9b from kube-system started at 2023-03-02 13:52:56 +0000 UTC (3 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: kube-proxy-nrj68 from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container kube-proxy ready: true, restart count 3
    Mar  2 14:25:48.347: INFO: nginx-proxy-aarnq-sc-k8s-node-srv2 from kube-system started at 2023-02-28 07:06:42 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container nginx-proxy ready: true, restart count 3
    Mar  2 14:25:48.347: INFO: node-local-dns-pwwsn from kube-system started at 2023-02-27 13:13:12 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container node-cache ready: true, restart count 3
    Mar  2 14:25:48.347: INFO: kured-879hb from kured started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container kured ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: kube-prometheus-stack-prometheus-node-exporter-bp2wd from monitoring started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: opensearch-curator-27962785-zjbfs from opensearch-system started at 2023-03-02 14:25:00 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container opensearch-curator ready: false, restart count 0
    Mar  2 14:25:48.347: INFO: sonobuoy from sonobuoy started at 2023-03-02 12:35:12 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: sonobuoy-e2e-job-eae18696d9844ddc from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container e2e ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-zf5bk from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: restic-tvgh4 from velero started at 2023-03-02 13:52:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container restic ready: true, restart count 0
    Mar  2 14:25:48.347: INFO: webhook-to-be-mutated from webhook-9835 started at 2023-03-02 14:24:18 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.347: INFO: 	Container example ready: false, restart count 0
    Mar  2 14:25:48.347: INFO: 
    Logging pods the apiserver thinks is on node aarnq-sc-k8s-node-srv3 before test
    Mar  2 14:25:48.381: INFO: cert-manager-cainjector-655cfbc4d-vc586 from cert-manager started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.382: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    Mar  2 14:25:48.382: INFO: dex-58d8c68494-gnrr5 from dex started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.382: INFO: 	Container dex ready: true, restart count 0
    Mar  2 14:25:48.382: INFO: falco-9v9b5 from falco started at 2023-02-27 15:52:43 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.382: INFO: 	Container falco ready: true, restart count 1
    Mar  2 14:25:48.383: INFO: falco-exporter-457cd from falco started at 2023-02-27 15:51:13 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.383: INFO: 	Container falco-exporter ready: true, restart count 3
    Mar  2 14:25:48.383: INFO: falco-falcosidekick-5d5c7d4db-hddwc from falco started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.383: INFO: 	Container falcosidekick ready: true, restart count 0
    Mar  2 14:25:48.383: INFO: aarnq-sc-logs-logs-compaction-27960390-dkhl6 from fluentd-system started at 2023-02-28 22:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.383: INFO: 	Container compaction ready: false, restart count 0
    Mar  2 14:25:48.383: INFO: aarnq-sc-logs-logs-retention-27960450-xntms from fluentd-system started at 2023-02-28 23:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.383: INFO: 	Container retention ready: false, restart count 0
    Mar  2 14:25:48.384: INFO: fluentd-forwarder-9smtw from fluentd-system started at 2023-02-27 15:38:34 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.384: INFO: 	Container fluentd-forwarder ready: true, restart count 1
    Mar  2 14:25:48.384: INFO: harbor-backup-cronjob-27960480-trvs5 from harbor started at 2023-03-01 00:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.384: INFO: 	Container run ready: false, restart count 0
    Mar  2 14:25:48.384: INFO: ingress-nginx-controller-4bgc8 from ingress-nginx started at 2023-02-27 13:54:29 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.384: INFO: 	Container controller ready: true, restart count 2
    Mar  2 14:25:48.385: INFO: ingress-nginx-default-backend-64599cb78d-t9m7m from ingress-nginx started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.385: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
    Mar  2 14:25:48.385: INFO: calico-accountant-wgpwj from kube-system started at 2023-02-27 13:50:52 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.385: INFO: 	Container calico-accountant ready: true, restart count 2
    Mar  2 14:25:48.385: INFO: calico-node-7vgvf from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.385: INFO: 	Container calico-node ready: true, restart count 2
    Mar  2 14:25:48.386: INFO: csi-cinder-controllerplugin-6fdb685467-qppqd from kube-system started at 2023-03-01 07:31:56 +0000 UTC (6 container statuses recorded)
    Mar  2 14:25:48.386: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  2 14:25:48.386: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar  2 14:25:48.386: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar  2 14:25:48.386: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar  2 14:25:48.386: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar  2 14:25:48.386: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  2 14:25:48.386: INFO: csi-cinder-nodeplugin-wn26q from kube-system started at 2023-02-27 13:22:16 +0000 UTC (3 container statuses recorded)
    Mar  2 14:25:48.387: INFO: 	Container cinder-csi-plugin ready: true, restart count 8
    Mar  2 14:25:48.387: INFO: 	Container liveness-probe ready: true, restart count 2
    Mar  2 14:25:48.387: INFO: 	Container node-driver-registrar ready: true, restart count 2
    Mar  2 14:25:48.387: INFO: kube-proxy-t9sqm from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.387: INFO: 	Container kube-proxy ready: true, restart count 2
    Mar  2 14:25:48.387: INFO: nginx-proxy-aarnq-sc-k8s-node-srv3 from kube-system started at 2023-02-27 13:14:14 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.387: INFO: 	Container nginx-proxy ready: true, restart count 2
    Mar  2 14:25:48.387: INFO: node-local-dns-jf9nv from kube-system started at 2023-02-27 13:13:14 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.388: INFO: 	Container node-cache ready: true, restart count 2
    Mar  2 14:25:48.388: INFO: kured-g9qpk from kured started at 2023-02-27 15:31:11 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.388: INFO: 	Container kured ready: true, restart count 3
    Mar  2 14:25:48.388: INFO: ciskubebench-exporter-68bcb66c46-wjl7g from monitoring started at 2023-03-02 13:52:30 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.388: INFO: 	Container metrics-collector ready: true, restart count 0
    Mar  2 14:25:48.388: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 14:25:48.388: INFO: grafana-label-enforcer-ff6966584-d9872 from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.389: INFO: 	Container prom-label-enforcer ready: true, restart count 0
    Mar  2 14:25:48.389: INFO: kube-prometheus-stack-kube-state-metrics-5584579f7d-jmrqj from monitoring started at 2023-03-01 07:31:57 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.389: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Mar  2 14:25:48.389: INFO: kube-prometheus-stack-operator-6bd84664f-wxlxc from monitoring started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.389: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Mar  2 14:25:48.389: INFO: kube-prometheus-stack-prometheus-node-exporter-9v6v5 from monitoring started at 2023-02-27 13:50:02 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.389: INFO: 	Container node-exporter ready: true, restart count 2
    Mar  2 14:25:48.390: INFO: prometheus-kube-prometheus-stack-prometheus-0 from monitoring started at 2023-03-01 07:32:02 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.390: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 14:25:48.390: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 14:25:48.390: INFO: starboard-operator-7f84bbf756-t9srl from monitoring started at 2023-03-02 13:52:30 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.390: INFO: 	Container starboard-operator ready: true, restart count 0
    Mar  2 14:25:48.390: INFO: vulnerability-exporter-8485469578-tcxmv from monitoring started at 2023-03-02 13:52:30 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.390: INFO: 	Container metrics-collector ready: true, restart count 0
    Mar  2 14:25:48.390: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 14:25:48.390: INFO: opensearch-master-1 from opensearch-system started at 2023-03-01 07:32:10 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.390: INFO: 	Container opensearch ready: true, restart count 0
    Mar  2 14:25:48.391: INFO: sonobuoy-systemd-logs-daemon-set-a2ea4c1134ba4899-m5t49 from sonobuoy started at 2023-03-02 12:35:25 +0000 UTC (2 container statuses recorded)
    Mar  2 14:25:48.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 14:25:48.391: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 14:25:48.391: INFO: thanos-query-query-frontend-6c58dbdc6-6g7jx from thanos started at 2023-03-01 07:31:56 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.391: INFO: 	Container query-frontend ready: true, restart count 0
    Mar  2 14:25:48.391: INFO: thanos-receiver-receive-1 from thanos started at 2023-03-01 07:32:11 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.391: INFO: 	Container receive ready: true, restart count 0
    Mar  2 14:25:48.391: INFO: thanos-receiver-storegateway-0 from thanos started at 2023-03-01 07:32:06 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.391: INFO: 	Container storegateway ready: true, restart count 0
    Mar  2 14:25:48.391: INFO: restic-hwpfm from velero started at 2023-02-27 13:13:47 +0000 UTC (1 container statuses recorded)
    Mar  2 14:25:48.391: INFO: 	Container restic ready: true, restart count 3
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 14:25:48.392
    Mar  2 14:25:48.402: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5707" to be "running"
    Mar  2 14:25:48.407: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.092213ms
    Mar  2 14:25:50.413: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01052716s
    Mar  2 14:25:52.415: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.013165929s
    Mar  2 14:25:52.415: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 14:25:52.42
    STEP: Trying to apply a random label on the found node. 03/02/23 14:25:52.44
    STEP: verifying the node has the label kubernetes.io/e2e-aa3d1aa4-0bb8-4526-8728-18f2bfb3e2ad 42 03/02/23 14:25:52.451
    STEP: Trying to relaunch the pod, now with labels. 03/02/23 14:25:52.461
    Mar  2 14:25:52.469: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5707" to be "not pending"
    Mar  2 14:25:52.510: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 40.952982ms
    Mar  2 14:25:54.518: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048525907s
    Mar  2 14:25:56.518: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.048426249s
    Mar  2 14:25:56.518: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-aa3d1aa4-0bb8-4526-8728-18f2bfb3e2ad off the node aarnq-sc-k8s-node-srv2 03/02/23 14:25:56.524
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-aa3d1aa4-0bb8-4526-8728-18f2bfb3e2ad 03/02/23 14:25:56.545
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 14:25:56.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5707" for this suite. 03/02/23 14:25:56.556
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:25:56.571
Mar  2 14:25:56.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename kubelet-test 03/02/23 14:25:56.576
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:56.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:56.6
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar  2 14:25:56.617: INFO: Waiting up to 5m0s for pod "busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a" in namespace "kubelet-test-896" to be "running and ready"
Mar  2 14:25:56.656: INFO: Pod "busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a": Phase="Pending", Reason="", readiness=false. Elapsed: 38.109193ms
Mar  2 14:25:56.656: INFO: The phase of Pod busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:25:58.664: INFO: Pod "busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a": Phase="Running", Reason="", readiness=true. Elapsed: 2.045764149s
Mar  2 14:25:58.664: INFO: The phase of Pod busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a is Running (Ready = true)
Mar  2 14:25:58.664: INFO: Pod "busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  2 14:25:58.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-896" for this suite. 03/02/23 14:25:58.685
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":355,"skipped":6627,"failed":0}
------------------------------
â€¢ [2.123 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:25:56.571
    Mar  2 14:25:56.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename kubelet-test 03/02/23 14:25:56.576
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:56.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:56.6
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar  2 14:25:56.617: INFO: Waiting up to 5m0s for pod "busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a" in namespace "kubelet-test-896" to be "running and ready"
    Mar  2 14:25:56.656: INFO: Pod "busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a": Phase="Pending", Reason="", readiness=false. Elapsed: 38.109193ms
    Mar  2 14:25:56.656: INFO: The phase of Pod busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:25:58.664: INFO: Pod "busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a": Phase="Running", Reason="", readiness=true. Elapsed: 2.045764149s
    Mar  2 14:25:58.664: INFO: The phase of Pod busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a is Running (Ready = true)
    Mar  2 14:25:58.664: INFO: Pod "busybox-scheduling-d676fd55-b5fe-4eeb-9f0c-a3686d23d80a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  2 14:25:58.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-896" for this suite. 03/02/23 14:25:58.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:25:58.696
Mar  2 14:25:58.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename disruption 03/02/23 14:25:58.697
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:58.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:58.739
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 03/02/23 14:25:58.743
STEP: Waiting for the pdb to be processed 03/02/23 14:25:58.748
STEP: updating the pdb 03/02/23 14:26:00.754
STEP: Waiting for the pdb to be processed 03/02/23 14:26:00.761
STEP: patching the pdb 03/02/23 14:26:02.775
STEP: Waiting for the pdb to be processed 03/02/23 14:26:02.791
STEP: Waiting for the pdb to be deleted 03/02/23 14:26:02.812
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  2 14:26:02.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3262" for this suite. 03/02/23 14:26:02.833
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":356,"skipped":6634,"failed":0}
------------------------------
â€¢ [4.150 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:25:58.696
    Mar  2 14:25:58.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename disruption 03/02/23 14:25:58.697
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:25:58.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:25:58.739
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 03/02/23 14:25:58.743
    STEP: Waiting for the pdb to be processed 03/02/23 14:25:58.748
    STEP: updating the pdb 03/02/23 14:26:00.754
    STEP: Waiting for the pdb to be processed 03/02/23 14:26:00.761
    STEP: patching the pdb 03/02/23 14:26:02.775
    STEP: Waiting for the pdb to be processed 03/02/23 14:26:02.791
    STEP: Waiting for the pdb to be deleted 03/02/23 14:26:02.812
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  2 14:26:02.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3262" for this suite. 03/02/23 14:26:02.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:26:02.858
Mar  2 14:26:02.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename security-context 03/02/23 14:26:02.86
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:02.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:02.904
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/02/23 14:26:02.908
Mar  2 14:26:02.929: INFO: Waiting up to 5m0s for pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00" in namespace "security-context-6016" to be "Succeeded or Failed"
Mar  2 14:26:02.964: INFO: Pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00": Phase="Pending", Reason="", readiness=false. Elapsed: 34.180099ms
Mar  2 14:26:04.969: INFO: Pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039164967s
Mar  2 14:26:06.971: INFO: Pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041112734s
Mar  2 14:26:08.970: INFO: Pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040935739s
STEP: Saw pod success 03/02/23 14:26:08.971
Mar  2 14:26:08.971: INFO: Pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00" satisfied condition "Succeeded or Failed"
Mar  2 14:26:08.978: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00 container test-container: <nil>
STEP: delete the pod 03/02/23 14:26:08.986
Mar  2 14:26:09.004: INFO: Waiting for pod security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00 to disappear
Mar  2 14:26:09.014: INFO: Pod security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 14:26:09.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6016" for this suite. 03/02/23 14:26:09.027
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":357,"skipped":6664,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.186 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:26:02.858
    Mar  2 14:26:02.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename security-context 03/02/23 14:26:02.86
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:02.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:02.904
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/02/23 14:26:02.908
    Mar  2 14:26:02.929: INFO: Waiting up to 5m0s for pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00" in namespace "security-context-6016" to be "Succeeded or Failed"
    Mar  2 14:26:02.964: INFO: Pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00": Phase="Pending", Reason="", readiness=false. Elapsed: 34.180099ms
    Mar  2 14:26:04.969: INFO: Pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039164967s
    Mar  2 14:26:06.971: INFO: Pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041112734s
    Mar  2 14:26:08.970: INFO: Pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040935739s
    STEP: Saw pod success 03/02/23 14:26:08.971
    Mar  2 14:26:08.971: INFO: Pod "security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00" satisfied condition "Succeeded or Failed"
    Mar  2 14:26:08.978: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00 container test-container: <nil>
    STEP: delete the pod 03/02/23 14:26:08.986
    Mar  2 14:26:09.004: INFO: Waiting for pod security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00 to disappear
    Mar  2 14:26:09.014: INFO: Pod security-context-0333ccb9-9351-4a08-ad20-ae7ffb732c00 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 14:26:09.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-6016" for this suite. 03/02/23 14:26:09.027
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:26:09.047
Mar  2 14:26:09.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 14:26:09.048
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:09.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:09.069
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 03/02/23 14:26:09.095
Mar  2 14:26:09.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576" in namespace "projected-7057" to be "Succeeded or Failed"
Mar  2 14:26:09.121: INFO: Pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576": Phase="Pending", Reason="", readiness=false. Elapsed: 10.400486ms
Mar  2 14:26:11.138: INFO: Pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027547072s
Mar  2 14:26:13.132: INFO: Pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021766307s
Mar  2 14:26:15.126: INFO: Pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015757993s
STEP: Saw pod success 03/02/23 14:26:15.126
Mar  2 14:26:15.126: INFO: Pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576" satisfied condition "Succeeded or Failed"
Mar  2 14:26:15.129: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576 container client-container: <nil>
STEP: delete the pod 03/02/23 14:26:15.138
Mar  2 14:26:15.171: INFO: Waiting for pod downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576 to disappear
Mar  2 14:26:15.186: INFO: Pod downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 14:26:15.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7057" for this suite. 03/02/23 14:26:15.194
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":358,"skipped":6668,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.153 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:26:09.047
    Mar  2 14:26:09.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 14:26:09.048
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:09.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:09.069
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 03/02/23 14:26:09.095
    Mar  2 14:26:09.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576" in namespace "projected-7057" to be "Succeeded or Failed"
    Mar  2 14:26:09.121: INFO: Pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576": Phase="Pending", Reason="", readiness=false. Elapsed: 10.400486ms
    Mar  2 14:26:11.138: INFO: Pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027547072s
    Mar  2 14:26:13.132: INFO: Pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021766307s
    Mar  2 14:26:15.126: INFO: Pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015757993s
    STEP: Saw pod success 03/02/23 14:26:15.126
    Mar  2 14:26:15.126: INFO: Pod "downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576" satisfied condition "Succeeded or Failed"
    Mar  2 14:26:15.129: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576 container client-container: <nil>
    STEP: delete the pod 03/02/23 14:26:15.138
    Mar  2 14:26:15.171: INFO: Waiting for pod downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576 to disappear
    Mar  2 14:26:15.186: INFO: Pod downwardapi-volume-6b74d8c9-6552-4376-b86f-d6ceee92c576 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 14:26:15.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7057" for this suite. 03/02/23 14:26:15.194
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:26:15.203
Mar  2 14:26:15.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename pods 03/02/23 14:26:15.204
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:15.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:15.228
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Mar  2 14:26:15.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: creating the pod 03/02/23 14:26:15.239
STEP: submitting the pod to kubernetes 03/02/23 14:26:15.24
Mar  2 14:26:15.248: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef" in namespace "pods-7331" to be "running and ready"
Mar  2 14:26:15.253: INFO: Pod "pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.228814ms
Mar  2 14:26:15.253: INFO: The phase of Pod pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:26:17.259: INFO: Pod "pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.011619959s
Mar  2 14:26:17.260: INFO: The phase of Pod pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef is Running (Ready = true)
Mar  2 14:26:17.260: INFO: Pod "pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 14:26:17.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7331" for this suite. 03/02/23 14:26:17.299
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":359,"skipped":6671,"failed":0}
------------------------------
â€¢ [2.105 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:26:15.203
    Mar  2 14:26:15.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename pods 03/02/23 14:26:15.204
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:15.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:15.228
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Mar  2 14:26:15.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: creating the pod 03/02/23 14:26:15.239
    STEP: submitting the pod to kubernetes 03/02/23 14:26:15.24
    Mar  2 14:26:15.248: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef" in namespace "pods-7331" to be "running and ready"
    Mar  2 14:26:15.253: INFO: Pod "pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.228814ms
    Mar  2 14:26:15.253: INFO: The phase of Pod pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:26:17.259: INFO: Pod "pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.011619959s
    Mar  2 14:26:17.260: INFO: The phase of Pod pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef is Running (Ready = true)
    Mar  2 14:26:17.260: INFO: Pod "pod-logs-websocket-7bb73196-0b95-4d8a-9f81-d3deadd091ef" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 14:26:17.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7331" for this suite. 03/02/23 14:26:17.299
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:26:17.308
Mar  2 14:26:17.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 14:26:17.31
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:17.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:17.342
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-5ee72234-e5e7-4a54-823f-afef873c835c 03/02/23 14:26:17.345
STEP: Creating a pod to test consume secrets 03/02/23 14:26:17.349
Mar  2 14:26:17.362: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5" in namespace "projected-1002" to be "Succeeded or Failed"
Mar  2 14:26:17.384: INFO: Pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.564068ms
Mar  2 14:26:19.422: INFO: Pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059630327s
Mar  2 14:26:21.510: INFO: Pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.147525764s
Mar  2 14:26:23.390: INFO: Pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028041377s
STEP: Saw pod success 03/02/23 14:26:23.391
Mar  2 14:26:23.391: INFO: Pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5" satisfied condition "Succeeded or Failed"
Mar  2 14:26:23.395: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 14:26:23.401
Mar  2 14:26:23.416: INFO: Waiting for pod pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5 to disappear
Mar  2 14:26:23.419: INFO: Pod pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 14:26:23.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1002" for this suite. 03/02/23 14:26:23.424
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":360,"skipped":6675,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.124 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:26:17.308
    Mar  2 14:26:17.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 14:26:17.31
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:17.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:17.342
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-5ee72234-e5e7-4a54-823f-afef873c835c 03/02/23 14:26:17.345
    STEP: Creating a pod to test consume secrets 03/02/23 14:26:17.349
    Mar  2 14:26:17.362: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5" in namespace "projected-1002" to be "Succeeded or Failed"
    Mar  2 14:26:17.384: INFO: Pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.564068ms
    Mar  2 14:26:19.422: INFO: Pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059630327s
    Mar  2 14:26:21.510: INFO: Pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.147525764s
    Mar  2 14:26:23.390: INFO: Pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028041377s
    STEP: Saw pod success 03/02/23 14:26:23.391
    Mar  2 14:26:23.391: INFO: Pod "pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5" satisfied condition "Succeeded or Failed"
    Mar  2 14:26:23.395: INFO: Trying to get logs from node aarnq-sc-k8s-node-srv2 pod pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 14:26:23.401
    Mar  2 14:26:23.416: INFO: Waiting for pod pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5 to disappear
    Mar  2 14:26:23.419: INFO: Pod pod-projected-secrets-c5d0bcd5-23a0-418f-aa14-f6d67a7b9ba5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 14:26:23.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1002" for this suite. 03/02/23 14:26:23.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:26:23.439
Mar  2 14:26:23.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename projected 03/02/23 14:26:23.441
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:23.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:23.47
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-74c5472c-f034-48e3-989c-b16d44820c77 03/02/23 14:26:23.487
STEP: Creating configMap with name cm-test-opt-upd-12c46fca-3ace-4762-ac6c-fcd9bae29fad 03/02/23 14:26:23.492
STEP: Creating the pod 03/02/23 14:26:23.496
Mar  2 14:26:23.514: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be" in namespace "projected-2933" to be "running and ready"
Mar  2 14:26:23.529: INFO: Pod "pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be": Phase="Pending", Reason="", readiness=false. Elapsed: 14.825417ms
Mar  2 14:26:23.529: INFO: The phase of Pod pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:26:25.535: INFO: Pod "pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be": Phase="Running", Reason="", readiness=true. Elapsed: 2.020820233s
Mar  2 14:26:25.535: INFO: The phase of Pod pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be is Running (Ready = true)
Mar  2 14:26:25.535: INFO: Pod "pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-74c5472c-f034-48e3-989c-b16d44820c77 03/02/23 14:26:25.595
STEP: Updating configmap cm-test-opt-upd-12c46fca-3ace-4762-ac6c-fcd9bae29fad 03/02/23 14:26:25.606
STEP: Creating configMap with name cm-test-opt-create-0673c3f6-c669-4606-82f0-8e27f6475906 03/02/23 14:26:25.617
STEP: waiting to observe update in volume 03/02/23 14:26:25.622
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 14:26:27.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2933" for this suite. 03/02/23 14:26:27.672
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":361,"skipped":6704,"failed":0}
------------------------------
â€¢ [4.240 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:26:23.439
    Mar  2 14:26:23.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename projected 03/02/23 14:26:23.441
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:23.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:23.47
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-74c5472c-f034-48e3-989c-b16d44820c77 03/02/23 14:26:23.487
    STEP: Creating configMap with name cm-test-opt-upd-12c46fca-3ace-4762-ac6c-fcd9bae29fad 03/02/23 14:26:23.492
    STEP: Creating the pod 03/02/23 14:26:23.496
    Mar  2 14:26:23.514: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be" in namespace "projected-2933" to be "running and ready"
    Mar  2 14:26:23.529: INFO: Pod "pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be": Phase="Pending", Reason="", readiness=false. Elapsed: 14.825417ms
    Mar  2 14:26:23.529: INFO: The phase of Pod pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 14:26:25.535: INFO: Pod "pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be": Phase="Running", Reason="", readiness=true. Elapsed: 2.020820233s
    Mar  2 14:26:25.535: INFO: The phase of Pod pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be is Running (Ready = true)
    Mar  2 14:26:25.535: INFO: Pod "pod-projected-configmaps-299852ff-ba4c-4a8a-8969-c8ecfbb619be" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-74c5472c-f034-48e3-989c-b16d44820c77 03/02/23 14:26:25.595
    STEP: Updating configmap cm-test-opt-upd-12c46fca-3ace-4762-ac6c-fcd9bae29fad 03/02/23 14:26:25.606
    STEP: Creating configMap with name cm-test-opt-create-0673c3f6-c669-4606-82f0-8e27f6475906 03/02/23 14:26:25.617
    STEP: waiting to observe update in volume 03/02/23 14:26:25.622
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 14:26:27.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2933" for this suite. 03/02/23 14:26:27.672
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 14:26:27.682
Mar  2 14:26:27.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
STEP: Building a namespace api object, basename replication-controller 03/02/23 14:26:27.684
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:27.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:27.711
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa 03/02/23 14:26:27.713
Mar  2 14:26:27.727: INFO: Pod name my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa: Found 0 pods out of 1
Mar  2 14:26:32.730: INFO: Pod name my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa: Found 1 pods out of 1
Mar  2 14:26:32.730: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa" are running
Mar  2 14:26:32.730: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm" in namespace "replication-controller-1400" to be "running"
Mar  2 14:26:32.734: INFO: Pod "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm": Phase="Running", Reason="", readiness=true. Elapsed: 3.424481ms
Mar  2 14:26:32.734: INFO: Pod "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm" satisfied condition "running"
Mar  2 14:26:32.734: INFO: Pod "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 14:26:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 14:26:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 14:26:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 14:26:27 +0000 UTC Reason: Message:}])
Mar  2 14:26:32.735: INFO: Trying to dial the pod
Mar  2 14:26:37.755: INFO: Controller my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa: Got expected result from replica 1 [my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm]: "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  2 14:26:37.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1400" for this suite. 03/02/23 14:26:37.762
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":362,"skipped":6704,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.086 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 14:26:27.682
    Mar  2 14:26:27.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2815392237
    STEP: Building a namespace api object, basename replication-controller 03/02/23 14:26:27.684
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 14:26:27.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 14:26:27.711
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa 03/02/23 14:26:27.713
    Mar  2 14:26:27.727: INFO: Pod name my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa: Found 0 pods out of 1
    Mar  2 14:26:32.730: INFO: Pod name my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa: Found 1 pods out of 1
    Mar  2 14:26:32.730: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa" are running
    Mar  2 14:26:32.730: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm" in namespace "replication-controller-1400" to be "running"
    Mar  2 14:26:32.734: INFO: Pod "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm": Phase="Running", Reason="", readiness=true. Elapsed: 3.424481ms
    Mar  2 14:26:32.734: INFO: Pod "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm" satisfied condition "running"
    Mar  2 14:26:32.734: INFO: Pod "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 14:26:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 14:26:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 14:26:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 14:26:27 +0000 UTC Reason: Message:}])
    Mar  2 14:26:32.735: INFO: Trying to dial the pod
    Mar  2 14:26:37.755: INFO: Controller my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa: Got expected result from replica 1 [my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm]: "my-hostname-basic-9b57480e-91e1-4b9b-bf83-cbd12149d8aa-zzxkm", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  2 14:26:37.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1400" for this suite. 03/02/23 14:26:37.762
  << End Captured GinkgoWriter Output
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Mar  2 14:26:37.786: INFO: Running AfterSuite actions on all nodes
Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Mar  2 14:26:37.786: INFO: Running AfterSuite actions on node 1
Mar  2 14:26:37.786: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar  2 14:26:37.786: INFO: Running AfterSuite actions on all nodes
    Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Mar  2 14:26:37.786: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar  2 14:26:37.786: INFO: Running AfterSuite actions on node 1
    Mar  2 14:26:37.786: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.079 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 6650.489 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h50m50.906994574s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

